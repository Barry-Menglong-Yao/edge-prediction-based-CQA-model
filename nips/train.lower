there is experimental evidence that cortical neurons show avalanche activity with the intensity of firing events being distributed as a power-law . <eos> we present a biologically plausible extension of a neural network which exhibits a power-law avalanche distribution for a wide range of connectivity parameters .
we propose an algorithm that uses gaussian process regression to learn common hidden structure shared between corresponding sets of heterogenous observations . <eos> the observation spaces are linked via a single , reduced-dimensionality latent variable space . <eos> we present results from two datasets demonstrating the algorithms ? s ability to synthesize novel data from learned correspondences . <eos> we first show that the method can learn the nonlinear mapping between corresponding views of objects , filling in missing data as needed to synthesize novel views . <eos> we then show that the method can learn a mapping between human degrees of freedom and robotic degrees of freedom for a humanoid robot , allowing robotic imitation of human poses from motion capture data .
experimental data indicate that norepinephrine is critically involved in aspects of vigilance and attention . <eos> previously , we considered the function of this neuromodulatory system on a time scale of minutes and longer , and suggested that it signals global uncertainty arising from gross changes in environmental contingencies . <eos> however , norepinephrine is also known to be activated phasically by familiar stimuli in welllearned tasks . <eos> here , we extend our uncertainty-based treatment of norepinephrine to this phasic mode , proposing that it is involved in the detection and reaction to state uncertainty within a task . <eos> this role of norepinephrine can be understood through the metaphor of neural interrupts .
nested sampling is a new monte carlo method by skilling [ 1 ] intended for general bayesian computation . <eos> nested sampling provides a robust alternative to annealing-based methods for computing normalizing constants . <eos> it can also generate estimates of other quantities such as posterior expectations . <eos> the key technical requirement is an ability to draw samples uniformly from the prior subject to a constraint on the likelihood . <eos> we provide a demonstration with the potts model , an undirected graphical model .
compressed sensing is an emerging field based on the revelation that a small group of linear projections of a sparse signal contains enough information for reconstruction . <eos> in this paper we introduce a new theory for distributed compressed sensing ( dcs ) that enables new distributed coding algorithms for multi-signal ensembles that exploit both intra- and inter-signal correlation structures . <eos> the dcs theory rests on a new concept that we term the joint sparsity of a signal ensemble . <eos> we study three simple models for jointly sparse signals , propose algorithms for joint recovery of multiple signals from incoherent projections , and characterize theoretically and empirically the number of measurements per sensor required for accurate reconstruction . <eos> in some sense dcs is a framework for distributed compression of sources with memory , which has remained a challenging problem in information theory for some time . <eos> dcs is immediately applicable to a range of problems in sensor networks and arrays .
it is well-known that everything that is learnable in the difficult online setting , where an arbitrary sequences of examples must be labeled one at a time , is also learnable in the batch setting , where examples are drawn independently from a distribution . <eos> we show a result in the opposite direction . <eos> we give an efficient conversion algorithm from batch to online that is transductive : it uses future unlabeled data . <eos> this demonstrates the equivalence between what is properly and efficiently learnable in a batch model and a transductive online model .
we determine the asymptotic limit of the function computed by support vector machines ( svm ) and related algorithms that minimize a regularized empirical convex loss function in the reproducing kernel hilbert space of the gaussian rbf kernel , in the situation where the number of examples tends to infinity , the bandwidth of the gaussian kernel tends to 0 , and the regularization parameter is held fixed . <eos> non-asymptotic convergence bounds to this limit in the l2 sense are provided , together with upper bounds on the classification error that is shown to converge to the bayes risk , therefore proving the bayes-consistency of a variety of methods although the regularization term does not vanish . <eos> these results are particularly relevant to the one-class svm , for which the regularization can not vanish by construction , and which is shown for the first time to be a consistent density level set estimator .
nonnegative matrix approximation ( nnma ) is a recent technique for dimensionality reduction and data analysis that yields a parts based , sparse nonnegative representation for nonnegative input data . <eos> nnma has found a wide variety of applications , including text analysis , document clustering , face/image recognition , language modeling , speech processing and many others . <eos> despite these numerous applications , the algorithmic development for computing the nnma factors has been relatively deficient . <eos> this paper makes algorithmic progress by modeling and solving ( using multiplicative updates ) new generalized nnma problems that minimize bregman divergences between the input matrix and its lowrank approximation . <eos> the multiplicative update formulae in the pioneering work by lee and seung [ 11 ] arise as a special case of our algorithms . <eos> in addition , the paper shows how to use penalty functions for incorporating constraints other than nonnegativity into the problem . <eos> further , some interesting extensions to the use of linkfunctions for modeling nonlinear relationships are also discussed .
we describe a hierarchical compositional system for detecting deformable objects in images . <eos> objects are represented by graphical models . <eos> the algorithm uses a hierarchical tree where the root of the tree corresponds to the full object and lower-level elements of the tree correspond to simpler features . <eos> the algorithm proceeds by passing simple messages up and down the tree . <eos> the method works rapidly , in under a second , on 320 240 images . <eos> we demonstrate the approach on detecting cats , horses , and hands . <eos> the method works in the presence of background clutter and occlusions . <eos> our approach is contrasted with more traditional methods such as dynamic programming and belief propagation .
we consider a framework for semi-supervised learning using spectral decomposition based un-supervised kernel design . <eos> this approach subsumes a class of previously proposed semi-supervised learning methods on data graphs . <eos> we examine various theoretical properties of such methods . <eos> in particular , we derive a generalization performance bound , and obtain the optimal kernel design by minimizing the bound . <eos> based on the theoretical analysis , we are able to demonstrate why spectral kernel design based methods can often improve the predictive performance . <eos> experiments are used to illustrate the main consequences of our analysis .
we show that queyranne ? s algorithm for minimizing symmetric submodular functions can be used for clustering with a variety of different objective functions . <eos> two specific criteria that we consider in this paper are the single linkage and the minimum description length criteria . <eos> the first criterion tries to maximize the minimum distance between elements of different clusters , and is inherently ? discriminative ? . <eos> it is known that optimal clusterings into k clusters for any given k in polynomial time for this criterion can be computed . <eos> the second criterion seeks to minimize the description length of the clusters given a probabilistic generative model . <eos> we show that the optimal partitioning into 2 clusters , and approximate partitioning ( guaranteed to be within a factor of 2 of the the optimal ) for more clusters can be computed . <eos> to the best of our knowledge , this is the first time that a tractable algorithm for finding the optimal clustering with respect to the mdl criterion for 2 clusters has been given . <eos> besides the optimality result for the mdl criterion , the chief contribution of this paper is to show that the same algorithm can be used to optimize a broad class of criteria , and hence can be used for many application specific criterion for which efficient algorithm are not known .
this paper presents a non-asymptotic statistical analysis of kernel-pca with a focus different from the one proposed in previous work on this topic . <eos> here instead of considering the reconstruction error of kpca we are interested in approximation error bounds for the eigenspaces themselves . <eos> we prove an upper bound depending on the spacing between eigenvalues but not on the dimensionality of the eigenspace . <eos> as a consequence this allows to infer stability results for these estimated spaces .
in this paper , we show that the hinge loss can be interpreted as the neg-log-likelihood of a semi-parametric model of posterior probabilities . <eos> from this point of view , svms represent the parametric component of a semi-parametric model fitted by a maximum a posteriori estimation procedure . <eos> this connection enables to derive a mapping from svm scores to estimated posterior probabilities . <eos> unlike previous proposals , the suggested mapping is interval-valued , providing a set of posterior probabilities compatible with each svm score . <eos> this framework offers a new way to adapt the svm optimization problem to unbalanced classification , when decisions result in unequal ( asymmetric ) losses . <eos> experiments show improvements over state-of-the-art procedures .
this paper presents a new filter for online data association problems in high-dimensional spaces . <eos> the key innovation is a representation of the data association posterior in information form , in which the proximityof objects and tracks are expressed by numerical links . <eos> updating these links requires linear time , compared to exponential time required for computing the exact posterior probabilities . <eos> the paper derives the algorithm formally and provides comparative results using data obtained by a real-world camera array and by a large-scale sensor network simulation .
we discuss two intrinsic weaknesses of the spectral graph partitioning method , both of which have practical consequences . <eos> the first is that spectral embeddings tend to hide the best cuts from the commonly used hyperplane rounding method . <eos> rather than cleaning up the resulting suboptimal cuts with local search , we recommend the adoption of flow-based rounding . <eos> the second weakness is that for many ? power law ? <eos> graphs , the spectral method produces cuts that are highly unbalanced , thus decreasing the usefulness of the method for visualization ( see figure 4 ( b ) ) or as a basis for divide-and-conquer algorithms . <eos> these balance problems , which occur even though the spectral method ? s quotient-style objective function does encourage balance , can be fixed with a stricter balance constraint that turns the spectral mathematical program into an sdp that can be solved for million-node graphs by a method of burer and monteiro .
we introduce a new algorithm for off-policy temporal-difference learning with function approximation that has lower variance and requires less knowledge of the behavior policy than prior methods . <eos> we develop the notion of a recognizer , a filter on actions that distorts the behavior policy to produce a related target policy with low-variance importance-sampling corrections . <eos> we also consider target policies that are deviations from the state distribution of the behavior policy , such as potential temporally abstract options , which further reduces variance . <eos> this paper introduces recognizers and their potential advantages , then develops a full algorithm for linear function approximation and proves that its updates are in the same direction as on-policy td updates , which implies asymptotic convergence . <eos> even though our algorithm is based on importance sampling , we prove that it requires absolutely no knowledge of the behavior policy for the case of state-aggregation function approximators .
we present an infinite mixture model in which each component comprises a multivariate gaussian distribution over an input space , and a gaussian process model over an output space . <eos> our model is neatly able to deal with non-stationary covariance functions , discontinuities , multimodality and overlapping output signals . <eos> the work is similar to that by rasmussen and ghahramani [ 1 ] ; however , we use a full generative model over input and output space rather than just a conditional model . <eos> this allows us to deal with incomplete data , to perform inference over inverse functional mappings as well as for regression , and also leads to a more powerful and consistent bayesian specification of the effective ? gating network for the different experts .
in this paper , we present our design and experiments of a planar biped robot ( ? runbot ? ) <eos> under pure reflexive neuronal control . <eos> the goal of this study is to combine neuronal mechanisms with biomechanics to obtain very fast speed and the on-line learning of circuit parameters . <eos> our controller is built with biologically inspired sensor- and motor-neuron models , including local reflexes and not employing any kind of position or trajectory-tracking control algorithm . <eos> instead , this reflexive controller allows runbot to exploit its own natural dynamics during critical stages of its walking gait cycle . <eos> to our knowledge , this is the first time that dynamic biped walking is achieved using only a pure reflexive controller . <eos> in addition , this structure allows using a policy gradient reinforcement learning algorithm to tune the parameters of the reflexive controller in real-time during walking . <eos> this way runbot can reach a relative speed of 3.5 leg-lengths per second after a few minutes of online learning , which is faster than that of any other biped robot , and is also comparable to the fastest relative speed of human walking . <eos> in addition , the stability domain of stable walking is quite large supporting this design strategy .
we consider the problem of localizing a set of microphones together with a set of external acoustic events ( e.g. , hand claps ) , emitted at unknown times and unknown locations . <eos> we propose a solution that approximates this problem under a far field approximation defined in the calculus of affine geometry , and that relies on singular value decomposition ( svd ) to recover the affine structure of the problem . <eos> we then define low-dimensional optimization techniques for embedding the solution into euclidean geometry , and further techniques for recovering the locations and emission times of the acoustic events . <eos> the approach is useful for the calibration of ad-hoc microphone arrays and sensor networks .
given a redundant dictionary of basis vectors ( or atoms ) , our goal is to find maximally sparse representations of signals . <eos> previously , we have argued that a sparse bayesian learning ( sbl ) framework is particularly well-suited for this task , showing that it has far fewer local minima than other bayesian-inspired strategies . <eos> in this paper , we provide further evidence for this claim by proving a restricted equivalence condition , based on the distribution of the nonzero generating model weights , whereby the sbl solution will equal the maximally sparse representation . <eos> we also prove that if these nonzero weights are drawn from an approximate jeffreys prior , then with probability approaching one , our equivalence condition is satisfied . <eos> finally , we motivate the worst-case scenario for sbl and demonstrate that it is still better than the most widely used sparse representation algorithms . <eos> these include basis pursuit ( bp ) , which is based on a convex relaxation of the ? 0 ( quasi ) -norm , and orthogonal matching pursuit ( omp ) , a simple greedy strategy that iteratively selects basis vectors most aligned with the current residual .
motivated by the problem of learning to detect and recognize objects with minimal supervision , we develop a hierarchical probabilistic model for the spatial structure of visual scenes . <eos> in contrast with most existing models , our approach explicitly captures uncertainty in the number of object instances depicted in a given image . <eos> our scene model is based on the transformed dirichlet process ( tdp ) , a novel extension of the hierarchical dp in which a set of stochastically transformed mixture components are shared between multiple groups of data . <eos> for visual scenes , mixture components describe the spatial structure of visual features in an object ? centered coordinate frame , while transformations model the object positions in a particular image . <eos> learning and inference in the tdp , which has many potential applications beyond computer vision , is based on an empirically effective gibbs sampler . <eos> applied to a dataset of partially labeled street scenes , we show that the tdp ? s inclusion of spatial structure improves detection performance , flexibly exploiting partially labeled training images .
we demonstrate the first fully hardware implementation of retinotopic self-organization , from photon transduction to neural map formation . <eos> a silicon retina transduces patterned illumination into correlated spike trains that drive a population of silicon growth cones to automatically wire a topographic mapping by migrating toward sources of a diffusible guidance cue that is released by postsynaptic spikes . <eos> we varied the pattern of illumination to steer growth cones projected by different retinal ganglion cell types to self-organize segregated or coordinated retinotopic maps .
we propose a probabilistic model based on independent component analysis for learning multiple related tasks . <eos> in our model the task parameters are assumed to be generated from independent sources which account for the relatedness of the tasks . <eos> we use laplace distributions to model hidden sources which makes it possible to identify the hidden , independent components instead of just modeling correlations . <eos> furthermore , our model enjoys a sparsity property which makes it both parsimonious and robust . <eos> we also propose efficient algorithms for both empirical bayes method and point estimation . <eos> our experimental results on two multi-label text classification data sets show that the proposed approach is promising .
online learning algorithms are typically fast , memory efficient , and simple to implement . <eos> however , many common learning problems fit more naturally in the batch learning setting . <eos> the power of online learning algorithms can be exploited in batch settings by using online-to-batch conversions techniques which build a new batch algorithm from an existing online algorithm . <eos> we first give a unified overview of three existing online-to-batch conversion techniques which do not use training data in the conversion process . <eos> we then build upon these data-independent conversions to derive and analyze data-driven conversions . <eos> our conversions find hypotheses with a small risk by explicitly minimizing datadependent generalization bounds . <eos> we experimentally demonstrate the usefulness of our approach and in particular show that the data-driven conversions consistently outperform the data-independent conversions .
probabilistic modeling of correlated neural population firing activity is central to understanding the neural code and building practical decoding algorithms . <eos> no parametric models currently exist for modeling multivariate correlated neural data and the high dimensional nature of the data makes fully non-parametric methods impractical . <eos> to address these problems we propose an energy-based model in which the joint probability of neural activity is represented using learned functions of the 1d marginal histograms of the data . <eos> the parameters of the model are learned using contrastive divergence and an optimization procedure for finding appropriate marginal directions . <eos> we evaluate the method using real data recorded from a population of motor cortical neurons . <eos> in particular , we model the joint probability of population spiking times and 2d hand position and show that the likelihood of test data under our model is significantly higher than under other models . <eos> these results suggest that our model captures correlations in the firing activity . <eos> our rich probabilistic model of neural population activity is a step towards both measurement of the importance of correlations in neural coding and improved decoding of population activity .
this paper presents a novel technique for analyzing electromagnetic imaging data obtained using the stimulus evoked experimental paradigm . <eos> the technique is based on a probabilistic graphical model , which describes the data in terms of underlying evoked and interference sources , and explicitly models the stimulus evoked paradigm . <eos> a variational bayesian em algorithm infers the model from data , suppresses interference sources , and reconstructs the activity of separated individual brain sources . <eos> the new algorithm outperforms existing techniques on two real datasets , as well as on simulated data .
this paper proposes an algorithm to convert a t -stage stochastic decision problem with a continuous state space to a sequence of supervised learning problems . <eos> the optimization problem associated with the trajectory tree and random trajectory methods of kearns , mansour , and ng , 2000 , is solved using the gauss-seidel method . <eos> the algorithm breaks a multistage reinforcement learning problem into a sequence of single-stage reinforcement learning subproblems , each of which is solved via an exact reduction to a weighted-classification problem that can be solved using off-the-self methods . <eos> thus the algorithm converts a reinforcement learning problem into simpler supervised learning subproblems . <eos> it is shown that the method converges in a finite number of steps to a solution that can not be further improved by componentwise optimization . <eos> the implication of the proposed algorithm is that a plethora of classification methods can be applied to find policies in the reinforcement learning problem .
we consider the problem of constructing an aggregated estimator from a finite class of base functions which approximately minimizes a convex risk functional under the ? 1 constraint . <eos> for this purpose , we propose a stochastic procedure , the mirror descent , which performs gradient descent in the dual space . <eos> the generated estimates are additionally averaged in a recursive fashion with specific weights . <eos> mirror descent algorithms have been developed in different contexts and they are known to be particularly efficient in high dimensional problems . <eos> moreover their implementation is adapted to the online setting . <eos> the main result of the paper is the upper bound on the convergence rate for the generalization error .
sparse pca seeks approximate sparse eigenvectorswhose projections capture the maximal variance of data . <eos> as a cardinality-constrained and non-convex optimization problem , it is np-hard and is encountered in a wide range of applied fields , from bio-informatics to finance . <eos> recent progress has focused mainly on continuous approximation and convex relaxation of the hard cardinality constraint . <eos> in contrast , we consider an alternative discrete spectral formulation based on variational eigenvalue bounds and provide an effective greedy strategy as well as provably optimal solutions using branch-and-bound search . <eos> moreover , the exact methodology used reveals a simple renormalization step that improves approximate solutions obtained by any continuous method . <eos> the resulting performance gain of discrete algorithms is demonstrated on real-world benchmark data and in extensive monte carlo evaluation trials .
when trying to understand the brain , it is of fundamental importance to analyse ( e.g . from eeg/meg measurements ) what parts of the cortex interact with each other in order to infer more accurate models of brain activity . <eos> common techniques like blind source separation ( bss ) can estimate brain sources and single out artifacts by using the underlying assumption of source signal independence . <eos> however , physiologically interesting brain sources typically interact , so bss will ? by construction ? <eos> fail to characterize them properly . <eos> noting that there are truly interacting sources and signals that only seemingly interact due to effects of volume conduction , this work aims to contribute by distinguishing these effects . <eos> for this a new bss technique is proposed that uses anti-symmetrized cross-correlation matrices and subsequent diagonalization . <eos> the resulting decomposition consists of the truly interacting brain sources and suppresses any spurious interaction stemming from volume conduction . <eos> our new concept of interacting source analysis ( isa ) is successfully demonstrated on meg data .
recent experimental results suggest that dendritic and back-propagating spikes can influence synaptic plasticity in different ways [ 1 ] . <eos> in this study we investigate how these signals could temporally interact at dendrites leading to changing plasticity properties at local synapse clusters . <eos> similar to a previous study [ 2 ] , we employ a differential hebbian plasticity rule to emulate spike-timing dependent plasticity . <eos> we use dendritic ( d- ) and back-propagating ( bp- ) spikes as post-synaptic signals in the learning rule and investigate how their interaction will influence plasticity . <eos> we will analyze a situation where synapse plasticity characteristics change in the course of time , depending on the type of post-synaptic activity momentarily elicited . <eos> starting with weak synapses , which only elicit local d-spikes , a slow , unspecific growth process is induced . <eos> as soon as the soma begins to spike this process is replaced by fast synaptic changes as the consequence of the much stronger and sharper bp-spike , which now dominates the plasticity rule . <eos> this way a winner-take-all-mechanism emerges in a two-stage process , enhancing the best-correlated inputs . <eos> these results suggest that synaptic plasticity is a temporal changing process by which the computational properties of dendrites or complete neurons can be substantially augmented .
this paper introduces gaussian process dynamical models ( gpdm ) for nonlinear time series analysis . <eos> a gpdm comprises a low-dimensional latent space with associated dynamics , and a map from the latent space to an observation space . <eos> we marginalize out the model parameters in closed-form , using gaussian process ( gp ) priors for both the dynamics and the observation mappings . <eos> this results in a nonparametric model for dynamical systems that accounts for uncertainty in the model . <eos> we demonstrate the approach on human motion capture data in which each pose is 62-dimensional . <eos> despite the use of small data sets , the gpdm learns an effective representation of the nonlinear dynamics in these spaces . <eos> webpage : http : //www.dgp.toronto.edu/ jmwang/gpdm/
the observed physiological dynamics of an infant receiving intensive care are affected by many possible factors , including interventions to the baby , the operation of the monitoring equipment and the state of health . <eos> the factorial switching kalman filter can be used to infer the presence of such factors from a sequence of observations , and to estimate the true values where these observations have been corrupted . <eos> we apply this model to clinical time series data and show it to be effective in identifying a number of artifactual and physiological patterns .
the octopus arm is a highly versatile and complex limb . <eos> how the octopus controls such a hyper-redundant arm ( not to mention eight of them ! ) <eos> is as yet unknown . <eos> robotic arms based on the same mechanical principles may render present day robotic arms obsolete . <eos> in this paper , we tackle this control problem using an online reinforcement learning algorithm , based on a bayesian approach to policy evaluation known as gaussian process temporal difference ( gptd ) learning . <eos> our substitute for the real arm is a computer simulation of a 2-dimensional model of an octopus arm . <eos> even with the simplifications inherent to this model , the state space we face is a high-dimensional one . <eos> we apply a gptdbased algorithm to this domain , and demonstrate its operation on several learning tasks of varying degrees of difficulty .
we propose a mean-field approximation that dramatically reduces the computational complexity of solving stochastic dynamic games . <eos> we provide conditions that guarantee our method approximates an equilibrium as the number of agents grow . <eos> we then derive a performance bound to assess how well the approximation performs for any given number of agents . <eos> we apply our method to an important class of problems in applied microeconomics . <eos> we show with numerical experiments that we are able to greatly expand the set of economic problems that can be analyzed computationally .
we present a new kernel method for extracting semantic relations between entities in natural language text , based on a generalization of subsequence kernels . <eos> this kernel uses three types of subsequence patterns that are typically employed in natural language to assert relationships between two entities . <eos> experiments on extracting protein interactions from biomedical corpora and top-level relations from newspaper corpora demonstrate the advantages of this approach .
this paper presents a new sampling algorithm for approximating functions of variables representable as undirected graphical models of arbitrary connectivity with pairwise potentials , as well as for estimating the notoriously difficult partition function of the graph . <eos> the algorithm fits into the framework of sequential monte carlo methods rather than the more widely used mcmc , and relies on constructing a sequence of intermediate distributions which get closer to the desired one . <eos> while the idea of using temperedproposals is known , we construct a novel sequence of target distributions where , rather than dropping a global temperature parameter , we sequentially couple individual pairs of variables that are , initially , sampled exactly from a spanning tree of the variables . <eos> we present experimental results on inference and estimation of the partition function for sparse and densely-connected graphs .
we use the k-core decomposition to develop algorithms for the analysis of large scale complex networks . <eos> this decomposition , based on a recursive pruning of the least connected vertices , allows to disentangle the hierarchical structure of networks by progressively focusing on their central cores . <eos> by using this strategy we develop a general visualization algorithm that can be used to compare the structural properties of various networks and highlight their hierarchical structure . <eos> the low computational complexity of the algorithm , o ( n + e ) , where n is the size of the network , and e is the number of edges , makes it suitable for the visualization of very large sparse networks . <eos> we show how the proposed visualization tool allows to find specific structural fingerprints of networks .
we study the statistical convergence and consistency of regularized boosting methods , where the samples are not independent and identically distributed ( i.i.d . ) <eos> but come from empirical processes of stationary ? -mixing sequences . <eos> utilizing a technique that constructs a sequence of independent blocks close in distribution to the original samples , we prove the consistency of the composite classifiers resulting from a regularization achieved by restricting the 1-norm of the base classifiers weights . <eos> when compared to the i.i.d . <eos> case , the nature of sampling manifests in the consistency result only through generalization of the original condition on the growth of the regularization parameter .
in this paper we propose a new receiver for digital communications . <eos> we focus on the application of gaussian processes ( gps ) to the multiuser detection ( mud ) in code division multiple access ( cdma ) systems to solve the near-far problem . <eos> hence , we aim to reduce the interference from other users sharing the same frequency band . <eos> while usual approaches minimize the mean square error ( mmse ) to linearly retrieve the user of interest , we exploit the same criteria but in the design of a nonlinear mud . <eos> since the optimal solution is known to be nonlinear , the performance of this novel method clearly improves that of the mmse detectors . <eos> furthermore , the gp based mud achieves excellent interference suppression even for short training sequences . <eos> we also include some experiments to illustrate that other nonlinear detectors such as those based on support vector machines ( svms ) exhibit a worse performance .
fisher linear discriminant analysis ( lda ) can be sensitive to the problem data . <eos> robust fisher lda can systematically alleviate the sensitivity problem by explicitly incorporating a model of data uncertainty in a classification problem and optimizing for the worst-case scenario under this model . <eos> the main contribution of this paper is show that with general convex uncertainty models on the problem data , robust fisher lda can be carried out using convex optimization . <eos> for a certain type of product form uncertainty model , robust fisher lda can be carried out at a cost comparable to standard fisher lda . <eos> the method is demonstrated with some numerical examples . <eos> finally , we show how to extend these results to robust kernel fisher discriminant analysis , i.e. , robust fisher lda in a high dimensional feature space .
the classical bayes rule computes the posterior model probability from the prior probability and the data likelihood . <eos> we generalize this rule to the case when the prior is a density matrix ( symmetric positive definite and trace one ) and the data likelihood a covariance matrix . <eos> the classical bayes rule is retained as the special case when the matrices are diagonal . <eos> in the classical setting , the calculation of the probability of the data is an expected likelihood , where the expectation is over the prior distribution . <eos> in the generalized setting , this is replaced by an expected variance calculation where the variance is computed along the eigenvectors of the prior density matrix and the expectation is over the eigenvalues of the density matrix ( which form a probability vector ) . <eos> the variances along any direction is determined by the covariance matrix . <eos> curiously enough this expected variance calculation is a quantum measurement where the co-variance matrix specifies the instrument and the prior density matrix the mixture state of the particle . <eos> we motivate both the classical and the generalized bayes rule with a minimum relative entropy principle , where the kullbach-leibler version gives the classical bayes rule and umegaki ? s quantum relative entropy the new bayes rule for density matrices .
we present a simple and scalable algorithm for large-margin estimation of structured models , including an important class of markov networks and combinatorial models . <eos> we formulate the estimation problem as a convex-concave saddle-point problem and apply the extragradient method , yielding an algorithm with linear convergence using simple gradient and projection calculations . <eos> the projection step can be solved using combinatorial algorithms for min-cost quadratic flow . <eos> this makes the approach an efficient alternative to formulations based on reductions to a quadratic program ( qp ) . <eos> we present experiments on two very different structured prediction tasks : 3d image segmentation and word alignment , illustrating the favorable scaling properties of our algorithm .
we show how to learn a mahanalobis distance metric for k-nearest neighbor ( knn ) classification by semidefinite programming . <eos> the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin . <eos> on seven data sets of varying size and difficulty , we find that metrics trained in this way lead to significant improvements in knn classification ? for example , achieving a test error rate of 1.3 % on the mnist handwritten digits . <eos> as in support vector machines ( svms ) , the learning problem reduces to a convex optimization based on the hinge loss . <eos> unlike learning in svms , however , our framework requires no modification or extension for problems in multiway ( as opposed to binary ) classification .
we describe a novel method for learning templates for recognition and localization of objects drawn from categories . <eos> a generative model represents the configuration of multiple object parts with respect to an object coordinate system ; these parts in turn generate image features . <eos> the complexity of the model in the number of features is low , meaning our model is much more efficient to train than comparative methods . <eos> moreover , a variational approximation is introduced that allows learning to be orders of magnitude faster than previous approaches while incorporating many more features . <eos> this results in both accuracy and localization improvements . <eos> our model has been carefully tested on standard datasets ; we compare with a number of recent template models . <eos> in particular , we demonstrate state-of-the-art results for detection and localization .
images represent an important and abundant source of data . <eos> understanding their statistical structure has important applications such as image compression and restoration . <eos> in this paper we propose a particular kind of probabilistic model , dubbed the ? products of edge-perts model ? <eos> to describe the structure of wavelet transformed images . <eos> we develop a practical denoising algorithm based on a single edge-pert and show state-ofthe-art denoising performance on benchmark images .
we present a competitive analysis of some non-parametric bayesian algorithms in a worst-case online learning setting , where no probabilistic assumptions about the generation of the data are made . <eos> we consider models which use a gaussian process prior ( over the space of all functions ) and provide bounds on the regret ( under the log loss ) for commonly used non-parametric bayesian algorithms ? <eos> including gaussian regression and logistic regression ? <eos> which show how these algorithms can perform favorably under rather general conditions . <eos> these bounds explicitly handle the infinite dimensionality of these non-parametric classes in a natural way . <eos> we also make formal connections to the minimax and minimum description length ( mdl ) framework . <eos> here , we show precisely how bayesian gaussian regression is a minimax strategy .
the category of visual stimuli has been reliably decoded from patterns of neural activity in extrastriate visual cortex [ 1 ] . <eos> it has yet to be seen whether object identity can be inferred from this activity . <eos> we present fmri data measuring responses in human extrastriate cortex to a set of 12 distinct object images . <eos> we use a simple winner-take-all classifier , using half the data from each recording session as a training set , to evaluate encoding of object identity across fmri voxels . <eos> since this approach is sensitive to the inclusion of noisy voxels , we describe two methods for identifying subsets of voxels in the data which optimally distinguish object identity . <eos> one method characterizes the reliability of each voxel within subsets of the data , while another estimates the mutual information of each voxel with the stimulus set . <eos> we find that both metrics can identify subsets of the data which reliably encode object identity , even when noisy measurements are artificially added to the data . <eos> the mutual information metric is less efficient at this task , likely due to constraints in fmri data .
convexity has recently received a lot of attention in the machine learning community , and the lack of convexity has been seen as a major disadvantage of many learning algorithms , such as multi-layer artificial neural networks . <eos> we show that training multi-layer neural networks in which the number of hidden units is learned can be viewed as a convex optimization problem . <eos> this problem involves an infinite number of variables , but can be solved by incrementally inserting a hidden unit at a time , each time finding a linear classifier that minimizes a weighted sum of errors .
this paper provides a system-level analysis of a scalable distributed sensing model for networked sensors . <eos> in our system model , a data center acquires data from a bunch of l sensors which each independently encode their noisy observations of an original binary sequence , and transmit their encoded data sequences to the data center at a combined rate r , which is limited . <eos> supposing that the sensors use independent ldgm rate distortion codes , we show that the system performance can be evaluated for any given finite r when the number of sensors l goes to infinity . <eos> the analysis shows how the optimal strategy for the distributed sensing problem changes at critical values of the data rate r or the noise level .
survival in the natural world demands the selection of relevant visual cues to rapidly and reliably guide attention towards prey and predators in cluttered environments . <eos> we investigate whether our visual system selects cues that guide search in an optimal manner . <eos> we formally obtain the optimal cue selection strategy by maximizing the signal to noise ratio ( sn r ) between a search target and surrounding distractors . <eos> this optimal strategy successfully accounts for several phenomena in visual search behavior , including the effect of target-distractor discriminability , uncertainty in target ? s features , distractor heterogeneity , and linear separability . <eos> furthermore , the theory generates a new prediction , which we verify through psychophysical experiments with human subjects . <eos> our results provide direct experimental evidence that humans select visual cues so as to maximize sn r between the targets and surrounding clutter .
female crickets can locate males by phonotaxis to the mating song they produce . <eos> the behaviour and underlying physiology has been studied in some depth showing that the cricket auditory system solves this complex problem in a unique manner . <eos> we present an analogue very large scale integrated ( avlsi ) circuit model of this process and show that results from testing the circuit agree with simulation and what is known from the behaviour and physiology of the cricket auditory system . <eos> the avlsi circuitry is now being extended to use on a robot along with previously modelled neural circuitry to better understand the complete sensorimotor pathway .
we propose efficient algorithms for learning ranking functions from order constraints between sets ? i.e . classes ? of training samples . <eos> our algorithms may be used for maximizing the generalized wilcoxon mann whitney statistic that accounts for the partial ordering of the classes : special cases include maximizing the area under the roc curve for binary classification and its generalization for ordinal regression . <eos> experiments on public benchmarks indicate that : ( a ) the proposed algorithm is at least as accurate as the current state-of-the-art ; ( b ) computationally , it is several orders of magnitude faster and ? unlike current methods ? it is easily able to handle even large datasets with over 20,000 samples .
to investigate how top-down ( td ) and bottom-up ( bu ) information is weighted in the guidance of human search behavior , we manipulated the proportions of bu and td components in a saliency-based model . <eos> the model is biologically plausible and implements an artificial retina and a neuronal population code . <eos> the bu component is based on featurecontrast . <eos> the td component is defined by a feature-template match to a stored target representation . <eos> we compared the model ? s behavior at different mixtures of td and bu components to the eye movement behavior of human observers performing the identical search task . <eos> we found that a purely td model provides a much closer match to human behavior than any mixture model using bu information . <eos> only when biological constraints are removed ( e.g. , eliminating the retina ) did a bu/td mixture model begin to approximate human behavior .
the perceptron algorithm , despite its simplicity , often performs well on online classification tasks . <eos> the perceptron becomes especially effective when it is used in conjunction with kernels . <eos> however , a common difficulty encountered when implementing kernel-based online algorithms is the amount of memory required to store the online hypothesis , which may grow unboundedly . <eos> in this paper we present and analyze the forgetron algorithm for kernel-based online learning on a fixed memory budget . <eos> to our knowledge , this is the first online learning algorithm which , on one hand , maintains a strict limit on the number of examples it stores while , on the other hand , entertains a relative mistake bound . <eos> in addition to the formal results , we also present experiments with real datasets which underscore the merits of our approach .
we derive a bayesian ideal observer ( bio ) for detecting motion and solving the correspondence problem . <eos> we obtain barlow and tripathy ? s classic model as an approximation . <eos> our psychophysical experiments show that the trends of human performance are similar to the bayesian ideal , but overall human performance is far worse . <eos> we investigate ways to degrade the bayesian ideal but show that even extreme degradations do not approach human performance . <eos> instead we propose that humans perform motion tasks using generic , general purpose , models of motion . <eos> we perform more psychophysical experiments which are consistent with humans using a slow-and-smooth model and which rule out an alternative model using slowness .
we present a method for nonparametric regression that performs bandwidth selection and variable selection simultaneously . <eos> the approach is based on the technique of incrementally decreasing the bandwidth in directions where the gradient of the estimator with respect to bandwidth is large . <eos> when the unknown function satisfies a sparsity condition , our approach avoids the curse of dimensionality , achieving the optimal minimax rate of convergence , up to logarithmic factors , as if the relevant variables were known in advance . <eos> the method ? called rodeo ( regularization of derivative expectation operator ) ? conducts a sequence of hypothesis tests , and is easy to implement . <eos> a modified version that replaces hard with soft thresholding effectively solves a sequence of lasso problems .
we show that linear generalizations of rescorla-wagner can perform maximum likelihood estimation of the parameters of all generative models for causal reasoning . <eos> our approach involves augmenting variables to deal with conjunctions of causes , similar to the agumented model of rescorla . <eos> our results involve genericity assumptions on the distributions of causes . <eos> if these assumptions are violated , for example for the cheng causal power theory , then we show that a linear rescorla-wagner can estimate the parameters of the model up to a nonlinear transformtion . <eos> moreover , a nonlinear rescorla-wagner is able to estimate the parameters directly to within arbitrary accuracy . <eos> previous results can be used to determine convergence and to estimate convergence rates .
we present a series of theoretical arguments supporting the claim that a large class of modern learning algorithms that rely solely on the smoothness prior ? <eos> with similarity between examples expressed with a local kernel ? <eos> are sensitive to the curse of dimensionality , or more precisely to the variability of the target . <eos> our discussion covers supervised , semisupervised and unsupervised learning algorithms . <eos> these algorithms are found to be local in the sense that crucial properties of the learned function at x depend mostly on the neighbors of x in the training set . <eos> this makes them sensitive to the curse of dimensionality , well studied for classical non-parametric statistical learning . <eos> we show in the case of the gaussian kernel that when the function to be learned has many variations , these algorithms require a number of training examples proportional to the number of variations , which could be large even though there may exist short descriptions of the target function , i.e . their kolmogorov complexity may be low . <eos> this suggests that there exist non-local learning algorithms that at least have the potential to learn about such structured but apparently complex functions ( because locally they have many variations ) , while not using very specific prior domain knowledge .
given a directed graphical model with binary-valued hidden nodes and real-valued noisy observations , consider deciding upon the maximum a-posteriori ( map ) or the maximum posterior-marginal ( mpm ) assignment under the restriction that each node broadcasts only to its children exactly one single-bit message . <eos> we present a variational formulation , viewing the processing rules local to all nodes as degrees-of-freedom , that minimizes the loss in expected ( map or mpm ) performance subject to such online communication constraints . <eos> the approach leads to a novel message-passing algorithm to be executed offline , or before observations are realized , which mitigates the performance loss by iteratively coupling all rules in a manner implicitly driven by global statistics . <eos> we also provide ( i ) illustrative examples , ( ii ) assumptions that guarantee convergence and efficiency and ( iii ) connections to active research areas .
we present micropower mixed-signal vlsi hardware for real-time blind separation and localization of acoustic sources . <eos> gradient flow representation of the traveling wave signals acquired over a miniature ( 1cm diameter ) array of four microphones yields linearly mixed instantaneous observations of the time-differentiated sources , separated and localized by independent component analysis ( ica ) . <eos> the gradient flow and ica processors each measure 3mm ? <eos> 3mm in 0.5 ? m cmos , and consume 54 ? w and 180 ? w power , respectively , from a 3 v supply at 16 ks/s sampling rate . <eos> experiments demonstrate perceptually clear ( 12db ) separation and precise localization of two speech sources presented through speakers positioned at 1.5m from the array on a conference room table . <eos> analysis of the multipath residuals shows that they are spectrally diffuse , and void of the direct path .
fast and frugal heuristics are well studied models of bounded rationality . <eos> psychological research has proposed the take-the-best heuristic as a successful strategy in decision making with limited resources . <eos> take-thebest searches for a sufficiently good ordering of cues ( features ) in a task where objects are to be compared lexicographically . <eos> we investigate the complexity of the problem of approximating optimal cue permutations for lexicographic strategies . <eos> we show that no efficient algorithm can approximate the optimum to within any constant factor , if p 6= np . <eos> we further consider a greedy approach for building lexicographic strategies and derive tight bounds for the performance ratio of a new and simple algorithm . <eos> this algorithm is proven to perform better than take-the-best .
we present a model of edge and region grouping using a conditional random field built over a scale-invariant representation of images to integrate multiple cues . <eos> our model includes potentials that capture low-level similarity , mid-level curvilinear continuity and high-level object shape . <eos> maximum likelihood parameters for the model are learned from human labeled groundtruth on a large collection of horse images using belief propagation . <eos> using held out test data , we quantify the information gained by incorporating generic mid-level cues and high-level shape .
we present a bayesian framework for explaining how people reason about and predict the actions of an intentional agent , based on observing its behavior . <eos> action-understanding is cast as a problem of inverting a probabilistic generative model , which assumes that agents tend to act rationally in order to achieve their goals given the constraints of their environment . <eos> working in a simple sprite-world domain , we show how this model can be used to infer the goal of an agent and predict how the agent will act in novel situations or when environmental constraints change . <eos> the model provides a qualitative account of several kinds of inferences that preverbal infants have been shown to perform , and also fits quantitative predictions that adult observers make in a new experiment .
this paper proposes a new approach to feature selection based on a statistical feature mining technique for sequence and tree kernels . <eos> since natural language data take discrete structures , convolution kernels , such as sequence and tree kernels , are advantageous for both the concept and accuracy of many natural language processing tasks . <eos> however , experiments have shown that the best results can only be achieved when limited small sub-structures are dealt with by these kernels . <eos> this paper discusses this issue of convolution kernels and then proposes a statistical feature selection that enable us to use larger sub-structures effectively . <eos> the proposed method , in order to execute efficiently , can be embedded into an original kernel calculation process by using sub-structure mining algorithms . <eos> experiments on real nlp tasks confirm the problem in the conventional method and compare the performance of a conventional method to that of the proposed method .
inspired by ? google sets ? , we consider the problem of retrieving items from a concept or cluster , given a query consisting of a few items from that cluster . <eos> we formulate this as a bayesian inference problem and describe a very simple algorithm for solving it . <eos> our algorithm uses a modelbased concept of a cluster and ranks items using a score which evaluates the marginal probability that each item belongs to a cluster containing the query items . <eos> for exponential family models with conjugate priors this marginal probability is a simple function of sufficient statistics . <eos> we focus on sparse binary data and show that our score can be evaluated exactly using a single sparse matrix multiplication , making it possible to apply our algorithm to very large datasets . <eos> we evaluate our algorithm on three datasets : retrieving movies from eachmovie , finding completions of author sets from the nips dataset , and finding completions of sets of words appearing in the grolier encyclopedia . <eos> we compare to google sets and show that bayesian sets gives very reasonable set completions .
we propose a fast manifold learning algorithm based on the methodology of domain decomposition . <eos> starting with the set of sample points partitioned into two subdomains , we develop the solution of the interface problem that can glue the embeddings on the two subdomains into an embedding on the whole domain . <eos> we provide a detailed analysis to assess the errors produced by the gluing process using matrix perturbation theory . <eos> numerical examples are given to illustrate the efficiency and effectiveness of the proposed methods .
we propose a new bayesian method for spatial cluster detection , the ? bayesian spatial scan statistic , ? <eos> and compare this method to the standard ( frequentist ) scan statistic approach . <eos> we demonstrate that the bayesian statistic has several advantages over the frequentist approach , including increased power to detect clusters and ( since randomization testing is unnecessary ) much faster runtime . <eos> we evaluate the bayesian and frequentist methods on the task of prospective disease surveillance : detecting spatial clusters of disease cases resulting from emerging disease outbreaks . <eos> we demonstrate that our bayesian methods are successful in rapidly detecting outbreaks while keeping number of false positives low .
we present a probabilistic generative model of entity relationships and their attributes that simultaneously discovers groups among the entities and topics among the corresponding textual attributes . <eos> block-models of relationship data have been studied in social network analysis for some time . <eos> here we simultaneously cluster in several modalities at once , incorporating the attributes ( here , words ) associated with certain relationships . <eos> significantly , joint inference allows the discovery of topics to be guided by the emerging groups , and vice-versa . <eos> we present experimental results on two large data sets : sixteen years of bills put before the u.s. senate , comprising their corresponding text and voting records , and thirteen years of similar data from the united nations . <eos> we show that in comparison with traditional , separate latent-variable models for words , or blockstructures for votes , the group-topic model ? s joint inference discovers more cohesive groups and improved topics .
we analyze classification error on unseen cases , i.e . cases that are different from those in the training set . <eos> unlike standard generalization error , this off-training-set error may differ significantly from the empirical error with high probability even with large sample sizes . <eos> we derive a datadependent bound on the difference between off-training-set and standard generalization error . <eos> our result is based on a new bound on the missing mass , which for small samples is stronger than existing bounds based on good-turing estimators . <eos> as we demonstrate on uci data-sets , our bound gives nontrivial generalization guarantees in many practical cases . <eos> in light of these results , we show that certain claims made in the no free lunch literature are overly pessimistic .
spiking activity from neurophysiological experiments often exhibits dynamics beyond that driven by external stimulation , presumably reflecting the extensive recurrence of neural circuitry . <eos> characterizing these dynamics may reveal important features of neural computation , particularly during internally-driven cognitive operations . <eos> for example , the activity of premotor cortex ( pmd ) neurons during an instructed delay period separating movement-target specification and a movementinitiation cue is believed to be involved in motor planning . <eos> we show that the dynamics underlying this activity can be captured by a lowdimensional non-linear dynamical systems model , with underlying recurrent structure and stochastic point-process output . <eos> we present and validate latent variable methods that simultaneously estimate the system parameters and the trial-by-trial dynamical trajectories . <eos> these methods are applied to characterize the dynamics in pmd data recorded from a chronically-implanted 96-electrode array while monkeys perform delayed-reach tasks .
we investigate under what conditions a neuron can learn by experimentally supported rules for spike timing dependent plasticity ( stdp ) to predict the arrival times of strong ? teacher inputs to the same neuron . <eos> it turns out that in contrast to the famous perceptron convergence theorem , which predicts convergence of the perceptron learning rule for a simplified neuron model whenever a stable solution exists , no equally strong convergence guarantee can be given for spiking neurons with stdp . <eos> but we derive a criterion on the statistical dependency structure of input spike trains which characterizes exactly when learning with stdp will converge on average for a simple model of a spiking neuron . <eos> this criterion is reminiscent of the linear separability criterion of the perceptron convergence theorem , but it applies here to the rows of a correlation matrix related to the spike inputs . <eos> in addition we show through computer simulations for more realistic neuron models that the resulting analytically predicted positive learning results not only hold for the common interpretation of stdp where stdp changes the weights of synapses , but also for a more realistic interpretation suggested by experimental data where stdp modulates the initial release probability of dynamic synapses .
reinforcement learning by direct policy gradient estimation is attractive in theory but in practice leads to notoriously ill-behaved optimization problems . <eos> we improve its robustness and speed of convergence with stochastic meta-descent , a gain vector adaptation method that employs fast hessian-vector products . <eos> in our experiments the resulting algorithms outperform previously employed online stochastic , offline conjugate , and natural policy gradient methods .
we consider the problem of modeling a helicopter ? s dynamics based on state-action trajectories collected from it . <eos> the contribution of this paper is two-fold . <eos> first , we consider the linear models such as learned by cifer ( the industry standard in helicopter identification ) , and show that the linear parameterization makes certain properties of dynamical systems , such as inertia , fundamentally difficult to capture . <eos> we propose an alternative , acceleration based , parameterization that does not suffer from this deficiency , and that can be learned as efficiently from data . <eos> second , a markov decision process model of a helicopter ? s dynamics would explicitly model only the one-step transitions , but we are often interested in a model ? s predictive performance over longer timescales . <eos> in this paper , we present an efficient algorithm for ( approximately ) minimizing the prediction error over long time scales . <eos> we present empirical results on two different helicopters . <eos> although this work was motivated by the problem of modeling helicopters , the ideas presented here are general , and can be applied to modeling large classes of vehicular dynamics .
we consider regularized least-squares ( rls ) with a gaussian kernel . <eos> we prove that if we let the gaussian bandwidth while letting the regularization parameter 0 , the rls solution tends to a polynomial whose order is controlled by the rielative rates of decay of ? 12 and : if = ( 2k+1 ) , then , as ? , the rls solution tends to the kth order polynomial with minimal empirical error . <eos> we illustrate the result with an example .
kernel methods make it relatively easy to define complex highdimensional feature spaces . <eos> this raises the question of how we can identify the relevant subspaces for a particular learning task . <eos> when two views of the same phenomenon are available kernel canonical correlation analysis ( kcca ) has been shown to be an effective preprocessing step that can improve the performance of classification algorithms such as the support vector machine ( svm ) . <eos> this paper takes this observation to its logical conclusion and proposes a method that combines this two stage learning ( kcca followed by svm ) into a single optimisation termed svm-2k . <eos> we present both experimental and theoretical analysis of the approach showing encouraging results and insights .
a model of bottom-up overt attention is proposed based on the principle of maximizing information sampled from a scene . <eos> the proposed operation is based on shannon 's self-information measure and is achieved in a neural circuit , which is demonstrated as having close ties with the circuitry existent in the primate visual cortex . <eos> it is further shown that the proposed saliency measure may be extended to address issues that currently elude explanation in the domain of saliency based models . <eos> resu lts on natural images are compared with experimental eye tracking data revealing the efficacy of the model in predicting the deployment of overt attention as compared with existing efforts .
this paper presents a rigorous statistical analysis characterizing regimes in which active learning significantly outperforms classical passive learning . <eos> active learning algorithms are able to make queries or select sample locations in an online fashion , depending on the results of the previous queries . <eos> in some regimes , this extra flexibility leads to significantly faster rates of error decay than those possible in classical passive learning settings . <eos> the nature of these regimes is explored by studying fundamental performance limits of active and passive learning in two illustrative nonparametric function classes . <eos> in addition to examining the theoretical potential of active learning , this paper describes a practical algorithm capable of exploiting the extra flexibility of the active setting and provably improving upon the classical passive techniques . <eos> our active learning theory and methods show promise in a number of applications , including field estimation using wireless sensor networks and fault line detection .
a dynamic texture is a video model that treats a video as a sample from a spatio-temporal stochastic process , specifically a linear dynamical system . <eos> one problem associated with the dynamic texture is that it can not model video where there are multiple regions of distinct motion . <eos> in this work , we introduce the layered dynamic texture model , which addresses this problem . <eos> we also introduce a variant of the model , and present the em algorithm for learning each of the models . <eos> finally , we demonstrate the efficacy of the proposed model for the tasks of segmentation and synthesis of video .
this paper presents a new framework based on walks in a graph for analysis and inference in gaussian graphical models . <eos> the key idea is to decompose correlations between variables as a sum over all walks between those variables in the graph . <eos> the weight of each walk is given by a product of edgewise partial correlations . <eos> we provide a walk-sum interpretation of gaussian belief propagation in trees and of the approximate method of loopy belief propagation in graphs with cycles . <eos> this perspective leads to a better understanding of gaussian belief propagation and of its convergence in loopy graphs .
although variants of value iteration have been proposed for finding nash or correlated equilibria in general-sum markov games , these variants have not been shown to be effective in general . <eos> in this paper , we demonstrate by construction that existing variants of value iteration can not find stationary equilibrium policies in arbitrary general-sum markov games . <eos> instead , we propose an alternative interpretation of the output of value iteration based on a new ( non-stationary ) equilibrium concept that we call ? cyclic equilibria . ? <eos> we prove that value iteration identifies cyclic equilibria in a class of games in which it fails to find stationary equilibria . <eos> we also demonstrate empirically that value iteration finds cyclic equilibria in nearly all examples drawn from a random distribution of markov games .
brain-computer interface ( bci ) systems create a novel communication channel from the brain to an output device by bypassing conventional motor output pathways of nerves and muscles . <eos> therefore they could provide a new communication and control option for paralyzed patients . <eos> modern bci technology is essentially based on techniques for the classification of single-trial brain signals . <eos> here we present a novel technique that allows the simultaneous optimization of a spatial and a spectral filter enhancing discriminability of multi-channel eeg single-trials . <eos> the evaluation of 60 experiments involving 22 different subjects demonstrates the superiority of the proposed algorithm . <eos> apart from the enhanced classification , the spatial and/or the spectral filter that are determined by the algorithm can also be used for further analysis of the data , e.g. , for source localization of the respective brain rhythms .
this paper describes a highly successful application of mrfs to the problem of generating high-resolution range images . <eos> a new generation of range sensors combines the capture of low-resolution range images with the acquisition of registered high-resolution camera images . <eos> the mrf in this paper exploits the fact that discontinuities in range and coloring tend to co-align . <eos> this enables it to generate high-resolution , low-noise range images by integrating regular camera images into the range data . <eos> we show that by using such an mrf , we can substantially improve over existing range imaging technology .
we investigate the learning of the appearance of an object from a single image of it . <eos> instead of using a large number of pictures of the object to recognize , we use a labeled reference database of pictures of other objects to learn invariance to noise and variations in pose and illumination . <eos> this acquired knowledge is then used to predict if two pictures of new objects , which do not appear on the training pictures , actually display the same object . <eos> we propose a generic scheme called chopping to address this task . <eos> it relies on hundreds of random binary splits of the training set chosen to keep together the images of any given object . <eos> those splits are extended to the complete image space with a simple learning algorithm . <eos> given two images , the responses of the split predictors are combined with a bayesian rule into a posterior probability of similarity . <eos> experiments with the coil-100 database and with a database of 150 degraded latex symbols compare our method to a classical learning with several examples of the positive class and to a direct learning of the similarity .
we prove the strongest known bound for the risk of hypotheses selected from the ensemble generated by running a learning algorithm incrementally on the training data . <eos> our result is based on proof techniques that are remarkably different from the standard risk analysis based on uniform convergence arguments .
we present a novel cochlear model implemented in analog very large scale integration ( vlsi ) technology that emulates nonlinear active cochlear behavior . <eos> this silicon cochlea includes outer hair cell ( ohc ) electromotility through active bidirectional coupling ( abc ) , a mechanism we proposed in which ohc motile forces , through the microanatomical organization of the organ of corti , realize the cochlear amplifier . <eos> our chip measurements demonstrate that frequency responses become larger and more sharply tuned when abc is turned on ; the degree of the enhancement decreases with input intensity as abc includes saturation of ohc forces .
an increasing number of projects in neuroscience requires the statistical analysis of high dimensional data sets , as , for instance , in predicting behavior from neural ? ring or in operating arti ? cial devices from brain recordings in brain-machine interfaces . <eos> linear analysis techniques remain prevalent in such cases , but classical linear regression approaches are often numerically too fragile in high dimensions . <eos> in this paper , we address the question of whether emg data collected from arm movements of monkeys can be faithfully reconstructed with linear approaches from neural activity in primary motor cortex ( m1 ) . <eos> to achieve robust data analysis , we develop a full bayesian approach to linear regression that automatically detects and excludes irrelevant features in the data , regularizing against over ? tting . <eos> in comparison with ordinary least squares , stepwise regression , partial least squares , lasso regression and a brute force combinatorial search for the most predictive input features in the data , we demonstrate that the new bayesian method o ? ers a superior mixture of characteristics in terms of regularization against over ? tting , computational e ? ciency and ease of use , demonstrating its potential as a drop-in replacement for other linear regression techniques . <eos> as neuroscienti ? c results , our analyses demonstrate that emg data can be well predicted from m1 neurons , further opening the path for possible real-time interfaces between brains and machines .
reinforcement learning models have long promised to unify computational , psychological and neural accounts of appetitively conditioned behavior . <eos> however , the bulk of data on animal conditioning comes from free-operant experiments measuring how fast animals will work for reinforcement . <eos> existing reinforcement learning ( rl ) models are silent about these tasks , because they lack any notion of vigor . <eos> they thus fail to address the simple observation that hungrier animals will work harder for food , as well as stranger facts such as their sometimes greater productivity even when working for irrelevant outcomes such as water . <eos> here , we develop an rl framework for free-operant behavior , suggesting that subjects choose how vigorously to perform selected actions by optimally balancing the costs and benefits of quick responding . <eos> motivational states such as hunger shift these factors , skewing the tradeoff . <eos> this accounts normatively for the effects of motivation on response rates , as well as many other classic findings . <eos> finally , we suggest that tonic levels of dopamine may be involved in the computation linking motivational state to optimal responding , thereby explaining the complex vigor-related effects of pharmacological manipulation of dopamine .
linear text classification algorithms work by computing an inner product between a test document vector and a parameter vector . <eos> in many such algorithms , including naive bayes and most tfidf variants , the parameters are determined by some simple , closed-form , function of training set statistics ; we call this mapping mapping from statistics to parameters , the parameter function . <eos> much research in text classification over the last few decades has consisted of manual efforts to identify better parameter functions . <eos> in this paper , we propose an algorithm for automatically learning this function from related classification problems . <eos> the parameter function found by our algorithm then defines a new learning algorithm for text classification , which we can apply to novel classification tasks . <eos> we find that our learned classifier outperforms existing methods on a variety of multiclass text classification tasks .
in this paper , we aim at analyzing the characteristic of neuronal population responses to instantaneous or time-dependent inputs and the role of synapses in neural information processing . <eos> we have derived an evolution equation of the membrane potential density function with synaptic depression , and obtain the formulas for analytic computing the response of instantaneous re rate . <eos> through a technical analysis , we arrive at several signi cant conclusions : the background inputs play an important role in information processing and act as a switch betwee temporal integration and coincidence detection . <eos> the role of synapses can be regarded as a spatio-temporal lter ; it is important in neural information processing for the spatial distribution of synapses and the spatial and temporal relation of inputs . <eos> the instantaneous input frequency can affect the response amplitude and phase delay .
although non-parametric tests have already been proposed for that purpose , statistical significance tests for non-standard measures ( different from the classification error ) are less often used in the literature . <eos> this paper is an attempt at empirically verifying how these tests compare with more classical tests , on various conditions . <eos> more precisely , using a very large dataset to estimate the whole ? population ? , we analyzed the behavior of several statistical test , varying the class unbalance , the compared models , the performance measure , and the sample size . <eos> the main result is that providing big enough evaluation sets non-parametric tests are relatively reliable in all conditions .
we describe a vision-based obstacle avoidance system for off-road mobile robots . <eos> the system is trained from end to end to map raw input images to steering angles . <eos> it is trained in supervised mode to predict the steering angles provided by a human driver during training runs collected in a wide variety of terrains , weather conditions , lighting conditions , and obstacle types . <eos> the robot is a 50cm off-road truck , with two forwardpointing wireless color cameras . <eos> a remote computer processes the video and controls the robot via radio . <eos> the learning system is a large 6-layer convolutional network whose input is a single left/right pair of unprocessed low-resolution images . <eos> the robot exhibits an excellent ability to detect obstacles and navigate around them in real time at speeds of 2 m/s .
we present a non-linear , simple , yet effective , feature subset selection method for regression and use it in analyzing cortical neural activity . <eos> our algorithm involves a feature-weighted version of the k-nearest-neighbor algorithm . <eos> it is able to capture complex dependency of the target function on its input and makes use of the leave-one-out error as a natural regularization . <eos> we explain the characteristics of our algorithm on synthetic problems and use it in the context of predicting hand velocity from spikes recorded in motor cortex of a behaving monkey . <eos> by applying feature selection we are able to improve prediction quality and suggest a novel way of exploring neural data .
motor imagery attenuates eeg and rhythms over sensorimotor cortices . <eos> these amplitude changes are most successfully captured by the method of common spatial patterns ( csp ) and widely used in braincomputer interfaces ( bci ) . <eos> bci methods based on amplitude information , however , have not incoporated the rich phase dynamics in the eeg rhythm . <eos> this study reports on a bci method based on phase synchrony rate ( sr ) . <eos> sr , computed from binarized phase locking value , describes the number of discrete synchronization events within a window . <eos> statistical nonparametric tests show that srs contain significant differences between 2 types of motor imageries . <eos> classifiers trained on srs consistently demonstrate satisfactory results for all 5 subjects . <eos> it is further observed that , for 3 subjects , phase is more discriminative than amplitude in the first 1.5-2.0 s , which suggests that phase has the potential to boost the information transfer rate in bcis .
many real-world classification problems involve the prediction of multiple inter-dependent variables forming some structural dependency . <eos> recent progress in machine learning has mainly focused on supervised classification of such structured variables . <eos> in this paper , we investigate structured classification in a semi-supervised setting . <eos> we present a discriminative approach that utilizes the intrinsic geometry of input patterns revealed by unlabeled data points and we derive a maximum-margin formulation of semi-supervised learning for structured variables . <eos> unlike transductive algorithms , our formulation naturally extends to new test points .
given a probability measure p and a reference measure ? , one is often interested in the minimum ? -measure set with p -measure at least ? . <eos> minimum volume sets of this type summarize the regions of greatest probability mass of p , and are useful for detecting anomalies and constructing confidence regions . <eos> this paper addresses the problem of estimating minimum volume sets based on independent samples distributed according to p . <eos> other than these samples , no other information is available regarding p , but the reference measure ? <eos> is assumed to be known . <eos> we introduce rules for estimating minimum volume sets that parallel the empirical risk minimization and structural risk minimization principles in classification . <eos> as in classification , we show that the performances of our estimators are controlled by the rate of uniform convergence of empirical to true probabilities over the class from which the estimator is drawn . <eos> thus we obtain finite sample size performance bounds in terms of vc dimension and related quantities . <eos> we also demonstrate strong universal consistency and an oracle inequality . <eos> estimators based on histograms and dyadic partitions illustrate the proposed rules .
recurrent networks that perform a winner-take-all computation have been studied extensively . <eos> although some of these studies include spiking networks , they consider only analog input rates . <eos> we present results of this winner-take-all computation on a network of integrate-and-fire neurons which receives spike trains as inputs . <eos> we show how we can configure the connectivity in the network so that the winner is selected after a pre-determined number of input spikes . <eos> we discuss spiking inputs with both regular frequencies and poisson-distributed rates . <eos> the robustness of the computation was tested by implementing the winner-take-all network on an analog vlsi array of 64 integrate-and-fire neurons which have an innate variance in their operating parameters .
separation of music signals is an interesting but difficult problem . <eos> it is helpful for many other music researches such as audio content analysis . <eos> in this paper , a new music signal separation method is proposed , which is based on harmonic structure modeling . <eos> the main idea of harmonic structure modeling is that the harmonic structure of a music signal is stable , so a music signal can be represented by a harmonic structure model . <eos> accordingly , a corresponding separation algorithm is proposed . <eos> the main idea is to learn a harmonic structure model for each music signal in the mixture , and then separate signals by using these models to distinguish harmonic structures of different signals . <eos> experimental results show that the algorithm can separate signals and obtain not only a very high signalto-noise ratio ( snr ) but also a rather good subjective audio quality .
we propose a new linear method for dimension reduction to identify nongaussian components in high dimensional data . <eos> our method , ngca ( non-gaussian component analysis ) , uses a very general semi-parametric framework . <eos> in contrast to existing projection methods we define what is uninteresting ( gaussian ) : by projecting out uninterestingness , we can estimate the relevant non-gaussian subspace . <eos> we show that the estimation error of finding the non-gaussian components tends to zero at a parametric rate . <eos> once ngca components are identified and extracted , various tasks can be applied in the data analysis process , like data visualization , clustering , denoising or classification . <eos> a numerical study demonstrates the usefulness of our method .
functional magnetic resonance imaging ( fmri ) has enabled scientists to look into the active brain . <eos> however , interactivity between functional brain regions , is still little studied . <eos> in this paper , we contribute a novel framework for modeling the interactions between multiple active brain regions , using dynamic bayesian networks ( dbns ) as generative models for brain activation patterns . <eos> this framework is applied to modeling of neuronal circuits associated with reward . <eos> the novelty of our framework from a machine learning perspective lies in the use of dbns to reveal the brain connectivity and interactivity . <eos> such interactivity models which are derived from fmri data are then validated through a group classification task . <eos> we employ and compare four different types of dbns : parallel hidden markov models , coupled hidden markov models , fully-linked hidden markov models and dynamically multilinked hmms ( dml-hmm ) . <eos> moreover , we propose and compare two schemes of learning dml-hmms . <eos> experimental results show that by using dbns , group classification can be performed even if the dbns are constructed from as few as 5 brain regions . <eos> we also demonstrate that , by using the proposed learning algorithms , different dbn structures characterize drug addicted subjects vs. control subjects . <eos> this finding provides an independent test for the effect of psychopathology on brain function . <eos> in general , we demonstrate that incorporation of computer science principles into functional neuroimaging clinical studies provides a novel approach for probing human brain function .
in this paper we derive an algorithm that computes the entire solution path of the support vector regression , with essentially the same computational cost as ? tting one svr model . <eos> we also propose an unbiased estimate for the degrees of freedom of the svr model , which allows convenient selection of the regularization parameter .
we present a new gaussian process ( gp ) regression model whose covariance is parameterized by the the locations of m pseudo-input points , which we learn by a gradient based optimization . <eos> we take m n , where n is the number of real data points , and hence obtain a sparse regression method which has o ( m 2 n ) training cost and o ( m 2 ) prediction cost per test case . <eos> we also find hyperparameters of the covariance function in the same joint optimization . <eos> the method can be viewed as a bayesian regression model with particular input dependent noise . <eos> the method turns out to be closely related to several other sparse gp approaches , and we discuss the relation in detail . <eos> we finally demonstrate its performance on some large data sets , and make a direct comparison to other sparse gp methods . <eos> we show that our method can match full gp performance with small m , i.e . very sparse solutions , and it significantly outperforms other approaches in this regime .
integrate-and-fire-type models are usually criticized because of their simplicity . <eos> on the other hand , the integrate-and-fire model is the basis of most of the theoretical studies on spiking neuron models . <eos> here , we develop a sequential procedure to quantitatively evaluate an equivalent integrate-and-fire-type model based on intracellular recordings of cortical pyramidal neurons . <eos> we find that the resulting effective model is sufficient to predict the spike train of the real pyramidal neuron with high accuracy . <eos> in in vivo-like regimes , predicted and recorded traces are almost indistinguishable and a significant part of the spikes can be predicted at the correct timing . <eos> slow processes like spike-frequency adaptation are shown to be a key feature in this context since they are necessary for the model to connect between different driving regimes .
we describe a neuromorphic chip that uses binary synapses with spike timing-dependent plasticity ( stdp ) to learn stimulated patterns of activity and to compensate for variability in excitability . <eos> specifically , stdp preferentially potentiates ( turns on ) synapses that project from excitable neurons , which spike early , to lethargic neurons , which spike late . <eos> the additional excitatory synaptic current makes lethargic neurons spike earlier , thereby causing neurons that belong to the same pattern to spike in synchrony . <eos> once learned , an entire pattern can be recalled by stimulating a subset .
in this paper we propose a general framework to study the generalization properties of binary classifiers trained with data which may be dependent , but are deterministically generated upon a sample of independent examples . <eos> it provides generalization bounds for binary classification and some cases of ranking problems , and clarifies the relationship between these learning tasks .
we describe a hierarchy of motif-based kernels for multiple alignments of biological sequences , particularly suitable to process regulatory regions of genes . <eos> the kernels incorporate progressively more information , with the most complex kernel accounting for a multiple alignment of orthologous regions , the phylogenetic tree relating the species , and the prior knowledge that relevant sequence patterns occur in conserved motif blocks . <eos> these kernels can be used in the presence of a library of known transcription factor binding sites , or de novo by iterating over all k-mers of a given length . <eos> in the latter mode , a discriminative classifier built from such a kernel not only recognizes a given class of promoter regions , but as a side effect simultaneously identifies a collection of relevant , discriminative sequence motifs . <eos> we demonstrate the utility of the motif-based multiple alignment kernels by using a collection of aligned promoter regions from five yeast species to recognize classes of cell-cycle regulated genes . <eos> supplementary data is available at http : //noble.gs.washington.edu/proj/pkernel .
in this paper we propose a new basis selection criterion for building sparse gp regression models that provides promising gains in accuracy as well as efficiency over previous methods . <eos> our algorithm is much faster than that of smola and bartlett , while , in generalization it greatly outperforms the information gain approach proposed by seeger et al , especially on the quality of predictive distributions .
lasso regression tends to assign zero weights to most irrelevant or redundant features , and hence is a promising technique for feature selection . <eos> its limitation , however , is that it only offers solutions to linear models . <eos> kernel machines with feature scaling techniques have been studied for feature selection with non-linear models . <eos> however , such approaches require to solve hard non-convex optimization problems . <eos> this paper proposes a new approach named the feature vector machine ( fvm ) . <eos> it reformulates the standard lasso regression into a form isomorphic to svm , and this form can be easily extended for feature selection with non-linear models by introducing kernels defined on feature vectors . <eos> fvm generates sparse solutions in the nonlinear feature space and it is much more tractable compared to feature scaling kernel machines . <eos> our experiments with fvm on simulated data show encouraging results in identifying the small number of dominating features that are non-linearly correlated to the response , a task the standard lasso fails to complete .
the network topology of neurons in the brain exhibits an abundance of feedback connections , but the computational function of these feedback connections is largely unknown . <eos> we present a computational theory that characterizes the gain in computational power achieved through feedback in dynamical systems with fading memory . <eos> it implies that many such systems acquire through feedback universal computational capabilities for analog computing with a non-fading memory . <eos> in particular , we show that feedback enables such systems to process time-varying input streams in diverse ways according to rules that are implemented through internal states of the dynamical system . <eos> in contrast to previous attractor-based computational models for neural networks , these flexible internal states are high-dimensional attractors of the circuit dynamics , that still allow the circuit state to absorb new information from online input streams . <eos> in this way one arrives at novel models for working memory , integration of evidence , and reward expectation in cortical circuits . <eos> we show that they are applicable to circuits of conductance-based hodgkin-huxley ( hh ) neurons with high levels of noise that reflect experimental data on invivo conditions .
this paper addresses the issue of numerical computation in machine learning domains based on similarity metrics , such as kernel methods , spectral techniques and gaussian processes . <eos> it presents a general solution strategy based on krylov subspace iteration and fast n-body learning methods . <eos> the experiments show significant gains in computation and storage on datasets arising in image segmentation , object detection and dimensionality reduction . <eos> the paper also presents theoretical bounds on the stability of these methods .
we present a conditional temporal probabilistic framework for reconstructing 3d human motion in monocular video based on descriptors encoding image silhouette observations . <eos> for computational efficiency we restrict visual inference to low-dimensional kernel induced non-linear state spaces . <eos> our methodology ( kbme ) combines kernel pca-based non-linear dimensionality reduction ( kpca ) and conditional bayesian mixture of experts ( bme ) in order to learn complex multivalued predictors between observations and model hidden states . <eos> this is necessary for accurate , inverse , visual perception inferences , where several probable , distant 3d solutions exist due to noise or the uncertainty of monocular perspective projection . <eos> low-dimensional models are appropriate because many visual processes exhibit strong non-linear correlations in both the image observations and the target , hidden state variables . <eos> the learned predictors are temporally combined within a conditional graphical model in order to allow a principled propagation of uncertainty . <eos> we study several predictors and empirically show that the proposed algorithm positively compares with techniques based on regression , kernel dependency estimation ( kde ) or pca alone , and gives results competitive to those of high-dimensional mixture predictors at a fraction of their computational cost . <eos> we show that the method successfully reconstructs the complex 3d motion of humans in real monocular video sequences .
biological sensory systems are faced with the problem of encoding a high-fidelity sensory signal with a population of noisy , low-fidelity neurons . <eos> this problem can be expressed in information theoretic terms as coding and transmitting a multi-dimensional , analog signal over a set of noisy channels . <eos> previously , we have shown that robust , overcomplete codes can be learned by minimizing the reconstruction error with a constraint on the channel capacity . <eos> here , we present a theoretical analysis that characterizes the optimal linear coder and decoder for one- and twodimensional data . <eos> the analysis allows for an arbitrary number of coding units , thus including both under- and over-complete representations , and provides a number of important insights into optimal coding strategies . <eos> in particular , we show how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness . <eos> we also report numerical solutions for robust coding of highdimensional image data and show that these codes are substantially more robust compared against other image codes such as ica and wavelets .
humans make optimal perceptual decisions in noisy and ambiguous conditions . <eos> computations underlying such optimal behavior have been shown to rely on probabilistic inference according to generative models whose structure is usually taken to be known a priori . <eos> we argue that bayesian model selection is ideal for inferring similar and even more complex model structures from experience . <eos> we find in experiments that humans learn subtle statistical properties of visual scenes in a completely unsupervised manner . <eos> we show that these findings are well captured by bayesian model learning within a class of models that seek to explain observed variables by independent hidden causes .
a general analysis of the limiting distribution of neural network functions is performed , with emphasis on non-gaussian limits . <eos> we show that with i.i.d . <eos> symmetric stable output weights , and more generally with weights distributed from the normal domain of attraction of a stable variable , that the neural functions converge in distribution to stable processes . <eos> conditions are also investigated under which gaussian limits do occur when the weights are independent but not identically distributed . <eos> some particularly tractable classes of stable distributions are examined , and the possibility of learning with such processes .
clustering is a fundamental problem in machine learning and has been approached in many ways . <eos> two general and quite different approaches include iteratively fitting a mixture model ( e.g. , using em ) and linking together pairs of training cases that have high affinity ( e.g. , using spectral methods ) . <eos> pair-wise clustering algorithms need not compute sufficient statistics and avoid poor solutions by directly placing similar examples in the same cluster . <eos> however , many applications require that each cluster of data be accurately described by a prototype or model , so affinity-based clustering and its benefits can not be directly realized . <eos> we describe a technique called ? affinity propagation ? , which combines the advantages of both approaches . <eos> the method learns a mixture model of the data by recursively propagating affinity messages . <eos> we demonstrate affinity propagation on the problems of clustering image patches for image segmentation and learning mixtures of gene expression models from microarray data . <eos> we find that affinity propagation obtains better solutions than mixtures of gaussians , the k-medoids algorithm , spectral clustering and hierarchical clustering , and is both able to find a pre-specified number of clusters and is able to automatically determine the number of clusters . <eos> interestingly , affinity propagation can be viewed as belief propagation in a graphical model that accounts for pairwise training case likelihood functions and the identification of cluster centers .
we investigate the problem of automatically constructing efficient representations or basis functions for approximating value functions based on analyzing the structure and topology of the state space . <eos> in particular , two novel approaches to value function approximation are explored based on automatically constructing basis functions on state spaces that can be represented as graphs or manifolds : one approach uses the eigenfunctions of the laplacian , in effect performing a global fourier analysis on the graph ; the second approach is based on diffusion wavelets , which generalize classical wavelets to graphs using multiscale dilations induced by powers of a diffusion operator or random walk on the graph . <eos> together , these approaches form the foundation of a new generation of methods for solving large markov decision processes , in which the underlying representation and policies are simultaneously learned .
a standard method to obtain stochastic models for symbolic time series is to train state-emitting hidden markov models ( se-hmms ) with the baum-welch algorithm . <eos> based on observable operator models ( ooms ) , in the last few months a number of novel learning algorithms for similar purposes have been developed : ( 1,2 ) two versions of an ? efficiency sharpening ? <eos> ( es ) algorithm , which iteratively improves the statistical efficiency of a sequence of oom estimators , ( 3 ) a constrained gradient descent ml estimator for transition-emitting hmms ( te-hmms ) . <eos> we give an overview on these algorithms and compare them with se-hmm/em learning on synthetic and real-life data .
there is a long-standing controversy on the site of the cerebellar motor learning . <eos> different theories and experimental results suggest that either the cerebellar flocculus or the brainstem learns the task and stores the memory . <eos> with a dynamical system approach , we clarify the mechanism of transferring the memory generated in the flocculus to the brainstem and that of so-called savings phenomena . <eos> the brainstem learning must comply with a sort of hebbian rule depending on purkinje-cell activities . <eos> in contrast to earlier numerical models , our model is simple but it accommodates explanations and predictions of experimental situations as qualitative features of trajectories in the phase space of synaptic weights , without fine parameter tuning .
the two-thirds power law , an empirical law stating an inverse non-linear relationship between the tangential hand speed and the curvature of its trajectory during curved motion , is widely acknowledged to be an invariant of upper-limb movement . <eos> it has also been shown to exist in eyemotion , locomotion and was even demonstrated in motion perception and prediction . <eos> this ubiquity has fostered various attempts to uncover the origins of this empirical relationship . <eos> in these it was generally attributed either to smoothness in hand- or joint-space or to the result of mechanisms that damp noise inherent in the motor system to produce the smooth trajectories evident in healthy human motion . <eos> we show here that white gaussian noise also obeys this power-law . <eos> analysis of signal and noise combinations shows that trajectories that were synthetically created not to comply with the power-law are transformed to power-law compliant ones after combination with low levels of noise . <eos> furthermore , there exist colored noise types that drive non-power-law trajectories to power-law compliance and are not affected by smoothing . <eos> these results suggest caution when running experiments aimed at verifying the power-law or assuming its underlying existence without proper analysis of the noise . <eos> our results could also suggest that the power-law might be derived not from smoothness or smoothness-inducing mechanisms operating on the noise inherent in our motor system but rather from the correlated noise which is inherent in this motor system .
theories of visual attention commonly posit that early parallel processes extract conspicuous features such as color contrast and motion from the visual field . <eos> these features are then combined into a saliency map , and attention is directed to the most salient regions first . <eos> top-down attentional control is achieved by modulating the contribution of different feature types to the saliency map . <eos> a key source of data concerning attentional control comes from behavioral studies in which the effect of recent experience is examined as individuals repeatedly perform a perceptual discrimination task ( e.g. , ? what shape is the odd-colored object ? ? ) . <eos> the robust finding is that repetition of features of recent trials ( e.g. , target color ) facilitates performance . <eos> we view this facilitation as an adaptation to the statistical structure of the environment . <eos> we propose a probabilistic model of the environment that is updated after each trial . <eos> under the assumption that attentional control operates so as to make performance more efficient for more likely environmental states , we obtain parsimonious explanations for data from four different experiments . <eos> further , our model provides a rational explanation for why the influence of past experience on attentional control is short lived .
we consider approximate value iteration with a parameterized approximator in which the state space is partitioned and the optimal cost-to-go function over each partition is approximated by a constant . <eos> we establish performance loss bounds for policies derived from approximations associated with fixed points . <eos> these bounds identify benefits to having projection weights equal to the invariant distribution of the resulting policy . <eos> such projection weighting leads to the same fixed points as td ( 0 ) . <eos> our analysis also leads to the first performance loss bound for approximate value iteration with an average cost objective .
the problem of computing a resample estimate for the reconstruction error in pca is reformulated as an inference problem with the help of the replica method . <eos> using the expectation consistent ( ec ) approximation , the intractable inference problem can be solved efficiently using only two variational parameters . <eos> a perturbative correction to the result is computed and an alternative simplified derivation is also presented .
this paper explores two aspects of social network modeling . <eos> first , we generalize a successful static model of relationships into a dynamic model that accounts for friendships drifting over time . <eos> second , we show how to make it tractable to learn such models from data , even as the number of entities n gets large . <eos> the generalized model associates each entity with a point in p-dimensional euclidian latent space . <eos> the points can move as time progresses but large moves in latent space are improbable . <eos> observed links between entities are more likely if the entities are close in latent space . <eos> we show how to make such a model tractable ( subquadratic in the number of entities ) by the use of appropriate kernel functions for similarity in latent space ; the use of low dimensional kd-trees ; a new efficient dynamic adaptation of multidimensional scaling for a first pass of approximate projection of entities into latent space ; and an efficient conjugate gradient update rule for non-linear local optimization in which amortized time per entity during an update is o ( log n ) . <eos> we use both synthetic and real-world data on upto 11,000 entities which indicate linear scaling in computation time and improved performance over four alternative approaches . <eos> we also illustrate the system operating on twelve years of nips co-publication data . <eos> we present a detailed version of this work in [ 1 ] .
fusing multiple information sources can yield significant benefits to successfully accomplish learning tasks . <eos> many studies have focussed on fusing information in supervised learning contexts . <eos> we present an approach to utilize multiple information sources in the form of similarity data for unsupervised learning . <eos> based on similarity information , the clustering task is phrased as a non-negative matrix factorization problem of a mixture of similarity measurements . <eos> the tradeoff between the informativeness of data sources and the sparseness of their mixture is controlled by an entropy-based weighting mechanism . <eos> for the purpose of model selection , a stability-based approach is employed to ensure the selection of the most self-consistent hypothesis . <eos> the experiments demonstrate the performance of the method on toy as well as real world data sets .
we present a method for performing transductive inference on very large datasets . <eos> our algorithm is based on multiclass gaussian processes and is effective whenever the multiplication of the kernel matrix or its inverse with a vector can be computed sufficiently fast . <eos> this holds , for instance , for certain graph and string kernels . <eos> transduction is achieved by variational inference over the unlabeled data subject to a balancing constraint .
we define a probability distribution over equivalence classes of binary matrices with a finite number of rows and an unbounded number of columns . <eos> this distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially infinite array of features . <eos> we identify a simple generative process that results in the same distribution over equivalence classes , which we call the indian buffet process . <eos> we illustrate the use of this distribution as a prior in an infinite latent feature model , deriving a markov chain monte carlo algorithm for inference in this model and applying the algorithm to an image dataset .
predictive state representations ( psrs ) are a method of modeling dynamical systems using only observable data , such as actions and observations , to describe their model . <eos> psrs use predictions about the outcome of future tests to summarize the system state . <eos> the best existing techniques for discovery and learning of psrs use a monte carlo approach to explicitly estimate these outcome probabilities . <eos> in this paper , we present a new algorithm for discovery and learning of psrs that uses a gradient descent approach to compute the predictions for the current state . <eos> the algorithm takes advantage of the large amount of structure inherent in a valid prediction matrix to constrain its predictions . <eos> furthermore , the algorithm can be used online by an agent to constantly improve its prediction quality ; something that current state of the art discovery and learning algorithms are unable to do . <eos> we give empirical results to show that our constrained gradient algorithm is able to discover core tests using very small amounts of data , and with larger amounts of data can compute accurate predictions of the system dynamics .
there has been a surge of interest in learning non-linear manifold models to approximate high-dimensional data . <eos> both for computational complexity reasons and for generalization capability , sparsity is a desired feature in such models . <eos> this usually means dimensionality reduction , which naturally implies estimating the intrinsic dimension , but it can also mean selecting a subset of the data to use as landmarks , which is especially important because many existing algorithms have quadratic complexity in the number of observations . <eos> this paper presents an algorithm for selecting landmarks , based on lasso regression , which is well known to favor sparse approximations because it uses regularization with an l1 norm . <eos> as an added benefit , a continuous manifold parameterization , based on the landmarks , is also found . <eos> experimental results with synthetic and real data illustrate the algorithm .
based on a large scale spiking neuron model of the input layers 4c and of macaque , we identify neural mechanisms for the observed contrast dependent receptive field size of v1 cells . <eos> we observe a rich variety of mechanisms for the phenomenon and analyze them based on the relative gain of excitatory and inhibitory synaptic inputs . <eos> we observe an average growth in the spatial extent of excitation and inhibition for low contrast , as predicted from phenomenological models . <eos> however , contrary to phenomenological models , our simulation results suggest this is neither sufficient nor necessary to explain the phenomenon .
neurons can have rapidly changing spike train statistics dictated by the underlying network excitability or behavioural state of an animal . <eos> to estimate the time course of such state dynamics from single- or multiple neuron recordings , we have developed an algorithm that maximizes the likelihood of observed spike trains by optimizing the state lifetimes and the state-conditional interspike-interval ( isi ) distributions . <eos> our nonparametric algorithm is free of time-binning and spike-counting problems and has the computational complexity of a mixed-state markov model operating on a state sequence of length equal to the total number of recorded spikes . <eos> as an example , we fit a two-state model to paired recordings of premotor neurons in the sleeping songbird . <eos> we find that the two state-conditional isi functions are highly similar to the ones measured during waking and singing , respectively .
long-distance language modeling is important not only in speech recognition and machine translation , but also in high-dimensional discrete sequence modeling in general . <eos> however , the problem of context length has almost been neglected so far and a na ve bag-of-words history has been employed in natural language processing . <eos> in contrast , in this paper we view topic shifts within a text as a latent stochastic process to give an explicit probabilistic generative model that has partial exchangeability . <eos> we propose an online inference algorithm using particle filters to recognize topic shifts to employ the most appropriate length of context automatically . <eos> experiments on the bnc corpus showed consistent improvement over previous methods involving no chronological order .
our understanding of the input-output function of single cells has been substantially advanced by biophysically accurate multi-compartmental models . <eos> the large number of parameters needing hand tuning in these models has , however , somewhat hampered their applicability and interpretability . <eos> here we propose a simple and well-founded method for automatic estimation of many of these key parameters : 1 ) the spatial distribution of channel densities on the cell ? s membrane ; 2 ) the spatiotemporal pattern of synaptic input ; 3 ) the channels ? <eos> reversal potentials ; 4 ) the intercompartmental conductances ; and 5 ) the noise level in each compartment . <eos> we assume experimental access to : a ) the spatiotemporal voltage signal in the dendrite ( or some contiguous subpart thereof , e.g . via voltage sensitive imaging techniques ) , b ) an approximate kinetic description of the channels and synapses present in each compartment , and c ) the morphology of the part of the neuron under investigation . <eos> the key observation is that , given data a ) -c ) , all of the parameters 1 ) -4 ) may be simultaneously inferred by a version of constrained linear regression ; this regression , in turn , is efficiently solved using standard algorithms , without any ? local minima ? <eos> problems despite the large number of parameters and complex dynamics . <eos> the noise level 5 ) may also be estimated by standard techniques . <eos> we demonstrate the method ? s accuracy on several model datasets , and describe techniques for quantifying the uncertainty in our estimates .
a 5-layer neuromorphic vision processor whose components communicate spike events asychronously using the address-eventrepresentation ( aer ) is demonstrated . <eos> the system includes a retina chip , two convolution chips , a 2d winner-take-all chip , a delay line chip , a learning classifier chip , and a set of pcbs for computer interfacing and address space remappings . <eos> the components use a mixture of analog and digital computation and will learn to classify trajectories of a moving object . <eos> a complete experimental setup and measurements results are shown .
while classical kernel-based learning algorithms are based on a single kernel , in practice it is often desirable to use multiple kernels . <eos> lankriet et al . ( 2004 ) considered conic combinations of kernel matrices for classification , leading to a convex quadratically constraint quadratic program . <eos> we show that it can be rewritten as a semi-infinite linear program that can be efficiently solved by recycling the standard svm implementations . <eos> moreover , we generalize the formulation and our method to a larger class of problems , including regression and one-class classification . <eos> experimental results show that the proposed algorithm helps for automatic model selection , improving the interpretability of the learning result and works for hundred thousands of examples or hundreds of kernels to be combined .
while kernel canonical correlation analysis ( kernel cca ) has been applied in many problems , the asymptotic convergence of the functions estimated from a finite sample to the true functions has not yet been established . <eos> this paper gives a rigorous proof of the statistical convergence of kernel cca and a related method ( nocco ) , which provides a theoretical justification for these methods . <eos> the result also gives a sufficient condition on the decay of the regularization coefficient in the methods to ensure convergence .
this paper presents representation and logic for labeling contrast edges and ridges in visual scenes in terms of both surface occlusion ( border ownership ) and thinline objects . <eos> in natural scenes , thinline objects include sticks and wires , while in human graphical communication thinlines include connectors , dividers , and other abstract devices . <eos> our analysis is directed at both natural and graphical domains . <eos> the basic problem is to formulate the logic of the interactions among local image events , specifically contrast edges , ridges , junctions , and alignment relations , such as to encode the natural constraints among these events in visual scenes . <eos> in a sparse heterogeneous markov random field framework , we define a set of interpretation nodes and energy/potential functions among them . <eos> the minimum energy configuration found by loopy belief propagation is shown to correspond to preferred human interpretation across a wide range of prototypical examples including important illusory contour figures such as the kanizsa triangle , as well as more difficult examples . <eos> in practical terms , the approach delivers correct interpretations of inherently ambiguous hand-drawn box-and-connector diagrams at low computational cost .
we present a novel approach to the characterization of complex sensory neurons . <eos> one of the main goals of characterizing sensory neurons is to characterize dimensions in stimulus space to which the neurons are highly sensitive ( causing large gradients in the neural responses ) or alternatively dimensions in stimulus space to which the neuronal response are invariant ( defining iso-response manifolds ) . <eos> we formulate this problem as that of learning a geometry on stimulus space that is compatible with the neural responses : the distance between stimuli should be large when the responses they evoke are very different , and small when the responses they evoke are similar . <eos> here we show how to successfully train such distance functions using rather limited amount of information . <eos> the data consisted of the responses of neurons in primary auditory cortex ( a1 ) of anesthetized cats to 32 stimuli derived from natural sounds . <eos> for each neuron , a subset of all pairs of stimuli was selected such that the responses of the two stimuli in a pair were either very similar or very dissimilar . <eos> the distance function was trained to fit these constraints . <eos> the resulting distance functions generalized to predict the distances between the responses of a test stimulus and the trained stimuli .
under natural viewing conditions , small movements of the eye and body prevent the maintenance of a steady direction of gaze . <eos> it is known that stimuli tend to fade when they are stabilized on the retina for several seconds . <eos> however , it is unclear whether the physiological self-motion of the retinal image serves a visual purpose during the brief periods of natural visual fixation . <eos> this study examines the impact of fixational instability on the statistics of visual input to the retina and on the structure of neural activity in the early visual system . <eos> fixational instability introduces fluctuations in the retinal input signals that , in the presence of natural images , lack spatial correlations . <eos> these input fluctuations strongly influence neural activity in a model of the lgn . <eos> they decorrelate cell responses , even if the contrast sensitivity functions of simulated cells are not perfectly tuned to counter-balance the power-law spectrum of natural images . <eos> a decorrelation of neural activity has been proposed to be beneficial for discarding statistical redundancies in the input signals . <eos> fixational instability might , therefore , contribute to establishing efficient representations of natural stimuli .
we introduce a new model of genetic diversity which summarizes a large input dataset into an epitome , a short sequence or a small set of short sequences of probability distributions capturing many overlapping subsequences from the dataset . <eos> the epitome as a representation has already been used in modeling real-valued signals , such as images and audio . <eos> the discrete sequence model we introduce in this paper targets applications in genetics , from multiple alignment to recombination and mutation inference . <eos> in our experiments , we concentrate on modeling the diversity of hiv where the epitome emerges as a natural model for producing relatively small vaccines covering a large number of immune system targets known as epitopes . <eos> our experiments show that the epitome includes more epitopes than other vaccine designs of similar length , including cocktails of consensus strains , phylogenetic tree centers , and observed strains . <eos> we also discuss epitome designs that take into account uncertainty about tcell cross reactivity and epitope presentation . <eos> in our experiments , we find that vaccine optimization is fairly robust to these uncertainties .
we argue that when objects are characterized by many attributes , clustering them on the basis of a relatively small random subset of these attributes can capture information on the unobserved attributes as well . <eos> moreover , we show that under mild technical conditions , clustering the objects on the basis of such a random subset performs almost as well as clustering with the full attribute set . <eos> we prove a finite sample generalization theorems for this novel learning scheme that extends analogous results from the supervised learning setting . <eos> the scheme is demonstrated for collaborative filtering of users with movies rating as attributes .
we considered a gamma distribution of interspike intervals as a statistical model for neuronal spike generation . <eos> the model parameters consist of a time-dependent firing rate and a shape parameter that characterizes spiking irregularities of individual neurons . <eos> because the environment changes with time , observed data are generated from the time-dependent firing rate , which is an unknown function . <eos> a statistical model with an unknown function is called a semiparametric model , which is one of the unsolved problem in statistics and is generally very difficult to solve . <eos> we used a novel method of estimating functions in information geometry to estimate the shape parameter without estimating the unknown function . <eos> we analytically obtained an optimal estimating function for the shape parameter independent of the functional form of the firing rate . <eos> this estimation is efficient without fisher information loss and better than maximum likelihood estimation .
calculations that quantify the dependencies between variables are vital to many operations with graphical models , e.g. , active learning and sensitivity analysis . <eos> previously , pairwise information gain calculation has involved a cost quadratic in network size . <eos> in this work , we show how to perform a similar computation with cost linear in network size . <eos> the loss function that allows this is of a form amenable to computation by dynamic programming . <eos> the message-passing algorithm that results is described and empirical results demonstrate large speedups without decrease in accuracy . <eos> in the cost-sensitive domains examined , superior accuracy is achieved .
previous work has demonstrated that the image variations of many objects ( human faces in particular ) under variable lighting can be effectively modeled by low dimensional linear spaces . <eos> the typical linear subspace learning algorithms include principal component analysis ( pca ) , linear discriminant analysis ( lda ) , and locality preserving projection ( lpp ) . <eos> all of these methods consider an n1 ? <eos> n2 image as a high dimensional vector in rn1 ? n2 , while an image represented in the plane is intrinsically a matrix . <eos> in this paper , we propose a new algorithm called tensor subspace analysis ( tsa ) . <eos> tsa considers an image as the second order tensor in rn1 ? <eos> rn2 , where rn1 and rn2 are two vector spaces . <eos> the relationship between the column vectors of the image matrix and that between the row vectors can be naturally characterized by tsa . <eos> tsa detects the intrinsic local geometrical structure of the tensor space by learning a lower dimensional tensor subspace . <eos> we compare our proposed approach with pca , lda and lpp methods on two standard databases . <eos> experimental results demonstrate that tsa achieves better recognition rate , while being much more efficient .
we discuss a method for obtaining a subject ? s a priori beliefs from his/her behavior in a psychophysics context , under the assumption that the behavior is ( nearly ) optimal from a bayesian perspective . <eos> the method is nonparametric in the sense that we do not assume that the prior belongs to any fixed class of distributions ( e.g. , gaussian ) . <eos> despite this increased generality , the method is relatively simple to implement , being based in the simplest case on a linear programming algorithm , and more generally on a straightforward maximum likelihood or maximum a posteriori formulation , which turns out to be a convex optimization problem ( with no non-global local maxima ) in many important cases . <eos> in addition , we develop methods for analyzing the uncertainty of these estimates . <eos> we demonstrate the accuracy of the method in a simple simulated coin-flipping setting ; in particular , the method is able to precisely track the evolution of the subject ? s posterior distribution as more and more data are observed . <eos> we close by briefly discussing an interesting connection to recent models of neural population coding .
linear implementations of the efficient coding hypothesis , such as independent component analysis ( ica ) and sparse coding models , have provided functional explanations for properties of simple cells in v1 [ 1 , 2 ] . <eos> these models , however , ignore the non-linear behavior of neurons and fail to match individual and population properties of neural receptive fields in subtle but important ways . <eos> hierarchical models , including gaussian scale mixtures [ 3 , 4 ] and other generative statistical models [ 5 , 6 ] , can capture higher-order regularities in natural images and explain nonlinear aspects of neural processing such as normalization and context effects [ 6,7 ] . <eos> previously , it had been assumed that the lower level representation is independent of the hierarchy , and had been fixed when training these models . <eos> here we examine the optimal lower-level representations derived in the context of a hierarchical model and find that the resulting representations are strikingly different from those based on linear models . <eos> unlike the the basis functions and filters learned by ica or sparse coding , these functions individually more closely resemble simple cell receptive fields and collectively span a broad range of spatial scales . <eos> our work unifies several related approaches and observations about natural image structure and suggests that hierarchical models might yield better representations of image structure throughout the hierarchy .
we present a novel spectral clustering method that enables users to incorporate prior knowledge of the size of clusters into the clustering process . <eos> the cost function , which is named size regularized cut ( srcut ) , is defined as the sum of the inter-cluster similarity and a regularization term measuring the relative size of two clusters . <eos> finding a partition of the data set to minimize srcut is proved to be np-complete . <eos> an approximation algorithm is proposed to solve a relaxed version of the optimization problem as an eigenvalue problem . <eos> evaluations over different data sets demonstrate that the method is not sensitive to outliers and performs better than normalized cut .
we present an improvement to the dp-slam algorithm for simultaneous localization and mapping ( slam ) that maintains multiple hypotheses about densely populated maps ( one full map per particle in a particle filter ) in time that is linear in all significant algorithm parameters and takes constant ( amortized ) time per iteration . <eos> this means that the asymptotic complexity of the algorithm is no greater than that of a pure localization algorithm using a single map and the same number of particles . <eos> we also present a hierarchical extension of dp-slam that uses a two level particle filter which models drift in the particle filtering process itself . <eos> the hierarchical approach enables recovery from the inevitable drift that results from using a finite number of particles in a particle filter and permits the use of dp-slam in more challenging domains , while maintaining linear time asymptotic complexity .
in this paper , we provide a general theorem that establishes a correspondence between surrogate loss functions in classification and the family of f -divergences . <eos> moreover , we provide constructive procedures for determining the f -divergence induced by a given surrogate loss , and conversely for finding all surrogate loss functions that realize a given f -divergence . <eos> next we introduce the notion of universal equivalence among loss functions and corresponding f -divergences , and provide necessary and sufficient conditions for universal equivalence to hold . <eos> these ideas have applications to classification problems that also involve a component of experiment design ; in particular , we leverage our results to prove consistency of a procedure for learning a classifier under decentralization requirements .
topic models , such as latent dirichlet allocation ( lda ) , can be useful tools for the statistical analysis of document collections and other discrete data . <eos> the lda model assumes that the words of each document arise from a mixture of topics , each of which is a distribution over the vocabulary . <eos> a limitation of lda is the inability to model topic correlation even though , for example , a document about genetics is more likely to also be about disease than x-ray astronomy . <eos> this limitation stems from the use of the dirichlet distribution to model the variability among the topic proportions . <eos> in this paper we develop the correlated topic model ( ctm ) , where the topic proportions exhibit correlation via the logistic normal distribution [ 1 ] . <eos> we derive a mean-field variational inference algorithm for approximate posterior inference in this model , which is complicated by the fact that the logistic normal is not conjugate to the multinomial . <eos> the ctm gives a better fit than lda on a collection of ocred articles from the journal science . <eos> furthermore , the ctm provides a natural way of visualizing and exploring this and other unstructured data sets .
we extend radial basis function ( rbf ) networks to the scenario in which multiple correlated tasks are learned simultaneously , and present the corresponding learning algorithms . <eos> we develop the algorithms for learning the network structure , in either a supervised or unsupervised manner . <eos> training data may also be actively selected to improve the network ? s generalization to test data . <eos> experimental results based on real data demonstrate the advantage of the proposed algorithms and support our conclusions .
we develop an approach for estimation with gaussian markov processes that imposes a smoothness prior while allowing for discontinuities . <eos> instead of propagating information laterally between neighboring nodes in a graph , we study the posterior distribution of the hidden nodes as a whole ? how it is perturbed by invoking discontinuities , or weakening the edges , in the graph . <eos> we show that the resulting computation amounts to feed-forward fan-in operations reminiscent of v1 neurons . <eos> moreover , using suitable matrix preconditioners , the incurred matrix inverse and determinant can be approximated , without iteration , in the same computational style . <eos> simulation results illustrate the merits of this approach .
in supervised learning scenarios , feature selection has been studied widely in the literature . <eos> selecting features in unsupervised learning scenarios is a much harder problem , due to the absence of class labels that would guide the search for relevant information . <eos> and , almost all of previous unsupervised feature selection methods are wrappertechniques that require a learning algorithm to evaluate the candidate feature subsets . <eos> in this paper , we propose a filtermethod for feature selection which is independent of any learning algorithm . <eos> our method can be performed in either supervised or unsupervised fashion . <eos> the proposed method is based on the observation that , in many real world classification problems , data from the same class are often close to each other . <eos> the importance of a feature is evaluated by its power of locality preserving , or , laplacian score . <eos> we compare our method with data variance ( unsupervised ) and fisher score ( supervised ) on two data sets . <eos> experimental results demonstrate the effectiveness and efficiency of our algorithm .
probabilistic temporal planning attempts to find good policies for acting in domains with concurrent durative tasks , multiple uncertain outcomes , and limited resources . <eos> these domains are typically modelled as markov decision problems and solved using dynamic programming methods . <eos> this paper demonstrates the application of reinforcement learning in the form of a policy-gradient method to these domains . <eos> our emphasis is large domains that are infeasible for dynamic programming . <eos> our approach is to construct simple policies , or agents , for each planning task . <eos> the result is a general probabilistic temporal planner , named the factored policy-gradient planner ( fpg-planner ) , which can handle hundreds of tasks , optimising for probability of success , duration , and resource use .
learning patterns of human behavior from sensor data is extremely important for high-level activity inference . <eos> we show how to extract and label a person ? s activities and signi ? cant places from traces of gps data . <eos> in contrast to existing techniques , our approach simultaneously detects and classi ? es the signi ? cant locations of a person and takes the highlevel context into account . <eos> our system uses relational markov networks to represent the hierarchical activity model that encodes the complex relations among gps readings , activities and signi ? cant places . <eos> we apply fft-based message passing to perform ef ? cient summation over large numbers of nodes in the networks . <eos> we present experiments that show signi ? cant improvements over existing techniques .
we extend a previously developed bayesian framework for perception to account for sensory adaptation . <eos> we first note that the perceptual effects of adaptation seems inconsistent with an adjustment of the internally represented prior distribution . <eos> instead , we postulate that adaptation increases the signal-to-noise ratio of the measurements by adapting the operational range of the measurement stage to the input range . <eos> we show that this changes the likelihood function in such a way that the bayesian estimator model can account for reported perceptual behavior . <eos> in particular , we compare the model ? s predictions to human motion discrimination data and demonstrate that the model accounts for the commonly observed perceptual adaptation effects of repulsion and enhanced discriminability .
we propose consensus propagation , an asynchronous distributed protocol for averaging numbers across a network . <eos> we establish convergence , characterize the convergence rate for regular graphs , and demonstrate that the protocol exhibits better scaling properties than pairwise averaging , an alternative that has received much recent attention . <eos> consensus propagation can be viewed as a special case of belief propagation , and our results contribute to the belief propagation literature . <eos> in particular , beyond singly-connected graphs , there are very few classes of relevant problems for which belief propagation is known to converge .
to escape from the curse of dimensionality , we claim that one can learn non-local functions , in the sense that the value and shape of the learned function at x must be inferred using examples that may be far from x . <eos> with this objective , we present a non-local non-parametric density estimator . <eos> it builds upon previously proposed gaussian mixture models with regularized covariance matrices to take into account the local shape of the manifold . <eos> it also builds upon recent work on non-local estimators of the tangent plane of a manifold , which are able to generalize in places with little training data , unlike traditional , local , non-parametric models .
in this paper we consider the problem of finding sets of points that conform to a given underlying model from within a dense , noisy set of observations . <eos> this problem is motivated by the task of efficiently linking faint asteroid detections , but is applicable to a range of spatial queries . <eos> we survey current tree-based approaches , showing a trade-off exists between single tree and multiple tree algorithms . <eos> to this end , we present a new type of multiple tree algorithm that uses a variable number of trees to exploit the advantages of both approaches . <eos> we empirically show that this algorithm performs well using both simulated and astronomical data .
training a learning algorithm is a costly task . <eos> a major goal of active learning is to reduce this cost . <eos> in this paper we introduce a new algorithm , kqbc , which is capable of actively learning large scale problems by using selective sampling . <eos> the algorithm overcomes the costly sampling step of the well known query by committee ( qbc ) algorithm by projecting onto a low dimensional space . <eos> kqbc also enables the use of kernels , providing a simple way of extending qbc to the non-linear scenario . <eos> sampling the low dimension space is done using the hit and run random walk . <eos> we demonstrate the success of this novel algorithm by applying it to both artificial and a real world problems .
diffusion tensor magnetic resonance imaging ( dt-mri ) is a non invasive method for brain neuronal fibers delineation . <eos> here we show a modification for dt-mri that allows delineation of neuronal fibers which are infiltrated by edema . <eos> we use the muliple tensor variational ( mtv ) framework which replaces the diffusion model of dt-mri with a multiple component model and fits it to the signal attenuation with a variational regularization mechanism . <eos> in order to reduce free water contamination we estimate the free water compartment volume fraction in each voxel , remove it , and then calculate the anisotropy of the remaining compartment . <eos> the variational framework was applied on data collected with conventional clinical parameters , containing only six diffusion directions . <eos> by using the variational framework we were able to overcome the highly ill posed fitting . <eos> the results show that we were able to find fibers that were not found by dt-mri .
we present a model that learns the influence of interacting markov chains within a team . <eos> the proposed model is a dynamic bayesian network ( dbn ) with a two-level structure : individual-level and group-level . <eos> individual level models actions of each player , and the group-level models actions of the team as a whole . <eos> experiments on synthetic multi-player games and a multi-party meeting corpus show the effectiveness of the proposed model .
we introduce a method to automatically improve character models for a handwritten script without the use of transcriptions and using a minimum of document specific training data . <eos> we show that we can use searches for the words in a dictionary to identify portions of the document whose transcriptions are unambiguous . <eos> using templates extracted from those regions , we retrain our character prediction model to drastically improve our search retrieval performance for words in the document .
we initiate the study of learning from multiple sources of limited data , each of which may be corrupted at a different rate . <eos> we develop a complete theory of which data sources should be used for two fundamental problems : estimating the bias of a coin , and learning a classifier in the presence of label noise . <eos> in both cases , efficient algorithms are provided for computing the optimal subset of data .
we consider the task of depth estimation from a single monocular image . <eos> we take a supervised learning approach to this problem , in which we begin by collecting a training set of monocular images ( of unstructured outdoor environments which include forests , trees , buildings , etc . ) <eos> and their corresponding ground-truth depthmaps . <eos> then , we apply supervised learning to predict the depthmap as a function of the image . <eos> depth estimation is a challenging problem , since local features alone are insufficient to estimate depth at a point , and one needs to consider the global context of the image . <eos> our model uses a discriminatively-trained markov random field ( mrf ) that incorporates multiscale local- and global-image features , and models both depths at individual points as well as the relation between depths at different points . <eos> we show that , even on unstructured scenes , our algorithm is frequently able to recover fairly accurate depthmaps .
given a set of points and a set of prototypes representing them , how to create a graph of the prototypes whose topology accounts for that of the points ? <eos> this problem had not yet been explored in the framework of statistical learning theory . <eos> in this work , we propose a generative model based on the delaunay graph of the prototypes and the expectationmaximization algorithm to learn the parameters . <eos> this work is a first step towards the construction of a topological model of a set of points grounded on statistics .
while classical experiments on spike-timing dependent plasticity analyzed synaptic changes as a function of the timing of pairs of pre- and postsynaptic spikes , more recent experiments also point to the effect of spike triplets . <eos> here we develop a mathematical framework that allows us to characterize timing based learning rules . <eos> moreover , we identify a candidate learning rule with five variables ( and 5 free parameters ) that captures a variety of experimental data , including the dependence of potentiation and depression upon pre- and postsynaptic firing frequencies . <eos> the relation to the bienenstock-cooper-munro rule as well as to some timing-based rules is discussed .
we address the problem of robust , computationally-efficient design of biological experiments . <eos> classical optimal experiment design methods have not been widely adopted in biological practice , in part because the resulting designs can be very brittle if the nominal parameter estimates for the model are poor , and in part because of computational constraints . <eos> we present a method for robust experiment design based on a semidefinite programming relaxation . <eos> we present an application of this method to the design of experiments for a complex calcium signal transduction pathway , where we have found that the parameter estimates obtained from the robust design are better than those obtained from an ? optimal design .
consider the problem of joint parameter estimation and prediction in a markov random field : i.e. , the model parameters are estimated on the basis of an initial set of data , and then the fitted model is used to perform prediction ( e.g. , smoothing , denoising , interpolation ) on a new noisy observation . <eos> working in the computation-limited setting , we analyze a joint method in which the same convex variational relaxation is used to construct an m-estimator for fitting parameters , and to perform approximate marginalization for the prediction step . <eos> the key result of this paper is that in the computation-limited setting , using an inconsistent parameter estimator ( i.e. , an estimator that returns the wrongmodel even in the infinite data limit ) is provably beneficial , since the resulting errors can partially compensate for errors made by using an approximate prediction technique . <eos> en route to this result , we analyze the asymptotic properties of m-estimators based on convex variational relaxations , and establish a lipschitz stability property that holds for a broad class of variational methods . <eos> we show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product . <eos> 1 keywords : markov random fields ; variational method ; message-passing algorithms ; sum-product ; belief propagation ; parameter estimation ; learning .
a good image object detection algorithm is accurate , fast , and does not require exact locations of objects in a training set . <eos> we can create such an object detector by taking the architecture of the viola-jones detector cascade and training it with a new variant of boosting that we call milboost . <eos> milboost uses cost functions from the multiple instance learning literature combined with the anyboost framework . <eos> we adapt the feature selection criterion of milboost to optimize the performance of the viola-jones cascade . <eos> experiments show that the detection rate is up to 1.6 times better using milboost . <eos> this increased detection rate shows the advantage of simultaneously learning the locations and scales of the objects in the training set along with the parameters of the classifier .
this paper explores the statistical relationship between natural images and their underlying range ( depth ) images . <eos> we look at how this relationship changes over scale , and how this information can be used to enhance low resolution range data using a full resolution intensity image . <eos> based on our findings , we propose an extension to an existing technique known as shape recipes [ 3 ] , and the success of the two methods are compared using images and laser scans of real scenes . <eos> our extension is shown to provide a two-fold improvement over the current method . <eos> furthermore , we demonstrate that ideal linear shape-from-shading filters , when learned from natural scenes , may derive even more strength from shadow cues than from the traditional linear-lambertian shading cues .
in previous work we presented an efficient approach to computing kernel summations which arise in many machine learning methods such as kernel density estimation . <eos> this approach , dual-tree recursion with finitedifference approximation , generalized existing methods for similar problems arising in computational physics in two ways appropriate for statistical problems : toward distribution sensitivity and general dimension , partly by avoiding series expansions . <eos> while this proved to be the fastest practical method for multivariate kernel density estimation at the optimal bandwidth , it is much less efficient at larger-than-optimal bandwidths . <eos> in this work , we explore the extent to which the dual-tree approach can be integrated with multipole-like hermite expansions in order to achieve reasonable efficiency across all bandwidth scales , though only for low dimensionalities . <eos> in the process , we derive and demonstrate the first truly hierarchical fast gauss transforms , effectively combining the best tools from discrete algorithms and continuous approximation theory .
we study the problem of maximum entropy density estimation in the presence of known sample selection bias . <eos> we propose three bias correction approaches . <eos> the first one takes advantage of unbiased sufficient statistics which can be obtained from biased samples . <eos> the second one estimates the biased distribution and then factors the bias out . <eos> the third one approximates the second by only using samples from the sampling distribution . <eos> we provide guarantees for the first two approaches and evaluate the performance of all three approaches in synthetic experiments and on real data from species habitat modeling , where maxent has been successfully applied and where sample selection bias is a significant problem .
an analog focal-plane processor having a 128 128 photodiode array has been developed for directional edge filtering . <eos> it can perform 4 4-pixel kernel convolution for entire pixels only with 256 steps of simple analog processing . <eos> newly developed cyclic line access and row-parallel processing scheme in conjunction with the ? only-nearest-neighbor interconnects architecture has enabled a very simple implementation . <eos> a proof-of-concept chip was fabricated in a 0.35- m 2-poly 3-metal cmos technology and the edge filtering at a rate of 200 frames/sec . <eos> has been experimentally demonstrated .
we present a new connectionist model for constructive , intuitionistic modal reasoning . <eos> we use ensembles of neural networks to represent intuitionistic modal theories , and show that for each intuitionistic modal program there exists a corresponding neural network ensemble that computes the program . <eos> this provides a massively parallel model for intuitionistic modal reasoning , and sets the scene for integrated reasoning , knowledge representation , and learning of intuitionistic theories in neural networks , since the networks in the ensemble can be trained by examples using standard neural learning algorithms .
there have been many graph-based approaches for semi-supervised classification . <eos> one problem is that of hyperparameter learning : performance depends greatly on the hyperparameters of the similarity graph , transformation of the graph laplacian and the noise model . <eos> we present a bayesian framework for learning hyperparameters for graph-based semisupervised classification . <eos> given some labeled data , which can contain inaccurate labels , we pose the semi-supervised classification as an inference problem over the unknown labels . <eos> expectation propagation is used for approximate inference and the mean of the posterior is used for classification . <eos> the hyperparameters are learned using em for evidence maximization . <eos> we also show that the posterior mean can be written in terms of the kernel matrix , providing a bayesian classifier to classify new points . <eos> tests on synthetic and real datasets show cases where there are significant improvements in performance over the existing approaches .
we consider criteria for variational representations of non-gaussian latent variables , and derive variational em algorithms in general form . <eos> we establish a general equivalence among convex bounding methods , evidence based methods , and ensemble learning/variational bayes methods , which has previously been demonstrated only for particular cases .
we propose a simple information-theoretic approach to soft clustering based on maximizing the mutual information i ( x , y ) between the unknown cluster labels y and the training patterns x with respect to parameters of specifically constrained encoding distributions . <eos> the constraints are chosen such that patterns are likely to be clustered similarly if they lie close to specific unknown vectors in the feature space . <eos> the method may be conveniently applied to learning the optimal affinity matrix , which corresponds to learning parameters of the kernelized encoder . <eos> the procedure does not require computations of eigenvalues of the gram matrices , which makes it potentially attractive for clustering large data sets .
the variational bayesian framework has been widely used to approximate the bayesian learning . <eos> in various applications , it has provided computational tractability and good generalization performance . <eos> in this paper , we discuss the variational bayesian learning of the mixture of exponential families and provide some additional theoretical support by deriving the asymptotic form of the stochastic complexity . <eos> the stochastic complexity , which corresponds to the minimum free energy and a lower bound of the marginal likelihood , is a key quantity for model selection . <eos> it also enables us to discuss the e ? ect of hyperparameters and the accuracy of the variational bayesian approach as an approximation of the true bayesian learning .
recent neurophysiological evidence suggests the ability to interpret biological motion is facilitated by a neuronal `` mirror system '' which maps visual inputs to the pre-motor cortex . <eos> if the common architecture and circuitry of the cortices is taken to imply a common computation across multiple perceptual and cognitive modalities , this visual-motor interaction might be expected to have a unified computational basis . <eos> two essential tasks underlying such visual-motor cooperation are shown here to be simply expressed and directly solved as transformation-discovery inverse problems : ( a ) discriminating and determining the pose of a primed 3d object in a real-world scene , and ( b ) interpreting the 3d configuration of an articulated kinematic object in an image . <eos> the recently developed map-seeking method provides a mathematically tractable , cortically-plausible solution to these and a variety of other inverse problems which can be posed as the discovery of a composition of transformations between two patterns . <eos> the method relies on an ordering property of superpositions and on decomposition of the transformation spaces inherent in the generating processes of the problem .
we describe a generative model for handwritten digits that uses two pairs of opposing springs whose stiffnesses are controlled by a motor program . <eos> we show how neural networks can be trained to infer the motor programs required to accurately reconstruct the mnist digits . <eos> the inferred motor programs can be used directly for digit classification , but they can also be used in other ways . <eos> by adding noise to the motor program inferred from an mnist image we can generate a large set of very different images of the same class , thus enlarging the training set available to other methods . <eos> we can also use the motor programs as additional , highly informative outputs which reduce overfitting when training a feed-forward classifier .
a foundational problem in semi-supervised learning is the construction of a graph underlying the data . <eos> we propose to use a method which optimally combines a number of differently constructed graphs . <eos> for each of these graphs we associate a basic graph kernel . <eos> we then compute an optimal combined kernel . <eos> this kernel solves an extended regularization problem which requires a joint minimization over both the data and the set of graph kernels . <eos> we present encouraging results on different ocr tasks where the optimal combined kernel is computed from graphs constructed with a variety of distances functions and the ? k in nearest neighbors .
multiple visual cues are used by the visual system to analyze a scene ; achromatic cues include luminance , texture , contrast and motion . <eos> singlecell recordings have shown that the mammalian visual cortex contains neurons that respond similarly to scene structure ( e.g. , orientation of a boundary ) , regardless of the cue type conveying this information . <eos> this paper shows that cue-invariant response properties of simple- and complex-type cells can be learned from natural image data in an unsupervised manner . <eos> in order to do this , we also extend a previous conceptual model of cue invariance so that it can be applied to model simple- and complex-cell responses . <eos> our results relate cue-invariant response properties to natural image statistics , thereby showing how the statistical modeling approach can be used to model processing beyond the elemental response properties visual neurons . <eos> this work also demonstrates how to learn , from natural image data , more sophisticated feature detectors than those based on changes in mean luminance , thereby paving the way for new data-driven approaches to image processing and computer vision .
we present an efficient algorithm to actively select queries for learning the boundaries separating a function domain into regions where the function is above and below a given threshold . <eos> we develop experiment selection methods based on entropy , misclassification rate , variance , and their combinations , and show how they perform on a number of data sets . <eos> we then show how these algorithms are used to determine simultaneously valid 1 confidence intervals for seven cosmological parameters . <eos> experimentation shows that the algorithm reduces the computation necessary for the parameter estimation problem by an order of magnitude .
standard statistical models of language fail to capture one of the most striking properties of natural languages : the power-law distribution in the frequencies of word tokens . <eos> we present a framework for developing statistical models that generically produce power-laws , augmenting standard generative models with an adaptor that produces the appropriate pattern of token frequencies . <eos> we show that taking a particular stochastic process the pitman-yor process as an adaptor justifies the appearance of type frequencies in formal analyses of natural language , and improves the performance of a model for unsupervised learning of morphology .
this paper presents a diffusion based probabilistic interpretation of spectral clustering and dimensionality reduction algorithms that use the eigenvectors of the normalized graph laplacian . <eos> given the pairwise adjacency matrix of all points , we define a diffusion distance between any two data points and show that the low dimensional representation of the data by the first few eigenvectors of the corresponding markov matrix is optimal under a certain mean squared error criterion . <eos> furthermore , assuming that data points are random samples from a density p ( x ) = e ? u ( x ) we identify these eigenvectors as discrete approximations of eigenfunctions of a fokker-planck operator in a potential 2u ( x ) with reflecting boundary conditions . <eos> finally , applying known results regarding the eigenvalues and eigenfunctions of the continuous fokker-planck operator , we provide a mathematical justification for the success of spectral clustering and dimensional reduction algorithms based on these first few eigenvectors . <eos> this analysis elucidates , in terms of the characteristics of diffusion processes , many empirical findings regarding spectral clustering algorithms . <eos> keywords : algorithms and architectures , learning theory .
active learning is the problem in supervised learning to design the locations of training input points so that the generalization error is minimized . <eos> existing active learning methods often assume that the model used for learning is correctly specified , i.e. , the learning target function can be expressed by the model at hand . <eos> in many practical situations , however , this assumption may not be fulfilled . <eos> in this paper , we first show that the existing active learning method can be theoretically justified under slightly weaker condition : the model does not have to be correctly specified , but slightly misspecified models are also allowed . <eos> however , it turns out that the weakened condition is still restrictive in practice . <eos> to cope with this problem , we propose an alternative active learning method which can be theoretically justified for a wider class of misspecified models . <eos> thus , the proposed method has a broader range of applications than the existing method . <eos> numerical studies show that the proposed active learning method is robust against the misspecification of models and is thus reliable .
we introduce a technique for dimensionality estimation based on the notion of quantization dimension , which connects the asymptotic optimal quantization error for a probability distribution on a manifold to its intrinsic dimension . <eos> the definition of quantization dimension yields a family of estimation algorithms , whose limiting case is equivalent to a recent method based on packing numbers . <eos> using the formalism of high-rate vector quantization , we address issues of statistical consistency and analyze the behavior of our scheme in the presence of noise .
we measure the ability of human observers to predict the next datum in a sequence that is generated by a simple statistical process undergoing change at random points in time . <eos> accurate performance in this task requires the identification of changepoints . <eos> we assess individual differences between observers both empirically , and using two kinds of models : a bayesian approach for change detection and a family of cognitively plausible fast and frugal models . <eos> some individuals detect too many changes and hence perform sub-optimally due to excess variability . <eos> other individuals do not detect enough changes , and perform sub-optimally because they fail to notice short-term temporal trends .
we present an algorithm for learning a quadratic gaussian metric ( mahalanobis distance ) for use in classification tasks . <eos> our method relies on the simple geometric intuition that a good metric is one under which points in the same class are simultaneously near each other and far from points in the other classes . <eos> we construct a convex optimization problem whose solution generates such a metric by trying to collapse all examples in the same class to a single point and push examples in other classes infinitely far away . <eos> we show that when the metric we learn is used in simple classifiers , it yields substantial improvements over standard alternatives on a variety of problems . <eos> we also discuss how the learned metric may be used to obtain a compact low dimensional feature representation of the original input space , allowing more efficient classification with very little reduction in performance .
we propose a simple clustering framework on graphs encoding pairwise data similarities . <eos> unlike usual similarity-based methods , the approach softly assigns data to clusters in a probabilistic way . <eos> more importantly , a hierarchical clustering is naturally derived in this framework to gradually merge lower-level clusters into higher-level ones . <eos> a random walk analysis indicates that the algorithm exposes clustering structures in various resolutions , i.e. , a higher level statistically models a longer-term diffusion on graphs and thus discovers a more global clustering structure . <eos> finally we provide very encouraging experimental results .
we present a computational model of human eye movements in an object class detection task . <eos> the model combines state-of-the-art computer vision object class detection methods ( sift features trained using adaboost ) with a biologically plausible model of human eye movement to produce a sequence of simulated fixations , culminating with the acquisition of a target . <eos> we validated the model by comparing its behavior to the behavior of human observers performing the identical object class detection task ( looking for a teddy bear among visually complex nontarget objects ) . <eos> we found considerable agreement between the model and human data in multiple eye movement measures , including number of fixations , cumulative probability of fixating the target , and scanpath distance .
we propose a model by which dopamine ( da ) and norepinepherine ( ne ) combine to alternate behavior between relatively exploratory and exploitative modes . <eos> the model is developed for a target detection task for which there is extant single neuron recording data available from locus coeruleus ( lc ) ne neurons . <eos> an exploration-exploitation trade-off is elicited by regularly switching which of the two stimuli are rewarded . <eos> da functions within the model to change synaptic weights according to a reinforcement learning algorithm . <eos> exploration is mediated by the state of lc firing , with higher tonic and lower phasic activity producing greater response variability . <eos> the opposite state of lc function , with lower baseline firing rate and greater phasic responses , favors exploitative behavior . <eos> changes in lc firing mode result from combined measures of response conflict and reward rate , where response conflict is monitored using models of anterior cingulate cortex ( acc ) . <eos> increased long-term response conflict and decreased reward rate , which occurs following reward contingency switch , favors the higher tonic state of lc function and ne release . <eos> this increases exploration , and facilitates discovery of the new target .
we consider the scaling of the number of examples necessary to achieve good performance in distributed , cooperative , multi-agent reinforcement learning , as a function of the the number of agents n. we prove a worstcase lower bound showing that algorithms that rely solely on a global reward signal to learn policies confront a fundamental limit : they require a number of real-world examples that scales roughly linearly in the number of agents . <eos> for settings of interest with a very large number of agents , this is impractical . <eos> we demonstrate , however , that there is a class of algorithms that , by taking advantage of local reward signals in large distributed markov decision processes , are able to ensure good performance with a number of samples that scales as o ( log n ) . <eos> this makes them applicable even in settings with a very large number of agents n .
spectral clustering enjoys its success in both data clustering and semisupervised learning . <eos> but , most spectral clustering algorithms can not handle multi-class clustering problems directly . <eos> additional strategies are needed to extend spectral clustering algorithms to multi-class clustering problems . <eos> furthermore , most spectral clustering algorithms employ hard cluster membership , which is likely to be trapped by the local optimum . <eos> in this paper , we present a new spectral clustering algorithm , named ? soft cut ? . <eos> it improves the normalized cut algorithm by introducing soft membership , and can be efficiently computed using a bound optimization algorithm . <eos> our experiments with a variety of datasets have shown the promising performance of the proposed clustering algorithm .
we present a family of approximation techniques for probabilistic graphical models , based on the use of graphical preconditioners developed in the scientific computing literature . <eos> our framework yields rigorous upper and lower bounds on event probabilities and the log partition function of undirected graphical models , using non-iterative procedures that have low time complexity . <eos> as in mean field approaches , the approximations are built upon tractable subgraphs ; however , we recast the problem of optimizing the tractable distribution parameters and approximate inference in terms of the well-studied linear systems problem of obtaining a good matrix preconditioner . <eos> experiments are presented that compare the new approximation schemes to variational methods .
the problem of resource allocation in sparse graphs with real variables is studied using methods of statistical physics . <eos> an efficient distributed algorithm is devised on the basis of insight gained from the analysis and is examined using numerical simulations , showing excellent performance and full agreement with the theoretical results .
hybrid ? cmol integrated circuits , combining cmos subsystem with nanowire crossbars and simple two-terminal nanodevices , promise to extend the exponential moore-law development of microelectronics into the sub-10-nm range . <eos> we are developing neuromorphic network ( ? crossnet ? ) <eos> architectures for this future technology , in which neural cell bodies are implemented in cmos , nanowires are used as axons and dendrites , while nanodevices ( bistable latching switches ) are used as elementary synapses . <eos> we have shown how crossnets may be trained to perform pattern recovery and classification despite the limitations imposed by the cmol hardware . <eos> preliminary estimates have shown that cmol crossnets may be extremely dense ( ~10 7 cells per cm2 ) and operate approximately a million times faster than biological neural networks , at manageable power consumption . <eos> in conclusion , we discuss in brief possible short-term and long-term applications of the emerging technology .
humans are extremely adept at learning new skills by imitating the actions of others . <eos> a progression of imitative abilities has been observed in children , ranging from imitation of simple body movements to goalbased imitation based on inferring intent . <eos> in this paper , we show that the problem of goal-based imitation can be formulated as one of inferring goals and selecting actions using a learned probabilistic graphical model of the environment . <eos> we first describe algorithms for planning actions to achieve a goal state using probabilistic inference . <eos> we then describe how planning can be used to bootstrap the learning of goal-dependent policies by utilizing feedback from the environment . <eos> the resulting graphical model is then shown to be powerful enough to allow goal-based imitation . <eos> using a simple maze navigation task , we illustrate how an agent can infer the goals of an observed teacher and imitate the teacher even when the goals are uncertain and the demonstration is incomplete .
establishing correspondence between distinct objects is an important and nontrivial task : correctness of the correspondence hinges on properties which are difficult to capture in an a priori criterion . <eos> while previous work has used a priori criteria which in some cases led to very good results , the present paper explores whether it is possible to learn a combination of features that , for a given training set of aligned human heads , characterizes the notion of correct correspondence . <eos> by optimizing this criterion , we are then able to compute correspondence and morphs for novel heads .
stress and genetic background regulate different aspects of behavioral learning through the action of stress hormones and neuromodulators . <eos> in reinforcement learning ( rl ) models , meta-parameters such as learning rate , future reward discount factor , and exploitation-exploration factor , control learning dynamics and performance . <eos> they are hypothesized to be related to neuromodulatory levels in the brain . <eos> we found that many aspects of animal learning and performance can be described by simple rl models using dynamic control of the meta-parameters . <eos> to study the effects of stress and genotype , we carried out 5-hole-box light conditioning and morris water maze experiments with c57bl/6 and dba/2 mouse strains . <eos> the animals were exposed to different kinds of stress to evaluate its effects on immediate performance as well as on long-term memory . <eos> then , we used rl models to simulate their behavior . <eos> for each experimental session , we estimated a set of model meta-parameters that produced the best fit between the model and the animal performance . <eos> the dynamics of several estimated meta-parameters were qualitatively similar for the two simulated experiments , and with statistically significant differences between different genetic strains and stress conditions .
we propose new pac-bayes bounds for the risk of the weighted majority vote that depend on the mean and variance of the error of its associated gibbs classifier . <eos> we show that these bounds can be smaller than the risk of the gibbs classifier and can be arbitrarily close to zero even if the risk of the gibbs classifier is close to 1/2 . <eos> moreover , we show that these bounds can be uniformly estimated on the training data for all possible posteriors q . <eos> moreover , they can be improved by using a large sample of unlabelled data .
graph matching is a fundamental problem in computer vision and machine learning . <eos> we present two contributions . <eos> first , we give a new spectral relaxation technique for approximate solutions to matching problems , that naturally incorporates one-to-one or one-to-many constraints within the relaxation scheme . <eos> the second is a normalization procedure for existing graph matching scoring functions that can dramatically improve the matching accuracy . <eos> it is based on a reinterpretation of the graph matching compatibility matrix as a bipartite graph on edges for which we seek a bistochastic normalization . <eos> we evaluate our two contributions on a comprehensive test set of random graph matching problems , as well as on image correspondence problem . <eos> our normalization procedure can be used to improve the performance of many existing graph matching algorithms , including spectral matching , graduated assignment and semidefinite programming .
active learning refers to algorithmic frameworks aimed at selecting training data points in order to reduce the number of required training data points and/or improve the generalization performance of a learning method . <eos> in this paper , we present an asymptotic analysis of active learning for generalized linear models . <eos> our analysis holds under the common practical situation of model misspecification , and is based on realistic assumptions regarding the nature of the sampling distributions , which are usually neither independent nor identical . <eos> we derive unbiased estimators of generalization performance , as well as estimators of expected reduction in generalization error after adding a new training data point , that allow us to optimize its sampling distribution through a convex optimization problem . <eos> our analysis naturally leads to an algorithm for sequential active learning which is applicable for all tasks supported by generalized linear models ( e.g. , binary classification , multi-class classification , regression ) and can be applied in non-linear settings through the use of mercer kernels .
we introduce coherent point drift ( cpd ) , a novel probabilistic method for nonrigid registration of point sets . <eos> the registration is treated as a maximum likelihood ( ml ) estimation problem with motion coherence constraint over the velocity field such that one point set moves coherently to align with the second set . <eos> we formulate the motion coherence constraint and derive a solution of regularized ml estimation through the variational approach , which leads to an elegant kernel form . <eos> we also derive the em algorithm for the penalized ml optimization with deterministic annealing . <eos> the cpd method simultaneously finds both the non-rigid transformation and the correspondence between two point sets without making any prior assumption of the transformation model except that of motion coherence . <eos> this method can estimate complex non-linear non-rigid transformations , and is shown to be accurate on 2d and 3d examples and robust in the presence of outliers and missing points .
the risk , or probability of error , of the classifier produced by the adaboost algorithm is investigated . <eos> in particular , we consider the stopping strategy to be used in adaboost to achieve universal consistency . <eos> we show that provided adaboost is stopped after n iterations ? for sample size n and < 1 ? the sequence of risks of the classifiers it produces approaches the bayes risk if bayes risk l > 0 .
this paper describes a gaussian process framework for inferring pixel-wise disparity and bi-layer segmentation of a scene given a stereo pair of images . <eos> the gaussian process covariance is parameterized by a foreground-backgroundocclusion segmentation label to model both smooth regions and discontinuities . <eos> as such , we call our model a switched gaussian process . <eos> we propose a greedy incremental algorithm for adding observations from the data and assigning segmentation labels . <eos> two observation schedules are proposed : the first treats scanlines as independent , the second uses an active learning criterion to select a sparse subset of points to measure . <eos> we show that this probabilistic framework has comparable performance to the state-of-the-art .
we present a new statistical framework called hidden markov dirichlet process ( hmdp ) to jointly model the genetic recombinations among possibly infinite number of founders and the coalescence-with-mutation events in the resulting genealogies . <eos> the hmdp posits that a haplotype of genetic markers is generated by a sequence of recombination events that select an ancestor for each locus from an unbounded set of founders according to a 1st-order markov transition process . <eos> conjoining this process with a mutation model , our method accommodates both between-lineage recombination and within-lineage sequence variations , and leads to a compact and natural interpretation of the population structure and inheritance process underlying haplotype data . <eos> we have developed an efficient sampling algorithm for hmdp based on a two-level nested p ? olya urn scheme . <eos> on both simulated and real snp haplotype data , our method performs competitively or significantly better than extant methods in uncovering the recombination hotspots along chromosomal loci ; and in addition it also infers the ancestral genetic patterns and offers a highly accurate map of ancestral compositions of modern populations .
computational gene prediction using generative models has reached a plateau , with several groups converging to a generalized hidden markov model ( ghmm ) incorporating phylogenetic models of nucleotide sequence evolution . <eos> further improvements in gene calling accuracy are likely to come through new methods that incorporate additional data , both comparative and species specific . <eos> conditional random fields ( crfs ) , which directly model the conditional probability p ( y|x ) of a vector of hidden states conditioned on a set of observations , provide a unified framework for combining probabilistic and non-probabilistic information and have been shown to outperform hmms on sequence labeling tasks in natural language processing . <eos> we describe the use of crfs for comparative gene prediction . <eos> we implement a model that encapsulates both a phylogenetic-ghmm ( our baseline comparative model ) and additional non-probabilistic features . <eos> we tested our model on the genome sequence of the fungal human pathogen cryptococcus neoformans . <eos> our baseline comparative model displays accuracy comparable to the the best available gene prediction tool for this organism . <eos> moreover , we show that discriminative training and the incorporation of non-probabilistic evidence significantly improve performance . <eos> our software implementation , conrad , is freely available with an open source license at http : //www.broad.mit.edu/annotation/conrad/ .
a key challenge in designing analog-to-digital converters for cortically implanted prosthesis is to sense and process high-dimensional neural signals recorded by the micro-electrode arrays . <eos> in this paper , we describe a novel architecture for analog-to-digital ( a/d ) conversion that combines ? ? <eos> conversion with spatial de-correlation within a single module . <eos> the architecture called multiple-input multiple-output ( mimo ) ? ? <eos> is based on a min-max gradient descent optimization of a regularized linear cost function that naturally lends to an a/d formulation . <eos> using an online formulation , the architecture can adapt to slow variations in cross-channel correlations , observed due to relative motion of the microelectrodes with respect to the signal sources . <eos> experimental results with real recorded multi-channel neural data demonstrate the effectiveness of the proposed algorithm in alleviating cross-channel redundancy across electrodes and performing data-compression directly at the a/d converter .
planning in partially observable domains is a notoriously difficult problem . <eos> however , in many real-world scenarios , planning can be simplified by decomposing the task into a hierarchy of smaller planning problems . <eos> several approaches have been proposed to optimize a policy that decomposes according to a hierarchy specified a priori . <eos> in this paper , we investigate the problem of automatically discovering the hierarchy . <eos> more precisely , we frame the optimization of a hierarchical policy as a non-convex optimization problem that can be solved with general non-linear solvers , a mixed-integer non-linear approximation or a form of bounded hierarchical policy iteration . <eos> by encoding the hierarchical structure as variables of the optimization problem , we can automatically discover a hierarchy . <eos> our method is flexible enough to allow any parts of the hierarchy to be specified based on prior knowledge while letting the optimization discover the unknown parts . <eos> it can also discover hierarchical policies , including recursive policies , that are more compact ( potentially infinitely fewer parameters ) and often easier to understand given the decomposition induced by the hierarchy .
clustering , or factoring of a document collection attempts to explaineach observed document in terms of one or a small number of inferred prototypes . <eos> prior work demonstrated that when links exist between documents in the corpus ( as is the case with a collection of web pages or scientific papers ) , building a joint model of document contents and connections produces a better model than that built from contents or connections alone . <eos> many problems arise when trying to apply these joint models to corpus at the scale of the world wide web , however ; one of these is that the sheer overhead of representing a feature space on the order of billions of dimensions becomes impractical . <eos> we address this problem with a simple representational shift inspired by probabilistic relational models : instead of representing document linkage in terms of the identities of linking documents , we represent it by the explicit and inferred attributes of the linking documents . <eos> several surprising results come with this shift : in addition to being computationally more tractable , the new model produces factors that more cleanly decompose the document collection . <eos> we discuss several variations on this model and show how some can be seen as exact generalizations of the pagerank algorithm .
in this paper we are presenting a novel multivariate analysis method . <eos> our scheme is based on a novel kernel orthonormalized partial least squares ( pls ) variant for feature extraction , imposing sparsity constrains in the solution to improve scalability . <eos> the algorithm is tested on a benchmark of uci data sets , and on the analysis of integrated short-time music features for genre prediction . <eos> the upshot is that the method has strong expressive power even with rather few features , is clearly outperforming the ordinary kernel pls , and therefore is an appealing method for feature extraction of labelled data .
the quality measures used in information retrieval are particularly difficult to optimize directly , since they depend on the model scores only through the sorted order of the documents returned for a given query . <eos> thus , the derivatives of the cost with respect to the model parameters are either zero , or are undefined . <eos> in this paper , we propose a class of simple , flexible algorithms , called lambdarank , which avoids these difficulties by working with implicit cost functions . <eos> we describe lambdarank using neural network models , although the idea applies to any differentiable function class . <eos> we give necessary and sufficient conditions for the resulting implicit cost function to be convex , and we show that the general method has a simple mechanical interpretation . <eos> we demonstrate significantly improved accuracy , over a state-of-the-art ranking algorithm , on several datasets . <eos> we also show that lambdarank provides a method for significantly speeding up the training phase of that ranking algorithm . <eos> although this paper is directed towards ranking , the proposed method can be extended to any non-smooth and multivariate cost functions .
we consider the problem of learning accurate models from multiple sources of ? nearby data . <eos> given distinct samples from multiple data sources and estimates of the dissimilarities between these sources , we provide a general theory of which samples should be used to learn models for each source . <eos> this theory is applicable in a broad decision-theoretic learning framework , and yields results for classification and regression generally , and for density estimation within the exponential family . <eos> a key component of our approach is the development of approximate triangle inequalities for expected loss , which may be of independent interest .
using extensions of linear algebra concepts to reproducing kernel hilbert spaces ( rkhs ) , we define a unifying framework for random walk kernels on graphs . <eos> reduction to a sylvester equation allows us to compute many of these kernels in o ( n3 ) worst-case time . <eos> this includes kernels whose previous worst-case time complexity was o ( n6 ) , such as the geometric kernels of g ? artner et al . [ 1 ] and the marginal graph kernels of kashima et al . [ 2 ] . <eos> our algebra in rkhs allow us to exploit sparsity in directed and undirected graphs more effectively than previous methods , yielding sub-cubic computational complexity when combined with conjugate gradient solvers or fixed-point iterations . <eos> experiments on graphs from bioinformatics and other application domains show that our algorithms are often more than 1000 times faster than existing approaches .
data sets involving multiple groups with shared characteristics frequently arise in practice . <eos> in this paper we extend hierarchical dirichlet processes to model such data . <eos> each group is assumed to be generated from a template mixture model with group level variability in both the mixing proportions and the component parameters . <eos> variabilities in mixing proportions across groups are handled using hierarchical dirichlet processes , also allowing for automatic determination of the number of components . <eos> in addition , each group is allowed to have its own component parameters coming from a prior described by a template mixture model . <eos> this group-level variability in the component parameters is handled using a random effects model . <eos> we present a markov chain monte carlo ( mcmc ) sampling algorithm to estimate model parameters and demonstrate the method by applying it to the problem of modeling spatial brain activation patterns across multiple images collected via functional magnetic resonance imaging ( fmri ) .
we consider the machine vision task of pose estimation from static images , specifically for the case of articulated objects . <eos> this problem is hard because of the large number of degrees of freedom to be estimated . <eos> following a established line of research , pose estimation is framed as inference in a probabilistic model . <eos> in our experience however , the success of many approaches often lie in the power of the features . <eos> our primary contribution is a novel casting of visual inference as an iterative parsing process , where one sequentially learns better and better features tuned to a particular image . <eos> we show quantitative results for human pose estimation on a database of over 300 images that suggest our algorithm is competitive with or surpasses the state-of-the-art . <eos> since our procedure is quite general ( it does not rely on face or skin detection ) , we also use it to estimate the poses of horses in the weizmann database .
we propose a recursive algorithm for clustering trajectories lying in multiple moving hyperplanes . <eos> starting from a given or random initial condition , we use normalized gradient descent to update the coefficients of a time varying polynomial whose degree is the number of hyperplanes and whose derivatives at a trajectory give an estimate of the vector normal to the hyperplane containing that trajectory . <eos> as time proceeds , the estimates of the hyperplane normals are shown to track their true values in a stable fashion . <eos> the segmentation of the trajectories is then obtained by clustering their associated normal vectors . <eos> the final result is a simple recursive algorithm for segmenting a variable number of moving hyperplanes . <eos> we test our algorithm on the segmentation of dynamic scenes containing rigid motions and dynamic textures , e.g. , a bird floating on water . <eos> our method not only segments the bird motion from the surrounding water motion , but also determines patterns of motion in the scene ( e.g. , periodic motion ) directly from the temporal evolution of the estimated polynomial coefficients . <eos> our experiments also show that our method can deal with appearing and disappearing motions in the scene .
starting with the work of jaakkola and haussler , a variety of approaches have been proposed for coupling domain-specific generative models with statistical learning methods . <eos> the link is established by a kernel function which provides a similarity measure based inherently on the underlying model . <eos> in computational biology , the full promise of this framework has rarely ever been exploited , as most kernels are derived from very generic models , such as sequence profiles or hidden markov models . <eos> here , we introduce the mtreemix kernel , which is based on a generative model tailored to the underlying biological mechanism . <eos> specifically , the kernel quantifies the similarity of evolutionary escape from antiviral drug pressure between two viral sequence samples . <eos> we compare this novel kernel to a standard , evolution-agnostic amino acid encoding in the prediction of hiv drug resistance from genotype , using support vector regression . <eos> the results show significant improvements in predictive performance across 17 anti-hiv drugs . <eos> thus , in our study , the generative-discriminative paradigm is key to bridging the gap between population genetic modeling and clinical decision making .
sparse coding provides a class of algorithms for finding succinct representations of stimuli ; given only unlabeled input data , it discovers basis functions that capture higher-level features in the data . <eos> however , finding sparse codes remains a very difficult computational problem . <eos> in this paper , we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems : an l1 -regularized least squares problem and an l2 -constrained least squares problem . <eos> we propose novel algorithms to solve both of these optimization problems . <eos> our algorithms result in a significant speedup for sparse coding , allowing us to learn larger sparse codes than possible with previously described algorithms . <eos> we apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and , therefore , may provide a partial explanation for these two phenomena in v1 neurons .
we1 develop conditional random sampling ( crs ) , a technique particularly suitable for sparse data . <eos> in large-scale applications , the data are often highly sparse . <eos> crs combines sketching and sampling in that it converts sketches of the data into conditional random samples online in the estimation stage , with the sample size determined retrospectively . <eos> this paper focuses on approximating pairwise l2 and l1 distances and comparing crs with random projections . <eos> for boolean ( 0/1 ) data , crs is provably better than random projections . <eos> we show using real-world data that crs often outperforms random projections . <eos> this technique can be applied in learning , data mining , information retrieval , and database query optimizations .
we describe a method to learn to make sequential stopping decisions , such as those made along a processing pipeline . <eos> we envision a scenario in which a series of decisions must be made as to whether to continue to process . <eos> further processing costs time and resources , but may add value . <eos> our goal is to create , based on historic data , a series of decision rules ( one at each stage in the pipeline ) that decide , based on information gathered up to that point , whether to continue processing the part . <eos> we demonstrate how our framework encompasses problems from manufacturing to vision processing . <eos> we derive a quadratic ( in the number of decisions ) bound on testing performance and provide empirical results on object detection .
under the prediction model of learning , a prediction strategy is presented with an i.i.d . <eos> sample of n 1 points in x and corresponding labels from a concept f f , and aims to minimize the worst-case probability of erring on an nth point . <eos> by exploiting the structure of f , haussler et al . achieved a vc ( f ) /n bound for the natural one-inclusion prediction strategy , improving on bounds implied by pac-type results by a o ( log n ) factor . <eos> the key data structure in their result is the natural subgraph of the hypercube ? the one-inclusion graph ; the key step is a d = vc ( f ) bound on one-inclusion graph density . <eos> the first main result of this n n ? 1 paper is a density bound of n ? d ? 1 / ( ? d ) < d , which positively resolves a conjecture of kuzmin & warmuth relating to their unlabeled peeling compression scheme and also leads to an improved mistake bound for the randomized ( deterministic ) one-inclusion strategy for all d ( for d ? ( n ) ) . <eos> the proof uses a new form of vc-invariant shifting and a group-theoretic symmetrization . <eos> our second main result is a k-class analogue of the d/n mistake bound , replacing the vc-dimension by the pollard pseudo-dimension and the one-inclusion strategy by its natural hypergraph generalization . <eos> this bound on expected risk improves on known pac-based results by a factor of o ( log n ) and is shown to be optimal up to a o ( log k ) factor . <eos> the combinatorial technique of shifting takes a central role in understanding the one-inclusion ( hyper ) graph and is a running theme throughout .
discriminative learning methods for classification perform well when training and test data are drawn from the same distribution . <eos> in many situations , though , we have labeled training data for a source domain , and we wish to learn a classifier which performs well on a target domain with a different distribution . <eos> under what conditions can we adapt a classifier trained on the source domain for use in the target domain ? <eos> intuitively , a good feature representation is a crucial factor in the success of domain adaptation . <eos> we formalize this intuition theoretically with a generalization bound for domain adaption . <eos> our theory illustrates the tradeoffs inherent in designing a representation for domain adaptation and gives a new justification for a recently proposed model . <eos> it also points toward a promising new model for domain adaptation : one which explicitly minimizes the difference between the source and target domains , while at the same time maximizing the margin of the training set .
we describe and analyze an algorithmic framework for online classification where each online trial consists of multiple prediction tasks that are tied together . <eos> we tackle the problem of updating the online hypothesis by defining a projection problem in which each prediction task corresponds to a single linear constraint . <eos> these constraints are tied together through a single slack parameter . <eos> we then introduce a general method for approximately solving the problem by projecting simultaneously and independently on each constraint which corresponds to a prediction sub-problem , and then averaging the individual solutions . <eos> we show that this approach constitutes a feasible , albeit not necessarily optimal , solution for the original projection problem . <eos> we derive concrete simultaneous projection schemes and analyze them in the mistake bound model . <eos> we demonstrate the power of the proposed algorithm in experiments with online multiclass text categorization . <eos> our experiments indicate that a combination of class-dependent features with the simultaneous projection method outperforms previously studied algorithms .
up to now even subjects that are experts in the use of machine learning based bci systems still have to undergo a calibration session of about 20-30 min . <eos> from this data their ( movement ) intentions are so far infered . <eos> we now propose a new paradigm that allows to completely omit such calibration and instead transfer knowledge from prior sessions . <eos> to achieve this goal we first define normalized csp features and distances in-between . <eos> second , we derive prototypical features across sessions : ( a ) by clustering or ( b ) by feature concatenation methods . <eos> finally , we construct a classifier based on these individualized prototypes and show that , indeed , classifiers can be successfully transferred to a new session for a number of subjects .
cooperative competitive networks are believed to play a central role in cortical processing and have been shown to exhibit a wide set of useful computational properties . <eos> we propose a vlsi implementation of a spiking cooperative competitive network and show how it can perform context dependent computation both in the mean firing rate domain and in spike timing correlation space . <eos> in the mean rate case the network amplifies the activity of neurons belonging to the selected stimulus and suppresses the activity of neurons receiving weaker stimuli . <eos> in the event correlation case , the recurrent network amplifies with a higher gain the correlation between neurons which receive highly correlated inputs while leaving the mean firing rate unaltered . <eos> we describe the network architecture and present experimental data demonstrating its context dependent computation capabilities .
we consider single-class classification ( scc ) as a two-person game between the learner and an adversary . <eos> in this game the target distribution is completely known to the learner and the learner ? s goal is to construct a classifier capable of guaranteeing a given tolerance for the false-positive error while minimizing the false negative error . <eos> we identify both ? hard and ? soft optimal classification strategies for different types of games and demonstrate that soft classification can provide a significant advantage . <eos> our optimal strategies and bounds provide worst-case lower bounds for standard , finite-sample scc and also motivate new approaches to solving scc .
we describe hidden semi-markov support vector machines ( shm svms ) , an extension of hm svms to semi-markov chains . <eos> this allows us to predict segmentations of sequences based on segment-based features measuring properties such as the length of the segment . <eos> we propose a novel technique to partition the problem into sub-problems . <eos> the independently obtained partial solutions can then be recombined in an efficient way , which allows us to solve label sequence learning problems with several thousands of labeled sequences . <eos> we have tested our algorithm for predicting gene structures , an important problem in computational biology . <eos> results on a well-known model organism illustrate the great potential of shm svms in computational biology .
geometrically based methods for various tasks of machine learning have attracted considerable attention over the last few years . <eos> in this paper we show convergence of eigenvectors of the point cloud laplacian to the eigenfunctions of the laplace-beltrami operator on the underlying manifold , thus establishing the first convergence results for a spectral dimensionality reduction algorithm in the manifold setting .
we consider methods that try to find a good policy for a markov decision process by choosing one from a given class . <eos> the policy is chosen based on its empirical performance in simulations . <eos> we are interested in conditions on the complexity of the policy class that ensure the success of such simulation based policy search methods . <eos> we show that under bounds on the amount of computation involved in computing policies , transition dynamics and rewards , uniform convergence of empirical estimates to true value functions occurs . <eos> previously , such results were derived by assuming boundedness of pseudodimension and lipschitz continuity . <eos> these assumptions and ours are both stronger than the usual combinatorial complexity measures . <eos> we show , via minimax inequalities , that this is essential : boundedness of pseudodimension or fat-shattering dimension alone is not sufficient .
we introduce two methods to improve convergence of the kernel hebbian algorithm ( kha ) for iterative kernel pca . <eos> kha has a scalar gain parameter which is either held constant or decreased as 1/t , leading to slow convergence . <eos> our kha/et algorithm accelerates kha by incorporating the reciprocal of the current estimated eigenvalues as a gain vector . <eos> we then derive and apply stochastic metadescent ( smd ) to kha/et ; this further speeds convergence by performing gain adaptation in rkhs . <eos> experimental results for kernel pca and spectral clustering of usps digits as well as motion capture and image de-noising problems confirm that our methods converge substantially faster than conventional kha .
we consider the problem of training a conditional random field ( crf ) to maximize per-label predictive accuracy on a training set , an approach motivated by the principle of empirical risk minimization . <eos> we give a gradient-based procedure for minimizing an arbitrarily accurate approximation of the empirical risk under a hamming loss function . <eos> in experiments with both simulated and real data , our optimization procedure gives significantly better testing performance than several current approaches for crf training , especially in situations of high label noise .
policy gradient methods are reinforcement learning algorithms that adapt a parameterized policy by following a performance gradient estimate . <eos> conventional policy gradient methods use monte-carlo techniques to estimate this gradient . <eos> since monte carlo methods tend to have high variance , a large number of samples is required , resulting in slow convergence . <eos> in this paper , we propose a bayesian framework that models the policy gradient as a gaussian process . <eos> this reduces the number of samples needed to obtain accurate gradient estimates . <eos> moreover , estimates of the natural gradient as well as a measure of the uncertainty in the gradient estimates are provided at little extra cost .
techniques such as probabilistic topic models and latent-semantic indexing have been shown to be broadly useful at automatically extracting the topical or semantic content of documents , or more generally for dimension-reduction of sparse count data . <eos> these types of models and algorithms can be viewed as generating an abstraction from the words in a document to a lower-dimensional latent variable representation that captures what the document is generally about beyond the specific words it contains . <eos> in this paper we propose a new probabilistic model that tempers this approach by representing each document as a combination of ( a ) a background distribution over common words , ( b ) a mixture distribution over general topics , and ( c ) a distribution over words that are treated as being specific to that document . <eos> we illustrate how this model can be used for information retrieval by matching documents both at a general topic level and at a specific word level , providing an advantage over techniques that only match documents at a general level ( such as topic models or latent-sematic indexing ) or that only match documents at the specific word level ( such as tf-idf ) .
chemical reaction networks by which individual cells gather and process information about their chemical environments have been dubbed ? signal transduction networks . <eos> despite this suggestive terminology , there have been few attempts to analyze chemical signaling systems with the quantitative tools of information theory . <eos> gradient sensing in the social amoeba dictyostelium discoideum is a well characterized signal transduction system in which a cell estimates the direction of a source of diffusing chemoattractant molecules based on the spatiotemporal sequence of ligand-receptor binding events at the cell membrane . <eos> using monte carlo techniques ( mcell ) we construct a simulation in which a collection of individual ligand particles undergoing brownian diffusion in a three-dimensional volume interact with receptors on the surface of a static amoeboid cell . <eos> adapting a method for estimation of spike train entropies described by victor ( originally due to kozachenko and leonenko ) , we estimate lower bounds on the mutual information between the transmitted signal ( direction of ligand source ) and the received signal ( spatiotemporal pattern of receptor binding/unbinding events ) . <eos> hence we provide a quantitative framework for addressing the question : how much could the cell know , and when could it know it we show that the time course of the mutual information between the cell ? s surface receptors and the ( unknown ) gradient direction is consistent with experimentally measured cellular response times . <eos> we find that the acquisition of directional information depends strongly on the time constant at which the intracellular response is filtered .
we present a robust distributed algorithm for approximate probabilistic inference in dynamical systems , such as sensor networks and teams of mobile robots . <eos> using assumed density filtering , the network nodes maintain a tractable representation of the belief state in a distributed fashion . <eos> at each time step , the nodes coordinate to condition this distribution on the observations made throughout the network , and to advance this estimate to the next time step . <eos> in addition , we identify a significant challenge for probabilistic inference in dynamical systems : message losses or network partitions can cause nodes to have inconsistent beliefs about the current state of the system . <eos> we address this problem by developing distributed algorithms that guarantee that nodes will reach an informative consistent distribution when communication is re-established . <eos> we present a suite of experimental results on real-world sensor data for two real sensor network deployments : one with 25 cameras and another with 54 temperature sensors .
we consider the problem of denoising a noisily sampled submanifold m in rd , where the submanifold m is a priori unknown and we are only given a noisy point sample . <eos> the presented denoising algorithm is based on a graph-based diffusion process of the point sample . <eos> we analyze this diffusion process using recent results about the convergence of graph laplacians . <eos> in the experiments we show that our method is capable of dealing with non-trivial high-dimensional noise . <eos> moreover using the denoising algorithm as pre-processing method we can improve the results of a semi-supervised learning algorithm .
we introduce a gaussian process ( gp ) framework , stochastic relational models ( srm ) , for learning social , physical , and other relational phenomena where interactions between entities are observed . <eos> the key idea is to model the stochastic structure of entity relationships ( i.e. , links ) via a tensor interaction of multiple gps , each defined on one type of entities . <eos> these models in fact define a set of nonparametric priors on infinite dimensional tensor matrices , where each element represents a relationship between a tuple of entities . <eos> by maximizing the marginalized likelihood , information is exchanged between the participating gps through the entire relational network , so that the dependency structure of links is messaged to the dependency of entities , reflected by the adapted gp kernels . <eos> the framework offers a discriminative approach to link prediction , namely , predicting the existences , strengths , or types of relationships based on the partially observed linkage network as well as the attributes of entities ( if given ) . <eos> we discuss properties and variants of srm and derive an efficient learning algorithm . <eos> very encouraging experimental results are achieved on a toy problem and a user-movie preference link prediction task . <eos> in the end we discuss extensions of srm to general relational learning tasks .
we propose a generic algorithm for computation of similarity measures for sequential data . <eos> the algorithm uses generalized suffix trees for efficient calculation of various kernel , distance and non-metric similarity functions . <eos> its worst-case run-time is linear in the length of sequences and independent of the underlying embedding language , which can cover words , k-grams or all contained subsequences . <eos> experiments with network intrusion detection , dna analysis and text processing applications demonstrate the utility of distances and similarity coefficients for sequences as alternatives to classical kernel functions .
one of the intuitions underlying many graph-based methods for clustering and semi-supervised learning , is that class or cluster boundaries pass through areas of low probability density . <eos> in this paper we provide some formal analysis of that notion for a probability distribution . <eos> we introduce a notion of weighted boundary volume , which measures the length of the class/cluster boundary weighted by the density of the underlying probability distribution . <eos> we show that sizes of the cuts of certain commonly used data adjacency graphs converge to this continuous weighted volume of the boundary . <eos> keywords : clustering , semi-supervised learning
we introduce a class of mpds which greatly simplify reinforcement learning . <eos> they have discrete state spaces and continuous control spaces . <eos> the controls have the effect of rescaling the transition probabilities of an underlying markov chain . <eos> a control cost penalizing kl divergence between controlled and uncontrolled transition probabilities makes the minimization problem convex , and allows analytical computation of the optimal controls given the optimal value function . <eos> an exponential transformation of the optimal value function makes the minimized bellman equation linear . <eos> apart from their theoretical signi cance , the new mdps enable ef cient approximations to traditional mdps . <eos> shortest path problems are approximated to arbitrary precision with largest eigenvalue problems , yielding an o ( n ) algorithm . <eos> accurate approximations to generic mdps are obtained via continuous embedding reminiscent of lp relaxation in integer programming . <eos> offpolicy learning of the optimal value function is possible without need for stateaction values ; the new algorithm ( z-learning ) outperforms q-learning . <eos> this work was supported by nsf grant ecs ? 0524761 .
markov networks are commonly used in a wide variety of applications , ranging from computer vision , to natural language , to computational biology . <eos> in most current applications , even those that rely heavily on learned models , the structure of the markov network is constructed by hand , due to the lack of effective algorithms for learning markov network structure from data . <eos> in this paper , we provide a computationally efficient method for learning markov network structure from data . <eos> our method is based on the use of l1 regularization on the weights of the log-linear model , which has the effect of biasing the model towards solutions where many of the parameters are zero . <eos> this formulation converts the markov network learning problem into a convex optimization problem in a continuous space , which can be solved using efficient gradient methods . <eos> a key issue in this setting is the ( unavoidable ) use of approximate inference , which can lead to errors in the gradient computation when the network structure is dense . <eos> thus , we explore the use of different feature introduction schemes and compare their performance . <eos> we provide results for our method on synthetic data , and on two real world data sets : pixel values in the mnist data , and genetic sequence variations in the human hapmap data . <eos> we show that our l1 -based method achieves considerably higher generalization performance than the more standard l2 -based method ( a gaussian parameter prior ) or pure maximum-likelihood learning . <eos> we also show that we can learn mrf network structure at a computational cost that is not much greater than learning parameters alone , demonstrating the existence of a feasible method for this important problem .
the increasingly popular independent component analysis ( ica ) may only be applied to data following the generative ica model in order to guarantee algorithmindependent and theoretically valid results . <eos> subspace ica models generalize the assumption of component independence to independence between groups of components . <eos> they are attractive candidates for dimensionality reduction methods , however are currently limited by the assumption of equal group sizes or less general semi-parametric models . <eos> by introducing the concept of irreducible independent subspaces or components , we present a generalization to a parameter-free mixture model . <eos> moreover , we relieve the condition of at-most-one-gaussian by including previous results on non-gaussian component analysis . <eos> after introducing this general model , we discuss joint block diagonalization with unknown block sizes , on which we base a simple extension of jade to algorithmically perform the subspace analysis . <eos> simulations confirm the feasibility of the algorithm .
semi-supervised learning is more powerful than supervised learning by using both labeled and unlabeled data . <eos> in particular , the manifold regularization framework , together with kernel methods , leads to the laplacian svm ( lapsvm ) that has demonstrated state-of-the-art performance . <eos> however , the lapsvm solution typically involves kernel expansions of all the labeled and unlabeled examples , and is slow on testing . <eos> moreover , existing semi-supervised learning methods , including the lapsvm , can only handle a small number of unlabeled examples . <eos> in this paper , we integrate manifold regularization with the core vector machine , which has been used for large-scale supervised and unsupervised learning . <eos> by using a sparsified manifold regularizer and formulating as a center-constrained minimum enclosing ball problem , the proposed method produces sparse solutions with low time and space complexities . <eos> experimental results show that it is much faster than the lapsvm , and can handle a million unlabeled examples on a standard pc ; while the lapsvm can only handle several thousand patterns .
survival in a non-stationary , potentially adversarial environment requires animals to detect sensory changes rapidly yet accurately , two oft competing desiderata . <eos> neurons subserving such detections are faced with the corresponding challenge to discern realchanges in inputs as quickly as possible , while ignoring noisy fluctuations . <eos> mathematically , this is an example of a change-detection problem that is actively researched in the controlled stochastic processes community . <eos> in this paper , we utilize sophisticated tools developed in that community to formalize an instantiation of the problem faced by the nervous system , and characterize the bayes-optimal decision policy under certain assumptions . <eos> we will derive from this optimal strategy an information accumulation and decision process that remarkably resembles the dynamics of a leaky integrate-and-fire neuron . <eos> this correspondence suggests that neurons are optimized for tracking input changes , and sheds new light on the computational import of intracellular properties such as resting membrane potential , voltage-dependent conductance , and post-spike reset voltage . <eos> we also explore the influence that factors such as timing , uncertainty , neuromodulation , and reward should and do have on neuronal dynamics and sensitivity , as the optimal decision strategy depends critically on these factors .
we consider the well-studied problem of learning decision lists using few examples when many irrelevant features are present . <eos> we show that smooth boosting algorithms such as madaboost can efficiently learn decision lists of length k over n boolean variables using poly ( k , log n ) many examples provided that the marginal distribution over the relevant variables is ? not too concentrated ? <eos> in an l 2 -norm sense . <eos> using a recent result of h ? astad , we extend the analysis to obtain a similar ( though quantitatively weaker ) result for learning arbitrary linear threshold functions with k nonzero coefficients . <eos> experimental results indicate that the use of a smooth boosting algorithm , which plays a crucial role in our analysis , has an impact on the actual performance of the algorithm .
this paper proposes a new approach to model-based clustering under prior knowledge . <eos> the proposed formulation can be interpreted from two different angles : as penalized logistic regression , where the class labels are only indirectly observed ( via the probability density of each class ) ; as finite mixture learning under a grouping prior . <eos> to estimate the parameters of the proposed model , we derive a ( generalized ) em algorithm with a closed-form e-step , in contrast with other recent approaches to semi-supervised probabilistic clustering which require gibbs sampling or suboptimal shortcuts . <eos> we show that our approach is ideally suited for image segmentation : it avoids the combinatorial nature markov random field priors , and opens the door to more sophisticated spatial priors ( e.g. , wavelet-based ) in a simple and computationally efficient way . <eos> finally , we extend our formulation to work in unsupervised , semi-supervised , or discriminative modes .
in previous studies , quadratic modelling of natural images has resulted in cell models that react strongly to edges and bars . <eos> here we apply quadratic independent component analysis to natural image patches , and show that up to a small approximation error , the estimated components are computing conjunctions of two linear features . <eos> these conjunctive features appear to represent not only edges and bars , but also inherently two-dimensional stimuli , such as corners . <eos> in addition , we show that for many of the components , the underlying linear features have essentially v1 simple cell receptive field characteristics . <eos> our results indicate that the development of the v2 cells preferring angles and corners may be partly explainable by the principle of unsupervised sparse coding of natural images .
we consider the problem of grasping novel objects , specifically ones that are being seen for the first time through vision . <eos> we present a learning algorithm that neither requires , nor tries to build , a 3-d model of the object . <eos> instead it predicts , directly as a function of the images , a point at which to grasp the object . <eos> our algorithm is trained via supervised learning , using synthetic images for the training set . <eos> we demonstrate on a robotic manipulation platform that this approach successfully grasps a wide variety of objects , such as wine glasses , duct tape , markers , a translucent box , jugs , knife-cutters , cellphones , keys , screwdrivers , staplers , toothbrushes , a thick coil of wire , a strangely shaped power horn , and others , none of which were seen in the training set .
although there has been substantial progress in understanding the neurophysiological mechanisms of stereopsis , how neurons interact in a network during stereo computation remains unclear . <eos> computational models on stereopsis suggest local competition and long-range cooperation are important for resolving ambiguity during stereo matching . <eos> to test these predictions , we simultaneously recorded from multiple neurons in v1 of awake , behaving macaques while presenting surfaces of different depths rendered in dynamic random dot stereograms . <eos> we found that the interaction between pairs of neurons was a function of similarity in receptive fields , as well as of the input stimulus . <eos> neurons coding the same depth experienced common inhibition early in their responses for stimuli presented at their nonpreferred disparities . <eos> they experienced mutual facilitation later in their responses for stimulation at their preferred disparity . <eos> these findings are consistent with a local competition mechanism that first removes gross mismatches , and a global cooperative mechanism that further refines depth estimates .
we consider how a search engine should select advertisements to display with search results , in order to maximize its revenue . <eos> under the standard pay-per-clickarrangement , revenue depends on how well the displayed advertisements appeal to users . <eos> the main difficulty stems from new advertisements whose degree of appeal has yet to be determined . <eos> often the only reliable way of determining appeal is exploration via display to users , which detracts from exploitation of other advertisements known to have high appeal . <eos> budget constraints and finite advertisement lifetimes make it necessary to explore as well as exploit . <eos> in this paper we study the tradeoff between exploration and exploitation , modeling advertisement placement as a multi-armed bandit problem . <eos> we extend traditional bandit formulations to account for budget constraints that occur in search engine advertising markets , and derive theoretical bounds on the performance of a family of algorithms . <eos> we measure empirical performance via extensive experiments over real-world data .
we propose a new technique for sampling the solutions of combinatorial problems in a near-uniform manner . <eos> we focus on problems specified as a boolean formula , i.e. , on sat instances . <eos> sampling for sat problems has been shown to have interesting connections with probabilistic reasoning , making practical sampling algorithms for sat highly desirable . <eos> the best current approaches are based on markov chain monte carlo methods , which have some practical limitations . <eos> our approach exploits combinatorial properties of random parity ( xor ) constraints to prune away solutions near-uniformly . <eos> the final sample is identified amongst the remaining ones using a state-of-the-art sat solver . <eos> the resulting sampling distribution is provably arbitrarily close to uniform . <eos> our experiments show that our technique achieves a significantly better sampling quality than the best alternative .
kernel-based regularized learning seeks a model in a hypothesis space by minimizing the empirical error and the model ? s complexity . <eos> based on the representer theorem , the solution consists of a linear combination of translates of a kernel . <eos> this paper investigates a generalized form of representer theorem for kernel-based learning . <eos> after mapping predefined features and translates of a kernel simultaneously onto a hypothesis space by a specific way of constructing kernels , we proposed a new algorithm by utilizing a generalized regularizer which leaves part of the space unregularized . <eos> using a squared-loss function in calculating the empirical error , a simple convex solution is obtained which combines predefined features with translates of the kernel . <eos> empirical evaluations have confirmed the effectiveness of the algorithm for supervised learning tasks .
the study of point cloud data sampled from a stratification , a collection of manifolds with possible different dimensions , is pursued in this paper . <eos> we present a technique for simultaneously soft clustering and estimating the mixed dimensionality and density of such structures . <eos> the framework is based on a maximum likelihood estimation of a poisson mixture model . <eos> the presentation of the approach is completed with artificial and real examples demonstrating the importance of extending manifold learning to stratification learning .
in this paper , we propose a novel exemplar-based approach to extract dynamic foreground regions from a changing background within a collection of images or a video sequence . <eos> by using image segmentation as a pre-processing step , we convert this traditional pixel-wise labeling problem into a lower-dimensional supervised , binary labeling procedure on image segments . <eos> our approach consists of three steps . <eos> first , a set of random image patches are spatially and adaptively sampled within each segment . <eos> second , these sets of extracted samples are formed into two ? bags of patches ? <eos> to model the foreground/background appearance , respectively . <eos> we perform a novel bidirectional consistency check between new patches from incoming frames and current ? bags of patches ? <eos> to reject outliers , control model rigidity and make the model adaptive to new observations . <eos> within each bag , image patches are further partitioned and resampled to create an evolving appearance model . <eos> finally , the foreground/background decision over segments in an image is formulated using an aggregation function defined on the similarity measurements of sampled patches relative to the foreground and background models . <eos> the essence of the algorithm is conceptually simple and can be easily implemented within a few hundred lines of matlab code . <eos> we evaluate and validate the proposed approach by extensive real examples of the object-level image mapping and tracking within a variety of challenging environments . <eos> we also show that it is straightforward to apply our problem formulation on non-rigid object tracking with difficult surveillance videos .
we present a hierarchical bayesian model for sets of related , but different , classes of time series data . <eos> our model performs alignment simultaneously across all classes , while detecting and characterizing class-specific differences . <eos> during inference the model produces , for each class , a distribution over a canonical representation of the class . <eos> these class-specific canonical representations are automatically aligned to one another ? <eos> preserving common sub-structures , and highlighting differences . <eos> we apply our model to compare and contrast solenoid valve current data , and also , liquid-chromatography-ultraviolet-diode array data from a study of the plant arabidopsis thaliana .
independent component analysis ( ica ) is a popular method for extracting independent features from visual data . <eos> however , as a fundamentally linear technique , there is always nonlinear residual redundancy that is not captured by ica . <eos> hence there have been many attempts to try to create a hierarchical version of ica , but so far none of the approaches have a natural way to apply them more than once . <eos> here we show that there is a relatively simple technique that transforms the absolute values of the outputs of a previous application of ica into a normal distribution , to which ica maybe applied again . <eos> this results in a recursive ica algorithm that may be applied any number of times in order to extract higher order structure from previous layers .
in supervised learning there is a typical presumption that the training and test points are taken from the same distribution . <eos> in practice this assumption is commonly violated . <eos> the situations where the training and test data are from different distributions is called covariate shift . <eos> recent work has examined techniques for dealing with covariate shift in terms of minimisation of generalisation error . <eos> as yet the literature lacks a bayesian generative perspective on this problem . <eos> this paper tackles this issue for regression models . <eos> recent work on covariate shift can be understood in terms of mixture regression . <eos> using this view , we obtain a general approach to regression under covariate shift , which reproduces previous work as a special case . <eos> the main advantages of this new formulation over previous models for covariate shift are that we no longer need to presume the test and training densities are known , the regression and density estimation are combined into a single procedure , and previous methods are reproduced as special cases of this procedure , shedding light on the implicit assumptions the methods are making .
we present a generalization of dynamic bayesian networks to concisely describe complex probability distributions such as in problems with multiple interacting variable-length streams of random variables . <eos> our framework incorporates recent graphical model constructs to account for existence uncertainty , value-specific independence , aggregation relationships , and local and global constraints , while still retaining a bayesian network interpretation and efficient inference and learning techniques . <eos> we introduce one such general technique , which is an extension of value elimination , a backtracking search inference algorithm . <eos> multi-dynamic bayesian networks are motivated by our work on statistical machine translation ( mt ) . <eos> we present results on mt word alignment in support of our claim that mdbns are a promising framework for the rapid prototyping of new mt systems .
we describe an unsupervised method for learning a probabilistic grammar of an object from a set of training examples . <eos> our approach is invariant to the scale and rotation of the objects . <eos> we illustrate our approach using thirteen objects from the caltech 101 database . <eos> in addition , we learn the model of a hybrid object class where we do not know the specific object or its position , scale or pose . <eos> this is illustrated by learning a hybrid class consisting of faces , motorbikes , and airplanes . <eos> the individual objects can be recovered as different aspects of the grammar for the object class . <eos> in all cases , we validate our results by learning the probability grammars from training datasets and evaluating them on the test datasets . <eos> we compare our method to alternative approaches . <eos> the advantages of our approach is the speed of inference ( under one second ) , the parsing of the object , and increased accuracy of performance . <eos> moreover , our approach is very general and can be applied to a large range of objects and structures .
structural equation models can be seen as an extension of gaussian belief networks to cyclic graphs , and we show they can be understood generatively as the model for the joint distribution of long term average equilibrium activity of gaussian dynamic belief networks . <eos> most use of structural equation models in fmri involves postulating a particular structure and comparing learnt parameters across different groups . <eos> in this paper it is argued that there are situations where priors about structure are not firm or exhaustive , and given sufficient data , it is worth investigating learning network structure as part of the approach to connectivity analysis . <eos> first we demonstrate structure learning on a toy problem . <eos> we then show that for particular fmri data the simple models usually assumed are not supported . <eos> we show that is is possible to learn sensible structural equation models that can provide modelling benefits , but that are not necessarily going to be the same as a true causal model , and suggest the combination of prior models and learning or the use of temporal information from dynamic models may provide more benefits than learning structural equations alone .
linear gaussian state-space models are widely used and a bayesian treatment of parameters is therefore of considerable interest . <eos> the approximate variational bayesian method applied to these models is an attractive approach , used successfully in applications ranging from acoustics to bioinformatics . <eos> the most challenging aspect of implementing the method is in performing inference on the hidden state sequence of the model . <eos> we show how to convert the inference problem so that standard kalman filtering/smoothing recursions from the literature may be applied . <eos> this is in contrast to previously published approaches based on belief propagation . <eos> our framework both simplifies and unifies the inference problem , so that future applications may be more easily developed . <eos> we demonstrate the elegance of the approach on bayesian temporal ica , with an application to finding independent dynamical processes underlying noisy eeg signals .
we consider the problem of learning classifiers for structurally incomplete data , where some objects have a subset of features inherently absent due to complex relationships between the features . <eos> the common approach for handling missing features is to begin with a preprocessing phase that completes the missing features , and then use a standard classification procedure . <eos> in this paper we show how incomplete data can be classified directly without any completion of the missing features using a max-margin learning framework . <eos> we formulate this task using a geometrically-inspired objective function , and discuss two optimization approaches : the linearly separable case is written as a set of convex feasibility problems , and the non-separable case has a non-convex objective that we optimize iteratively . <eos> by avoiding the pre-processing phase in which the data is completed , these approaches offer considerable computational savings . <eos> more importantly , we show that by elegantly handling complex patterns of missing values , our approach is both competitive with other methods when the values are missing at random and outperforms them when the missing values have non-trivial structure . <eos> we demonstrate our results on two real-world problems : edge prediction in metabolic pathways , and automobile detection in natural images .
dirichlet process ( dp ) mixture models are promising candidates for clustering applications where the number of clusters is unknown a priori . <eos> due to computational considerations these models are unfortunately unsuitable for large scale data-mining applications . <eos> we propose a class of deterministic accelerated dp mixture models that can routinely handle millions of data-cases . <eos> the speedup is achieved by incorporating kd-trees into a variational bayesian algorithm for dp mixtures in the stick-breaking representation , similar to that of blei and jordan ( 2005 ) . <eos> our algorithm differs in the use of kd-trees and in the way we handle truncation : we only assume that the variational distributions are fixed at their priors after a certain level . <eos> experiments show that speedups relative to the standard variational algorithm can be significant .
blind source separation , i.e . the extraction of unknown sources from a set of given signals , is relevant for many applications . <eos> a special case of this problem is dimension reduction , where the goal is to approximate a given set of signals by superpositions of a minimal number of sources . <eos> since in this case the signals outnumber the sources the problem is over-determined . <eos> most popular approaches for addressing this problem are based on purely linear mixing models . <eos> however , many applications like the modeling of acoustic signals , emg signals , or movement trajectories , require temporal shift-invariance of the extracted components . <eos> this case has only rarely been treated in the computational literature , and specifically for the case of dimension reduction almost no algorithms have been proposed . <eos> we present a new algorithm for the solution of this problem , which is based on a timefrequency transformation ( wigner-ville distribution ) of the generative model . <eos> we show that this algorithm outperforms classical source separation algorithms for linear mixtures , and also a related method for mixtures with delays . <eos> in addition , applying the new algorithm to trajectories of human gaits , we demonstrate that it is suitable for the extraction of spatio-temporal components that are easier to interpret than components extracted with other classical algorithms .
we introduce a method for approximate smoothed inference in a class of switching linear dynamical systems , based on a novel form of gaussian sum smoother . <eos> this class includes the switching kalman filter and the more general case of switch transitions dependent on the continuous latent state . <eos> the method improves on the standard kim smoothing approach by dispensing with one of the key approximations , thus making fuller use of the available future information . <eos> whilst the only central assumption required is projection to a mixture of gaussians , we show that an additional conditional independence assumption results in a simpler but stable and accurate alternative . <eos> unlike the alternative unstable expectation propagation procedure , our method consists only of a single forward and backward pass and is reminiscent of the standard smoothing correctionrecursions in the simpler linear dynamical system . <eos> the algorithm performs well on both toy experiments and in a large scale application to noise robust speech recognition .
selective attention is the strategy used by biological sensory systems to solve the problem of limited parallel processing capacity : salient subregions of the input stimuli are serially processed , while non ? salient regions are suppressed . <eos> we present an mixed mode analog/digital very large scale integration implementation of a building block for a multi ? chip neuromorphic hardware model of selective attention . <eos> we describe the chip ? s architecture and its behavior , when its is part of a multi ? chip system with a spiking retina as input , and show how it can be used to implement in real-time flexible models of bottom-up attention .
we present a novel , semi-supervised approach to training discriminative random fields ( drfs ) that efficiently exploits labeled and unlabeled training data to achieve improved accuracy in a variety of image processing tasks . <eos> we formulate drf training as a form of map estimation that combines conditional loglikelihood on labeled data , given a data-dependent prior , with a conditional entropy regularizer defined on unlabeled data . <eos> although the training objective is no longer concave , we develop an efficient local optimization procedure that produces classifiers that are more accurate than ones based on standard supervised drf training . <eos> we then apply our semi-supervised approach to train drfs to segment both synthetic and real data sets , and demonstrate significant improvements over supervised drfs in each case .
pyramid intersection is an efficient method for computing an approximate partial matching between two sets of feature vectors . <eos> we introduce a novel pyramid embedding based on a hierarchy of non-uniformly shaped bins that takes advantage of the underlying structure of the feature space and remains accurate even for sets with high-dimensional feature vectors . <eos> the matching similarity is computed in linear time and forms a mercer kernel . <eos> whereas previous matching approximation algorithms suffer from distortion factors that increase linearly with the feature dimension , we demonstrate that our approach can maintain constant accuracy even as the feature dimension increases . <eos> when used as a kernel in a discriminative classifier , our approach achieves improved object recognition results over a state-of-the-art set kernel .
it has been established that a neuron reproduces highly precise spike response to identical ? uctuating input currents . <eos> we wish to accurately predict the ? ring times of a given neuron for any input current . <eos> for this purpose we adopt a model that mimics the dynamics of the membrane potential , and then take a cue from its dynamics for predicting the spike occurrence for a novel input current . <eos> it is found that the prediction is signi ? cantly improved by observing the state space of the membrane potential and its time derivative ( s ) in advance of a possible spike , in comparison to simply thresholding an instantaneous value of the estimated potential .
data sets that characterize human activity over time through collections of timestamped events or counts are of increasing interest in application areas as humancomputer interaction , video surveillance , and web data analysis . <eos> we propose a non-parametric bayesian framework for modeling collections of such data . <eos> in particular , we use a dirichlet process framework for learning a set of intensity functions corresponding to different categories , which form a basis set for representing individual time-periods ( e.g. , several days ) depending on which categories the time-periods are assigned to . <eos> this allows the model to learn in a data-driven fashion what factorsare generating the observations on a particular day , including ( for example ) weekday versus weekend effects or day-specific effects corresponding to unique ( single-day ) occurrences of unusual behavior , sharing information where appropriate to obtain improved estimates of the behavior associated with each category . <eos> applications to real ? world data sets of count data involving both vehicles and people are used to illustrate the technique .
we introduce binary matrix factorization , a novel model for unsupervised matrix decomposition . <eos> the decomposition is learned by fitting a non-parametric bayesian probabilistic model with binary latent variables to a matrix of dyadic data . <eos> unlike bi-clustering models , which assign each row or column to a single cluster based on a categorical hidden feature , our binary feature model reflects the prior belief that items and attributes can be associated with more than one latent cluster at a time . <eos> we provide simple learning and inference rules for this new model and show how to extend it to an infinite model in which the number of features is not a priori fixed but is allowed to grow with the size of the data .
we consider the problem of constructing a function whose zero set is to represent a surface , given sample points with surface normal vectors . <eos> the contributions include a novel means of regularising multi-scale compactly supported basis functions that leads to the desirable properties previously only associated with fully supported bases , and show equivalence to a gaussian process with modified covariance function . <eos> we also provide a regularisation framework for simpler and more direct treatment of surface normals , along with a corresponding generalisation of the representer theorem . <eos> we demonstrate the techniques on 3d problems of up to 14 million data points , as well as 4d time series data .
we present a new algorithm , locally smooth manifold learning ( lsml ) , that learns a warping function from a point on an manifold to its neighbors . <eos> important characteristics of lsml include the ability to recover the structure of the manifold in sparsely populated regions and beyond the support of the provided data . <eos> applications of our proposed technique include embedding with a natural out-of-sample extension and tasks such as tangent distance estimation , frame rate up-conversion , video compression and motion transfer .
we present a computational bayesian approach for wiener diffusion models , which are prominent accounts of response time distributions in decision-making . <eos> we first develop a general closed-form analytic approximation to the response time distributions for one-dimensional diffusion processes , and derive the required wiener diffusion as a special case . <eos> we use this result to undertake bayesian modeling of benchmark data , using posterior sampling to draw inferences about the interesting psychological parameters . <eos> with the aid of the benchmark data , we show the bayesian account has several advantages , including dealing naturally with the parameter variation needed to account for some key features of the data , and providing quantitative measures to guide decisions about model construction .
this paper develops a multi-frame image super-resolution approach from a bayesian view-point by marginalizing over the unknown registration parameters relating the set of input low-resolution views . <eos> in tipping and bishop ? s bayesian image super-resolution approach [ 16 ] , the marginalization was over the superresolution image , necessitating the use of an unfavorable image prior . <eos> by integrating over the registration parameters rather than the high-resolution image , our method allows for more realistic prior distributions , and also reduces the dimension of the integral considerably , removing the main computational bottleneck of the other algorithm . <eos> in addition to the motion model used by tipping and bishop , illumination components are introduced into the generative model , allowing us to handle changes in lighting as well as motion . <eos> we show results on real and synthetic datasets to illustrate the efficacy of this approach .
we present two new algorithms for online learning in reproducing kernel hilbert spaces . <eos> our first algorithm , ilk ( implicit online learning with kernels ) , employs a new , implicit update technique that can be applied to a wide variety of convex loss functions . <eos> we then introduce a bounded memory version , silk ( sparse ilk ) , that maintains a compact representation of the predictor without compromising solution quality , even in non-stationary environments . <eos> we prove loss bounds and analyze the convergence rate of both . <eos> experimental evidence shows that our proposed algorithms outperform current methods on synthetic and real data .
we derive a cost functional for estimating the relationship between highdimensional observations and the low-dimensional process that generated them with no input-output examples . <eos> limiting our search to invertible observation functions confers numerous benefits , including a compact representation and no suboptimal local minima . <eos> our approximation algorithms for optimizing this cost functional are fast and give diagnostic bounds on the quality of their solution . <eos> our method can be viewed as a manifold learning algorithm that utilizes a prior on the low-dimensional manifold coordinates . <eos> the benefits of taking advantage of such priors in manifold learning and searching for the inverse observation functions in system identification are demonstrated empirically by learning to track moving targets from raw measurements in a sensor network setting and in an rfid tracking experiment .
a reliable motion estimation algorithm must function under a wide range of conditions . <eos> one regime , which we consider here , is the case of moving objects with contours but no visible texture . <eos> tracking distinctive features such as corners can disambiguate the motion of contours , but spurious features such as t-junctions can be badly misleading . <eos> it is difficult to determine the reliability of motion from local measurements , since a full rank covariance matrix can result from both real and spurious features . <eos> we propose a novel approach that avoids these points altogether , and derives global motion estimates by utilizing information from three levels of contour analysis : edgelets , boundary fragments and contours . <eos> boundary fragment are chains of orientated edgelets , for which we derive motion estimates from local evidence . <eos> the uncertainties of the local estimates are disambiguated after the boundary fragments are properly grouped into contours . <eos> the grouping is done by constructing a graphical model and marginalizing it using importance sampling . <eos> we propose two equivalent representations in this graphical model , reversible switch variables attached to the ends of fragments and fragment chains , to capture both local and global statistics of boundaries . <eos> our system is successfully applied to both synthetic and real video sequences containing high-contrast boundaries and textureless regions . <eos> the system produces good motion estimates along with properly grouped and completed contours .
bayesian inference has become increasingly important in statistical machine learning . <eos> exact bayesian calculations are often not feasible in practice , however . <eos> a number of approximate bayesian methods have been proposed to make such calculations practical , among them the variational bayesian ( vb ) approach . <eos> the vb approach , while useful , can nevertheless suffer from slow convergence to the approximate solution . <eos> to address this problem , we propose parameter-expanded variational bayesian ( px-vb ) methods to speed up vb . <eos> the new algorithm is inspired by parameter-expanded expectation maximization ( px-em ) and parameterexpanded data augmentation ( px-da ) . <eos> similar to px-em and -da , px-vb expands a model with auxiliary variables to reduce the coupling between variables in the original model . <eos> we analyze the convergence rates of vb and px-vb and demonstrate the superior convergence rates of px-vb in variational probit regression and automatic relevance determination .
semi-supervised learning algorithms have been successfully applied in many applications with scarce labeled data , by utilizing the unlabeled data . <eos> one important category is graph based semi-supervised learning algorithms , for which the performance depends considerably on the quality of the graph , or its hyperparameters . <eos> in this paper , we deal with the less explored problem of learning the graphs . <eos> we propose a graph learning method for the harmonic energy minimization method ; this is done by minimizing the leave-one-out prediction error on labeled data points . <eos> we use a gradient based method and designed an efficient algorithm which significantly accelerates the calculation of the gradient by applying the matrix inversion lemma and using careful pre-computation . <eos> experimental results show that the graph learning method is effective in improving the performance of the classification algorithm .
we propose a highly efficient framework for kernel multi-class models with a large and structured set of classes . <eos> kernel parameters are learned automatically by maximizing the cross-validation log likelihood , and predictive probabilities are estimated . <eos> we demonstrate our approach on large scale text classification tasks with hierarchical class structure , achieving state-of-the-art results in an order of magnitude less time than previous work .
our motor system changes due to causes that span multiple timescales . <eos> for example , muscle response can change because of fatigue , a condition where the disturbance has a fast timescale or because of disease where the disturbance is much slower . <eos> here we hypothesize that the nervous system adapts in a way that reflects the temporal properties of such potential disturbances . <eos> according to a bayesian formulation of this idea , movement error results in a credit assignment problem : what timescale is responsible for this disturbance ? <eos> the adaptation schedule influences the behavior of the optimal learner , changing estimates at different timescales as well as the uncertainty . <eos> a system that adapts in this way predicts many properties observed in saccadic gain adaptation . <eos> it well predicts the timecourses of motor adaptation in cases of partial sensory deprivation and reversals of the adaptation direction .
in this paper , we formalize multi-instance multi-label learning , where each training example is associated with not only multiple instances but also multiple class labels . <eos> such a problem can occur in many real-world tasks , e.g . an image usually contains multiple patches each of which can be described by a feature vector , and the image can belong to multiple categories since its semantics can be recognized in different ways . <eos> we analyze the relationship between multi-instance multi-label learning and the learning frameworks of traditional supervised learning , multiinstance learning and multi-label learning . <eos> then , we propose the m iml b oost and m iml s vm algorithms which achieve good performance in an application to scene classification .
in this paper we focus on the issue of normalization of the affinity matrix in spectral clustering . <eos> we show that the difference between n-cuts and ratio-cuts is in the error measure being used ( relative-entropy versus l1 norm ) in finding the closest doubly-stochastic matrix to the input affinity matrix . <eos> we then develop a scheme for finding the optimal , under frobenius norm , doubly-stochastic approximation using von-neumann ? s successive projections lemma . <eos> the new normalization scheme is simple and efficient and provides superior clustering performance over many of the standardized tests .
neural motor prostheses ( nmps ) require the accurate decoding of motor cortical population activity for the control of an artificial motor system . <eos> previous work on cortical decoding for nmps has focused on the recovery of hand kinematics . <eos> human nmps however may require the control of computer cursors or robotic devices with very different physical and dynamical properties . <eos> here we show that the firing rates of cells in the primary motor cortex of non-human primates can be used to control the parameters of an artificial physical system exhibiting realistic dynamics . <eos> the model represents 2d hand motion in terms of a point mass connected to a system of idealized springs . <eos> the nonlinear spring coefficients are estimated from the firing rates of neurons in the motor cortex . <eos> we evaluate linear and a nonlinear decoding algorithms using neural recordings from two monkeys performing two different tasks . <eos> we found that the decoded spring coefficients produced accurate hand trajectories compared with state-of-the-art methods for direct decoding of hand kinematics . <eos> furthermore , using a physically-based system produced decoded movements that were more naturalin that their frequency spectrum more closely matched that of natural hand movements .
we study the problem of parameter estimation in continuous density hidden markov models ( cd-hmms ) for automatic speech recognition ( asr ) . <eos> as in support vector machines , we propose a learning algorithm based on the goal of margin maximization . <eos> unlike earlier work on max-margin markov networks , our approach is specifically geared to the modeling of real-valued observations ( such as acoustic feature vectors ) using gaussian mixture models . <eos> unlike previous discriminative frameworks for asr , such as maximum mutual information and minimum classification error , our framework leads to a convex optimization , without any spurious local minima . <eos> the objective function for large margin training of cd-hmms is defined over a parameter space of positive semidefinite matrices . <eos> its optimization can be performed efficiently with simple gradient-based methods that scale well to large problems . <eos> we obtain competitive results for phonetic recognition on the timit speech corpus .
we present a learning algorithm for undiscounted reinforcement learning . <eos> our interest lies in bounds for the algorithm ? s online performance after some finite number of steps . <eos> in the spirit of similar methods already successfully applied for the exploration-exploitation tradeoff in multi-armed bandit problems , we use upper confidence bounds to show that our ucrl algorithm achieves logarithmic online regret in the number of steps taken with respect to an optimal policy .
computation of a satisfactory control policy for a markov decision process when the parameters of the model are not exactly known is a problem encountered in many practical applications . <eos> the traditional robust approach is based on a worstcase analysis and may lead to an overly conservative policy . <eos> in this paper we consider the tradeoff between nominal performance and the worst case performance over all possible models . <eos> based on parametric linear programming , we propose a method that computes the whole set of pareto efficient policies in the performancerobustness plane when only the reward parameters are subject to uncertainty . <eos> in the more general case when the transition probabilities are also subject to error , we show that the strategy with the optimaltradeoff might be non-markovian and hence is in general not tractable .
patch-based appearance models are used in a wide range of computer vision applications . <eos> to learn such models it has previously been necessary to specify a suitable set of patch sizes and shapes by hand . <eos> in the jigsaw model presented here , the shape , size and appearance of patches are learned automatically from the repeated structures in a set of training images . <eos> by learning such irregularly shaped ? jigsaw pieces ? , we are able to discover both the shape and the appearance of object parts without supervision . <eos> when applied to face images , for example , the learned jigsaw pieces are surprisingly strongly associated with face parts of different shapes and scales such as eyes , noses , eyebrows and cheeks , to name a few . <eos> we conclude that learning the shape of the patch not only improves the accuracy of appearance-based part detection but also allows for shape-based part detection . <eos> this enables parts of similar appearance but different shapes to be distinguished ; for example , while foreheads and cheeks are both skin colored , they have markedly different shapes .
perceptual bistability refers to the phenomenon of spontaneously switching between two or more interpretations of an image under continuous viewing . <eos> although switching behavior is increasingly well characterized , the origins remain elusive . <eos> we propose that perceptual switching naturally arises from the brain ? s search for best interpretations while performing bayesian inference . <eos> in particular , we propose that the brain explores a posterior distribution over image interpretations at a rapid time scale via a sampling-like process and updates its interpretation when a sampled interpretation is better than the discounted value of its current interpretation . <eos> we formalize the theory , explicitly derive switching rate distributions and discuss qualitative properties of the theory including the effect of changes in the posterior distribution on switching rates . <eos> finally , predictions of the theory are shown to be consistent with measured changes in human switching dynamics to necker cube stimuli induced by context .
we propose a family of kernels for structured objects which is based on the bag-ofcomponents paradigm . <eos> however , rather than decomposing each complex object into the single histogram of its components , we use for each object a family of nested histograms , where each histogram in this hierarchy describes the object seen from an increasingly granular perspective . <eos> we use this hierarchy of histograms to define elementary kernels which can detect coarse and fine similarities between the objects . <eos> we compute through an efficient averaging trick a mixture of such specific kernels , to propose a final kernel value which weights efficiently local and global matches . <eos> we propose experimental results on an image retrieval experiment which show that this mixture is an effective template procedure to be used with kernels on histograms .
we consider the problem of inferring the structure of a network from cooccurrence data : observations that indicate which nodes occur in a signaling pathway but do not directly reveal node order within the pathway . <eos> this problem is motivated by network inference problems arising in computational biology and communication systems , in which it is difficult or impossible to obtain precise time ordering information . <eos> without order information , every permutation of the activated nodes leads to a different feasible solution , resulting in combinatorial explosion of the feasible set . <eos> however , physical principles underlying most networked systems suggest that not all feasible solutions are equally likely . <eos> intuitively , nodes that co-occur more frequently are probably more closely connected . <eos> building on this intuition , we model path co-occurrences as randomly shuffled samples of a random walk on the network . <eos> we derive a computationally efficient network inference algorithm and , via novel concentration inequalities for importance sampling estimators , prove that a polynomial complexity monte carlo version of the algorithm converges with high probability .
this paper proposes a pac-bayes bound to measure the performance of support vector machine ( svm ) classifiers . <eos> the bound is based on learning a prior over the distribution of classifiers with a part of the training samples . <eos> experimental work shows that this bound is tighter than the original pac-bayes , resulting in an enhancement of the predictive capabilities of the pac-bayes bound . <eos> in addition , it is shown that the use of this bound as a means to estimate the hyperparameters of the classifier compares favourably with cross validation in terms of accuracy of the model , while saving a lot of computational burden .
we consider the task of tuning hyperparameters in svm models based on minimizing a smooth performance validation function , e.g. , smoothed k-fold crossvalidation error , using non-linear optimization techniques . <eos> the key computation in this approach is that of the gradient of the validation function with respect to hyperparameters . <eos> we show that for large-scale problems involving a wide choice of kernel-based models and validation functions , this computation can be very efficiently done ; often within just a fraction of the training time . <eos> empirical results show that a near-optimal set of hyperparameters can be identified by our approach with very few training rounds and gradient computations . <eos> .
in many areas of science and engineering , the problem arises how to discover low dimensional representations of high dimensional data . <eos> recently , a number of researchers have converged on common solutions to this problem using methods from convex optimization . <eos> in particular , many results have been obtained by constructing semidefinite programs ( sdps ) with low rank solutions . <eos> while the rank of matrix variables in sdps can not be directly constrained , it has been observed that low rank solutions emerge naturally by computing high variance or maximal trace solutions that respect local distance constraints . <eos> in this paper , we show how to solve very large problems of this type by a matrix factorization that leads to much smaller sdps than those previously studied . <eos> the matrix factorization is derived by expanding the solution of the original problem in terms of the bottom eigenvectors of a graph laplacian . <eos> the smaller sdps obtained from this matrix factorization yield very good approximations to solutions of the original problem . <eos> moreover , these approximations can be further refined by conjugate gradient descent . <eos> we illustrate the approach on localization in large scale sensor networks , where optimizations involving tens of thousands of nodes can be solved in just a few minutes .
we have developed a novel algorithm for integrating source localization and noise suppression based on a probabilistic graphical model of stimulus-evoked meg/eeg data . <eos> our algorithm localizes multiple dipoles while suppressing noise sources with the computational complexity equivalent to a single dipole scan , and is therefore more efficient than traditional multidipole fitting procedures . <eos> in simulation , the algorithm can accurately localize and estimate the time course of several simultaneously-active dipoles , with rotating or fixed orientation , at noise levels typical for averaged meg data . <eos> furthermore , the algorithm is superior to beamforming techniques , which we show to be an approximation to our graphical model , in estimation of temporally correlated sources . <eos> success of this algorithm for localizing auditory cortex in a tumor patient and for localizing an epileptic spike source are also demonstrated .
everyday inductive reasoning draws on many kinds of knowledge , including knowledge about relationships between properties and knowledge about relationships between objects . <eos> previous accounts of inductive reasoning generally focus on just one kind of knowledge : models of causal reasoning often focus on relationships between properties , and models of similarity-based reasoning often focus on similarity relationships between objects . <eos> we present a bayesian model of inductive reasoning that incorporates both kinds of knowledge , and show that it accounts well for human inferences about the properties of biological species .
we consider the problem of detecting humans and classifying their pose from a single image . <eos> specifically , our goal is to devise a statistical model that simultaneously answers two questions : 1 ) is there a human in the image ? <eos> and , if so , 2 ) what is a low-dimensional representation of her pose ? <eos> we investigate models that can be learned in an unsupervised manner on unlabeled images of human poses , and provide information that can be used to match the pose of a new image to the ones present in the training set . <eos> starting from a set of descriptors recently proposed for human detection , we apply the latent dirichlet allocation framework to model the statistics of these features , and use the resulting model to answer the above questions . <eos> we show how our model can efficiently describe the space of images of humans with their pose , by providing an effective representation of poses for tasks such as classification and matching , while performing remarkably well in human/non human decision problems , thus enabling its use for human detection . <eos> we validate the model with extensive quantitative experiments and comparisons with other approaches on human detection and pose matching .
we present a general model-independent approach to the analysis of data in cases when these data do not appear in the form of co-occurrence of two variables x , y , but rather as a sample of values of an unknown ( stochastic ) function z ( x , y ) . <eos> for example , in gene expression data , the expression level z is a function of gene x and condition y ; or in movie ratings data the rating z is a function of viewer x and movie y . <eos> the approach represents a consistent extension of the information bottleneck method that has previously relied on the availability of co-occurrence statistics . <eos> by altering the relevance variable we eliminate the need in the sample of joint distribution of all input variables . <eos> this new formulation also enables simple mdl-like model complexity control and prediction of missing values of z . <eos> the approach is analyzed and shown to be on a par with the best known clustering algorithms for a wide range of domains . <eos> for the prediction of missing values ( collaborative filtering ) it improves the currently best known results .
by adopting gaussian process priors a fully bayesian solution to the problem of integrating possibly heterogeneous data sets within a classification setting is presented . <eos> approximate inference schemes employing variational & expectation propagation based methods are developed and rigorously assessed . <eos> we demonstrate our approach to integrating multiple data sets on a large scale protein fold prediction problem where we infer the optimal combinations of covariance functions and achieve state-of-the-art performance without resorting to any ad hoc parameter tuning and classifier combination .
we establish a general oracle inequality for clipped approximate minimizers of regularized empirical risks and apply this inequality to support vector machine ( svm ) type algorithms . <eos> we then show that for svms using gaussian rbf kernels for classification this oracle inequality leads to learning rates that are faster than the ones established in [ 9 ] . <eos> finally , we use our oracle inequality to show that a simple parameter selection approach based on a validation set can yield the same fast learning rates without knowing the noise exponents which were required to be known a-priori in [ 9 ] .
we show that the relevant information about a classification problem in feature space is contained up to negligible error in a finite number of leading kernel pca components if the kernel matches the underlying learning problem . <eos> thus , kernels not only transform data sets such that good generalization can be achieved even by linear discriminant functions , but this transformation is also performed in a manner which makes economic use of feature space dimensions . <eos> in the best case , kernels provide efficient implicit representations of the data to perform classification . <eos> practically , we propose an algorithm which enables us to recover the subspace and dimensionality relevant for good classification . <eos> our algorithm can therefore be applied ( 1 ) to analyze the interplay of data set and kernel in a geometric fashion , ( 2 ) to help in model selection , and to ( 3 ) de-noise in feature space in order to yield better classification results .
given a set f of classifiers and a probability distribution over their domain , one can define a metric by taking the distance between a pair of classifiers to be the probability that they classify a random item differently . <eos> we prove bounds on the sample complexity of pac learning in terms of the doubling dimension of this metric . <eos> these bounds imply known bounds on the sample complexity of learning halfspaces with respect to the uniform distribution that are optimal up to a constant factor . <eos> we prove a bound that holds for any algorithm that outputs a classifier with zero error whenever this is possible ; this bound is in terms of the maximum of the doubling dimension and the vc-dimension of f , and strengthens the best known bound in terms of the vc-dimension alone . <eos> we show that there is no bound on the doubling dimension in terms of the vc-dimension of f ( in contrast with the metric dimension ) .
spectral clustering methods are common graph-based approaches to clustering of data . <eos> spectral clustering algorithms typically start from local information encoded in a weighted graph on the data and cluster according to the global eigenvectors of the corresponding ( normalized ) similarity matrix . <eos> one contribution of this paper is to present fundamental limitations of this general local to global approach . <eos> we show that based only on local information , the normalized cut functional is not a suitable measure for the quality of clustering . <eos> further , even with a suitable similarity measure , we show that the first few eigenvectors of such adjacency matrices can not successfully cluster datasets that contain structures at different scales of size and density . <eos> based on these findings , a second contribution of this paper is a novel diffusion based measure to evaluate the coherence of individual clusters . <eos> our measure can be used in conjunction with any bottom-up graph-based clustering method , it is scale-free and can determine coherent clusters at all scales . <eos> we present both synthetic examples and real image segmentation problems where various spectral clustering algorithms fail . <eos> in contrast , using this coherence measure finds the expected clusters at all scales . <eos> keywords : clustering , kernels , learning theory .
the extraction of statistically independent components from high-dimensional multi-sensory input streams is assumed to be an essential component of sensory processing in the brain . <eos> such independent component analysis ( or blind source separation ) could provide a less redundant representation of information about the external world . <eos> another powerful processing strategy is to extract preferentially those components from high-dimensional input streams that are related to other information sources , such as internal predictions or proprioceptive feedback . <eos> this strategy allows the optimization of internal representation according to the information bottleneck method . <eos> however , concrete learning rules that implement these general unsupervised learning principles for spiking neurons are still missing . <eos> we show how both information bottleneck optimization and the extraction of independent components can in principle be implemented with stochastically spiking neurons with refractoriness . <eos> the new learning rule that achieves this is derived from abstract information optimization principles .
we introduce a game-theoretic model for network formation inspired by earlier stochastic models that mix localized and long-distance connectivity . <eos> in this model , players may purchase edges at distance d at a cost of d , and wish to minimize the sum of their edge purchases and their average distance to other players . <eos> in this model , we show there is a striking ? small world threshold phenomenon : in two dimensions , if < 2 then every nash equilibrium results in a network of constant diameter ( independent of network size ) , and if > 2 then every nash equilibrium results in a network whose diameter grows as a root of the network size , and thus is unbounded . <eos> we contrast our results with those of kleinberg [ 8 ] in a stochastic model , and empirically investigate the ? navigability of equilibrium networks . <eos> our theoretical results all generalize to higher dimensions .
maximum margin clustering was proposed lately and has shown promising performance in recent studies [ 1 , 2 ] . <eos> it extends the theory of support vector machine to unsupervised learning . <eos> despite its good performance , there are three major problems with maximum margin clustering that question its efficiency for real-world applications . <eos> first , it is computationally expensive and difficult to scale to large-scale datasets because the number of parameters in maximum margin clustering is quadratic in the number of examples . <eos> second , it requires data preprocessing to ensure that any clustering boundary will pass through the origins , which makes it unsuitable for clustering unbalanced dataset . <eos> third , it is sensitive to the choice of kernel functions , and requires external procedure to determine the appropriate values for the parameters of kernel functions . <eos> in this paper , we propose ? generalized maximum margin clustering ? <eos> framework that addresses the above three problems simultaneously . <eos> the new framework generalizes the maximum margin clustering algorithm by allowing any clustering boundaries including those not passing through the origins . <eos> it significantly improves the computational efficiency by reducing the number of parameters . <eos> furthermore , the new framework is able to automatically determine the appropriate kernel matrix without any labeled data . <eos> finally , we show a formal connection between maximum margin clustering and spectral clustering . <eos> we demonstrate the efficiency of the generalized maximum margin clustering algorithm using both synthetic datasets and real datasets from the uci repository .
finite mixture model is a powerful tool in many statistical learning problems . <eos> in this paper , we propose a general , structure-preserving approach to reduce its model complexity , which can bring significant computational benefits in many applications . <eos> the basic idea is to group the original mixture components into compact clusters , and then minimize an upper bound on the approximation error between the original and simplified models . <eos> by adopting the l2 norm as the distance measure between mixture models , we can derive closed-form solutions that are more robust and reliable than using the kl-based distance measure . <eos> moreover , the complexity of our algorithm is only linear in the sample size and dimensionality . <eos> experiments on density estimation and clustering-based image segmentation demonstrate its outstanding performance in terms of both speed and accuracy .
in many modern large-scale learning applications , the amount of unlabeled data far exceeds that of labeled data . <eos> a common instance of this problem is the transductive setting where the unlabeled test points are known to the learning algorithm . <eos> this paper presents a study of regression problems in that setting . <eos> it presents explicit vc-dimension error bounds for transductive regression that hold for all bounded loss functions and coincide with the tight classification bounds of vapnik when applied to classification . <eos> it also presents a new transductive regression algorithm inspired by our bound that admits a primal and kernelized closedform solution and deals efficiently with large amounts of unlabeled data . <eos> the algorithm exploits the position of unlabeled points to locally estimate their labels and then uses a global optimization to ensure robust predictions . <eos> our study also includes the results of experiments with several publicly available regression data sets with up to 20,000 unlabeled examples . <eos> the comparison with other transductive regression algorithms shows that it performs well and that it can scale to large data sets .
we consider the scenario where training and test data are drawn from different distributions , commonly referred to as sample selection bias . <eos> most algorithms for this setting try to first recover sampling distributions and then make appropriate corrections based on the distribution estimate . <eos> we present a nonparametric method which directly produces resampling weights without distribution estimation . <eos> our method works by matching distributions between training and testing sets in feature space . <eos> experimental results demonstrate that our method works well in practice .
image congealing ( ic ) is a non-parametric method for the joint alignment of a collection of images affected by systematic and unwanted deformations . <eos> the method attempts to undo the deformations by minimizing a measure of complexity of the image ensemble , such as the averaged per-pixel entropy . <eos> this enables alignment without an explicit model of the aligned dataset as required by other methods ( e.g . transformed component analysis ) . <eos> while ic is simple and general , it may introduce degenerate solutions when the transformations allow minimizing the complexity of the data by collapsing them to a constant . <eos> such solutions need to be explicitly removed by regularization . <eos> in this paper we propose an alternative formulation which solves this regularization issue on a more principled ground . <eos> we make the simple observation that alignment should simplify the data while preserving the useful information carried by them . <eos> therefore we trade off fidelity and complexity of the aligned ensemble rather than minimizing the complexity alone . <eos> this eliminates the need for an explicit regularization of the transformations , and has a number of other useful properties such as noise suppression . <eos> we show the modeling and computational benefits of the approach to the some of the problems on which ic has been demonstrated .
the objects in many real-world domains can be organized into hierarchies , where each internal node picks out a category of objects . <eos> given a collection of features and relations defined over a set of objects , an annotated hierarchy includes a specification of the categories that are most useful for describing each individual feature and relation . <eos> we define a generative model for annotated hierarchies and the features and relations that they describe , and develop a markov chain monte carlo scheme for learning annotated hierarchies . <eos> we show that our model discovers interpretable structure in several real-world data sets .
we propose a non-linear generative model for human motion data that uses an undirected model with binary latent variables and real-valued visiblevariables that represent joint angles . <eos> the latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps . <eos> such an architecture makes on-line inference efficient and allows us to use a simple approximate learning procedure . <eos> after training , the model finds a single set of parameters that simultaneously capture several different kinds of motion . <eos> we demonstrate the power of our approach by synthesizing various motion sequences and by performing on-line filling in of data lost during motion capture . <eos> website : http : //www.cs.toronto.edu/ ? gwtaylor/publications/nips2006mhmublv/
we present a novel algorithm called pg-means which is able to learn the number of clusters in a classical gaussian mixture model . <eos> our method is robust and efficient ; it uses statistical hypothesis tests on one-dimensional projections of the data and model to determine if the examples are well represented by the model . <eos> in so doing , we are applying a statistical test for the entire model at once , not just on a per-cluster basis . <eos> we show that our method works well in difficult cases such as non-gaussian data , overlapping clusters , eccentric clusters , high dimension , and many true clusters . <eos> further , our new method provides a much more stable estimate of the number of clusters than existing methods .
bob offers a face-detection web service where clients can submit their images for analysis . <eos> alice would very much like to use the service , but is reluctant to reveal the content of her images to bob . <eos> bob , for his part , is reluctant to release his face detector , as he spent a lot of time , energy and money constructing it . <eos> secure multiparty computations use cryptographic tools to solve this problem without leaking any information . <eos> unfortunately , these methods are slow to compute and we introduce a couple of machine learning techniques that allow the parties to solve the problem while leaking a controlled amount of information . <eos> the first method is an information-bottleneck variant of adaboost that lets bob find a subset of features that are enough for classifying an image patch , but not enough to actually reconstruct it . <eos> the second machine learning technique is active learning that allows alice to construct an online classifier , based on a small number of calls to bob ? s face detector . <eos> she can then use her online classifier as a fast rejector before using a cryptographically secure classifier on the remaining image patches .
online convex programming has recently emerged as a powerful primitive for designing machine learning algorithms . <eos> for example , ocp can be used for learning a linear classifier , dynamically rebalancing a binary search tree , finding the shortest path in a graph with unknown edge lengths , solving a structured classification problem , or finding a good strategy in an extensive-form game . <eos> several researchers have designed no-regret algorithms for ocp . <eos> but , compared to algorithms for special cases of ocp such as learning from expert advice , these algorithms are not very numerous or flexible . <eos> in learning from expert advice , one tool which has proved particularly valuable is the correspondence between no-regret algorithms and convex potential functions : by reasoning about these potential functions , researchers have designed algorithms with a wide variety of useful guarantees such as good performance when the target hypothesis is sparse . <eos> until now , there has been no such recipe for the more general ocp problem , and therefore no ability to tune ocp algorithms to take advantage of properties of the problem or data . <eos> in this paper we derive a new class of no-regret learning algorithms for ocp . <eos> these lagrangian hedging algorithms are based on a general class of potential functions , and are a direct generalization of known learning rules like weighted majority and external-regret matching . <eos> in addition to proving regret bounds , we demonstrate our algorithms learning to play one-card poker .
some of the most effective recent methods for content-based image classification work by extracting dense or sparse local image descriptors , quantizing them according to a coding rule such as k-means vector quantization , accumulating histograms of the resulting ? visual word ? <eos> codes over the image , and classifying these with a conventional classifier such as an svm . <eos> large numbers of descriptors and large codebooks are needed for good results and this becomes slow using k-means . <eos> we introduce extremely randomized clustering forests ? <eos> ensembles of randomly created clustering trees ? <eos> and show that these provide more accurate results , much faster training and testing and good resistance to background clutter in several state-of-the-art image classification tasks .
we develop a bayesian sum-of-treesmodel , named bart , where each tree is constrained by a prior to be a weak learner . <eos> fitting and inference are accomplished via an iterative backfitting mcmc algorithm . <eos> this model is motivated by ensemble methods in general , and boosting algorithms in particular . <eos> like boosting , each weak learner ( i.e. , each weak tree ) contributes a small amount to the overall model . <eos> however , our procedure is defined by a statistical model : a prior and a likelihood , while boosting is defined by an algorithm . <eos> this model-based approach enables a full and accurate assessment of uncertainty in model predictions , while remaining highly competitive in terms of predictive accuracy .
we address the problem of blind motion deblurring from a single image , caused by a few moving objects . <eos> in such situations only part of the image may be blurred , and the scene consists of layers blurred in different degrees . <eos> most of of existing blind deconvolution research concentrates at recovering a single blurring kernel for the entire image . <eos> however , in the case of different motions , the blur can not be modeled with a single kernel , and trying to deconvolve the entire image with the same kernel will cause serious artifacts . <eos> thus , the task of deblurring needs to involve segmentation of the image into regions with different blurs . <eos> our approach relies on the observation that the statistics of derivative filters in images are significantly changed by blur . <eos> assuming the blur results from a constant velocity motion , we can limit the search to one dimensional box filter blurs . <eos> this enables us to model the expected derivatives distributions as a function of the width of the blur kernel . <eos> those distributions are surprisingly powerful in discriminating regions with different blurs . <eos> the approach produces convincing deconvolution results on real world images with rich texture .
the standard support vector machine formulation does not provide its user with the ability to explicitly control the number of support vectors used to define the generated classifier . <eos> we present a modified version of svm that allows the user to set a budget parameter b and focuses on minimizing the loss attained by the b worst-classified examples while ignoring the remaining examples . <eos> this idea can be used to derive sparse versions of both l1-svm and l2-svm . <eos> technically , we obtain these new svm variants by replacing the 1-norm in the standard svm formulation with various interpolation-norms . <eos> we also adapt the smo optimization algorithm to our setting and report on some preliminary experimental results .
current road-traffic optimisation practice around the world is a combination of hand tuned policies with a small degree of automatic adaption . <eos> even state-ofthe-art research controllers need good models of the road traffic , which can not be obtained directly from existing sensors . <eos> we use a policy-gradient reinforcement learning approach to directly optimise the traffic signals , mapping currently deployed sensor observations to control signals . <eos> our trained controllers are ( theoretically ) compatible with the traffic system used in sydney and many other cities around the world . <eos> we apply two policy-gradient methods : ( 1 ) the recent natural actor-critic algorithm , and ( 2 ) a vanilla policy-gradient algorithm for comparison . <eos> along the way we extend natural-actor critic approaches to work for distributed and online infinite-horizon problems .
metric learning has been shown to significantly improve the accuracy of k-nearest neighbor ( knn ) classification . <eos> in problems involving thousands of features , distance learning algorithms can not be used due to overfitting and high computational complexity . <eos> in such cases , previous work has relied on a two-step solution : first apply dimensionality reduction methods to the data , and then learn a metric in the resulting low-dimensional subspace . <eos> in this paper we show that better classification performance can be achieved by unifying the objectives of dimensionality reduction and metric learning . <eos> we propose a method that solves for the low-dimensional projection of the inputs , which minimizes a metric objective aimed at separating points in different classes by a large margin . <eos> this projection is defined by a significantly smaller number of parameters than metrics learned in input space , and thus our optimization reduces the risks of overfitting . <eos> theory and results are presented for both a linear as well as a kernelized version of the algorithm . <eos> overall , we achieve classification rates similar , and in several cases superior , to those of support vector machines .
the ill-posed nature of the meg/eeg source localization problem requires the incorporation of prior assumptions when choosing an appropriate solution out of an infinite set of candidates . <eos> bayesian methods are useful in this capacity because they allow these assumptions to be explicitly quantified . <eos> recently , a number of empirical bayesian approaches have been proposed that attempt a form of model selection by using the data to guide the search for an appropriate prior . <eos> while seemingly quite different in many respects , we apply a unifying framework based on automatic relevance determination ( ard ) that elucidates various attributes of these methods and suggests directions for improvement . <eos> we also derive theoretical properties of this methodology related to convergence , local minima , and localization bias and explore connections with established algorithms .
attempting to model human categorization and similarity judgements is both a very interesting but also an exceedingly difficult challenge . <eos> some of the difficulty arises because of conflicting evidence whether human categorization and similarity judgements should or should not be modelled as to operate on a mental representation that is essentially metric . <eos> intuitively , this has a strong appeal as it would allow ( dis ) similarity to be represented geometrically as distance in some internal space . <eos> here we show how a single stimulus , carefully constructed in a psychophysical experiment , introduces l2 violations in what used to be an internal similarity space that could be adequately modelled as euclidean . <eos> we term this one influential data point a conflictual judgement . <eos> we present an algorithm of how to analyse such data and how to identify the crucial point . <eos> thus there may not be a strict dichotomy between either a metric or a non-metric internal space but rather degrees to which potentially large subsets of stimuli are represented metrically with a small subset causing a global violation of metricity .
we develop and analyze game-theoretic algorithms for predicting coordinate binding of multiple dna binding regulators . <eos> the allocation of proteins to local neighborhoods and to sites is carried out with resource constraints while explicating competing and coordinate binding relations among proteins with affinity to the site or region . <eos> the focus of this paper is on mathematical foundations of the approach . <eos> we also briefly demonstrate the approach in the context of the ? -phage switch .
we present new theoretical and empirical results with the ilstd algorithm for policy evaluation in reinforcement learning with linear function approximation . <eos> ilstd is an incremental method for achieving results similar to lstd , the dataefficient , least-squares version of temporal difference learning , without incurring the full cost of the lstd computation . <eos> lstd is o ( n2 ) , where n is the number of parameters in the linear function approximator , while ilstd is o ( n ) . <eos> in this paper , we generalize the previous ilstd algorithm and present three new results : ( 1 ) the first convergence proof for an ilstd algorithm ; ( 2 ) an extension to incorporate eligibility traces without changing the asymptotic computational complexity ; and ( 3 ) the first empirical results with an ilstd algorithm for a problem ( mountain car ) with feature vectors large enough ( n = 10 , 000 ) to show substantial computational advantages over lstd .
go is an ancient board game that poses unique opportunities and challenges for ai and machine learning . <eos> here we develop a machine learning approach to go , and related board games , focusing primarily on the problem of learning a good evaluation function in a scalable way . <eos> scalability is essential at multiple levels , from the library of local tactical patterns , to the integration of patterns across the board , to the size of the board itself . <eos> the system we propose is capable of automatically learning the propensity of local patterns from a library of games . <eos> propensity and other local tactical information are fed into a recursive neural network , derived from a bayesian network architecture . <eos> the network integrates local information across the board and produces local outputs that represent local territory ownership probabilities . <eos> the aggregation of these probabilities provides an effective strategic evaluation function that is an estimate of the expected area at the end ( or at other stages ) of the game . <eos> local area targets for training can be derived from datasets of human games . <eos> a system trained using only 9 9 amateur game data performs surprisingly well on a test set derived from 19 19 professional game data . <eos> possible directions for further improvements are briefly discussed .
a new bottom-up visual saliency model , graph-based visual saliency ( gbvs ) , is proposed . <eos> it consists of two steps : rst forming activation maps on certain feature channels , and then normalizing them in a way which highlights conspicuity and admits combination with other maps . <eos> the model is simple , and biologically plausible insofar as it is naturally parallelized . <eos> this model powerfully predicts human xations on 749 variations of 108 natural images , achieving 98 % of the roc area of a human-based control , whereas the classical algorithms of itti & koch ( [ 2 ] , [ 3 ] , [ 4 ] ) achieve only 84 % .
we use multi-electrode recordings from cat primary visual cortex and investigate whether a simple linear classifier can extract information about the presented stimuli . <eos> we find that information is extractable and that it even lasts for several hundred milliseconds after the stimulus has been removed . <eos> in a fast sequence of stimulus presentation , information about both new and old stimuli is present simultaneously and nonlinear relations between these stimuli can be extracted . <eos> these results suggest nonlinear properties of cortical representations . <eos> the important implications of these properties for the nonlinear brain theory are discussed .
learning by imitation represents an important mechanism for rapid acquisition of new behaviors in humans and robots . <eos> a critical requirement for learning by imitation is the ability to handle uncertainty arising from the observation process as well as the imitator ? s own dynamics and interactions with the environment . <eos> in this paper , we present a new probabilistic method for inferring imitative actions that takes into account both the observations of the teacher as well as the imitator ? s dynamics . <eos> our key contribution is a nonparametric learning method which generalizes to systems with very different dynamics . <eos> rather than relying on a known forward model of the dynamics , our approach learns a nonparametric forward model via exploration . <eos> leveraging advances in approximate inference in graphical models , we show how the learned forward model can be directly used to plan an imitating sequence . <eos> we provide experimental results for two systems : a biomechanical model of the human arm and a 25-degrees-of-freedom humanoid robot . <eos> we demonstrate that the proposed method can be used to learn appropriate motor inputs to the model arm which imitates the desired movements . <eos> a second set of results demonstrates dynamically stable full-body imitation of a human teacher by the humanoid robot .
in real-world planning problems , we must reason not only about our own goals , but about the goals of other agents with which we may interact . <eos> often these agents goals are neither completely aligned with our own nor directly opposed to them . <eos> instead there are opportunities for cooperation : by joining forces , the agents can all achieve higher utility than they could separately . <eos> but , in order to cooperate , the agents must negotiate a mutually acceptable plan from among the many possible ones , and each agent must trust that the others will follow their parts of the deal . <eos> research in multi-agent planning has often avoided the problem of making sure that all agents have an incentive to follow a proposed joint plan . <eos> on the other hand , while game theoretic algorithms handle incentives correctly , they often don ? t scale to large planning problems . <eos> in this paper we attempt to bridge the gap between these two lines of research : we present an e ? cient game-theoretic approximate planning algorithm , along with a negotiation protocol which encourages agents to compute and agree on joint plans that are fair and optimal in a sense de ? ned below . <eos> we demonstrate our algorithm and protocol on two simple robotic planning problems.1
we propose a new method for constructing hyperkenels and define two promising special cases that can be computed in closed form . <eos> these we call the gaussian and wishart hyperkernels . <eos> the former is especially attractive in that it has an interpretable regularization scheme reminiscent of that of the gaussian rbf kernel . <eos> we discuss how kernel learning can be used not just for improving the performance of classification and regression methods , but also as a stand-alone algorithm for dimensionality reduction and relational or metric learning .
we study the problem of online prediction of a noisy labeling of a graph with the perceptron . <eos> we address both label noise and concept noise . <eos> graph learning is framed as an instance of prediction on a finite set . <eos> to treat label noise we show that the hinge loss bounds derived by gentile [ 1 ] for online perceptron learning can be transformed to relative mistake bounds with an optimal leading constant when applied to prediction on a finite set . <eos> these bounds depend crucially on the norm of the learned concept . <eos> often the norm of a concept can vary dramatically with only small perturbations in a labeling . <eos> we analyze a simple transformation that stabilizes the norm under perturbations . <eos> we derive an upper bound that depends only on natural properties of the graph ? <eos> the graph diameter and the cut size of a partitioning of the graph ? <eos> which are only indirectly dependent on the size of the graph . <eos> the impossibility of such bounds for the graph geodesic nearest neighbors algorithm will be demonstrated .
this paper introduces adaptor grammars , a class of probabilistic models of language that generalize probabilistic context-free grammars ( pcfgs ) . <eos> adaptor grammars augment the probabilistic rules of pcfgs with adaptorsthat can induce dependencies among successive uses . <eos> with a particular choice of adaptor , based on the pitman-yor process , nonparametric bayesian models of language using dirichlet processes and hierarchical dirichlet processes can be written as simple grammars . <eos> we present a general-purpose inference algorithm for adaptor grammars , making it easy to define and use such models , and illustrate how several existing nonparametric bayesian models can be expressed within this framework .
many neural areas , notably , the hippocampus , show structured , dynamical , population behavior such as coordinated oscillations . <eos> it has long been observed that such oscillations provide a substrate for representing analog information in the firing phases of neurons relative to the underlying population rhythm . <eos> however , it has become increasingly clear that it is essential for neural populations to represent uncertainty about the information they capture , and the substantial recent work on neural codes for uncertainty has omitted any analysis of oscillatory systems . <eos> here , we observe that , since neurons in an oscillatory network need not only fire once in each cycle ( or even at all ) , uncertainty about the analog quantities each neuron represents by its firing phase might naturally be reported through the degree of concentration of the spikes that it fires . <eos> we apply this theory to memory in a model of oscillatory associative recall in hippocampal area ca3 . <eos> although it is not well treated in the literature , representing and manipulating uncertainty is fundamental to competent memory ; our theory enables us to view ca3 as an effective uncertainty-aware , retrieval system .
in this paper , we present a subspace method for learning nonlinear dynamical systems based on stochastic realization , in which state vectors are chosen using kernel canonical correlation analysis , and then state-space systems are identified through regression with the state vectors . <eos> we construct the theoretical underpinning and derive a concrete algorithm for nonlinear identification . <eos> the obtained algorithm needs no iterative optimization procedure and can be implemented on the basis of fast and reliable numerical schemes . <eos> the simulation result shows that our algorithm can express dynamics with a high degree of accuracy .
we describe a nonnegative variant of the ? sparse pca problem . <eos> the goal is to create a low dimensional representation from a collection of points which on the one hand maximizes the variance of the projected points and on the other uses only parts of the original coordinates , and thereby creating a sparse representation . <eos> what distinguishes our problem from other sparse pca formulations is that the projection involves only nonnegative weights of the original coordinates a desired quality in various fields , including economics , bioinformatics and computer vision . <eos> adding nonnegativity contributes to sparseness , where it enforces a partitioning of the original coordinates among the new axes . <eos> we describe a simple yet efficient iterative coordinate-descent type of scheme which converges to a local optimum of our optimization criteria , giving good results on large real world datasets .
we present a probabilistic model applied to the fmri video rating prediction task of the pittsburgh brain activity interpretation competition ( pbaic ) [ 2 ] . <eos> our goal is to predict a time series of subjective , semantic ratings of a movie given functional mri data acquired during viewing by three subjects . <eos> our method uses conditionally trained gaussian markov random fields , which model both the relationships between the subjects ? <eos> fmri voxel measurements and the ratings , as well as the dependencies of the ratings across time steps and between subjects . <eos> we also employed non-traditional methods for feature selection and regularization that exploit the spatial structure of voxel activity in the brain . <eos> the model displayed good performance in predicting the scored ratings for the three subjects in test data sets , and a variant of this model was the third place entrant to the 2006 pbaic .
the neurons of the neocortex communicate by asynchronous events called action potentials ( or ? spikes ? ) . <eos> however , for simplicity of simulation , most models of processing by cortical neural networks have assumed that the activations of their neurons can be approximated by event rates rather than taking account of individual spikes . <eos> the obstacle to exploring the more detailed spike processing of these networks has been reduced considerably in recent years by the development of hybrid analog-digital very-large scale integrated ( hvlsi ) neural networks composed of spiking neurons that are able to operate in real-time . <eos> in this paper we describe such a hvlsi neural network that performs an interesting task of selective attentional processing that was previously described for a simulated pointer-maprate model by hahnloser and colleagues . <eos> we found that most of the computational features of their rate model can be reproduced in the spiking implementation ; but , that spike-based processing requires a modification of the original network architecture in order to memorize a previously attended target .
we describe an algorithmic framework for an abstract game which we term a convex repeated game . <eos> we show that various online learning and boosting algorithms can be all derived as special cases of our algorithmic framework . <eos> this unified view explains the properties of existing algorithms and also enables us to derive several new interesting algorithms . <eos> our algorithmic framework stems from a connection that we build between the notions of regret in game theory and weak duality in convex optimization .
correlation between instances is often modelled via a kernel function using input attributes of the instances . <eos> relational knowledge can further reveal additional pairwise correlations between variables of interest . <eos> in this paper , we develop a class of models which incorporates both reciprocal relational information and input attributes using gaussian process techniques . <eos> this approach provides a novel non-parametric bayesian framework with a data-dependent covariance function for supervised learning tasks . <eos> we also apply this framework to semi-supervised learning . <eos> experimental results on several real world data sets verify the usefulness of this algorithm .
this paper presents an algorithm for synthesis of human motion in specified styles . <eos> we use a theory of movement observation ( laban movement analysis ) to describe movement styles as points in a multi-dimensional perceptual space . <eos> we cast the task of learning to synthesize desired movement styles as a regression problem : sequences generated via space-time interpolation of motion capture data are used to learn a nonlinear mapping between animation parameters and movement styles in perceptual space . <eos> we demonstrate that the learned model can apply a variety of motion styles to pre-recorded motion sequences and it can extrapolate styles not originally included in the training data .
we propose two statistical tests to determine if two samples are from different distributions . <eos> our test statistic is in both cases the distance between the means of the two samples mapped into a reproducing kernel hilbert space ( rkhs ) . <eos> the first test is based on a large deviation bound for the test statistic , while the second is based on the asymptotic distribution of this statistic . <eos> the test statistic can be computed in o ( m2 ) time . <eos> we apply our approach to a variety of problems , including attribute matching for databases using the hungarian marriage method , where our test performs strongly . <eos> we also demonstrate excellent performance when comparing distributions over graphs , for which no alternative tests currently exist .
this work presents a method for estimating human facial attractiveness , based on supervised learning techniques . <eos> numerous facial features that describe facial geometry , color and texture , combined with an average human attractiveness score for each facial image , are used to train various predictors . <eos> facial attractiveness ratings produced by the final predictor are found to be highly correlated with human ratings , markedly improving previous machine learning achievements . <eos> simulated psychophysical experiments with virtually manipulated images reveal preferences in the machine 's judgments which are remarkably similar to those of humans . <eos> these experiments shed new light on existing theories of facial attractiveness such as the averageness , smoothness and symmetry hypotheses . <eos> it is intriguing to find that a machine trained explicitly to capture an operational performance criteria such as attractiveness rating , implicitly captures basic human psychophysical biases characterizing the perception of facial attractiveness in general .
we describe a novel unsupervised method for learning sparse , overcomplete features . <eos> the model uses a linear encoder , and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector . <eos> given an input , the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output . <eos> learning proceeds in a two-phase em-like fashion : ( 1 ) compute the minimum-energy code vector , ( 2 ) adjust the parameters of the encoder and decoder so as to decrease the energy . <eos> the model produces ? stroke detectors ? <eos> when trained on handwritten numerals , and gabor-like filters when trained on natural image patches . <eos> inference and learning are very fast , requiring no preprocessing , and no expensive sampling . <eos> using the proposed unsupervised method to initialize the first layer of a convolutional network , we achieved an error rate slightly lower than the best reported result on the mnist dataset . <eos> finally , an extension of the method is described to learn topographical filter maps .
latent dirichlet allocation ( lda ) is a bayesian network that has recently gained much popularity in applications ranging from document modeling to computer vision . <eos> due to the large scale nature of these applications , current inference procedures like variational bayes and gibbs sampling have been found lacking . <eos> in this paper we propose the collapsed variational bayesian inference algorithm for lda , and show that it is computationally efficient , easy to implement and significantly more accurate than standard variational bayesian inference for lda .
efficient coding models predict that the optimal code for natural images is a population of oriented gabor receptive fields . <eos> these results match response properties of neurons in primary visual cortex , but not those in the retina . <eos> does the retina use an optimal code , and if so , what is it optimized for ? <eos> previous theories of retinal coding have assumed that the goal is to encode the maximal amount of information about the sensory signal . <eos> however , the image sampled by retinal photoreceptors is degraded both by the optics of the eye and by the photoreceptor noise . <eos> therefore , de-blurring and de-noising of the retinal signal should be important aspects of retinal coding . <eos> furthermore , the ideal retinal code should be robust to neural noise and make optimal use of all available neurons . <eos> here we present a theoretical framework to derive codes that simultaneously satisfy all of these desiderata . <eos> when optimized for natural images , the model yields filters that show strong similarities to retinal ganglion cell ( rgc ) receptive fields . <eos> importantly , the characteristics of receptive fields vary with retinal eccentricities where the optical blur and the number of rgcs are significantly different . <eos> the proposed model provides a unified account of retinal coding , and more generally , it may be viewed as an extension of the wiener filter with an arbitrary number of noisy units .
we present a local learning approach for clustering . <eos> the basic idea is that a good clustering result should have the property that the cluster label of each data point can be well predicted based on its neighboring data and their cluster labels , using current supervised learning methods . <eos> an optimization problem is formulated such that its solution has the above property . <eos> relaxation and eigen-decomposition are applied to solve this optimization problem . <eos> we also briefly investigate the parameter selection issue and provide a simple parameter selection method for the proposed algorithm . <eos> experimental results are provided to validate the effectiveness of the proposed approach .
we phrase k-means clustering as an empirical risk minimization procedure over a class hk and explicitly calculate the covering number for this class . <eos> next , we show that stability of k-means clustering is characterized by the geometry of hk with respect to the underlying distribution . <eos> we prove that in the case of a unique global minimizer , the clustering solution is stable with respect to complete changes of the data , while for the case of multiple minimizers , the change of ? <eos> ( n1/2 ) samples defines the transition between stability and instability . <eos> while for a finite number of minimizers this result follows from multinomial distribution estimates , the case of infinite minimizers requires more refined tools . <eos> we conclude by proving that stability of the functions in hk implies stability of the actual centers of the clusters . <eos> since stability is often used for selecting the number of clusters in practice , we hope that our analysis serves as a starting point for finding theoretically grounded recipes for the choice of k .
in general , the problem of computing a maximum a posteriori ( map ) assignment in a markov random field ( mrf ) is computationally intractable . <eos> however , in certain subclasses of mrf , an optimal or close-to-optimal assignment can be found very efficiently using combinatorial optimization algorithms : certain mrfs with mutual exclusion constraints can be solved using bipartite matching , and mrfs with regular potentials can be solved using minimum cut methods . <eos> however , these solutions do not apply to the many mrfs that contain such tractable components as sub-networks , but also other non-complying potentials . <eos> in this paper , we present a new method , called c ompose , for exploiting combinatorial optimization for sub-networks within the context of a max-product belief propagation algorithm . <eos> c ompose uses combinatorial optimization for computing exact maxmarginals for an entire sub-network ; these can then be used for inference in the context of the network as a whole . <eos> we describe highly efficient methods for computing max-marginals for subnetworks corresponding both to bipartite matchings and to regular networks . <eos> we present results on both synthetic and real networks encoding correspondence problems between images , which involve both matching constraints and pairwise geometric constraints . <eos> we compare to a range of current methods , showing that the ability of c ompose to transmit information globally across the network leads to improved convergence , decreased running time , and higher-scoring assignments .
despite all the attention paid to variational methods based on sum-product message passing ( loopy belief propagation , tree-reweighted sum-product ) , these methods are still bound to inference on a small set of probabilistic models . <eos> mean field approximations have been applied to a broader set of problems , but the solutions are often poor . <eos> we propose a new class of conditionally-specified variational approximations based on mean field theory . <eos> while not usable on their own , combined with sequential monte carlo they produce guaranteed improvements over conventional mean field . <eos> moreover , experiments on a well-studied problem ? <eos> inferring the stable configurations of the ising spin glass ? show that the solutions can be significantly better than those obtained using sum-product-based methods .
modelling the dynamics of transcriptional processes in the cell requires the knowledge of a number of key biological quantities . <eos> while some of them are relatively easy to measure , such as mrna decay rates and mrna abundance levels , it is still very hard to measure the active concentration levels of the transcription factor proteins that drive the process and the sensitivity of target genes to these concentrations . <eos> in this paper we show how these quantities for a given transcription factor can be inferred from gene expression levels of a set of known target genes . <eos> we treat the protein concentration as a latent function with a gaussian process prior , and include the sensitivities , mrna decay rates and baseline expression levels as hyperparameters . <eos> we apply this procedure to a human leukemia dataset , focusing on the tumour repressor p53 and obtaining results in good accordance with recent biological studies .
we provide a pac-bayesian bound for the expected loss of convex combinations of classifiers under a wide class of loss functions ( which includes the exponential loss and the logistic loss ) . <eos> our numerical experiments with adaboost indicate that the proposed upper bound , computed on the training set , behaves very similarly as the true loss estimated on the testing set .
we propose a new kernel-based data transformation technique . <eos> it is founded on the principle of maximum entropy ( maxent ) preservation , hence named kernel maxent . <eos> the key measure is renyi ? s entropy estimated via parzen windowing . <eos> we show that kernel maxent is based on eigenvectors , and is in that sense similar to kernel pca , but may produce strikingly different transformed data sets . <eos> an enhanced spectral clustering algorithm is proposed , by replacing kernel pca by kernel maxent as an intermediate step . <eos> this has a major impact on performance .
this paper addresses the bottom-up influence of local image information on human eye movements . <eos> most existing computational models use a set of biologically plausible linear filters , e.g. , gabor or difference-of-gaussians filters as a front-end , the outputs of which are nonlinearly combined into a real number that indicates visual saliency . <eos> unfortunately , this requires many design parameters such as the number , type , and size of the front-end filters , as well as the choice of nonlinearities , weighting and normalization schemes etc. , for which biological plausibility can not always be justified . <eos> as a result , these parameters have to be chosen in a more or less ad hoc way . <eos> here , we propose to learn a visual saliency model directly from human eye movement data . <eos> the model is rather simplistic and essentially parameter-free , and therefore contrasts recent developments in the field that usually aim at higher prediction rates at the cost of additional parameters and increasing model complexity . <eos> experimental results show that ? despite the lack of any biological prior knowledge ? our model performs comparably to existing approaches , and in fact learns image features that resemble findings from several previous studies . <eos> in particular , its maximally excitatory stimuli have center-surround structure , similar to receptive fields in the early human visual system .
in biological neurons , the timing of a spike depends on the timing of synaptic currents , in a way that is classically described by the phase response curve . <eos> this has implications for temporal coding : an action potential that arrives on a synapse has an implicit meaning , that depends on the position of the postsynaptic neuron on the firing cycle . <eos> here we show that this implicit code can be used to perform computations . <eos> using theta neurons , we derive a spike-timing dependent learning rule from an error criterion . <eos> we demonstrate how to train an auto-encoder neural network using this rule .
many computer aided diagnosis ( cad ) problems can be best modelled as a multiple-instance learning ( mil ) problem with unbalanced data : i.e . , the training data typically consists of a few positive bags , and a very large number of negative instances . <eos> existing mil algorithms are much too computationally expensive for these datasets . <eos> we describe ch , a framework for learning a convex hull representation of multiple instances that is significantly faster than existing mil algorithms . <eos> our ch framework applies to any standard hyperplane-based learning algorithm , and for some algorithms , is guaranteed to find the global optimal solution . <eos> experimental studies on two different cad applications further demonstrate that the proposed algorithm significantly improves diagnostic accuracy when compared to both mil and traditional classifiers . <eos> although not designed for standard mil problems ( which have both positive and negative bags and relatively balanced datasets ) , comparisons against other mil methods on benchmark problems also indicate that the proposed method is competitive with the state-of-the-art .
we present a reduction framework from ordinal regression to binary classification based on extended examples . <eos> the framework consists of three steps : extracting extended examples from the original examples , learning a binary classifier on the extended examples with any binary classification algorithm , and constructing a ranking rule from the binary classifier . <eos> a weighted 0/1 loss of the binary classifier would then bound the mislabeling cost of the ranking rule . <eos> our framework allows not only to design good ordinal regression algorithms based on well-tuned binary classification approaches , but also to derive new generalization bounds for ordinal regression from known bounds for binary classification . <eos> in addition , our framework unifies many existing ordinal regression algorithms , such as perceptron ranking and support vector ordinal regression . <eos> when compared empirically on benchmark data sets , some of our newly designed algorithms enjoy advantages in terms of both training speed and generalization performance over existing algorithms , which demonstrates the usefulness of our framework .
in this paper we introduce and experiment with a framework for learning local perceptual distance functions for visual recognition . <eos> we learn a distance function for each training image as a combination of elementary distances between patch-based visual features . <eos> we apply these combined local distance functions to the tasks of image retrieval and classification of novel images . <eos> on the caltech 101 object recognition benchmark , we achieve 60.3 % mean recognition across classes using 15 training images per class , which is better than the best published performance by zhang , et al .
we propose a new approach for measuring similarity between two signals , which is applicable to many machine learning tasks , and to many signal types . <eos> we say that a signal s1 is ? similar to a signal s2 if it is ? easy to compose s1 from few large contiguous chunks of s2 . <eos> obviously , if we use small enough pieces , then any signal can be composed of any other . <eos> therefore , the larger those pieces are , the more similar s1 is to s2 . <eos> this induces a local similarity score at every point in the signal , based on the size of its supported surrounding region . <eos> these local scores can in turn be accumulated in a principled information-theoretic way into a global similarity score of the entire s1 to s2 . <eos> ? similarity by composition can be applied between pairs of signals , between groups of signals , and also between different portions of the same signal . <eos> it can therefore be employed in a wide variety of machine learning problems ( clustering , classification , retrieval , segmentation , attention , saliency , labelling , etc . <eos> ) , and can be applied to a wide range of signal types ( images , video , audio , biological data , etc . ) <eos> we show a few such examples .
we usually endow the investigated objects with pairwise relationships , which can be illustrated as graphs . <eos> in many real-world problems , however , relationships among the objects of our interest are more complex than pairwise . <eos> naively squeezing the complex relationships into pairwise ones will inevitably lead to loss of information which can be expected valuable for our learning tasks however . <eos> therefore we consider using hypergraphs instead to completely represent complex relationships among the objects of our interest , and thus the problem of learning with hypergraphs arises . <eos> our main contribution in this paper is to generalize the powerful methodology of spectral clustering which originally operates on undirected graphs to hypergraphs , and further develop algorithms for hypergraph embedding and transductive classification on the basis of the spectral hypergraph clustering approach . <eos> our experiments on a number of benchmarks showed the advantages of hypergraphs over usual graphs .
if language users are rational , they might choose to structure their utterances so as to optimize communicative properties . <eos> in particular , information-theoretic and psycholinguistic considerations suggest that this may include maximizing the uniformity of information density in an utterance . <eos> we investigate this possibility in the context of syntactic reduction , where the speaker has the option of either marking a higher-order unit ( a phrase ) with an extra word , or leaving it unmarked . <eos> we demonstrate that speakers are more likely to reduce less information-dense phrases . <eos> in a second step , we combine a stochastic model of structured utterance production with a logistic-regression model of syntactic reduction to study which types of cues speakers employ when estimating the predictability of upcoming elements . <eos> we demonstrate that the trend toward predictability-sensitive syntactic reduction ( jaeger , 2006 ) is robust in the face of a wide variety of control variables , and present evidence that speakers use both surface and structural cues for predictability estimation .
in this paper , application of sparse representation ( factorization ) of signals over an overcomplete basis ( dictionary ) for signal classification is discussed . <eos> searching for the sparse representation of a signal over an overcomplete dictionary is achieved by optimizing an objective function that includes two terms : one that measures the signal reconstruction error and another that measures the sparsity . <eos> this objective function works well in applications where signals need to be reconstructed , like coding and denoising . <eos> on the other hand , discriminative methods , such as linear discriminative analysis ( lda ) , are better suited for classification tasks . <eos> however , discriminative methods are usually sensitive to corruption in signals due to lacking crucial properties for signal reconstruction . <eos> in this paper , we present a theoretical framework for signal classification with sparse representation . <eos> the approach combines the discrimination power of the discriminative methods with the reconstruction property and the sparsity of the sparse representation that enables one to deal with signal corruptions : noise , missing data and outliers . <eos> the proposed approach is therefore capable of robust classification with a sparse representation of signals . <eos> the theoretical results are demonstrated with signal classification tasks , showing that the proposed approach outperforms the standard discriminative methods and the standard sparse representation in the case of corrupted signals .
we address the problem of sub-ordinate class recognition , like the distinction between different types of motorcycles . <eos> our approach is motivated by observations from cognitive psychology , which identify parts as the defining component of basic level categories ( like motorcycles ) , while sub-ordinate categories are more often defined by part properties ( like ? jagged wheels ? ) . <eos> accordingly , we suggest a two-stage algorithm : first , a relational part based object model is learnt using unsegmented object images from the inclusive class ( e.g. , motorcycles in general ) . <eos> the model is then used to build a class-specific vector representation for images , where each entry corresponds to a model ? s part . <eos> in the second stage we train a standard discriminative classifier to classify subclass instances ( e.g. , cross motorcycles ) based on the class-specific vector representation . <eos> we describe extensive experimental results with several subclasses . <eos> the proposed algorithm typically gives better results than a competing one-step algorithm , or a two stage algorithm where classification is based on a model of the sub-ordinate class .
the locally linear embedding ( lle ) is improved by introducing multiple linearly independent local weight vectors for each neighborhood . <eos> we characterize the reconstruction weights and show the existence of the linearly independent weight vectors at each neighborhood . <eos> the modified locally linear embedding ( mlle ) proposed in this paper is much stable . <eos> it can retrieve the ideal embedding if mlle is applied on data points sampled from an isometric manifold . <eos> mlle is also compared with the local tangent space alignment ( ltsa ) . <eos> numerical examples are given that show the improvement and efficiency of mlle .
the performance of eeg-based brain-computer-interfaces ( bcis ) critically depends on the extraction of features from the eeg carrying information relevant for the classification of different mental states . <eos> for bcis employing imaginary movements of different limbs , the method of common spatial patterns ( csp ) has been shown to achieve excellent classification results . <eos> the csp-algorithm however suffers from a lack of robustness , requiring training data without artifacts for good performance . <eos> to overcome this lack of robustness , we propose an adaptive spatial filter that replaces the training data in the csp approach by a-priori information . <eos> more specifically , we design an adaptive spatial filter that maximizes the ratio of the variance of the electric field originating in a predefined region of interest ( roi ) and the overall variance of the measured eeg . <eos> since it is known that the component of the eeg used for discriminating imaginary movements originates in the motor cortex , we design two adaptive spatial filters with the rois centered in the hand areas of the left and right motor cortex . <eos> we then use these to classify eeg data recorded during imaginary movements of the right and left hand of three subjects , and show that the adaptive spatial filters outperform the csp-algorithm , enabling classification rates of up to 94.7 % without artifact rejection .
we propose a novel framework for the classification of single trial electroencephalography ( eeg ) , based on regularized logistic regression . <eos> framed in this robust statistical framework no prior feature extraction or outlier removal is required . <eos> we present two variations of parameterizing the regression function : ( a ) with a full rank symmetric matrix coefficient and ( b ) as a difference of two rank=1 matrices . <eos> in the first case , the problem is convex and the logistic regression is optimal under a generative model . <eos> the latter case is shown to be related to the common spatial pattern ( csp ) algorithm , which is a popular technique in brain computer interfacing . <eos> the regression coefficients can also be topographically mapped onto the scalp similarly to csp projections , which allows neuro-physiological interpretation . <eos> simulations on 162 bci datasets demonstrate that classification accuracy and robustness compares favorably against conventional csp based classifiers .
semi-supervised svms ( s3 vm ) attempt to learn low-density separators by maximizing the margin over labeled and unlabeled examples . <eos> the associated optimization problem is non-convex . <eos> to examine the full potential of s3 vms modulo local minima problems in current implementations , we apply branch and bound techniques for obtaining exact , globally optimal solutions . <eos> empirical evidence suggests that the globally optimal solution can return excellent generalization performance in situations where other implementations fail completely . <eos> while our current implementation is only applicable to small datasets , we discuss variants that can potentially lead to practically useful algorithms .
the additive clustering model is widely used to infer the features of a set of stimuli from their similarities , on the assumption that similarity is a weighted linear function of common features . <eos> this paper develops a fully bayesian formulation of the additive clustering model , using methods from nonparametric bayesian statistics to allow the number of features to vary . <eos> we use this to explore several approaches to parameter estimation , showing that the nonparametric bayesian approach provides a straightforward way to obtain estimates of both the number of features used in producing similarity judgments and their importance .
gaussian data is pervasive and many learning algorithms ( e.g. , k-means ) model their inputs as a single sample drawn from a multivariate gaussian . <eos> however , in many real-life settings , each input object is best described by multiple samples drawn from a multivariate gaussian . <eos> such data can arise , for example , in a movie review database where each movie is rated by several users , or in time-series domains such as sensor networks . <eos> here , each input can be naturally described by both a mean vector and covariance matrix which parameterize the gaussian distribution . <eos> in this paper , we consider the problem of clustering such input objects , each represented as a multivariate gaussian . <eos> we formulate the problem using an information theoretic approach and draw several interesting theoretical connections to bregman divergences and also bregman matrix divergences . <eos> we evaluate our method across several domains , including synthetic data , sensor network data , and a statistical debugging application .
we focus on the problem of estimating the graph structure associated with a discrete markov random field . <eos> we describe a method based on ` 1 regularized logistic regression , in which the neighborhood of any given node is estimated by performing logistic regression subject to an ` 1 -constraint . <eos> our framework applies to the high-dimensional setting , in which both the number of nodes p and maximum neighborhood sizes d are allowed to grow as a function of the number of observations n. our main result is to establish sufficient conditions on the triple ( n , p , d ) for the method to succeed in consistently estimating the neighborhood of every node in the graph simultaneously . <eos> under certain mutual incoherence conditions analogous to those imposed in previous work on linear regression , we prove that consistent neighborhood selection can be obtained as long as the number of observations n grows more quickly than 6d6 log d + 2d5 log p , thereby establishing that logarithmic growth in the number of samples n relative to graph size p is sufficient to achieve neighborhood consistency . <eos> keywords : graphical models ; markov random fields ; structure learning ; ` 1 -regularization ; model selection ; convex risk minimization ; high-dimensional asymptotics ; concentration .
parsing and translating natural languages can be viewed as problems of predicting tree structures . <eos> for machine learning approaches to these predictions , the diversity and high dimensionality of the structures involved mandate very large training sets . <eos> this paper presents a purely discriminative learning method that scales up well to problems of this size . <eos> its accuracy was at least as good as other comparable methods on a standard parsing task . <eos> to our knowledge , it is the first purely discriminative learning algorithm for translation with treestructured models . <eos> unlike other popular methods , this method does not require a great deal of feature engineering a priori , because it performs feature selection over a compound feature space as it learns . <eos> experiments demonstrate the method ? s versatility , accuracy , and efficiency . <eos> relevant software is freely available at http : //nlp.cs.nyu.edu/parser and http : //nlp.cs.nyu.edu/genpar .
the time-histogram method is a handy tool for capturing the instantaneous rate of spike occurrence . <eos> in most of the neurophysiological literature , the bin size that critically determines the goodness of the fit of the time-histogram to the underlying rate has been selected by individual researchers in an unsystematic manner . <eos> we propose an objective method for selecting the bin size of a time-histogram from the spike data , so that the time-histogram best approximates the unknown underlying rate . <eos> the resolution of the histogram increases , or the optimal bin size decreases , with the number of spike sequences sampled . <eos> it is notable that the optimal bin size diverges if only a small number of experimental trials are available from a moderately fluctuating rate process . <eos> in this case , any attempt to characterize the underlying spike rate will lead to spurious results . <eos> given a paucity of data , our method can also suggest how many more trials are needed until the set of data can be analyzed with the required resolution .
many recent studies analyze how data from different modalities can be combined . <eos> often this is modeled as a system that optimally combines several sources of information about the same variable . <eos> however , it has long been realized that this information combining depends on the interpretation of the data . <eos> two cues that are perceived by different modalities can have different causal relationships : ( 1 ) they can both have the same cause , in this case we should fully integrate both cues into a joint estimate . <eos> ( 2 ) they can have distinct causes , in which case information should be processed independently . <eos> in many cases we will not know if there is one joint cause or two independent causes that are responsible for the cues . <eos> here we model this situation as a bayesian estimation problem . <eos> we are thus able to explain some experiments on visual auditory cue combination as well as some experiments on visual proprioceptive cue integration . <eos> our analysis shows that the problem solved by people when they combine cues to produce a movement is much more complicated than is usually assumed , because they need to infer the causal structure that is underlying their sensory experience .
a central question in game theory and artificial intelligence is how a rational agent should behave in a complex environment , given that it can not perform unbounded computations . <eos> we study strategic aspects of this question by formulating a simple model of a game with additional costs ( computational or otherwise ) for each strategy . <eos> first we connect this to zero-sum games , proving a counter-intuitive generalization of the classic min-max theorem to zero-sum games with the addition of strategy costs . <eos> we then show that potential games with strategy costs remain potential games . <eos> both zero-sum and potential games with strategy costs maintain a very appealing property : simple learning dynamics converge to equilibrium .
we present a method for learning a low-dimensional representation which is shared across a set of multiple related tasks . <eos> the method builds upon the wellknown 1-norm regularization problem using a new regularizer which controls the number of learned features common for all the tasks . <eos> we show that this problem is equivalent to a convex optimization problem and develop an iterative algorithm for solving it . <eos> the algorithm has a simple interpretation : it alternately performs a supervised and an unsupervised step , where in the latter step we learn commonacross-tasks representations and in the former step we learn task-specific functions using these representations . <eos> we report experiments on a simulated and a real data set which demonstrate that the proposed method dramatically improves the performance relative to learning each task independently . <eos> our algorithm can also be used , as a special case , to simply select not learn a few common features across the tasks .
we design an on-line algorithm for principal component analysis . <eos> in each trial the current instance is projected onto a probabilistically chosen low dimensional subspace . <eos> the total expected quadratic approximation error equals the total quadratic approximation error of the best subspace chosen in hindsight plus some additional term that grows linearly in dimension of the subspace but logarithmically in the dimension of the instances .
we introduce a novel adaptive non-parametric anomaly detection approach , called gem , that is based on the minimal covering properties of k-point entropic graphs when constructed on n training samples from a nominal probability distribution . <eos> such graphs have the property that as n their span recovers the entropy minimizing set that supports at least = k/n ( 100 ) % of the mass of the lebesgue part of the distribution . <eos> when a test sample falls outside of the entropy minimizing set an anomaly can be declared at a statistical level of significance = 1 ? . <eos> a method for implementing this non-parametric anomaly detector is proposed that approximates this minimum entropy set by the influence region of a k-point entropic graph built on the training data . <eos> by implementing an incremental leave-one-out k-nearest neighbor graph on resampled subsets of the training data gem can efficiently detect outliers at a given level of significance and compute their empirical p-values . <eos> we illustrate gem for several simulated and real data sets in high dimensional feature spaces .
correspondence algorithms typically struggle with shapes that display part-based variation . <eos> we present a probabilistic approach that matches shapes using independent part transformations , where the parts themselves are learnt during matching . <eos> ideas from semi-supervised learning are used to bias the algorithm towards finding ? perceptually valid part structures . <eos> shapes are represented by unlabeled point sets of arbitrary size and a background component is used to handle occlusion , local dissimilarity and clutter . <eos> thus , unlike many shape matching techniques , our approach can be applied to shapes extracted from real images . <eos> model parameters are estimated using an em algorithm that alternates between finding a soft correspondence and computing the optimal part transformations using procrustes analysis .
many unsupervised learning problems can be expressed as a form of matrix factorization , reconstructing an observed data matrix as the product of two matrices of latent variables . <eos> a standard challenge in solving these problems is determining the dimensionality of the latent matrices . <eos> nonparametric bayesian matrix factorization is one way of dealing with this challenge , yielding a posterior distribution over possible factorizations of unbounded dimensionality . <eos> a drawback to this approach is that posterior estimation is typically done using gibbs sampling , which can be slow for large problems and when conjugate priors can not be used . <eos> as an alternative , we present a particle filter for posterior estimation in nonparametric bayesian matrix factorization models . <eos> we illustrate this approach with two matrix factorization models and show favorable performance relative to gibbs sampling .
we consider a general form of transductive learning on graphs with laplacian regularization , and derive margin-based generalization bounds using appropriate geometric properties of the graph . <eos> we use this analysis to obtain a better understanding of the role of normalization of the graph laplacian matrix as well as the effect of dimension reduction . <eos> the results suggest a limitation of the standard degree-based normalization . <eos> we propose a remedy from our analysis and demonstrate empirically that the remedy leads to improved classification performance .
scoring structures of undirected graphical models by means of evaluating the marginal likelihood is very hard . <eos> the main reason is the presence of the partition function which is intractable to evaluate , let alone integrate over . <eos> we propose to approximate the marginal likelihood by employing two levels of approximation : we assume normality of the posterior ( the laplace approximation ) and approximate all remaining intractable quantities using belief propagation and the linear response approximation . <eos> this results in a fast procedure for model scoring . <eos> empirically , we find that our procedure has about two orders of magnitude better accuracy than standard bic methods for small datasets , but deteriorates when the size of the dataset grows .
we are at the beginning of the multicore era . <eos> computers will have increasingly many cores ( processors ) , but there is still no good programming framework for these architectures , and thus no simple and unified way for machine learning to take advantage of the potential speed up . <eos> in this paper , we develop a broadly applicable parallel programming method , one that is easily applied to many different learning algorithms . <eos> our work is in distinct contrast to the tradition in machine learning of designing ( often ingenious ) ways to speed up a single algorithm at a time . <eos> specifically , we show that algorithms that fit the statistical query model [ 15 ] can be written in a certain ? summation form , ? <eos> which allows them to be easily parallelized on multicore computers . <eos> we adapt google ? s map-reduce [ 7 ] paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression ( lwlr ) , k-means , logistic regression ( lr ) , naive bayes ( nb ) , svm , ica , pca , gaussian discriminant analysis ( gda ) , em , and backpropagation ( nn ) . <eos> our experimental results show basically linear speedup with an increasing number of processors .
autonomous helicopter flight is widely regarded to be a highly challenging control problem . <eos> this paper presents the first successful autonomous completion on a real rc helicopter of the following four aerobatic maneuvers : forward flip and sideways roll at low speed , tail-in funnel , and nose-in funnel . <eos> our experimental results significantly extend the state of the art in autonomous helicopter flight . <eos> we used the following approach : first we had a pilot fly the helicopter to help us find a helicopter dynamics model and a reward ( cost ) function . <eos> then we used a reinforcement learning ( optimal control ) algorithm to find a controller that is optimized for the resulting model and reward function . <eos> more specifically , we used differential dynamic programming ( ddp ) , an extension of the linear quadratic regulator ( lqr ) .
we examine the problem of predicting local sentiment flow in documents , and its application to several areas of text analysis . <eos> formally , the problem is stated as predicting an ordinal sequence based on a sequence of word sets . <eos> in the spirit of isotonic regression , we develop a variant of conditional random fields that is wellsuited to handle this problem . <eos> using the m ? obius transform , we express the model as a simple convex optimization problem . <eos> experiments demonstrate the model and its applications to sentiment prediction , style analysis , and text summarization .
we rigorously establish a close relationship between message passing algorithms and models of neurodynamics by showing that the equations of a continuous hopfield network can be derived from the equations of belief propagation on a binary markov random field . <eos> as hopfield networks are equipped with a lyapunov function , convergence is guaranteed . <eos> as a consequence , in the limit of many weak connections per neuron , hopfield networks exactly implement a continuous-time variant of belief propagation starting from message initialisations that prevent from running into convergence problems . <eos> our results lead to a better understanding of the role of message passing algorithms in real biological neural networks .
the maximum margin planning ( mmp ) ( ratliff et al. , 2006 ) algorithm solves imitation learning problems by learning linear mappings from features to cost functions in a planning domain . <eos> the learned policy is the result of minimum-cost planning using these cost functions . <eos> these mappings are chosen so that example policies ( or trajectories ) given by a teacher appear to be lower cost ( with a lossscaled margin ) than any other policy for a given planning domain . <eos> we provide a novel approach , m mp b oost , based on the functional gradient descent view of boosting ( mason et al. , 1999 ; friedman , 1999a ) that extends mmp by ? boosting in new features . <eos> this approach uses simple binary classification or regression to improve performance of mmp imitation learning , and naturally extends to the class of structured maximum margin prediction problems . <eos> ( taskar et al. , 2005 ) our technique is applied to navigation and planning problems for outdoor mobile robots and robotic legged locomotion .
multinomial logistic regression provides the standard penalised maximumlikelihood solution to multi-class pattern recognition problems . <eos> more recently , the development of sparse multinomial logistic regression models has found application in text processing and microarray classification , where explicit identification of the most informative features is of value . <eos> in this paper , we propose a sparse multinomial logistic regression method , in which the sparsity arises from the use of a laplace prior , but where the usual regularisation parameter is integrated out analytically . <eos> evaluation over a range of benchmark datasets reveals this approach results in similar generalisation performance to that obtained using cross-validation , but at greatly reduced computational expense .
we consider the problem of network anomaly detection in large distributed systems . <eos> in this setting , principal component analysis ( pca ) has been proposed as a method for discovering anomalies by continuously tracking the projection of the data onto a residual subspace . <eos> this method was shown to work well empirically in highly aggregated networks , that is , those with a limited number of large nodes and at coarse time scales . <eos> this approach , however , has scalability limitations . <eos> to overcome these limitations , we develop a pca-based anomaly detector in which adaptive local data filters send to a coordinator just enough data to enable accurate global detection . <eos> our method is based on a stochastic matrix perturbation analysis that characterizes the tradeoff between the accuracy of anomaly detection and the amount of data communicated over the network .
we present a method for binary on-line classification of triggered but temporally blurred events that are embedded in noisy time series in the context of on-line discrimination between left and right imaginary hand-movement . <eos> in particular the goal of the binary classification problem is to obtain the decision , as fast and as reliably as possible from the recorded eeg single trials . <eos> to provide a probabilistic decision at every time-point t the presented method gathers information from two distinct sequences of features across time . <eos> in order to incorporate decisions from prior time-points we suggest an appropriate weighting scheme , that emphasizes time instances , providing a higher discriminatory power between the instantaneous class distributions of each feature , where the discriminatory power is quantified in terms of the bayes error of misclassification . <eos> the effectiveness of this procedure is verified by its successful application in the 3rd bci competition . <eos> disclosure of the data after the competition revealed this approach to be superior with single trial error rates as low as 10.7 , 11.5 and 16.7 % for the three different subjects under study .
we study a setting that is motivated by the problem of filtering spam messages for many users . <eos> each user receives messages according to an individual , unknown distribution , reflected only in the unlabeled inbox . <eos> the spam filter for a user is required to perform well with respect to this distribution . <eos> labeled messages from publicly available sources can be utilized , but they are governed by a distinct distribution , not adequately representing most inboxes . <eos> we devise a method that minimizes a loss function with respect to a user ? s personal distribution based on the available biased sample . <eos> a nonparametric hierarchical bayesian model furthermore generalizes across users by learning a common prior which is imposed on new email accounts . <eos> empirically , we observe that bias-corrected learning outperforms naive reliance on the assumption of independent and identically distributed data ; dirichlet-enhanced generalization across users outperforms a single ( ? one size fits all ? ) <eos> filter as well as independent filters for all users .
we present a method for localizing and separating sound sources in stereo recordings that is robust to reverberation and does not make any assumptions about the source statistics . <eos> the method consists of a probabilistic model of binaural multisource recordings and an expectation maximization algorithm for finding the maximum likelihood parameters of that model . <eos> these parameters include distributions over delays and assignments of time-frequency regions to sources . <eos> we evaluate this method against two comparable algorithms on simulations of simultaneous speech from two or three sources . <eos> our method outperforms the others in anechoic conditions and performs as well as the better of the two in the presence of reverberation .
categorization is a central activity of human cognition . <eos> when an individual is asked to categorize a sequence of items , context effects arise : categorization of one item influences category decisions for subsequent items . <eos> specifically , when experimental subjects are shown an exemplar of some target category , the category prototype appears to be pulled toward the exemplar , and the prototypes of all nontarget categories appear to be pushed away . <eos> these push and pull effects diminish with experience , and likely reflect long-term learning of category boundaries . <eos> we propose and evaluate four principled probabilistic ( bayesian ) accounts of context effects in categorization . <eos> in all four accounts , the probability of an exemplar given a category is encoded as a gaussian density in feature space , and categorization involves computing category posteriors given an exemplar . <eos> the models differ in how the uncertainty distribution of category prototypes is represented ( localist or distributed ) , and how it is updated following each experience ( using a maximum likelihood gradient ascent , or a kalman filter update ) . <eos> we find that the distributed maximum-likelihood model can explain the key experimental phenomena . <eos> further , the model predicts other phenomena that were confirmed via reanalysis of the experimental data .
we propose a compact , low power vlsi network of spiking neurons which can learn to classify complex patterns of mean firing rates on ? line and in real ? time . <eos> the network of integrate-and-fire neurons is connected by bistable synapses that can change their weight using a local spike ? based plasticity mechanism . <eos> learning is supervised by a teacher which provides an extra input to the output neurons during training . <eos> the synaptic weights are updated only if the current generated by the plastic synapses does not match the output desired by the teacher ( as in the perceptron learning rule ) . <eos> we present experimental results that demonstrate how this vlsi network is able to robustly classify uncorrelated linearly separable spatial patterns of mean firing rates .
in many structured prediction problems , the highest-scoring labeling is hard to compute exactly , leading to the use of approximate inference methods . <eos> however , when inference is used in a learning algorithm , a good approximation of the score may not be sufficient . <eos> we show in particular that learning can fail even with an approximate inference method with rigorous approximation guarantees . <eos> there are two reasons for this . <eos> first , approximate methods can effectively reduce the expressivity of an underlying model by making it impossible to choose parameters that reliably give good predictions . <eos> second , approximations can respond to parameter changes in such a way that standard learning algorithms are misled . <eos> in contrast , we give two positive results in the form of learning bounds for the use of lp-relaxed inference in structured perceptron and empirical risk minimization settings . <eos> we argue that without understanding combinations of inference and learning , such as these , that are appropriately compatible , learning performance under approximate inference can not be guaranteed .
it is known that determinining whether a dec-pomdp , namely , a cooperative partially observable stochastic game ( posg ) , has a cooperative strategy with positive expected reward is complete for nexp . <eos> it was not known until now how cooperation affected that complexity . <eos> we show that , for competitive posgs , the complexity of determining whether one team has a positive-expected-reward strategy is complete for the class nexp with an oracle for np .
we present the first truly polynomial algorithm for learning the structure of bounded-treewidth junction trees -- an attractive subclass of probabilistic graphical models that permits both the compact representation of probability distributions and efficient exact inference . <eos> for a constant treewidth , our algorithm has polynomial time and sample complexity , and provides strong theoretical guarantees in terms of $ kl $ divergence from the true distribution . <eos> we also present a lazy extension of our approach that leads to very significant speed ups in practice , and demonstrate the viability of our method empirically , on several real world datasets . <eos> one of our key new theoretical insights is a method for bounding the conditional mutual information of arbitrarily large sets of random variables with only a polynomial number of mutual information computations on fixed-size subsets of variables , when the underlying distribution can be approximated by a bounded treewidth junction tree .
for infants , early word learning is a chicken-and-egg problem . <eos> one way to learn a word is to observe that it co-occurs with a particular referent across different situations . <eos> another way is to use the social context of an utterance to infer the intended referent of a word . <eos> here we present a bayesian model of cross-situational word learning , and an extension of this model that also learns which social cues are relevant to determining reference . <eos> we test our model on a small corpus of mother-infant interaction and find it performs better than competing models . <eos> finally , we show that our model accounts for experimental phenomena including mutual exclusivity , fast-mapping , and generalization from social cues .
machine learning contains many computational bottlenecks in the form of nested summations over datasets . <eos> kernel estimators and other methods are burdened by these expensive computations . <eos> exact evaluation is typically o ( n2 ) or higher , which severely limits application to large datasets . <eos> we present a multi-stage stratified monte carlo method for approximating such summations with probabilistic relative error control . <eos> the essential idea is fast approximation by sampling in trees . <eos> this method differs from many previous scalability techniques ( such as standard multi-tree methods ) in that its error is stochastic , but we derive conditions for error control and demonstrate that they work . <eos> further , we give a theoretical sample complexity for the method that is independent of dataset size , and show that this appears to hold in experiments , where speedups reach as high as 1014 , many orders of magnitude beyond the previous state of the art .
semi-supervised inductive learning concerns how to learn a decision rule from a data set containing both labeled and unlabeled data . <eos> several boosting algorithms have been extended to semi-supervised learning with various strategies . <eos> to our knowledge , however , none of them takes local smoothness constraints among data into account during ensemble learning . <eos> in this paper , we introduce a local smoothness regularizer to semi-supervised boosting algorithms based on the universal optimization framework of margin cost functionals . <eos> our regularizer is applicable to existing semi-supervised boosting algorithms to improve their generalization and speed up their training . <eos> comparative results on synthetic , benchmark and real world tasks demonstrate the effectiveness of our local smoothness regularizer . <eos> we discuss relevant issues and relate our regularizer to previous work .
we show that under suitable assumptions ( primarily linearization ) a simple and perspicuous online learning rule for information bottleneck optimization with spiking neurons can be derived . <eos> this rule performs on common benchmark tasks as well as a rather complex rule that has previously been proposed \cite { klampfletal:07b } . <eos> furthermore , the transparency of this new learning rule makes a theoretical analysis of its convergence properties feasible . <eos> a variation of this learning rule ( with sign changes ) provides a theoretically founded method for performing principal component analysis { ( pca ) } with spiking neurons . <eos> by applying this rule to an ensemble of neurons , different principal components of the input can be extracted . <eos> in addition , it is possible to preferentially extract those principal components from incoming signals $ x $ that are related or are not related to some additional target signal $ y_t $ . <eos> in a biological interpretation , this target signal $ y_t $ ( also called relevance variable ) could represent proprioceptive feedback , input from other sensory modalities , or top-down signals .
under natural viewing conditions , human observers shift their gaze to allocate processing resources to subsets of the visual input . <eos> many computational models have aimed at predicting such voluntary attentional shifts . <eos> although the importance of high level stimulus properties ( higher order statistics , semantics ) stands undisputed , most models are based on low-level features of the input alone . <eos> in this study we recorded eye-movements of human observers while they viewed photographs of natural scenes . <eos> about two thirds of the stimuli contained at least one person . <eos> we demonstrate that a combined model of face detection and low-level saliency clearly outperforms a low-level model in predicting locations humans fixate . <eos> this is reflected in our finding fact that observes , even when not instructed to look for anything particular , fixate on a face with a probability of over 80 % within their first two fixations ( 500ms ) . <eos> remarkably , the model 's predictive performance in images that do not contain faces is not impaired by spurious face detector responses , which is suggestive of a bottom-up mechanism for face detection . <eos> in summary , we provide a novel computational approach which combines high level object knowledge ( in our case : face locations ) with low-level features to successfully predict the allocation of attentional resources .
the expectation maximization ( em ) algorithm is a widely used maximum likelihood estimation procedure for statistical models when the values of some of the variables in the model are not observed . <eos> very often , however , our aim is primarily to find a model that assigns values to the latent variables that have intended meaning for our data and maximizing expected likelihood only sometimes accomplishes this . <eos> unfortunately , it is typically difficult to add even simple a-priori information about latent variables in graphical models without making the models overly complex or intractable . <eos> in this paper , we present an efficient , principled way to inject rich constraints on the posteriors of latent variables into the em algorithm . <eos> our method can be used to learn tractable graphical models that satisfy additional , otherwise intractable constraints . <eos> focusing on clustering and the alignment problem for statistical machine translation , we show that simple , intuitive posterior constraints can greatly improve the performance over standard baselines and be competitive with more complex , intractable models .
large repositories of source code create new challenges and opportunities for statistical machine learning . <eos> here we first develop an infrastructure for the automated crawling , parsing , and database storage of open source software . <eos> the infrastructure allows us to gather internet-scale source code . <eos> for instance , in one experiment , we gather 4,632 java projects from sourceforge and apache totaling over 38 million lines of code from 9,250 developers . <eos> simple statistical analyses of the data first reveal robust power-law behavior for package , sloc , and method call distributions . <eos> we then develop and apply unsupervised author-topic , probabilistic models to automatically discover the topics embedded in the code and extract topic-word and author-topic distributions . <eos> in addition to serving as a convenient summary for program function and developer activities , these and other related distributions provide a statistical and information-theoretic basis for quantifying and analyzing developer similarity and competence , topic scattering , and document tangling , with direct applications to software engineering . <eos> finally , by combining software textual content with structural information captured by our coderank approach , we are able to significantly improve software retrieval performance , increasing the auc metric to 0.86 -- roughly 10-30 % better than previous approaches based on text alone .
we construct a biologically motivated stochastic differential model of the neural and hemodynamic activity underlying the observed blood oxygen level dependent ( bold ) signal in functional magnetic resonance imaging ( fmri ) . <eos> the model poses a difficult parameter estimation problem , both theoretically due to the nonlinearity and divergence of the differential system , and computationally due to its time and space complexity . <eos> we adapt a particle filter and smoother to the task , and discuss some of the practical approaches used to tackle the difficulties , including use of sparse matrices and parallelisation . <eos> results demonstrate the tractability of the approach in its application to an effective connectivity study .
rosetta is one of the leading algorithms for protein structure prediction today . <eos> it is a monte carlo energy minimization method requiring many random restarts to find structures with low energy . <eos> in this paper we present a resampling technique for structure prediction of small alpha/beta proteins using rosetta . <eos> from an initial round of rosetta sampling , we learn properties of the energy landscape that guide a subsequent round of sampling toward lower-energy structures . <eos> rather than attempt to fit the full energy landscape , we use feature selection methods ? both l1-regularized linear regression and decision trees ? to identify structural features that give rise to low energy . <eos> we then enrich these structural features in the second sampling round . <eos> results are presented across a benchmark set of nine small alpha/beta proteins demonstrating that our methods seldom impair , and frequently improve , rosetta ? s performance .
independent component analysis ( ica ) is a powerful method to decouple signals . <eos> most of the algorithms performing ica do not consider the temporal correlations of the signal , but only higher moments of its amplitude distribution . <eos> moreover , they require some preprocessing of the data ( whitening ) so as to remove second order correlations . <eos> in this paper , we are interested in understanding the neural mechanism responsible for solving ica . <eos> we present an online learning rule that exploits delayed correlations in the input . <eos> this rule performs ica by detecting joint variations in the firing rates of pre- and postsynaptic neurons , similar to a local rate-based hebbian learning rule .
statistical models on full and partial rankings of n items are often of limited practical use for large n due to computational consideration . <eos> we explore the use of non-parametric models for partially ranked data and derive efficient procedures for their use for large n. the derivations are largely possible through combinatorial and algebraic manipulations based on the lattice of partial rankings . <eos> in particular , we demonstrate for the first time a non-parametric coherent and consistent model capable of efficiently aggregating partially ranked data of different types .
we present a theoretical study on the discriminative clustering framework , recently proposed for simultaneous subspace selection via linear discriminant analysis ( lda ) and clustering . <eos> empirical results have shown its favorable performance in comparison with several other popular clustering algorithms . <eos> however , the inherent relationship between subspace selection and clustering in this framework is not well understood , due to the iterative nature of the algorithm . <eos> we show in this paper that this iterative subspace selection and clustering is equivalent to kernel k-means with a specific kernel gram matrix . <eos> this provides significant and new insights into the nature of this subspace selection procedure . <eos> based on this equivalence relationship , we propose the discriminative k-means ( diskmeans ) algorithm for simultaneous lda subspace selection and clustering , as well as an automatic parameter estimation procedure . <eos> we also present the nonlinear extension of diskmeans using kernels . <eos> we show that the learning of the kernel matrix over a convex set of pre-specified kernel matrices can be incorporated into the clustering formulation . <eos> the connection between diskmeans and several other clustering algorithms is also analyzed . <eos> the presented theories and algorithms are evaluated through experiments on a collection of benchmark data sets .
in order to represent state in controlled , partially observable , stochastic dynamical systems , some sort of sufficient statistic for history is necessary . <eos> predictive representations of state ( psrs ) capture state as statistics of the future . <eos> we introduce a new model of such systems called the ? exponential family psr , ? <eos> which defines as state the time-varying parameters of an exponential family distribution which models n sequential observations in the future . <eos> this choice of state representation explicitly connects psrs to state-of-the-art probabilistic modeling , which allows us to take advantage of current efforts in high-dimensional density estimation , and in particular , graphical models and maximum entropy models . <eos> we present a parameter learning algorithm based on maximum likelihood , and we show how a variety of current approximate inference methods apply . <eos> we evaluate the quality of our model with reinforcement learning by directly evaluating the control performance of the model .
we present epoch-greedy , an algorithm for multi-armed bandits with observable side information . <eos> epoch-greedy has the following properties : no knowledge of a time horizon $ t $ is necessary . <eos> the regret incurred by epoch-greedy is controlled by a sample complexity bound for a hypothesis class . <eos> the regret scales as $ o ( t^ { 2/3 } s^ { 1/3 } ) $ or better ( sometimes , much better ) . <eos> here $ s $ is the complexity term in a sample complexity bound for standard supervised learning .
recently , we have introduced a novel approach to dynamic programming and reinforcement learning that is based on maintaining explicit representations of stationary distributions instead of value functions . <eos> in this paper , we investigate the convergence properties of these dual algorithms both theoretically and empirically , and show how they can be scaled up by incorporating function approximation .
we investigate quantile regression based on the pinball loss and the ? -insensitive loss . <eos> for the pinball loss a condition on the data-generating distribution p is given that ensures that the conditional quantiles are approximated with respect to k k1 . <eos> this result is then used to derive an oracle inequality for an svm based on the pinball loss . <eos> moreover , we show that svms based on the ? -insensitive loss estimate the conditional median only under certain conditions on p .
clustering is often formulated as the maximum likelihood estimation of a mixture model that explains the data . <eos> the em algorithm widely used to solve the resulting optimization problem is inherently a gradient-descent method and is sensitive to initialization . <eos> the resulting solution is a local optimum in the neighborhood of the initial guess . <eos> this sensitivity to initialization presents a significant challenge in clustering large data sets into many clusters . <eos> in this paper , we present a different approach to approximate mixture fitting for clustering . <eos> we introduce an exemplar-based likelihood function that approximates the exact likelihood . <eos> this formulation leads to a convex minimization problem and an efficient algorithm with guaranteed convergence to the globally optimal solution . <eos> the resulting clustering can be thought of as a probabilistic mapping of the data points to the set of exemplars that minimizes the average distance and the information-theoretic cost of mapping . <eos> we present experimental results illustrating the performance of our algorithm and its comparison with the conventional approach to mixture model clustering .
to accelerate the training of kernel machines , we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods . <eos> the features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shiftinvariant kernel . <eos> we explore two sets of random features , provide convergence bounds on their ability to approximate various radial basis kernels , and show that in large-scale classification and regression tasks linear machine learning algorithms applied to these features outperform state-of-the-art large-scale kernel machines .
permutations are ubiquitous in many real world problems , such as voting , rankings and data association . <eos> representing uncertainty over permutations is challenging , since there are n ! <eos> possibilities , and typical compact representations such as graphical models can not efficiently capture the mutual exclusivity constraints associated with permutations . <eos> in this paper , we use the low-frequencyterms of a fourier decomposition to represent such distributions compactly . <eos> we present kronecker conditioning , a general and efficient approach for maintaining these distributions directly in the fourier domain . <eos> low order fourier-based approximations can lead to functions that do not correspond to valid distributions . <eos> to address this problem , we present an efficient quadratic program defined directly in the fourier domain to project the approximation onto a relaxed form of the marginal polytope . <eos> we demonstrate the effectiveness of our approach on a real camera-based multi-people tracking setting .
brain-computer interfaces can suffer from a large variance of the subject conditions within and across sessions . <eos> for example vigilance fluctuations in the individual , variable task involvement , workload etc . <eos> alter the characteristics of eeg signals and thus challenge a stable bci operation . <eos> in the present work we aim to define features based on a variant of the common spatial patterns ( csp ) algorithm that are constructed invariant with respect to such nonstationarities . <eos> we enforce invariance properties by adding terms to the denominator of a rayleigh coefficient representation of csp such as disturbance covariance matrices from fluctuations in visual processing . <eos> in this manner physiological prior knowledge can be used to shape the classification engine for bci . <eos> as a proof of concept we present a bci classifier that is robust to changes in the level of parietal -activity . <eos> in other words , the eeg decoding still works when there are lapses in vigilance .
we utilize the ensemble of trees framework , a tractable mixture over superexponential number of tree-structured distributions [ 1 ] , to develop a new model for multivariate density estimation . <eos> the model is based on a construction of treestructured copulas ? <eos> multivariate distributions with uniform on [ 0 , 1 ] marginals . <eos> by averaging over all possible tree structures , the new model can approximate distributions with complex variable dependencies . <eos> we propose an em algorithm to estimate the parameters for these tree-averaged models for both the real-valued and the categorical case . <eos> based on the tree-averaged framework , we propose a new model for joint precipitation amounts data on networks of rain stations .
we present a new local approximation algorithm for computing map and log-partition function for arbitrary exponential family distribution represented by a finite-valued pair-wise markov random field ( mrf ) , say g. our algorithm is based on decomposing g into appropriately chosen small components ; computing estimates locally in each of these components and then producing a good global solution . <eos> we prove that the algorithm can provide approximate solution within arbitrary accuracy when $ g $ excludes some finite sized graph as its minor and g has bounded degree : all planar graphs with bounded degree are examples of such graphs . <eos> the running time of the algorithm is $ \theta ( n ) $ ( n is the number of nodes in g ) , with constant dependent on accuracy , degree of graph and size of the graph that is excluded as a minor ( constant for planar graphs ) . <eos> our algorithm for minor-excluded graphs uses the decomposition scheme of klein , plotkin and rao ( 1993 ) . <eos> in general , our algorithm works with any decomposition scheme and provides quantifiable approximation guarantee that depends on the decomposition scheme .
learning the common structure shared by a set of supervised tasks is an important practical and theoretical problem . <eos> knowledge of this structure may lead to better generalization performance on the tasks and may also facilitate learning new tasks . <eos> we propose a framework for solving this problem , which is based on regularization with spectral functions of matrices . <eos> this class of regularization problems exhibits appealing computational properties and can be optimized efficiently by an alternating minimization algorithm . <eos> in addition , we provide a necessary and sufficient condition for convexity of the regularizer . <eos> we analyze concrete examples of the framework , which are equivalent to regularization with lp matrix norms . <eos> experiments on two real data sets indicate that the algorithm scales well with the number of tasks and improves on state of the art statistical performance .
we propose a new approach for dealing with the estimation of the location of change-points in one-dimensional piecewise constant signals observed in white noise . <eos> our approach consists in reframing this task in a variable selection context . <eos> we use a penalized least-squares criterion with a l1-type penalty for this purpose . <eos> we prove that , in an appropriate asymptotic framework , this method provides consistent estimators of the change-points . <eos> then , we explain how to implement this method in practice by combining the lar algorithm and a reduced version of the dynamic programming algorithm and we apply it to synthetic and real data .
in this paper we investigate multi-task learning in the context of gaussian processes ( gp ) . <eos> we propose a model that learns a shared covariance function on input-dependent features and a ? free-form covariance matrix over tasks . <eos> this allows for good flexibility when modelling inter-task dependencies while avoiding the need for large amounts of data for training . <eos> we show that under the assumption of noise-free observations and a block design , predictions for a given task only depend on its target values and therefore a cancellation of inter-task transfer occurs . <eos> we evaluate the benefits of our model on two practical applications : a compiler performance prediction problem and an exam score prediction task . <eos> additionally , we make use of gp approximations and properties of our model in order to provide scalability to large data sets .
we propose a model that leverages the millions of clicks received by web search engines , to predict document relevance . <eos> this allows the comparison of ranking functions when clicks are available but complete relevance judgments are not . <eos> after an initial training phase using a set of relevance judgments paired with click data , we show that our model can predict the relevance score of documents that have not been judged . <eos> these predictions can be used to evaluate the performance of a search engine , using our novel formalization of the confidence of the standard evaluation metric discounted cumulative gain ( dcg ) , so comparisons can be made across time and datasets . <eos> this contrasts with previous methods which can provide only pair-wise relevance judgements between results shown for the same query . <eos> when no relevance judgments are available , we can identify the better of two ranked lists up to 82 % of the time , and with only two relevance judgments for each query , we can identify the better ranking up to 94 % of the time . <eos> while our experiments are on sponsored search results , which is the financial backbone of web search , our method is general enough to be applicable to algorithmic web search results as well . <eos> furthermore , we give an algorithm to guide the selection of additional documents to judge to improve confidence .
we propose a novel method for { \em linear } dimensionality reduction of manifold modeled data . <eos> first , we show that with a small number $ m $ of { \em random projections } of sample points in $ \reals^n $ belonging to an unknown $ k $ -dimensional euclidean manifold , the intrinsic dimension ( id ) of the sample set can be estimated to high accuracy . <eos> second , we rigorously prove that using only this set of random projections , we can estimate the structure of the underlying manifold . <eos> in both cases , the number random projections required is linear in $ k $ and logarithmic in $ n $ , meaning that $ k < m\ll n $ . <eos> to handle practical situations , we develop a greedy algorithm to estimate the smallest size of the projection space required to perform manifold learning . <eos> our method is particularly relevant in distributed sensing systems and leads to significant potential savings in data acquisition , storage and transmission costs .
machine learning techniques are increasingly being used to produce a wide-range of classifiers for complex real-world applications that involve nonuniform testing costs and misclassification costs . <eos> as the complexity of these applications grows , the management of resources during the learning and classification processes becomes a challenging task . <eos> in this work we introduce act ( anytime cost-sensitive trees ) , a novel framework for operating in such environments . <eos> act is an anytime algorithm that allows trading computation time for lower classification costs . <eos> it builds a tree top-down and exploits additional time resources to obtain better estimations for the utility of the different candidate splits . <eos> using sampling techniques act approximates for each candidate split the cost of the subtree under it and favors the one with a minimal cost . <eos> due to its stochastic nature act is expected to be able to escape local minima , into which greedy methods may be trapped . <eos> experiments with a variety of datasets were conducted to compare the performance of act to that of the state of the art cost-sensitive tree learners . <eos> the results show that for most domains act produces trees of significantly lower costs . <eos> act is also shown to exhibit good anytime behavior with diminishing returns .
we develop and analyze an algorithm for nonparametric estimation of divergence functionals and the density ratio of two probability distributions . <eos> our method is based on a variational characterization of f -divergences , which turns the estimation into a penalized convex risk minimization problem . <eos> we present a derivation of our kernel-based estimation algorithm and an analysis of convergence rates for the estimator . <eos> our simulation results demonstrate the convergence behavior of the method , which compares favorably with existing methods in the literature .
we present a new class of models for high-dimensional nonparametric regression and classification called sparse additive models ( spam ) . <eos> our methods combine ideas from sparse linear modeling and additive nonparametric regression . <eos> we derive a method for fitting the models that is effective even when the number of covariates is larger than the sample size . <eos> a statistical analysis of the properties of spam is given together with empirical results on synthetic and real data , showing that spam can be effective in fitting sparse nonparametric models in high dimensional data .
we present a probabilistic approach to language change in which word forms are represented by phoneme sequences that undergo stochastic edits along the branches of a phylogenetic tree . <eos> our framework combines the advantages of the classical comparative method with the robustness of corpus-based probabilistic models . <eos> we use this framework to explore the consequences of two different schemes for defining probabilistic models of phonological change , evaluating these schemes using the reconstruction of ancient word forms in romance languages . <eos> the result is an efficient inference procedure for automatically inferring ancient word forms from modern languages , which can be generalized to support inferences about linguistic phylogenies .
we provide a provably efficient algorithm for learning markov decision processes ( mdps ) with continuous state and action spaces in the online setting . <eos> specifically , we take a model-based approach and show that a special type of online linear regression allows us to learn mdps with ( possibly kernalized ) linearly parameterized dynamics . <eos> this result builds on kearns and singh 's work that provides a provably efficient algorithm for finite state mdps . <eos> our approach is not restricted to the linear setting , and is applicable to other classes of continuous mdps .
a semi-supervised multitask learning ( mtl ) framework is presented , in which m parameterized semi-supervised classifiers , each associated with one of m partially labeled data manifolds , are learned jointly under the constraint of a softsharing prior imposed over the parameters of the classifiers . <eos> the unlabeled data are utilized by basing classifier learning on neighborhoods , induced by a markov random walk over a graph representation of each manifold . <eos> experimental results on real data sets demonstrate that semi-supervised mtl yields significant improvements in generalization performance over either semi-supervised single-task learning ( stl ) or supervised mtl .
we address the problem of adaptive sensor control in dynamic resource-constrained sensor networks . <eos> we focus on a meteorological sensing network comprising radars that can perform sector scanning rather than always scanning 360 degrees . <eos> we compare three sector scanning strategies . <eos> the sit-and-spin strategy always scans 360 degrees . <eos> the limited lookahead strategy additionally uses the expected environmental state k decision epochs in the future , as predicted from kalman filters , in its decision-making . <eos> the full lookahead strategy uses all expected future states by casting the problem as a markov decision process and using reinforcement learning to estimate the optimal scan strategy . <eos> we show that the main benefits of using a lookahead strategy are when there are multiple meteorological phenomena in the environment , and when the maximum radius of any phenomenon is sufficiently smaller than the radius of the radars . <eos> we also show that there is a trade-off between the average quality with which a phenomenon is scanned and the number of decision epochs before which a phenomenon is rescanned .
we present a novel message passing algorithm for approximating the map problem in graphical models . <eos> the algorithm is similar in structure to max-product but unlike max-product it always converges , and can be proven to find the exact map solution in various settings . <eos> the algorithm is derived via block coordinate descent in a dual of the lp relaxation of map , but does not require any tunable parameters such as step size or tree weights . <eos> we also describe a generalization of the method to cluster based potentials . <eos> the new method is tested on synthetic and real-world problems , and compares favorably with previous approaches .
although kernel measures of independence have been widely applied in machine learning ( notably in kernel ica ) , there is as yet no method to determine whether they have detected statistically significant dependence . <eos> we provide a novel test of the independence hypothesis for one particular kernel independence measure , the hilbert-schmidt independence criterion ( hsic ) . <eos> the resulting test costs o ( m2 ) , where m is the sample size . <eos> we demonstrate that this test outperforms established contingency table and functional correlation-based tests , and that this advantage is greater for multivariate data . <eos> finally , we show the hsic test also applies to text ( and to structured data more generally ) , for which no other independence test presently exists .
support vector machines ( svms ) suffer from a widely recognized scalability problem in both memory use and computational time . <eos> to improve scalability , we have developed a parallel svm algorithm ( psvm ) , which reduces memory use through performing a row-based , approximate matrix factorization , and which loads only essential data to each machine to perform parallel computation . <eos> let $ n $ denote the number of training instances , $ p $ the reduced matrix dimension after factorization ( $ p $ is significantly smaller than $ n $ ) , and $ m $ the number of machines . <eos> psvm reduces the memory requirement from $ \mo $ ( $ n^2 $ ) to $ \mo $ ( $ np/m $ ) , and improves computation time to $ \mo $ ( $ np^2/m $ ) . <eos> empirical studies on up to $ 500 $ computers shows psvm to be effective .
it is becoming increasingly important to learn from a partially-observed random matrix and predict its missing elements . <eos> we assume that the entire matrix is a single sample drawn from a matrix-variate t distribution and suggest a matrix-variate t model ( mvtm ) to predict those missing elements . <eos> we show that mvtm generalizes a range of known probabilistic models , and automatically performs model selection to encourage sparse predictive models . <eos> due to the non-conjugacy of its prior , it is difficult to make predictions by computing the mode or mean of the posterior distribution . <eos> we suggest an optimization method that sequentially minimizes a convex upper-bound of the log-likelihood , which is very efficient and scalable . <eos> the experiments on a toy data and eachmovie dataset show a good predictive accuracy of the model .
biological movement is built up of sub-blocks or motion primitives . <eos> such primitives provide a compact representation of movement which is also desirable in robotic control applications . <eos> we analyse handwriting data to gain a better understanding of use of primitives and their timings in biological movements . <eos> inference of the shape and the timing of primitives can be done using a factorial hmm based model , allowing the handwriting to be represented in primitive timing space . <eos> this representation provides a distribution of spikes corresponding to the primitive activations , which can also be modelled using hmm architectures . <eos> we show how the coupling of the low level primitive model , and the higher level timing model during inference can produce good reconstructions of handwriting , with shared primitives for all characters modelled . <eos> this coupled model also captures the variance profile of the dataset which is accounted for by spike timing jitter . <eos> the timing code provides a compact representation of the movement while generating a movement without an explicit timing model produces a scribbling style of output .
although theorists have interpreted classical conditioning as a laboratory model of bayesian belief updating , a recent reanalysis showed that the key features that theoretical models capture about learning are artifacts of averaging over subjects . <eos> rather than learning smoothly to asymptote ( reflecting , according to bayesian models , the gradual tradeoff from prior to posterior as data accumulate ) , subjects learn suddenly and their predictions fluctuate perpetually . <eos> we suggest that abrupt and unstable learning can be modeled by assuming subjects are conducting inference using sequential monte carlo sampling with a small number of samples ? <eos> one , in our simulations . <eos> ensemble behavior resembles exact bayesian models since , as in particle filters , it averages over many samples . <eos> further , the model is capable of exhibiting sophisticated behaviors like retrospective revaluation at the ensemble level , even given minimally sophisticated individuals that do not track uncertainty in their beliefs over trials .
we study the following question : is the two-dimensional structure of images a very strong prior or is it something that can be learned with a few examples of natural images ? <eos> if someone gave us a learning task involving images for which the two-dimensional topology of pixels was not known , could we discover it automatically and exploit it ? <eos> for example suppose that the pixels had been permuted in a fixed but unknown way , could we recover the relative two-dimensional location of pixels on images ? <eos> the surprising result presented here is that not only the answer is yes but that about as few as a thousand images are enough to approximately recover the relative locations of about a thousand pixels . <eos> this is achieved using a manifold learning algorithm applied to pixels associated with a measure of distributional similarity between pixel intensities . <eos> we compare different topology-extraction approaches and show how having the two-dimensional topology can be exploited .
bayesian models of multisensory perception traditionally address the problem of estimating an underlying variable that is assumed to be the cause of the two sensory signals . <eos> the brain , however , has to solve a more general problem : it also has to establish which signals come from the same source and should be integrated , and which ones do not and should be segregated . <eos> in the last couple of years , a few models have been proposed to solve this problem in a bayesian fashion . <eos> one of these has the strength that it formalizes the causal structure of sensory signals . <eos> we first compare these models on a formal level . <eos> furthermore , we conduct a psychophysics experiment to test human performance in an auditory-visual spatial localization task in which integration is not mandatory . <eos> we find that the causal bayesian inference model accounts for the data better than other models . <eos> keywords : causal inference , bayesian methods , visual perception .
many existing approaches to collaborative filtering can neither handle very large datasets nor easily deal with users who have very few ratings . <eos> in this paper we present the probabilistic matrix factorization ( pmf ) model which scales linearly with the number of observations and , more importantly , performs well on the large , sparse , and very imbalanced netflix dataset . <eos> we further extend the pmf model to include an adaptive prior on the model parameters and show how the model capacity can be controlled automatically . <eos> finally , we introduce a constrained version of the pmf model that is based on the assumption that users who have rated similar sets of movies are likely to have similar preferences . <eos> the resulting model is able to generalize considerably better for users with very few ratings . <eos> when the predictions of multiple pmf models are linearly combined with the predictions of restricted boltzmann machines models , we achieve an error rate of 0.8861 , that is nearly 7 % better than the score of netflix ? s own system .
a new algorithm for on-line learning linear-threshold functions is proposed which efficiently combines second-order statistics about the data with the logarithmic behavior '' of multiplicative/dual-norm algorithms . <eos> an initial theoretical analysis is provided suggesting that our algorithm might be viewed as a standard perceptron algorithm operating on a transformed sequence of examples with improved margin properties . <eos> we also report on experiments carried out on datasets from diverse domains , with the goal of comparing to known perceptron algorithms ( first-order , second-order , additive , multiplicative ) . <eos> our learning procedure seems to generalize quite well , and converges faster than the corresponding multiplicative baseline algorithms . ''
fair discriminative pedestrian finders are now available . <eos> in fact , these pedestrian finders make most errors on pedestrians in configurations that are uncommon in the training data , for example , mounting a bicycle . <eos> this is undesirable . <eos> however , the human configuration can itself be estimated discriminatively using structure learning . <eos> we demonstrate a pedestrian finder which first finds the most likely human pose in the window using a discriminative procedure trained with structure learning on a small dataset . <eos> we then present features ( local histogram of oriented gradient and local pca of gradient ) based on that configuration to an svm classifier . <eos> we show , using the inria person dataset , that estimates of configuration significantly improve the accuracy of a discriminative pedestrian finder .
we show how to use unlabeled data and a deep belief net ( dbn ) to learn a good covariance kernel for a gaussian process . <eos> we first learn a deep generative model of the unlabeled data using the fast , greedy algorithm introduced by hinton et.al . <eos> if the data is high-dimensional and highly-structured , a gaussian kernel applied to the top layer of features in the dbn works much better than a similar kernel applied to the raw input . <eos> performance at both regression and classification can then be further improved by using backpropagation through the dbn to discriminatively fine-tune the covariance kernel .
empirical risk minimization offers well-known learning guarantees when training and test data come from the same domain . <eos> in the real world , though , we often wish to adapt a classifier from a source domain with a large amount of training data to different target domain with very little training data . <eos> in this work we give uniform convergence bounds for algorithms that minimize a convex combination of source and target empirical risk . <eos> the bounds explicitly model the inherent trade-off between training on a large but inaccurate source data set and a small but accurate target training set . <eos> our theory also gives results when we have multiple source domains , each of which may have a different number of instances , and we exhibit cases in which minimizing a non-uniform combination of source risks can achieve much lower target error than standard empirical risk minimization .
on-line handwriting recognition is unusual among sequence labelling tasks in that the underlying generator of the observed data , i.e . the movement of the pen , is recorded directly . <eos> however , the raw data can be difficult to interpret because each letter is spread over many pen locations . <eos> as a consequence , sophisticated pre-processing is required to obtain inputs suitable for conventional sequence labelling algorithms , such as hmms . <eos> in this paper we describe a system capable of directly transcribing raw on-line handwriting data . <eos> the system consists of a recurrent neural network trained for sequence labelling , combined with a probabilistic language model . <eos> in experiments on an unconstrained on-line database , we record excellent results using either raw or pre-processed data , well outperforming a benchmark hmm in both cases .
many formal models of cognition implicitly use subjective probability distributions to capture the assumptions of human learners . <eos> most applications of these models determine these distributions indirectly . <eos> we propose a method for directly determining the assumptions of human learners by sampling from subjective probability distributions . <eos> using a correspondence between a model of human choice and markov chain monte carlo ( mcmc ) , we describe a method for sampling from the distributions over objects that people associate with different categories . <eos> in our task , subjects choose whether to accept or reject a proposed change to an object . <eos> the task is constructed so that these decisions follow an mcmc acceptance rule , defining a markov chain for which the stationary distribution is the category distribution . <eos> we test this procedure for both artificial categories acquired in the laboratory , and natural categories acquired from experience .
this paper considers kernels invariant to translation , rotation and dilation . <eos> we show that no non-trivial positive definite ( p.d . ) <eos> kernels exist which are radial and dilation invariant , only conditionally positive definite ( c.p.d . ) <eos> ones . <eos> accordingly , we discuss the c.p.d . <eos> case and provide some novel analysis , including an elementary derivation of a c.p.d . <eos> representer theorem . <eos> on the practical side , we give a support vector machine ( s.v.m . ) <eos> algorithm for arbitrary c.p.d . <eos> kernels . <eos> for the thinplate kernel this leads to a classifier with only one parameter ( the amount of regularisation ) , which we demonstrate to be as effective as an s.v.m . <eos> with the gaussian kernel , even though the gaussian involves a second parameter ( the length scale ) .
the peristimulus time historgram ( psth ) and its more continuous cousin , the spike density function ( sdf ) are staples in the analytic toolkit of neurophysiologists . <eos> the former is usually obtained by binning spiketrains , whereas the standard method for the latter is smoothing with a gaussian kernel . <eos> selection of a bin with or a kernel size is often done in an relatively arbitrary fashion , even though there have been recent attempts to remedy this situation \cite { shimazakibinningnips2006 , shimazakibinningneco2007 } . <eos> we develop an exact bayesian , generative model approach to estimating pshts and demonstate its superiority to competing methods . <eos> further advantages of our scheme include automatic complexity control and error bars on its predictions .
we present a probabilistic generative model of visual attributes , together with an efficient learning algorithm . <eos> attributes are visual qualities of objects , such as ? red ? , ? striped ? , or ? spotted ? . <eos> the model sees attributes as patterns of image segments , repeatedly sharing some characteristic properties . <eos> these can be any combination of appearance , shape , or the layout of segments within the pattern . <eos> moreover , attributes with general appearance are taken into account , such as the pattern of alternation of any two colors which is characteristic for stripes . <eos> to enable learning from unsegmented training images , the model is learnt discriminatively , by optimizing a likelihood ratio . <eos> as demonstrated in the experimental evaluation , our model can learn in a weakly supervised setting and encompasses a broad range of attributes . <eos> we show that attributes can be learnt starting from a text query to google image search , and can then be used to recognize the attribute and determine its spatial extent in novel real-world images .
incorporating invariances into a learning algorithm is a common problem in machine learning . <eos> we provide a convex formulation which can deal with arbitrary loss functions and arbitrary losses . <eos> in addition , it is a drop-in replacement for most optimization algorithms for kernels , including solvers of the svmstruct family . <eos> the advantage of our setting is that it relies on column generation instead of modifying the underlying optimization problem directly .
we propose an active learning algorithm that learns a continuous valuation model from discrete preferences . <eos> the algorithm automatically decides what items are best presented to an individual in order to find the item that they value highly in as few trials as possible , and exploits quirks of human psychology to minimize time and cognitive burden . <eos> to do this , our algorithm maximizes the expected improvement at each query without accurately modelling the entire valuation surface , which would be needlessly expensive . <eos> the problem is particularly difficult because the space of choices is infinite . <eos> we demonstrate the effectiveness of the new algorithm compared to related active learning methods . <eos> we also embed the algorithm within a decision making tool for assisting digital artists in rendering materials . <eos> the tool finds the best parameters while minimizing the number of queries .
stimulus selectivity of sensory neurons is often characterized by estimating their receptive field properties such as orientation selectivity . <eos> receptive fields are usually derived from the mean ( or covariance ) of the spike-triggered stimulus ensemble . <eos> this approach treats each spike as an independent message but does not take into account that information might be conveyed through patterns of neural activity that are distributed across space or time . <eos> can we find a concise description for the processing of a whole population of neurons analogous to the receptive field for single neurons ? <eos> here , we present a generalization of the linear receptive field which is not bound to be triggered on individual spikes but can be meaningfully linked to distributed response patterns . <eos> more precisely , we seek to identify those stimulus features and the corresponding patterns of neural activity that are most reliably coupled . <eos> we use an extension of reverse-correlation methods based on canonical correlation analysis . <eos> the resulting population receptive fields span the subspace of stimuli that is most informative about the population response . <eos> we evaluate our approach using both neuronal models and multi-electrode recordings from rabbit retinal ganglion cells . <eos> we show how the model can be extended to capture nonlinear stimulus-response relationships using kernel canonical correlation analysis , which makes it possible to test different coding mechanisms . <eos> our technique can also be used to calculate receptive fields from multi-dimensional neural measurements such as those obtained from dynamic imaging methods .
we extend position and phase-shift tuning , concepts already well established in the disparity energy neuron literature , to motion energy neurons . <eos> we show that reichardt-like detectors can be considered examples of position tuning , and that motion energy filters whose complex valued spatio-temporal receptive fields are space-time separable can be considered examples of phase tuning . <eos> by combining these two types of detectors , we obtain an architecture for constructing motion energy neurons whose center frequencies can be adjusted by both phase and position shifts . <eos> similar to recently described neurons in the primary visual cortex , these new motion energy neurons exhibit tuning that is between purely spacetime separable and purely speed tuned . <eos> we propose a functional role for this intermediate level of tuning by demonstrating that comparisons between pairs of these motion energy neurons can reliably discriminate between inputs whose velocities lie above or below a given reference velocity .
in bioinformatics it is often desirable to combine data from various measurement sources and thus structured feature vectors are to be analyzed that possess different intrinsic blocking characteristics ( e.g. , different patterns of missing values , observation noise levels , effective intrinsic dimensionalities ) . <eos> we propose a new machine learning tool , heterogeneous component analysis ( hca ) , for feature extraction in order to better understand the factors that underlie such complex structured heterogeneous data . <eos> hca is a linear block-wise sparse bayesian pca based not only on a probabilistic model with block-wise residual variance terms but also on a bayesian treatment of a block-wise sparse factor-loading matrix . <eos> we study various algorithms that implement our hca concept extracting sparse heterogeneous structure by obtaining common components for the blocks and specific components within each block . <eos> simulations on toy and bioinformatics data underline the usefulness of the proposed structured matrix factorization concept .
dynamic bayesian networks are structured representations of stochastic processes . <eos> despite their structure , exact inference in dbns is generally intractable . <eos> one approach to approximate inference involves grouping the variables in the process into smaller factors and keeping independent beliefs over these factors . <eos> in this paper we present several techniques for decomposing a dynamic bayesian network automatically to enable factored inference . <eos> we examine a number of features of a dbn that capture different types of dependencies that will cause error in factored inference . <eos> an empirical comparison shows that the most useful of these is a heuristic that estimates the mutual information introduced between factors by one step of belief propagation . <eos> in addition to features computed over entire factors , for efficiency we explored scores computed over pairs of variables . <eos> we present search methods that use these features , pairwise and not , to find a factorization , and we compare their results on several datasets . <eos> automatic factorization extends the applicability of factored inference to large , complex models that are undesirable to factor by hand . <eos> moreover , tests on real dbns show that automatic factorization can achieve significantly lower error in some cases .
many perceptual processes and neural computations , such as speech recognition , motor control and learning , depend on the ability to measure and mark the passage of time . <eos> however , the processes that make such temporal judgements possible are unknown . <eos> a number of different hypothetical mechanisms have been advanced , all of which depend on the known , temporally predictable evolution of a neural or psychological state , possibly through oscillations or the gradual decay of a memory trace . <eos> alternatively , judgements of elapsed time might be based on observations of temporally structured , but stochastic processes . <eos> such processes need not be specific to the sense of time ; typical neural and sensory processes contain at least some statistical structure across a range of time scales . <eos> here , we investigate the statistical properties of an estimator of elapsed time which is based on a simple family of stochastic process .
many tasks ( e.g. , clustering ) in machine learning only require the l distances instead of the original data . <eos> for dimension reductions in the l norm ( 0 < 2 ) , the method of stable random projections can efficiently compute the l distances in massive datasets ( e.g. , the web or massive data streams ) in one pass of the data . <eos> the estimation task for stable random projections has been an interesting topic . <eos> we propose a simple estimator based on the fractional power of the samples ( projected data ) , which is surprisingly near-optimal in terms of the asymptotic variance . <eos> in fact , it achieves the cram ? er-rao bound when = 2 and = 0+ . <eos> this new result will be useful when applying stable random projections to distancebased clustering , classifications , kernels , massive data streams etc .
reliably recovering 3d human pose from monocular video requires constraints that bias the estimates towards typical human poses and motions . <eos> we define priors for people tracking using a laplacian eigenmaps latent variable model ( lelvm ) . <eos> lelvm is a probabilistic dimensionality reduction model that naturally combines the advantages of latent variable models -- -definining a multimodal probability density for latent and observed variables , and globally differentiable nonlinear mappings for reconstruction and dimensionality reduction -- -with those of spectral manifold learning methods -- -no local optima , ability to unfold highly nonlinear manifolds , and good practical scaling to latent spaces of high dimension . <eos> lelvm is computationally efficient , simple to learn from sparse training data , and compatible with standard probabilistic trackers such as particle filters . <eos> we analyze the performance of a lelvm-based probabilistic sigma point mixture tracker in several real and synthetic human motion sequences and demonstrate that lelvm provides sufficient constraints for robust operation in the presence of missing , noisy and ambiguous image measurements .
over the past few years , the notion of stability in data clustering has received growing attention as a cluster validation criterion in a sample-based framework . <eos> however , recent work has shown that as the sample size increases , any clustering model will usually become asymptotically stable . <eos> this led to the conclusion that stability is lacking as a theoretical and practical tool . <eos> the discrepancy between this conclusion and the success of stability in practice has remained an open question , which we attempt to address . <eos> our theoretical approach is that stability , as used by cluster validation algorithms , is similar in certain respects to measures of generalization in a model-selection framework . <eos> in such cases , the model chosen governs the convergence rate of generalization bounds . <eos> by arguing that these rates are more important than the sample size , we are led to the prediction that stability-based cluster validation algorithms should not degrade with increasing sample size , despite the asymptotic universal stability . <eos> this prediction is substantiated by a theoretical analysis as well as some empirical results . <eos> we conclude that stability remains a meaningful cluster validation criterion over finite samples .
in transfer learning we aim to solve new problems using fewer examples using information gained from solving related problems . <eos> transfer learning has been successful in practice , and extensive pac analysis of these methods has been developed . <eos> however it is not yet clear how to define relatedness between tasks . <eos> this is considered as a major problem as it is conceptually troubling and it makes it unclear how much information to transfer and when and how to transfer it . <eos> in this paper we propose to measure the amount of information one task contains about another using conditional kolmogorov complexity between the tasks . <eos> we show how existing theory neatly solves the problem of measuring relatedness and transferring the rightamount of information in sequential transfer learning in a bayesian setting . <eos> the theory also suggests that , in a very formal and precise sense , no other reasonable transfer method can do much better than our kolmogorov complexity theoretic transfer method , and that sequential transfer is always justified . <eos> we also develop a practical approximation to the method and use it to transfer information between 8 arbitrarily chosen databases from the uci ml repository .
neural spike trains present challenges to analytical efforts due to their noisy , spiking nature . <eos> many studies of neuroscientific and neural prosthetic importance rely on a smoothed , denoised estimate of the spike train ? s underlying firing rate . <eos> current techniques to find time-varying firing rates require ad hoc choices of parameters , offer no confidence intervals on their estimates , and can obscure potentially important single trial variability . <eos> we present a new method , based on a gaussian process prior , for inferring probabilistically optimal estimates of firing rate functions underlying single or multiple neural spike trains . <eos> we test the performance of the method on simulated data and experimentally gathered neural spike trains , and we demonstrate improvements over conventional estimators .
we present a globally convergent method for regularized risk minimization problems . <eos> our method applies to support vector estimation , regression , gaussian processes , and any other regularized risk minimization setting which leads to a convex optimization problem . <eos> svmperf can be shown to be a special case of our approach . <eos> in addition to the unified framework we present tight convergence bounds , which show that our algorithm converges in o ( 1/ ) steps to  precision for general convex problems and in o ( log ( 1/ ) ) steps for continuously differentiable problems . <eos> we demonstrate in experiments the performance of our approach .
we study a pattern classification algorithm which has recently been proposed by vapnik and coworkers . <eos> it builds on a new inductive principle which assumes that in addition to positive and negative data , a third class of data is available , termed the universum . <eos> we assay the behavior of the algorithm by establishing links with fisher discriminant analysis and oriented pca , as well as with an svm in a projected subspace ( or , equivalently , with a data-dependent reduced kernel ) . <eos> we also provide experimental results .
semantic memory refers to our knowledge of facts and relationships between concepts . <eos> a successful semantic memory depends on inferring relationships between items that are not explicitly taught . <eos> recent mathematical modeling of episodic memory argues that episodic recall relies on retrieval of a gradually-changing representation of temporal context . <eos> we show that retrieved context enables the development of a global memory space that reflects relationships between all items that have been previously learned . <eos> when newly-learned information is integrated into this structure , it is placed in some relationship to all other items , even if that relationship has not been explicitly learned . <eos> we demonstrate this effect for global semantic structures shaped topologically as a ring , and as a two-dimensional sheet . <eos> we also examined the utility of this learning algorithm for learning a more realistic semantic space by training it on a large pool of synonym pairs . <eos> retrieved context enabled the model to inferrelationships between synonym pairs that had not yet been presented .
we consider continuous state , continuous action batch reinforcement learning where the goal is to learn a good policy from a sufficiently rich trajectory generated by another policy . <eos> we study a variant of fitted q-iteration , where the greedy action selection is replaced by searching for a policy in a restricted set of candidate policies by maximizing the average action values . <eos> we provide a rigorous theoretical analysis of this algorithm , proving what we believe is the first finite-time bounds for value-function based algorithms for continuous state- and action-space problems .
guided by the goal of obtaining an optimization algorithm that is both fast and yielding good generalization , we study the descent direction maximizing the decrease in generalization error or the probability of not increasing generalization error . <eos> the surprising result is that from both the bayesian and frequentist perspectives this can yield the natural gradient direction . <eos> although that direction can be very expensive to compute we develop an efficient , general , online approximation to the natural gradient descent which is suited to large scale problems . <eos> we report experimental results showing much faster convergence in computation time and in number of iterations with tonga ( topmoumoute online natural gradient algorithm ) than with stochastic gradient descent , even on very large datasets .
an important problem in many fields is the analysis of counts data to extract meaningful latent components . <eos> methods like probabilistic latent semantic analysis ( plsa ) and latent dirichlet allocation ( lda ) have been proposed for this purpose . <eos> however , they are limited in the number of components they can extract and also do not have a provision to control the expressiveness '' of the extracted components . <eos> in this paper , we present a learning formulation to address these limitations by employing the notion of sparsity . <eos> we start with the plsa framework and use an entropic prior in a maximum a posteriori formulation to enforce sparsity . <eos> we show that this allows the extraction of overcomplete sets of latent components which better characterize the data . <eos> we present experimental evidence of the utility of such representations . ''
traditional analysis methods for single-trial classification of electro-encephalography ( eeg ) focus on two types of paradigms : phase locked methods , in which the amplitude of the signal is used as the feature for classification , i.e . event related potentials ; and second order methods , in which the feature of interest is the power of the signal , i.e event related ( de ) synchronization . <eos> the process of deciding which paradigm to use is ad hoc and is driven by knowledge of neurological findings . <eos> here we propose a unified method in which the algorithm learns the best first and second order spatial and temporal features for classification of eeg based on a bilinear model . <eos> the efficiency of the method is demonstrated in simulated and real eeg from a benchmark data set for brain computer interface .
it has been shown that adapting a dictionary of basis functions to the statistics of natural images so as to maximize sparsity in the coefficients results in a set of dictionary elements whose spatial properties resemble those of v1 ( primary visual cortex ) receptive fields . <eos> however , the resulting sparse coefficients still exhibit pronounced statistical dependencies , thus violating the independence assumption of the sparse coding model . <eos> here , we propose a model that attempts to capture the dependencies among the basis function coefficients by including a pairwise coupling term in the prior over the coefficient activity states . <eos> when adapted to the statistics of natural images , the coupling terms learn a combination of facilitatory and inhibitory interactions among neighboring basis functions . <eos> these learned interactions may offer an explanation for the function of horizontal connections in v1 , and we discuss the implications of our findings for physiological experiments .
this paper studies boosting algorithms that make a single pass over a set of base classifiers . <eos> we first analyze a one-pass algorithm in the setting of boosting with diverse base classifiers . <eos> our guarantee is the same as the best proved for any boosting algorithm , but our one-pass algorithm is much faster than previous approaches . <eos> we next exhibit a random source of examples for which a pickyvariant of adaboost that skips poor base classifiers can outperform the standard adaboost algorithm , which uses every base classifier , by an exponential factor . <eos> experiments with reuters and synthetic data show that one-pass boosting can substantially improve on the accuracy of naive bayes , and that picky boosting can sometimes lead to a further improvement in accuracy .
the notion of algorithmic stability has been used effectively in the past to derive tight generalization bounds . <eos> a key advantage of these bounds is that they are de- signed for specific learning algorithms , exploiting their particular properties . <eos> but , as in much of learning theory , existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed ( i.i.d . ) . <eos> in many machine learning applications , however , this assumption does not hold . <eos> the observations received by the learning algorithm often have some inherent temporal dependence , which is clear in system diagnosis or time series prediction problems . <eos> this paper studies the scenario where the observations are drawn from a station- ary beta-mixing sequence , which implies a dependence between observations that weaken over time . <eos> it proves novel stability-based generalization bounds that hold even with this more general setting . <eos> these bounds strictly generalize the bounds given in the i.i.d . <eos> case . <eos> we also illustrate their application in the case of several general classes of learning algorithms , including support vector regression and kernel ridge regression .
we investigate the use of message-passing algorithms for the problem of finding the max-weight independent set ( mwis ) in a graph . <eos> first , we study the performance of loopy max-product belief propagation . <eos> we show that , if it converges , the quality of the estimate is closely related to the tightness of an lp relaxation of the mwis problem . <eos> we use this relationship to obtain sufficient conditions for correctness of the estimate . <eos> we then develop a modification of max-product ? <eos> one that converges to an optimal solution of the dual of the mwis problem . <eos> we also develop a simple iterative algorithm for estimating the max-weight independent set from this dual solution . <eos> we show that the mwis estimate obtained using these two algorithms in conjunction is correct when the graph is bipartite and the mwis is unique . <eos> finally , we show that any problem of map estimation for probability distributions over finite domains can be reduced to an mwis problem . <eos> we believe this reduction will yield new insights and algorithms for map estimation .
many algorithms have been recently developed for reducing dimensionality by projecting data onto an intrinsic non-linear manifold . <eos> unfortunately , existing algorithms often lose significant precision in this transformation . <eos> manifold sculpting is a new algorithm that iteratively reduces dimensionality by simulating surface tension in local neighborhoods . <eos> we present several experiments that show manifold sculpting yields more accurate results than existing algorithms with both generated and natural data-sets . <eos> manifold sculpting is also able to benefit from both prior dimensionality reduction efforts .
this paper compares a family of methods for characterizing neural feature selectivity with natural stimuli in the framework of the linear-nonlinear model . <eos> in this model , the neural firing rate is a nonlinear function of a small number of relevant stimulus components . <eos> the relevant stimulus dimensions can be found by maximizing one of the family of objective functions , r ? enyi divergences of different orders [ 1 , 2 ] . <eos> we show that maximizing one of them , r ? enyi divergence of order 2 , is equivalent to least-square fitting of the linear-nonlinear model to neural data . <eos> next , we derive reconstruction errors in relevant dimensions found by maximizing r ? enyi divergences of arbitrary order in the asymptotic limit of large spike numbers . <eos> we find that the smallest errors are obtained with r ? enyi divergence of order 1 , also known as kullback-leibler divergence . <eos> this corresponds to finding relevant dimensions by maximizing mutual information [ 2 ] . <eos> we numerically test how these optimization schemes perform in the regime of low signal-to-noise ratio ( small number of spikes and increasing neural noise ) for model visual neurons . <eos> we find that optimization schemes based on either least square fitting or information maximization perform well even when number of spikes is small . <eos> information maximization provides slightly , but significantly , better reconstructions than least square fitting . <eos> this makes the problem of finding relevant dimensions , together with the problem of lossy compression [ 3 ] , one of examples where informationtheoretic measures are no more data limited than those derived from least squares .
this paper explores the use of a maximal average margin ( mam ) optimality principle for the design of learning algorithms . <eos> it is shown that the application of this risk minimization principle results in a class of ( computationally ) simple learning machines similar to the classical parzen window classifier . <eos> a direct relation with the rademacher complexities is established , as such facilitating analysis and providing a notion of certainty of prediction . <eos> this analysis is related to support vector machines by means of a margin transformation . <eos> the power of the mam principle is illustrated further by application to ordinal regression tasks , resulting in an $ o ( n ) $ algorithm able to process large datasets in reasonable time .
sound localization by barn owls is commonly modeled as a matching procedure where localization cues derived from auditory inputs are compared to stored templates . <eos> while the matching models can explain properties of neural responses , no model explains how the owl resolves spatial ambiguity in the localization cues to produce accurate localization near the center of gaze . <eos> here , we examine two models for the barn owl 's sound localization behavior . <eos> first , we consider a maximum likelihood estimator in order to further evaluate the cue matching model . <eos> second , we consider a maximum a posteriori estimator to test if a bayesian model with a prior that emphasizes directions near the center of gaze can reproduce the owl 's localization behavior . <eos> we show that the maximum likelihood estimator can not reproduce the owl 's behavior , while the maximum a posteriori estimator is able to match the behavior . <eos> this result suggests that the standard cue matching model will not be sufficient to explain sound localization behavior in the barn owl . <eos> the bayesian model provides a new framework for analyzing sound localization in the barn owl and leads to predictions about the owl 's localization behavior .
a discriminative method is proposed for learning monotonic transformations of the training data while jointly estimating a large-margin classifier . <eos> in many domains such as document classification , image histogram classification and gene microarray experiments , fixed monotonic transformations can be useful as a preprocessing step . <eos> however , most classifiers only explore these transformations through manual trial and error or via prior domain knowledge . <eos> the proposed method learns monotonic transformations automatically while training a large-margin classifier without any prior knowledge of the domain . <eos> a monotonic piecewise linear function is learned which transforms data for subsequent processing by a linear hyperplane classifier . <eos> two algorithmic implementations of the method are formalized . <eos> the first solves a convergent alternating sequence of quadratic and linear programs until it obtains a locally optimal solution . <eos> an improved algorithm is then derived using a convex semidefinite relaxation that overcomes initialization issues in the greedy optimization problem . <eos> the effectiveness of these learned transformations on synthetic problems , text data and image data is demonstrated .
the learning of probabilistic models with many hidden variables and nondecomposable dependencies is an important and challenging problem . <eos> in contrast to traditional approaches based on approximate inference in a single intractable model , our approach is to train a set of tractable submodels by encouraging them to agree on the hidden variables . <eos> this allows us to capture non-decomposable aspects of the data while still maintaining tractability . <eos> we propose an objective function for our approach , derive em-style algorithms for parameter estimation , and demonstrate their effectiveness on three challenging real-world learning tasks .
we show that any weak ranker that can achieve an area under the roc curve slightly better than 1/2 ( which can be achieved by random guessing ) can be efficiently boosted to achieve an area under the roc curve arbitrarily close to 1 . <eos> we further show that this boosting can be performed even in the presence of independent misclassification noise , given access to a noise-tolerant weak ranker .
when training and test samples follow different input distributions ( i.e. , the situation called \emph { covariate shift } ) , the maximum likelihood estimator is known to lose its consistency . <eos> for regaining consistency , the log-likelihood terms need to be weighted according to the \emph { importance } ( i.e. , the ratio of test and training input densities ) . <eos> thus , accurately estimating the importance is one of the key tasks in covariate shift adaptation . <eos> a naive approach is to first estimate training and test input densities and then estimate the importance by the ratio of the density estimates . <eos> however , since density estimation is a hard problem , this approach tends to perform poorly especially in high dimensional cases . <eos> in this paper , we propose a direct importance estimation method that does not require the input density estimates . <eos> our method is equipped with a natural model selection procedure so tuning parameters such as the kernel width can be objectively optimized . <eos> this is an advantage over a recently developed method of direct importance estimation . <eos> simulations illustrate the usefulness of our approach .
we study the relation between notions of game-theoretic equilibria which are based on stability under a set of deviations , and empirical equilibria which are reached by rational players . <eos> rational players are modelled by players using no regret algorithms , which guarantee that their payoff in the long run is almost as much as the most they could hope to achieve by consistently deviating from the algorithm 's suggested action . <eos> we show that for a given set of deviations over the strategy set of a player , it is possible to efficiently approximate fixed points of a given deviation if and only if there exist efficient no regret algorithms resistant to the deviations . <eos> further , we show that if all players use a no regret algorithm , then the empirical distribution of their plays converges to an equilibrium .
planning in partially observable environments remains a challenging problem , despite significant recent advances in offline approximation techniques . <eos> a few online methods have also been proposed recently , and proven to be remarkably scalable , but without the theoretical guarantees of their offline counterparts . <eos> thus it seems natural to try to unify offline and online techniques , preserving the theoretical properties of the former , and exploiting the scalability of the latter . <eos> in this paper , we provide theoretical guarantees on an anytime algorithm for pomdps which aims to reduce the error made by approximate offline value iteration algorithms through the use of an efficient online searching procedure . <eos> the algorithm uses search heuristics based on an error analysis of lookahead search , to guide the online search towards reachable beliefs with the most potential to reduce error . <eos> we provide a general theorem showing that these search heuristics are admissible , and lead to complete and epsilon-optimal algorithms . <eos> this is , to the best of our knowledge , the strongest theoretical result available for online pomdp solution methods . <eos> we also provide empirical evidence showing that our approach is also practical , and can find ( provably ) near-optimal solutions in reasonable time .
electrical power management in large-scale it systems such as commercial datacenters is an application area of rapidly growing interest from both an economic and ecological perspective , with billions of dollars and millions of metric tons of co2 emissions at stake annually . <eos> businesses want to save power without sacrificing performance . <eos> this paper presents a reinforcement learning approach to simultaneous online management of both performance and power consumption . <eos> we apply rl in a realistic laboratory testbed using a blade cluster and dynamically varying http workload running on a commercial web applications middleware platform . <eos> we embed a cpu frequency controller in the blade servers firmware , and we train policies for this controller using a multi-criteria reward signal depending on both application performance and cpu power consumption . <eos> our testbed scenario posed a number of challenges to successful use of rl , including multiple disparate reward functions , limited decision sampling rates , and pathologies arising when using multiple sensor readings as state variables . <eos> we describe innovative practical solutions to these challenges , and demonstrate clear performance improvements over both hand-designed policies as well as obvious ? cookbook rl implementations .
in a multiple instance ( mi ) learning problem , instances are naturally organized into bags and it is the bags , instead of individual instances , that are labeled for training . <eos> mi learners assume that every instance in a bag labeled negative is actually negative , whereas at least one instance in a bag labeled positive is actually positive . <eos> we present a framework for active learning in the multiple-instance setting . <eos> in particular , we consider the case in which an mi learner is allowed to selectively query unlabeled instances in positive bags . <eos> this approach is well motivated in domains in which it is inexpensive to acquire bag labels and possible , but expensive , to acquire instance labels . <eos> we describe a method for learning from labels at mixed levels of granularity , and introduce two active query selection strategies motivated by the mi setting . <eos> our experiments show that learning from instance labels can significantly improve performance of a basic mi learning algorithm in two multiple-instance domains : content-based image recognition and text classification .
we consider apprenticeship learning ? learning from expert demonstrations ? in the setting of large , complex domains . <eos> past work in apprenticeship learning requires that the expert demonstrate complete trajectories through the domain . <eos> however , in many problems even an expert has difficulty controlling the system , which makes this approach infeasible . <eos> for example , consider the task of teaching a quadruped robot to navigate over extreme terrain ; demonstrating an optimal policy ( i.e. , an optimal set of foot locations over the entire terrain ) is a highly non-trivial task , even for an expert . <eos> in this paper we propose a method for hierarchical apprenticeship learning , which allows the algorithm to accept isolated advice at different hierarchical levels of the control task . <eos> this type of advice is often feasible for experts to give , even if the expert is unable to demonstrate complete trajectories . <eos> this allows us to extend the apprenticeship learning paradigm to much larger , more challenging domains . <eos> in particular , in this paper we apply the hierarchical apprenticeship learning algorithm to the task of quadruped locomotion over extreme terrain , and achieve , to the best of our knowledge , results superior to any previously published work .
current object recognition systems can only recognize a limited number of object categories ; scaling up to many categories is the next challenge . <eos> we seek to build a system to recognize and localize many different object categories in complex scenes . <eos> we achieve this through a simple approach : by matching the input image , in an appropriate representation , to images in a large training set of labeled images . <eos> due to regularities in object identities across similar scenes , the retrieved matches provide hypotheses for object identities and locations . <eos> we build a probabilistic model to transfer the labels from the retrieval set to the input image . <eos> we demonstrate the effectiveness of this approach and study algorithm component contributions using held-out test sets from the labelme database .
motivated by stochastic systems in which observed evidence and conditional dependencies between states of the network change over time , and certain quantities of interest ( marginal distributions , likelihood estimates etc . ) <eos> must be updated , we study the problem of adaptive inference in tree-structured bayesian networks . <eos> we describe an algorithm for adaptive inference that handles a broad range of changes to the network and is able to maintain marginal distributions , map estimates , and data likelihoods in all expected logarithmic time . <eos> we give an implementation of our algorithm and provide experiments that show that the algorithm can yield up to two orders of magnitude speedups on answering queries and responding to dynamic changes over the sum-product algorithm .
point process encoding models provide powerful statistical methods for understanding the responses of neurons to sensory stimuli . <eos> although these models have been successfully applied to neurons in the early sensory pathway , they have fared less well capturing the response properties of neurons in deeper brain areas , owing in part to the fact that they do not take into account multiple stages of processing . <eos> here we introduce a new twist on the point-process modeling approach : we include unobserved as well as observed spiking neurons in a joint encoding model . <eos> the resulting model exhibits richer dynamics and more highly nonlinear response properties , making it more powerful and more flexible for fitting neural data . <eos> more importantly , it allows us to estimate connectivity patterns among neurons ( both observed and unobserved ) , and may provide insight into how networks process sensory input . <eos> we formulate the estimation procedure using variational em and the wake-sleep algorithm , and illustrate the model ? s performance using a simulated example network consisting of two coupled neurons .
we investigate a new , convex relaxation of an expectation-maximization ( em ) variant that approximates a standard objective while eliminating local minima . <eos> first , a cautionary result is presented , showing that any convex relaxation of em over hidden variables must give trivial results if any dependence on the missing values is retained . <eos> although this appears to be a strong negative outcome , we then demonstrate how the problem can be bypassed by using equivalence relations instead of value assignments over hidden variables . <eos> in particular , we develop new algorithms for estimating exponential conditional models that only require equivalence relation information over the variable values . <eos> this reformulation leads to an exact expression for em variants in a wide range of problems . <eos> we then develop a semidefinite relaxation that yields global training by eliminating local minima .
we present four new reinforcement learning algorithms based on actor-critic and natural-gradient ideas , and provide their convergence proofs . <eos> actor-critic reinforcement learning methods are online approximations to policy iteration in which the value-function parameters are estimated using temporal difference learning and the policy parameters are updated by stochastic gradient descent . <eos> methods based on policy gradients in this way are of special interest because of their compatibility with function approximation methods , which are needed to handle large or infinite state spaces . <eos> the use of temporal difference learning in this way is of interest because in many applications it dramatically reduces the variance of the gradient estimates . <eos> the use of the natural gradient is of interest because it can produce better conditioned parameterizations and has been shown to further reduce variance in some cases . <eos> our results extend prior two-timescale convergence results for actor-critic methods by konda and tsitsiklis by using temporal difference learning in the actor and by incorporating natural gradients , and they extend prior empirical studies of natural actor-critic methods by peters , vijayakumar and schaal by providing the first convergence proofs and the first fully incremental algorithms .
we describe a novel noisy-logical distribution for representing the distribution of a binary output variable conditioned on multiple binary input variables . <eos> the distribution is represented in terms of noisy-or 's and noisy-and-not 's of causal features which are conjunctions of the binary inputs . <eos> the standard noisy-or and noisy-and-not models , used in causal reasoning and artificial intelligence , are special cases of the noisy-logical distribution . <eos> we prove that the noisy-logical distribution is complete in the sense that it can represent all conditional distributions provided a sufficient number of causal factors are used . <eos> we illustrate the noisy-logical distribution by showing that it can account for new experimental findings on how humans perform causal reasoning in more complex contexts . <eos> finally , we speculate on the use of the noisy-logical distribution for causal reasoning and artificial intelligence .
we propose a bayesian undirected graphical model for co-training , or more generally for semi-supervised multi-view learning . <eos> this makes explicit the previously unstated assumptions of a large class of co-training type algorithms , and also clarifies the circumstances under which these assumptions fail . <eos> building upon new insights from this model , we propose an improved method for co-training , which is a novel co-training kernel for gaussian process classifiers . <eos> the resulting approach is convex and avoids local-maxima problems , unlike some previous multi-view learning methods . <eos> furthermore , it can automatically estimate how much each view should be trusted , and thus accommodate noisy or unreliable views . <eos> experiments on toy data and real world data sets illustrate the benefits of this approach .
we describe an analog-vlsi neural network for face recognition based on subspace methods . <eos> the system uses a dimensionality-reduction network whose coefficients can be either programmed or learned on-chip to perform pca , or programmed to perform lda . <eos> a second network with userprogrammed coefficients performs classification with manhattan distances . <eos> the system uses on-chip compensation techniques to reduce the effects of device mismatch . <eos> using the orl database with 12x12-pixel images , our circuit achieves up to 85 % classification performance ( 98 % of an equivalent software implementation ) .
speech dereverberation remains an open problem after more than three decades of research . <eos> the most challenging step in speech dereverberation is blind channel identification ( bci ) . <eos> although many bci approaches have been developed , their performance is still far from satisfactory for practical applications . <eos> the main difficulty in bci lies in finding an appropriate acoustic model , which not only can effectively resolve solution degeneracies due to the lack of knowledge of the source , but also robustly models real acoustic environments . <eos> this paper proposes a sparse acoustic room impulse response ( rir ) model for bci , that is , an acoustic rir can be modeled by a sparse fir filter . <eos> under this model , we show how to formulate the bci of a single-input multiple-output ( simo ) system into a l1 norm regularized least squares ( ls ) problem , which is convex and can be solved efficiently with guaranteed global convergence . <eos> the sparseness of solutions is controlled by l1 -norm regularization parameters . <eos> we propose a sparse learning scheme that infers the optimal l1 -norm regularization parameters directly from microphone observations under a bayesian framework . <eos> our results show that the proposed approach is effective and robust , and it yields source estimates in real acoustic environments with high fidelity to anechoic chamber measurements .
we present a new analysis for the combination of binary classifiers . <eos> we propose a theoretical framework based on the neyman-pearson lemma to analyze combinations of classifiers . <eos> in particular , we give a method for finding the optimal decision rule for a combination of classifiers and prove that it has the optimal roc curve . <eos> we also show how our method generalizes and improves on previous work on combining classifiers and generating roc curves .
the classical hypothesis , that bottom-up saliency is a center-surround process , is combined with a more recent hypothesis that all saliency decisions are optimal in a decision-theoretic sense . <eos> the combined hypothesis is denoted as discriminant center-surround saliency , and the corresponding optimal saliency architecture is derived . <eos> this architecture equates the saliency of each image location to the discriminant power of a set of features with respect to the classification problem that opposes stimuli at center and surround , at that location . <eos> it is shown that the resulting saliency detector makes accurate quantitative predictions for various aspects of the psychophysics of human saliency , including non-linear properties beyond the reach of previous saliency models . <eos> furthermore , it is shown that discriminant center-surround saliency can be easily generalized to various stimulus modalities ( such as color , orientation and motion ) , and provides optimal solutions for many other saliency problems of interest for computer vision . <eos> optimal solutions , under this hypothesis , are derived for a number of the former ( including static natural images , dense motion fields , and even dynamic textures ) , and applied to a number of the latter ( the prediction of human eye fixations , motion-based saliency in the presence of ego-motion , and motion-based saliency in the presence of highly dynamic backgrounds ) . <eos> in result , discriminant saliency is shown to predict eye fixations better than previous models , and produce background subtraction algorithms that outperform the state-of-the-art in computer vision .
cascade detectors have been shown to operate extremely rapidly , with high accuracy , and have important applications such as face detection . <eos> driven by this success , cascade earning has been an area of active research in recent years . <eos> nevertheless , there are still challenging technical problems during the training process of cascade detectors . <eos> in particular , determining the optimal target detection rate for each stage of the cascade remains an unsolved issue . <eos> in this paper , we propose the multiple instance pruning ( mip ) algorithm for soft cascades . <eos> this algorithm computes a set of thresholds which aggressively terminate computation with no reduction in detection rate or increase in false positive rate on the training dataset . <eos> the algorithm is based on two key insights : i ) examples that are destined to be rejected by the complete classifier can be safely pruned early ; ii ) face detection is a multiple instance learning problem . <eos> the mip process is fully automatic and requires no assumptions of probability distributions , statistical independence , or ad hoc intermediate rejection targets . <eos> experimental results on the mit+cmu dataset demonstrate significant performance advantages .
we introduce a new bayesian model for hierarchical clustering based on a prior over trees called kingman ? s coalescent . <eos> we develop novel greedy and sequential monte carlo inferences which operate in a bottom-up agglomerative fashion . <eos> we show experimentally the superiority of our algorithms over the state-of-the-art , and demonstrate our approach in document clustering and phylolinguistics .
content-based image suggestion ( cbis ) targets the recommendation of products based on user preferences on the visual content of images . <eos> in this paper , we motivate both feature selection and model order identification as two key issues for a successful cbis . <eos> we propose a generative model in which the visual features and users are clustered into separate classes . <eos> we identify the number of both user and image classes with the simultaneous selection of relevant visual features using the message length approach . <eos> the goal is to ensure an accurate prediction of ratings for multidimensional non-gaussian and continuous image descriptors . <eos> experiments on a collected data have demonstrated the merits of our approach .
conditional random fields ( crfs ) are an effective tool for a variety of different data segmentation and labeling tasks including visual scene interpretation , which seeks to partition images into their constituent semantic-level regions and assign appropriate class labels to each region . <eos> for accurate labeling it is important to capture the global context of the image as well as local information . <eos> we introduce a crf based scene labeling model that incorporates both local features and features aggregated over the whole image or large sections of it . <eos> secondly , traditional crf learning requires fully labeled datasets which can be costly and troublesome to produce . <eos> we introduce a method for learning crfs from datasets with many unlabeled nodes by marginalizing out the unknown labels so that the log-likelihood of the known ones can be maximized by gradient ascent . <eos> loopy belief propagation is used to approximate the marginals needed for the gradient and log-likelihood calculations and the bethe free-energy approximation to the log-likelihood is monitored to control the step size . <eos> our experimental results show that effective models can be learned from fragmentary labelings and that incorporating top-down aggregate features significantly improves the segmentations . <eos> the resulting segmentations are compared to the state-of-the-art on three different image datasets .
we present a novel linear clustering framework ( diffrac ) which relies on a linear discriminative cost function and a convex relaxation of a combinatorial optimization problem . <eos> the large convex optimization problem is solved through a sequence of lower dimensional singular value decompositions . <eos> this framework has several attractive properties : ( 1 ) although apparently similar to k-means , it exhibits superior clustering performance than k-means , in particular in terms of robustness to noise . <eos> ( 2 ) it can be readily extended to non linear clustering if the discriminative cost function is based on positive definite kernels , and can then be seen as an alternative to spectral clustering . <eos> ( 3 ) prior information on the partition is easily incorporated , leading to state-of-the-art performance for semi-supervised learning , for clustering or classification . <eos> we present empirical evaluations of our algorithms on synthetic and real medium-scale datasets .
we cast the ranking problem as ( 1 ) multiple classification ( ? mc ? ) <eos> ( 2 ) multiple ordinal classification , which lead to computationally tractable learning algorithms for relevance ranking in web search . <eos> we consider the dcg criterion ( discounted cumulative gain ) , a standard quality measure in information retrieval . <eos> our approach is motivated by the fact that perfect classifications result in perfect dcg scores and the dcg errors are bounded by classification errors . <eos> we propose using the expected relevance to convert class probabilities into ranking scores . <eos> the class probabilities are learned using a gradient boosting tree algorithm . <eos> evaluations on large-scale datasets show that our approach can improve lambdarank [ 5 ] and the regressions-based ranker [ 6 ] , in terms of the ( normalized ) dcg scores . <eos> an efficient implementation of the boosting tree algorithm is also presented .
estimation of three-dimensional articulated human pose and motion from images is a central problem in computer vision . <eos> much of the previous work has been limited by the use of crude generative models of humans represented as articulated collections of simple parts such as cylinders . <eos> automatic initialization of such models has proved difficult and most approaches assume that the size and shape of the body parts are known a priori . <eos> in this paper we propose a method for automatically recovering a detailed parametric model of non-rigid body shape and pose from monocular imagery . <eos> specifically , we represent the body using a parameterized triangulated mesh model that is learned from a database of human range scans . <eos> we demonstrate a discriminative method to directly recover the model parameters from monocular images using a conditional mixture of kernel regressors . <eos> this predicted pose and shape are used to initialize a generative model for more detailed pose and shape estimation . <eos> the resulting approach allows fully automatic pose and shape recovery from monocular and multi-camera imagery . <eos> experimental results show that our method is capable of robustly recovering articulated pose , shape and biometric measurements ( e.g . height , weight , etc . ) <eos> in both calibrated and uncalibrated camera environments .
many tasks in speech processing involve classification of long term characteristics of a speech segment such as language , speaker , dialect , or topic . <eos> a natural technique for determining these characteristics is to first convert the input speech into a sequence of tokens such as words , phones , etc . <eos> from these tokens , we can then look for distinctive phrases , keywords , that characterize the speech . <eos> in many applications , a set of distinctive keywords may not be known a priori . <eos> in this case , an automatic method of building up keywords from short context units such as phones is desirable . <eos> we propose a method for construction of keywords based upon support vector machines . <eos> we cast the problem of keyword selection as a feature selection problem for n-grams of phones . <eos> we propose an alternating filter-wrapper method that builds successively longer keywords . <eos> application of this method on a language recognition task shows that the technique produces interesting and significant qualitative and quantitative results .
we describe a neurobiologically plausible model to implement dynamic routing using the concept of neuronal communication through neuronal coherence . <eos> the model has a three-tier architecture : a raw input tier , a routing control tier , and an invariant output tier . <eos> the correct mapping between input and output tiers is realized by an appropriate alignment of the phases of their respective background oscillations by the routing control units . <eos> we present an example architecture , implemented on a neuromorphic chip , that is able to achieve circular-shift invariance . <eos> a simple extension to our model can accomplish circular-shift dynamic routing with only o ( n ) connections , compared to o ( n 2 ) connections required by traditional models .
we give a new class of outer bounds on the marginal polytope , and propose a cutting-plane algorithm for efficiently optimizing over these constraints . <eos> when combined with a concave upper bound on the entropy , this gives a new variational inference algorithm for probabilistic inference in discrete markov random fields ( mrfs ) . <eos> valid constraints on the marginal polytope are derived through a series of projections onto the cut polytope . <eos> as a result , we obtain tighter upper bounds on the log-partition function . <eos> we also show empirically that the approximations of the marginals are significantly more accurate when using the tighter outer bounds . <eos> finally , we demonstrate the advantage of the new constraints for finding the map assignment in protein structure prediction .
we consider the estimation problem in gaussian graphical models with arbitrary structure . <eos> we analyze the embedded trees algorithm , which solves a sequence of problems on tractable subgraphs thereby leading to the solution of the estimation problem on an intractable graph . <eos> our analysis is based on the recently developed walk-sum interpretation of gaussian estimation . <eos> we show that non-stationary iterations of the embedded trees algorithm using any sequence of subgraphs converge in walk-summable models . <eos> based on walk-sum calculations , we develop adaptive methods that optimize the choice of subgraphs used at each iteration with a view to achieving maximum reduction in error . <eos> these adaptive procedures provide a significant speedup in convergence over stationary iterative methods , and also appear to converge in a larger class of models .
when predicting class labels for objects within a relational database , it is often helpful to consider a model for relationships : this allows for information between class labels to be shared and to improve prediction performance . <eos> however , there are different ways by which objects can be related within a relational database . <eos> one traditional way corresponds to a markov network structure : each existing relation is represented by an undirected edge . <eos> this encodes that , conditioned on input features , each object label is independent of other object labels given its neighbors in the graph . <eos> however , there is no reason why markov networks should be the only representation of choice for symmetric dependence structures . <eos> here we discuss the case when relationships are postulated to exist due to hidden common causes . <eos> we discuss how the resulting graphical model differs from markov networks , and how it describes different types of real-world relational processes . <eos> a bayesian nonparametric classification model is built upon this graphical representation and evaluated with several empirical studies .
bayesian model averaging , model selection and their approximations such as bic are generally statistically consistent , but sometimes achieve slower rates of convergence than other methods such as aic and leave-one-out cross-validation . <eos> on the other hand , these other methods can be inconsistent . <eos> we identify the catch-up phenomenon as a novel explanation for the slow convergence of bayesian methods . <eos> based on this analysis we define the switch-distribution , a modification of the bayesian model averaging distribution . <eos> we prove that in many situations model selection and prediction based on the switch-distribution is both consistent and achieves optimal convergence rates , thereby resolving the aic-bic dilemma . <eos> the method is practical ; we give an efficient algorithm .
in recent years , the language model latent dirichlet allocation ( lda ) , which clusters co-occurring words into topics , has been widely appled in the computer vision field . <eos> however , many of these applications have difficulty with modeling the spatial and temporal structure among visual words , since lda assumes that a document is a `` bag-of-words '' . <eos> it is also critical to properly design `` words '' and documentswhen using a language model to solve vision problems . <eos> in this paper , we propose a topic model spatial latent dirichlet allocation ( slda ) , which better encodes spatial structure among visual words that are essential for solving many vision problems . <eos> the spatial information is not encoded in the value of visual words but in the design of documents . <eos> instead of knowing the partition of words into documents \textit { a priori } , the word-document assignment becomes a random hidden variable in slda . <eos> there is a generative procedure , where knowledge of spatial structure can be flexibly added as a prior , grouping visual words which are close in space into the same document . <eos> we use slda to discover objects from a collection of images , and show it achieves better performance than lda .
we describe an efficient learning procedure for multilayer generative models that combine the best aspects of markov random fields and deep , directed belief nets . <eos> the generative models can be learned one layer at a time and when learning is complete they have a very fast inference procedure for computing a good approximation to the posterior distribution in all of the hidden layers . <eos> each hidden layer has its own mrf whose energy function is modulated by the top-down directed connections from the layer above . <eos> to generate from the model , each layer in turn must settle to equilibrium given its top-down input . <eos> we show that this type of model is good at capturing the statistics of patches of natural images .
recent research has studied the role of sparsity in high dimensional regression and signal reconstruction , establishing theoretical limits for recovering sparse models from sparse data . <eos> in this paper we study a variant of this problem where the original $ n $ input variables are compressed by a random linear transformation to $ m \ll n $ examples in $ p $ dimensions , and establish conditions under which a sparse linear model can be successfully recovered from the compressed data . <eos> a primary motivation for this compression procedure is to anonymize the data and preserve privacy by revealing little information about the original data . <eos> we characterize the number of random projections that are required for $ \ell_1 $ -regularized compressed regression to identify the nonzero coefficients in the true model with probability approaching one , a property called `` sparsistence . '' <eos> in addition , we show that $ \ell_1 $ -regularized compressed regression asymptotically predicts as well as an oracle linear model , a property called `` persistence . '' <eos> finally , we characterize the privacy properties of the compression procedure in information-theoretic terms , establishing upper bounds on the rate of information communicated between the compressed and uncompressed data that decay to zero .
we present a nonparametric bayesian method of estimating variable order markov processes up to a theoretically infinite order . <eos> by extending a stick-breaking prior , which is usually defined on a unit interval , verticallyto the trees of infinite depth associated with a hierarchical chinese restaurant process , our model directly infers the hidden orders of markov dependencies from which each symbol originated . <eos> experiments on character and word sequences in natural language showed that the model has a comparative performance with an exponentially large full-order model , while computationally much efficient in both time and space . <eos> we expect that this basic model will also extend to the variable order hierarchical clustering of general data .
diffusion processes are a family of continuous-time continuous-state stochastic processes that are in general only partially observed . <eos> the joint estimation of the forcing parameters and the system noise ( volatility ) in these dynamical systems is a crucial , but non-trivial task , especially when the system is nonlinear and multi-modal . <eos> we propose a variational treatment of diffusion processes , which allows us to estimate these parameters by simple gradient techniques and which is computationally less demanding than most mcmc approaches . <eos> furthermore , our parameter inference scheme does not break down when the time step gets smaller , unlike most current approaches . <eos> finally , we show how a cheap estimate of the posterior over the parameters can be constructed based on the variational free energy .
we consider the ensemble clustering problem where the task is to aggregatemultiple clustering solutions into a single consolidated clustering that maximizes the shared information among given clustering solutions . <eos> we obtain several new results for this problem . <eos> first , we note that the notion of agreement under such circumstances can be better captured using an agreement measure based on a 2d string encoding rather than voting strategy based methods proposed in literature . <eos> using this generalization , we first derive a nonlinear optimization model to maximize the new agreement measure . <eos> we then show that our optimization problem can be transformed into a strict 0-1 semidefinite program ( sdp ) via novel convexification techniques which can subsequently be relaxed to a polynomial time solvable sdp . <eos> our experiments indicate improvements not only in terms of the proposed agreement measure but also the existing agreement measures based on voting strategies . <eos> we discuss evaluations on clustering and image segmentation databases .
in this paper we develop a gaussian process ( gp ) framework to model a collection of reciprocal random variables defined on the \emph { edges } of a network . <eos> we show how to construct gp priors , i.e. , ~covariance functions , on the edges of directed , undirected , and bipartite graphs . <eos> the model suggests an intimate connection between \emph { link prediction } and \emph { transfer learning } , which were traditionally considered two separate research topics . <eos> though a straightforward gp inference has a very high complexity , we develop an efficient learning algorithm that can handle a large number of observations . <eos> the experimental results on several real-world data sets verify superior learning capacity .
loopy belief propagation has been employed in a wide variety of applications with great empirical success , but it comes with few theoretical guarantees . <eos> in this paper we investigate the use of the max-product form of belief propagation for weighted matching problems on general graphs . <eos> we show that max-product converges to the correct answer if the linear programming ( lp ) relaxation of the weighted matching problem is tight and does not converge if the lp relaxation is loose . <eos> this provides an exact characterization of max-product performance and reveals connections to the widely used optimization technique of lp relaxation . <eos> in addition , we demonstrate that max-product is effective in solving practical weighted matching problems in a distributed fashion by applying it to the problem of self-organization in sensor networks .
using multiple regularization hyperparameters is an effective method for managing model complexity in problems where input features have varying amounts of noise . <eos> while algorithms for choosing multiple hyperparameters are often used in neural networks and support vector machines , they are not common in structured prediction tasks , such as sequence labeling or parsing . <eos> in this paper , we consider the problem of learning regularization hyperparameters for log-linear models , a class of probabilistic models for structured prediction tasks which includes conditional random fields ( crfs ) . <eos> using an implicit differentiation trick , we derive an efficient gradient-based method for learning gaussian regularization priors with multiple hyperparameters . <eos> in both simulations and the real-world task of computational rna secondary structure prediction , we find that multiple hyperparameter learning provides a significant boost in accuracy compared to models learned using only a single regularization hyperparameter .
the present work aims to model the correspondence between facial motion and speech . <eos> the face and sound are modelled separately , with phonemes being the link between both . <eos> we propose a sequential model and evaluate its suitability for the generation of the facial animation from a sequence of phonemes , which we obtain from speech . <eos> we evaluate the results both by computing the error between generated sequences and real video , as well as with a rigorous double-blind test with human subjects . <eos> experiments show that our model compares favourably to other existing methods and that the sequences generated are comparable to real video sequences .
a method is proposed for semiparametric estimation where parametric and nonparametric criteria are exploited in density estimation and unsupervised learning . <eos> this is accomplished by making sampling assumptions on a dataset that smoothly interpolate between the extreme of independently distributed ( or id ) sample data ( as in nonparametric kernel density estimators ) to the extreme of independent identically distributed ( or iid ) sample data . <eos> this article makes independent similarly distributed ( or isd ) sampling assumptions and interpolates between these two using a scalar parameter . <eos> the parameter controls a bhattacharyya affinity penalty between pairs of distributions on samples . <eos> surprisingly , the isd method maintains certain consistency and unimodality properties akin to maximum likelihood estimation . <eos> the proposed isd scheme is an alternative for handling nonstationarity in data without making drastic hidden variable assumptions which often make estimation difficult and laden with local optima . <eos> experiments in density estimation on a variety of datasets confirm the value of isd over iid estimation , id estimation and mixture modeling .
this paper describes a new model for human visual classification that enables the recovery of image features that explain human subjects ' performance on different visual classification tasks . <eos> unlike previous methods , this algorithm does not model their performance with a single linear classifier operating on raw image pixels . <eos> instead , it models classification as the combination of multiple feature detectors . <eos> this approach extracts more information about human visual classification than has been previously possible with other methods and provides a foundation for further exploration .
we derive an equation for temporal difference learning from statistical principles . <eos> specifically , we start with the variational principle and then bootstrap to produce an updating rule for discounted state value estimates . <eos> the resulting equation is similar to the standard equation for temporal difference learning with eligibility traces , so called td ( ) , however it lacks the parameter that specifies the learning rate . <eos> in the place of this free parameter there is now an equation for the learning rate that is specific to each state transition . <eos> we experimentally test this new learning rule against td ( ? ) <eos> and find that it offers superior performance in various settings . <eos> finally , we make some preliminary investigations into how to extend our new temporal difference algorithm to reinforcement learning . <eos> to do this we combine our update equation with both watkins q ( ? ) <eos> and sarsa ( ? ) <eos> and find that it again offers superior performance without a learning rate parameter .
point-based algorithms have been surprisingly successful in computing approximately optimal solutions for partially observable markov decision processes ( pomdps ) in high dimensional belief spaces . <eos> in this work , we seek to understand the belief-space properties that allow some pomdp problems to be approximated efficiently and thus help to explain the point-based algorithms ? <eos> success often observed in the experiments . <eos> we show that an approximately optimal pomdp solution can be computed in time polynomial in the covering number of a reachable belief space , which is the subset of the belief space reachable from a given belief point . <eos> we also show that under the weaker condition of having a small covering number for an optimal reachable space , which is the subset of the belief space reachable under an optimal policy , computing an approximately optimal solution is np-hard . <eos> however , given a suitable set of points that coveran optimal reachable space well , an approximate solution can be computed in polynomial time . <eos> the covering number highlights several interesting properties that reduce the complexity of pomdp planning in practice , e.g. , fully observed state variables , beliefs with sparse support , smooth beliefs , and circulant state-transition matrices .
web servers on the internet need to maintain high reliability , but the cause of intermittent failures of web transactions is non-obvious . <eos> we use bayesian inference to diagnose problems with web services . <eos> this diagnosis problem is far larger than any previously attempted : it requires inference of 10^4 possible faults from 10^5 observations . <eos> further , such inference must be performed in less than a second . <eos> inference can be done at this speed by combining a variational approximation , a mean-field approximation , and the use of stochastic gradient descent to optimize a variational cost function . <eos> we use this fast inference to diagnose a time series of anomalous http requests taken from a real web service . <eos> the inference is fast enough to analyze network logs with billions of entries in a matter of hours .
we study the problem of an apprentice learning to behave in an environment with an unknown reward function by observing the behavior of an expert . <eos> we follow on the work of abbeel and ng [ 1 ] who considered a framework in which the true reward function is assumed to be a linear combination of a set of known and observable features . <eos> we give a new algorithm that , like theirs , is guaranteed to learn a policy that is nearly as good as the expert ? s , given enough examples . <eos> however , unlike their algorithm , we show that ours may produce a policy that is substantially better than the expert ? s . <eos> moreover , our algorithm is computationally faster , is easier to implement , and can be applied even in the absence of an expert . <eos> the method is based on a game-theoretic view of the problem , which leads naturally to a direct application of the multiplicative-weights algorithm of freund and schapire [ 2 ] for playing repeated matrix games . <eos> in addition to our formal presentation and analysis of the new algorithm , we sketch how the method can be applied when the transition function itself is unknown , and we provide an experimental demonstration of the algorithm on a toy video-game environment .
this article discusses a latent variable model for inference and prediction of symmetric relational data . <eos> the model , based on the idea of the eigenvalue decomposition , represents the relationship between two nodes as the weighted inner-product of node-specific vectors of latent characteristics . <eos> this `` eigenmodel '' generalizes other popular latent variable models , such as latent class and distance models : it is shown mathematically that any latent class or distance model has a representation as an eigenmodel , but not vice-versa . <eos> the practical implications of this are examined in the context of three real datasets , for which the eigenmodel has as good or better out-of-sample predictive performance than the other two models .
active learning sequentially selects unlabeled instances to label with the goal of reducing the effort needed to learn a good classifier . <eos> most previous studies in active learning have focused on selecting one unlabeled instance at one time while retraining in each iteration . <eos> however , single instance selection systems are unable to exploit a parallelized labeler when one is available . <eos> recently a few batch mode active learning approaches have been proposed that select a set of most informative unlabeled instances in each iteration , guided by some heuristic scores . <eos> in this paper , we propose a discriminative batch mode active learning approach that formulates the instance selection task as a continuous optimization problem over auxiliary instance selection variables . <eos> the optimization is formuated to maximize the discriminative classification performance of the target classifier , while also taking the unlabeled data into account . <eos> although the objective is not convex , we can manipulate a quasi-newton method to obtain a good local solution . <eos> our empirical studies on uci datasets show that the proposed active learning is more effective than current state-of-the art batch mode active learning algorithms .
markov jump processes play an important role in a large number of application domains . <eos> however , realistic systems are analytically intractable and they have traditionally been analysed using simulation based techniques , which do not provide a framework for statistical inference . <eos> we propose a mean field approximation to perform posterior inference and parameter estimation . <eos> the approximation allows a practical solution to the inference problem , { while still retaining a good degree of accuracy . } <eos> we illustrate our approach on two biologically motivated systems .
the control of high-dimensional , continuous , non-linear systems is a key problem in reinforcement learning and control . <eos> local , trajectory-based methods , using techniques such as differential dynamic programming ( ddp ) are not directly subject to the curse of dimensionality , but generate only local controllers . <eos> in this paper , we introduce receding horizon ddp ( rh-ddp ) , an extension to the classic ddp algorithm , which allows us to construct stable and robust controllers based on a library of local-control trajectories . <eos> we demonstrate the effectiveness of our approach on a series of high-dimensional control problems using a simulated multi-link swimming robot . <eos> these experiments show that our approach effectively circumvents dimensionality issues , and is capable of dealing effectively with problems with ( at least ) 34 state and 14 action dimensions .
it is becoming increasingly evident that organisms acting in uncertain dynamical environments often employ exact or approximate bayesian statistical calculations in order to continuously estimate the environmental state , integrate information from multiple sensory modalities , form predictions and choose actions . <eos> what is less clear is how these putative computations are implemented by cortical neural networks . <eos> an additional level of complexity is introduced because these networks observe the world through spike trains received from primary sensory afferents , rather than directly . <eos> a recent line of research has described mechanisms by which such computations can be implemented using a network of neurons whose activity directly represents a probability distribution across the possible ? world states ? . <eos> much of this work , however , uses various approximations , which severely restrict the domain of applicability of these implementations . <eos> here we make use of rigorous mathematical results from the theory of continuous time point process filtering , and show how optimal real-time state estimation and prediction may be implemented in a general setting using linear neural networks . <eos> we demonstrate the applicability of the approach with several examples , and relate the required network properties to the statistical nature of the environment , thereby quantifying the compatibility of a given network with its environment .
generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons . <eos> here we present a bayesian treatment of such models . <eos> using the expectation propagation algorithm , we are able to approximate the full posterior distribution over all weights . <eos> in addition , we use a laplacian prior to favor sparse solutions . <eos> therefore , stimulus features that do not critically influence neural activity will be assigned zero weights and thus be effectively excluded by the model . <eos> this feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities . <eos> the posterior distribution can be used to obtain confidence intervals which makes it possible to assess the statistical significance of the solution . <eos> in neural data analysis , the available amount of experimental measurements is often limited whereas the parameter space is large . <eos> in such a situation , both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential . <eos> we apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical significance of functional couplings between neurons . <eos> furthermore we used the sparsity of the laplace prior to select those filters from a spike-triggered covariance analysis that are most informative about the neural response .
people perform a remarkable range of tasks that require search of the visual environment for a target item among distractors . <eos> the guided search model ( wolfe , 1994 , 2007 ) , or gs , is perhaps the best developed psychological account of human visual search . <eos> to prioritize search , gs assigns saliency to locations in the visual field . <eos> saliency is a linear combination of activations from retinotopic maps representing primitive visual features . <eos> gs includes heuristics for setting the gain coefficient associated with each map . <eos> variants of gs have formalized the notion of optimization as a principle of attentional control ( e.g. , baldwin & mozer , 2006 ; cave , 1999 ; navalpakkam & itti , 2006 ; rao et al. , 2002 ) , but every gs-like model must be ? dumbed down ? <eos> to match human data , e.g. , by corrupting the saliency map with noise and by imposing arbitrary restrictions on gain modulation . <eos> we propose a principled probabilistic formulation of gs , called experience-guided search ( egs ) , based on a generative model of the environment that makes three claims : ( 1 ) feature detectors produce poisson spike trains whose rates are conditioned on feature type and whether the feature belongs to a target or distractor ; ( 2 ) the environment and/or task is nonstationary and can change over a sequence of trials ; and ( 3 ) a prior specifies that features are more likely to be present for target than for distractors . <eos> through experience , egs infers latent environment variables that determine the gains for guiding search . <eos> control is thus cast as probabilistic inference , not optimization . <eos> we show that egs can replicate a range of human data from visual search , including data that gs does not address .
we provide provably privacy-preserving versions of belief propagation , gibbs sampling , and other local algorithms ? <eos> distributed multiparty protocols in which each party or vertex learns only its final local value , and absolutely nothing else .
we consider the learning task consisting in predicting as well as the best function in a finite reference set g up to the smallest possible additive term . <eos> if r ( g ) denotes the generalization error of a prediction function g , under reasonable assumptions on the loss function ( typically satisfied by the least square loss when the output is bounded ) , it is known that the progressive mixture rule g_n satisfies e r ( g_n ) < min_ { g in g } r ( g ) + cst ( log|g| ) /n where n denotes the size of the training set , e denotes the expectation wrt the training set distribution . <eos> this work shows that , surprisingly , for appropriate reference sets g , the deviation convergence rate of the progressive mixture rule is only no better than cst / sqrt { n } , and not the expected cst / n. it also provides an algorithm which does not suffer from this drawback .
this paper introduces kernels on attributed pointsets , which are sets of vectors embedded in an euclidean space . <eos> the embedding gives the notion of neighborhood , which is used to define positive semidefinite kernels on pointsets . <eos> two novel kernels on neighborhoods are proposed , one evaluating the attribute similarity and the other evaluating shape similarity . <eos> shape similarity function is motivated from spectral graph matching techniques . <eos> the kernels are tested on three real life applications : face recognition , photo album tagging , and shot annotation in video sequences , with encouraging results .
we present a general boosting method extending functional gradient boosting to optimize complex loss functions that are encountered in many machine learning problems . <eos> our approach is based on optimization of quadratic upper bounds of the loss functions which allows us to present a rigorous convergence analysis of the algorithm . <eos> more importantly , this general framework enables us to use a standard regression base learner such as decision trees for fitting any loss function . <eos> we illustrate an application of the proposed method in learning ranking functions for web search by combining both preference data and labeled data for training . <eos> we present experimental results for web search using data from a commercial search engine that show significant improvements of our proposed methods over some existing methods .
extensive games are a powerful model of multiagent decision-making scenarios with incomplete information . <eos> finding a nash equilibrium for very large instances of these games has received a great deal of recent attention . <eos> in this paper , we describe a new technique for solving large games based on regret minimization . <eos> in particular , we introduce the notion of counterfactual regret , which exploits the degree of incomplete information in an extensive game . <eos> we show how minimizing counterfactual regret minimizes overall regret , and therefore in self-play can be used to compute a nash equilibrium . <eos> we demonstrate this technique in the domain of poker , showing we can solve abstractions of limit texas hold ? em with as many as 1012 states , two orders of magnitude larger than previous methods .
maximum variance unfolding ( mvu ) is an effective heuristic for dimensionality reduction . <eos> it produces a low-dimensional representation of the data by maximizing the variance of their embeddings while preserving the local distances of the original data . <eos> we show that mvu also optimizes a statistical dependence measure which aims to retain the identity of individual observations under the distancepreserving constraints . <eos> this general view allows us to design coloredvariants of mvu , which produce low-dimensional representations for a given task , e.g . subject to class labels or other side information .
we describe a new algorithm , relaxed survey propagation ( rsp ) , for finding map configurations in markov random fields . <eos> we compare its performance with state-of-the-art algorithms including the max-product belief propagation , its sequential tree-reweighted variant , residual ( sum-product ) belief propagation , and tree-structured expectation propagation . <eos> we show that it outperforms all approaches for ising models with mixed couplings , as well as on a web person disambiguation task formulated as a supervised clustering problem .
we address the problem of factorial learning which associates a set of latent causes or features with the observed data . <eos> factorial models usually assume that each feature has a single occurrence in a given data point . <eos> however , there are data such as images where latent features have multiple occurrences , e.g . a visual object class can have multiple instances shown in the same image . <eos> to deal with such cases , we present a probability model over non-negative integer valued matrices with possibly unbounded number of columns . <eos> this model can play the role of the prior in an nonparametric bayesian learning scenario where both the latent features and the number of their occurrences are unknown . <eos> we use this prior together with a likelihood model for unsupervised learning from images using a markov chain monte carlo inference algorithm .
a general modeling framework is proposed that unifies nonparametric-bayesian models , topic-models and bayesian networks . <eos> this class of infinite state bayes nets ( isbn ) can be viewed as directed networks of ? hierarchical dirichlet processes ? <eos> ( hdps ) where the domain of the variables can be structured ( e.g . words in documents or features in images ) . <eos> we show that collapsed gibbs sampling can be done efficiently in these models by leveraging the structure of the bayes net and using the forward-filtering-backward-sampling algorithm for junction trees . <eos> existing models , such as nested-dp , pachinko allocation , mixed membership stochastic block models as well as a number of new models are described as isbns . <eos> two experiments have been performed to illustrate these ideas .
recent experimental studies have focused on the specialization of different neural structures for different types of instrumental behavior . <eos> recent theoretical work has provided normative accounts for why there should be more than one control system , and how the output of different controllers can be integrated . <eos> two particlar controllers have been identified , one associated with a forward model and the prefrontal cortex and a second associated with computationally simpler , habitual , actor-critic methods and part of the striatum . <eos> we argue here for the normative appropriateness of an additional , but so far marginalized control system , associated with episodic memory , and involving the hippocampus and medial temporal cortices . <eos> we analyze in depth a class of simple environments to show that episodic control should be useful in a range of cases characterized by complexity and inferential noise , and most particularly at the very early stages of learning , long before habitization has set in . <eos> we interpret data on the transfer of control from the hippocampus to the striatum in the light of this hypothesis .
computational models of visual cortex , and in particular those based on sparse coding , have enjoyed much recent attention . <eos> despite this currency , the question of how sparse or how over-complete a sparse representation should be , has gone without principled answer . <eos> here , we use bayesian model-selection methods to address these questions for a sparse-coding model based on a student-t prior . <eos> having validated our methods on toy data , we find that natural images are indeed best modelled by extremely sparse distributions ; although for the student-t prior , the associated optimal basis size is only modestly overcomplete .
motivated in part by the hierarchical organization of cortex , a number of algorithms have recently been proposed that try to learn hierarchical , or `` deep , '' structure from unlabeled data . <eos> while several authors have formally or informally compared their algorithms to computations performed in visual area v1 ( and the cochlea ) , little attempt has been made thus far to evaluate these algorithms in terms of their fidelity for mimicking computations at deeper levels in the cortical hierarchy . <eos> this paper presents an unsupervised learning model that faithfully mimics certain properties of visual area v2 . <eos> specifically , we develop a sparse variant of the deep belief networks of hinton et al . ( 2006 ) . <eos> we learn two layers of nodes in the network , and demonstrate that the first layer , similar to prior work on sparse coding and ica , results in localized , oriented , edge filters , similar to the gabor functions known to model v1 cell receptive fields . <eos> further , the second layer in our model encodes correlations of the first layer responses in the data . <eos> specifically , it picks up both collinear ( `` contour '' ) features as well as corners and junctions . <eos> more interestingly , in a quantitative comparison , the encoding of these more complex `` corner '' features matches well with the results from the ito & komatsu 's study of biological v2 responses . <eos> this suggests that our sparse variant of deep belief networks holds promise for modeling more higher-order features .
we present a simple new criterion for classification , based on principles from lossy data compression . <eos> the criterion assigns a test sample to the class that uses the minimum number of additional bits to code the test sample , subject to an allowable distortion . <eos> we prove asymptotic optimality of this criterion for gaussian data and analyze its relationships to classical classifiers . <eos> theoretical results provide new insights into relationships among popular classifiers such as map and rda , as well as unsupervised clustering methods based on lossy compression [ 13 ] . <eos> minimizing the lossy coding length induces a regularization effect which stabilizes the ( implicit ) density estimate in a small-sample setting . <eos> compression also provides a uniform means of handling classes of varying dimension . <eos> this simple classification criterion and its kernel and local versions perform competitively against existing classifiers on both synthetic examples and real imagery data such as handwritten digits and human faces , without requiring domain-specific information .
we investigate a family of inference problems on markov models , where many sample paths are drawn from a markov chain and partial information is revealed to an observer who attempts to reconstruct the sample paths . <eos> we present algorithms and hardness results for several variants of this problem which arise by revealing different information to the observer and imposing different requirements for the reconstruction of sample paths . <eos> our algorithms are analogous to the classical viterbi algorithm for hidden markov models , which finds the single most probable sample path given a sequence of observations . <eos> our work is motivated by an important application in ecology : inferring bird migration paths from a large database of observations .
we summarize the implementation of an analog vlsi chip hosting a network of 32 integrate-and-fire ( if ) neurons with spike-frequency adaptation and 2,048 hebbian plastic bistable spike-driven stochastic synapses endowed with a self-regulating mechanism which stops unnecessary synaptic changes . <eos> the synaptic matrix can be flexibly configured and provides both recurrent and aer-based connectivity with external , aer compliant devices . <eos> we demonstrate the ability of the network to efficiently classify overlapping patterns , thanks to the self-regulating mechanism .
we present a novel bayesian model for semi-supervised part-of-speech tagging . <eos> our model extends the latent dirichlet allocation model and incorporates the intuition that words ? <eos> distributions over tags , p ( t|w ) , are sparse . <eos> in addition we introduce a model for determining the set of possible tags of a word which captures important dependencies in the ambiguity classes of words . <eos> our model outperforms the best previously proposed model for this task on a standard dataset .
learning in real-world domains often requires to deal with continuous state and action spaces . <eos> although many solutions have been proposed to apply reinforcement learning algorithms to continuous state problems , the same techniques can be hardly extended to continuous action spaces , where , besides the computation of a good approximation of the value function , a fast method for the identification of the highest-valued action is needed . <eos> in this paper , we propose a novel actor-critic approach in which the policy of the actor is estimated through sequential monte carlo methods . <eos> the importance sampling step is performed on the basis of the values learned by the critic , while the resampling step modifies the actor ? s policy . <eos> the proposed approach has been empirically compared to other learning algorithms into several domains ; in this paper , we report results obtained in a control problem consisting of steering a boat across a river .
we study the rates of growth of the regret in online convex optimization . <eos> first , we show that a simple extension of the algorithm of hazan et al eliminates the need for a priori knowledge of the lower bound on the second derivatives of the observed functions . <eos> we then provide an algorithm , adaptive online gradient descent , which interpolates between the results of zinkevich for linear functions and of hazan et al for strongly convex functions , achieving intermediate rates ? <eos> between t and log t . <eos> furthermore , we show strong optimality of the algorithm . <eos> finally , we provide an extension of our results to general norms .
functional magnetic resonance imaging ( fmri ) provides an unprecedented window into the complex functioning of the human brain , typically detailing the activity of thousands of voxels during hundreds of sequential time points . <eos> unfortunately , the interpretation of fmri is complicated due both to the relatively unknown connection between the hemodynamic response and neural activity and the unknown spatiotemporal characteristics of the cognitive patterns themselves . <eos> here , we use data from the experience based cognition competition to compare global and local methods of prediction applying both linear and nonlinear techniques of dimensionality reduction . <eos> we build global low dimensional representations of an fmri dataset , using linear and nonlinear methods . <eos> we learn a set of time series that are implicit functions of the fmri data , and predict the values of these times series in the future from the knowledge of the fmri data only . <eos> we find effective , low-dimensional models based on the principal components of cognitive activity in classically-defined anatomical regions , the brodmann areas . <eos> furthermore for some of the stimuli , the top predictive regions were stable across subjects and episodes , including wernicke ? s area for verbal instructions , visual cortex for facial and body features , and visual-temporal regions ( brodmann area 7 ) for velocity . <eos> these interpretations and the relative simplicity of our approach provide a transparent and conceptual basis upon which to build more sophisticated techniques for fmri decoding . <eos> to our knowledge , this is the first time that classical areas have been used in fmri for an effective prediction of complex natural experience .
we study boosting in the filtering setting , where the booster draws examples from an oracle instead of using a fixed training set and so may train efficiently on very large datasets . <eos> our algorithm , which is based on a logistic regression technique proposed by collins , schapire , & singer , requires fewer assumptions to achieve bounds equivalent to or better than previous work . <eos> moreover , we give the first proof that the algorithm of collins et al . is a strong pac learner , albeit within the filtering setting . <eos> our proofs demonstrate the algorithm ? s strong theoretical properties for both classification and conditional probability estimation , and we validate these results through extensive experiments . <eos> empirically , our algorithm proves more robust to noise and overfitting than batch boosters in conditional probability estimation and proves competitive in classification .
a novel approach to measure the interdependence of two time series is proposed , referred to as ? stochastic event synchrony ? <eos> ( ses ) ; it quantifies the alignment of two point processes by means of the following parameters : time delay , variance of the timing jitter , fraction of spuriousevents , and average similarity of events . <eos> ses may be applied to generic one-dimensional and multi-dimensional point processes , however , the paper mainly focusses on point processes in time-frequency domain . <eos> the average event similarity is in that case described by two parameters : the average frequency offset between events in the time-frequency plane , and the variance of the frequency offset ( ? frequency jitter ? <eos> ) ; ses then consists of five parameters in total . <eos> those parameters quantify the synchrony of oscillatory events , and hence , they provide an alternative to existing synchrony measures that quantify amplitude or phase synchrony . <eos> the pairwise alignment of point processes is cast as a statistical inference problem , which is solved by applying the maxproduct algorithm on a graphical model . <eos> the ses parameters are determined from the resulting pairwise alignment by maximum a posteriori ( map ) estimation . <eos> the proposed interdependence measure is applied to the problem of detecting anomalies in eeg synchrony of mild cognitive impairment ( mci ) patients ; the results indicate that ses significantly improves the sensitivity of eeg in detecting mci .
this contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms . <eos> the analysis shows distinct tradeoffs for the case of small-scale and large-scale learning problems . <eos> small-scale learning problems are subject to the usual approximation -- estimation tradeoff . <eos> large-scale learning problems are subject to a qualitatively different tradeoff involving the computational complexity of the underlying optimization algorithms in non-trivial ways .
we introduce a functional representation of time series which allows forecasts to be performed over an unspecified horizon with progressively-revealed information sets . <eos> by virtue of using gaussian processes , a complete covariance matrix between forecasts at several time-steps is available . <eos> this information is put to use in an application to actively trade price spreads between commodity futures contracts . <eos> the approach delivers impressive out-of-sample risk-adjusted returns after transaction costs on a portfolio of 30 spreads .
we present an agnostic active learning algorithm for any hypothesis class of bounded vc dimension under arbitrary data distributions . <eos> most previous work on active learning either makes strong distributional assumptions , or else is computationally prohibitive . <eos> our algorithm extends the simple scheme of cohn , atlas , and ladner [ 1 ] to the agnostic setting , using reductions to supervised learning that harness generalization bounds in a simple but subtle manner . <eos> we provide a fall-back guarantee that bounds the algorithm ? s label complexity by the agnostic pac sample complexity . <eos> our analysis yields asymptotic label complexity improvements for certain hypothesis classes and distributions . <eos> we also demonstrate improvements experimentally .
we propose a method for reconstruction of human brain states directly from functional neuroimaging data . <eos> the method extends the traditional multivariate regression analysis of discretized fmri data to the domain of stochastic functional measurements , facilitating evaluation of brain responses to naturalistic stimuli and boosting the power of functional imaging . <eos> the method searches for sets of voxel timecourses that optimize a multivariate functional linear model in terms of rsquare-statistic . <eos> population based incremental learning is used to search for spatially distributed voxel clusters , taking into account the variation in haemodynamic lag across brain areas and among subjects by voxel-wise non-linear registration of stimuli to fmri data . <eos> the method captures spatially distributed brain responses to naturalistic stimuli without attempting to localize function . <eos> application of the method for prediction of naturalistic stimuli from new and unknown fmri data shows that the approach is capable of identifying distributed clusters of brain locations that are highly predictive of a specific stimuli .
in this paper we formulate a novel and/or graph representation capable of describing the different configurations of deformable articulated objects such as horses . <eos> the representation makes use of the summarization principle so that lower level nodes in the graph only pass on summary statistics to the higher level nodes . <eos> the probability distributions are invariant to position , orientation , and scale . <eos> we develop a novel inference algorithm that combined a bottom-up process for proposing configurations for horses together with a top-down process for refining and validating these proposals . <eos> the strategy of surround suppression is applied to ensure that the inference time is polynomial in the size of input data . <eos> the algorithm was applied to the tasks of detecting , segmenting and parsing horses . <eos> we demonstrate that the algorithm is fast and comparable with the state of the art approaches .
we introduce supervised latent dirichlet allocation ( slda ) , a statistical model of labelled documents . <eos> the model accommodates a variety of response types . <eos> we derive a maximum-likelihood procedure for parameter estimation , which relies on variational approximations to handle intractable posterior expectations . <eos> prediction problems motivate this research : we use the fitted model to predict response values for new documents . <eos> we test slda on two real-world problems : movie ratings predicted from reviews , and web page popularity predicted from text descriptions . <eos> we illustrate the benefits of slda versus modern regularized regression , as well as versus an unsupervised lda analysis followed by a separate regression .
we present an algorithm called optimistic linear programming ( olp ) for learning to optimize average reward in an irreducible but otherwise unknown markov decision process ( mdp ) . <eos> olp uses its experience so far to estimate the mdp . <eos> it chooses actions by optimistically maximizing estimated future rewards over a set of next-state transition probabilities that are close to the estimates : a computation that corresponds to solving linear programs . <eos> we show that the total expected reward obtained by olp up to time $ t $ is within $ c ( p ) \log t $ of the reward obtained by the optimal policy , where $ c ( p ) $ is an explicit , mdp-dependent constant . <eos> olp is closely related to an algorithm proposed by burnetas and katehakis with four key differences : olp is simpler , it does not require knowledge of the supports of transition probabilities and the proof of the regret bound is simpler , but our regret bound is a constant factor larger than the regret of their algorithm . <eos> olp is also similar in flavor to an algorithm recently proposed by auer and ortner . <eos> but olp is simpler and its regret bound has a better dependence on the size of the mdp .
we investigate the problem of learning a widely-used latent-variable model the latent dirichlet allocation ( lda ) or ? topic model using distributed computation , where each of  processors only sees  of the total data set . <eos> we propose two distributed inference schemes that are motivated from different perspectives . <eos> the first scheme uses local gibbs sampling on each processor with periodic updates ? it is simple to implement and can be viewed as an approximation to a single processor implementation of gibbs sampling . <eos> the second scheme relies on a hierarchical bayesian extension of the standard lda model to directly account for the fact that data are distributed across  processors ? it has a theoretical guarantee of convergence but is more complex to implement than the approximate method . <eos> using five real-world text corpora we show that distributed learning works very well for lda models , i.e. , perplexity and precision-recall scores for distributed learning are indistinguishable from those obtained with single-processor learning . <eos> our extensive experimental results include large-scale distributed computation on 1000 virtual processors ; and speedup experiments of learning topics in a 100-million word corpus using 16 processors .
we extend the bayesian skill rating system trueskill to infer entire time series of skills of players by smoothing through time instead of filtering . <eos> the skill of each participating player , say , every year is represented by a latent skill variable which is affected by the relevant game outcomes that year , and coupled with the skill variables of the previous and subsequent year . <eos> inference in the resulting factor graph is carried out by approximate message passing ( ep ) along the time series of skills . <eos> as before the system tracks the uncertainty about player skills , explicitly models draws , can deal with any number of competing entities and can infer individual skills from team results . <eos> we extend the system to estimate player-specific draw margins . <eos> based on these models we present an analysis of the skill curves of important players in the history of chess over the past 150 years . <eos> results include plots of players ? <eos> lifetime skill development as well as the ability to compare the skills of different players across time . <eos> our results indicate that a ) the overall playing strength has increased over the past 150 years , and b ) that modelling a player ? s ability to force a draw provides significantly better predictive power .
bayesian reinforcement learning has generated substantial interest recently , as it provides an elegant solution to the exploration-exploitation trade-off in reinforcement learning . <eos> however most investigations of bayesian reinforcement learning to date focus on the standard markov decision processes ( mdps ) . <eos> our goal is to extend these ideas to the more general partially observable mdp ( pomdp ) framework , where the state is a hidden variable . <eos> to address this problem , we introduce a new mathematical model , the bayes-adaptive pomdp . <eos> this new model allows us to ( 1 ) improve knowledge of the pomdp domain through interaction with the environment , and ( 2 ) plan optimal sequences of actions which can tradeoff between improving the model , identifying the state , and gathering reward . <eos> we show how the model can be finitely approximated while preserving the value function . <eos> we describe approximations for belief tracking and planning in this model . <eos> empirical results on two domains show that the model estimate and agent ? s return improve over time , as the agent learns better model estimates .
binocular fusion takes place over a limited region smaller than one degree of visual angle ( panum 's fusional area ) , which is on the order of the range of preferred disparities measured in populations of disparity-tuned neurons in the visual cortex . <eos> however , the actual range of binocular disparities encountered in natural scenes ranges over tens of degrees . <eos> this discrepancy suggests that there must be a mechanism for detecting whether the stimulus disparity is either inside or outside of the range of the preferred disparities in the population . <eos> here , we present a statistical framework to derive feature in a population of v1 disparity neuron to determine the stimulus disparity within the preferred disparity range of the neural population . <eos> when optimized for natural images , it yields a feature that can be explained by the normalization which is a common model in v1 neurons . <eos> we further makes use of the feature to estimate the disparity in natural images . <eos> our proposed model generates more correct estimates than coarse-to-fine multiple scales approaches and it can also identify regions with occlusion . <eos> the approach suggests another critical role for normalization in robust disparity estimation .
we propose to test for the homogeneity of two samples by using kernel fisher discriminant analysis . <eos> this provides us with a consistent nonparametric test statistic , for which we derive the asymptotic distribution under the null hypothesis . <eos> we give experimental evidence of the relevance of our method on both artificial and real datasets .
maximum entropy analysis of binary variables provides an elegant way for studying the role of pairwise correlations in neural populations . <eos> unfortunately , these approaches suffer from their poor scalability to high dimensions . <eos> in sensory coding , however , high-dimensional data is ubiquitous . <eos> here , we introduce a new approach using a near-maximum entropy model , that makes this type of analysis feasible for very high-dimensional data ? the model parameters can be derived in closed form and sampling is easy . <eos> therefore , our nearmaxent approach can serve as a tool for testing predictions from a pairwise maximum entropy model not only for low-dimensional marginals , but also for high dimensional measurements of more than thousand units . <eos> we demonstrate its usefulness by studying natural images with dichotomized pixel intensities . <eos> our results indicate that the statistics of such higher-dimensional measurements exhibit additional structure that are not predicted by pairwise correlations , despite the fact that pairwise correlations explain the lower-dimensional marginal statistics surprisingly well up to the limit of dimensionality where estimation of the full joint distribution is feasible .
we demonstrate that log-linear grammars with latent variables can be practically trained using discriminative methods . <eos> central to efficient discriminative training is a hierarchical pruning procedure which allows feature expectations to be efficiently approximated in a gradient-based procedure . <eos> we compare l1 and l2 regularization and show that l1 regularization is superior , requiring fewer iterations to converge , and yielding sparser solutions . <eos> on full-scale treebank parsing experiments , the discriminative latent models outperform both the comparable generative latent models as well as the discriminative non-latent baselines .
hierarchical penalization is a generic framework for incorporating prior information in the fitting of statistical models , when the explicative variables are organized in a hierarchical structure . <eos> the penalizer is a convex functional that performs soft selection at the group level , and shrinks variables within each group . <eos> this favors solutions with few leading terms in the final combination . <eos> the framework , originally derived for taking prior knowledge into account , is shown to be useful in linear regression , when several parameters are used to model the influence of one feature , or in kernel regression , for learning multiple kernels . <eos> keywords optimization : constrained and convex optimization . <eos> supervised learning : regression , kernel methods , sparsity and feature selection .
in this paper , we propose a method for support vector machine classification using indefinite kernels . <eos> instead of directly minimizing or stabilizing a nonconvex loss function , our method simultaneously finds the support vectors and a proxy kernel matrix used in computing the loss . <eos> this can be interpreted as a robust classification problem where the indefinite kernel matrix is treated as a noisy observation of the true positive semidefinite kernel . <eos> our formulation keeps the problem convex and relatively large problems can be solved efficiently using the analytic center cutting plane method . <eos> we compare the performance of our technique with other methods on several data sets .
we propose a new measure of conditional dependence of random variables , based on normalized cross-covariance operators on reproducing kernel hilbert spaces . <eos> unlike previous kernel dependence measures , the proposed criterion does not depend on the choice of kernel in the limit of infinite data , for a wide class of kernels . <eos> at the same time , it has a straightforward empirical estimate with good convergence behaviour . <eos> we discuss the theoretical properties of the measure , and demonstrate its application in experiments .
in many applications , one has to actively select among a set of expensive observations before making an informed decision . <eos> often , we want to select observations which perform well when evaluated with an objective function chosen by an adversary . <eos> examples include minimizing the maximum posterior variance in gaussian process regression , robust experimental design , and sensor placement for outbreak detection . <eos> in this paper , we present the submodular saturation algorithm , a simple and efficient algorithm with strong theoretical approximation guarantees for the case where the possible objective functions exhibit submodularity , an intuitive diminishing returns property . <eos> moreover , we prove that better approximation algorithms do not exist unless np-complete problems admit efficient algorithms . <eos> we evaluate our algorithm on several real-world problems . <eos> for gaussian process regression , our algorithm compares favorably with state-of-the-art heuristics described in the geostatistics literature , while being simpler , faster and providing theoretical guarantees . <eos> for robust experimental design , our algorithm performs favorably compared to sdp-based algorithms .
a wide variety of dirichlet-multinomial topicmodels have found interesting applications in recent years . <eos> while gibbs sampling remains an important method of inference in such models , variational techniques have certain advantages such as easy assessment of convergence , easy optimization without the need to maintain detailed balance , a bound on the marginal likelihood , and side-stepping of issues with topic-identifiability . <eos> the most accurate variational technique thus far , namely collapsed variational latent dirichlet allocation , did not deal with model selection nor did it include inference for hyperparameters . <eos> we address both issues by generalizing the technique , obtaining the first variational algorithm to deal with the hierarchical dirichlet process and to deal with hyperparameters of dirichlet variables . <eos> experiments show a significant improvement in accuracy .
a recently proposed formulation of the stochastic planning and control problem as one of parameter estimation for suitable artificial statistical models has led to the adoption of inference algorithms for this notoriously hard problem . <eos> at the algorithmic level , the focus has been on developing expectation-maximization ( em ) algorithms . <eos> in this paper , we begin by making the crucial observation that the stochastic control problem can be reinterpreted as one of trans-dimensional inference . <eos> with this new interpretation , we are able to propose a novel reversible jump markov chain monte carlo ( mcmc ) algorithm that is more efficient than its em counterparts . <eos> moreover , it enables us to implement full bayesian policy search , without the need for gradients and with one single markov chain . <eos> the new approach involves sampling directly from a distribution that is proportional to the reward and , consequently , performs better than classic simulations methods in situations where the reward is a rare event .
rare category detection is an open challenge for active learning , especially in the de-novo case ( no labeled examples ) , but of significant practical importance for data mining - e.g . detecting new financial transaction fraud patterns , where normal legitimate transactions dominate . <eos> this paper develops a new method for detecting an instance of each minority class via an unsupervised local-density-differential sampling strategy . <eos> essentially a variable-scale nearest neighbor process is used to optimize the probability of sampling tightly-grouped minority classes , subject to a local smoothness assumption of the majority class . <eos> results on both synthetic and real data sets are very positive , detecting each minority class with only a fraction of the actively sampled points required by random sampling and by pelleg ? s interleave method , the prior best technique in the sparse literature on this topic .
semi-supervised learning , i.e . learning from both labeled and unlabeled data has received significant attention in the machine learning literature in recent years . <eos> still our understanding of the theoretical foundations of the usefulness of unlabeled data remains somewhat limited . <eos> the simplest and the best understood situation is when the data is described by an identifiable mixture model , and where each class comes from a pure component . <eos> this natural setup and its implications ware analyzed in [ 11 , 5 ] . <eos> one important result was that in certain regimes , labeled data becomes exponentially more valuable than unlabeled data . <eos> however , in most realistic situations , one would not expect that the data comes from a parametric mixture distribution with identifiable components . <eos> there have been recent efforts to analyze the non-parametric situation , for example , ? cluster and ? manifold assumptions have been suggested as a basis for analysis . <eos> still , a satisfactory and fairly complete theoretical understanding of the nonparametric problem , similar to that in [ 11 , 5 ] has not yet been developed . <eos> in this paper we investigate an intermediate situation , when the data comes from a probability distribution , which can be modeled , but not perfectly , by an identifiable mixture distribution . <eos> this seems applicable to many situation , when , for example , a mixture of gaussians is used to model the data . <eos> the contribution of this paper is an analysis of the role of labeled and unlabeled data depending on the amount of imperfection in the model .
we propose a gaussian process ( gp ) framework for robust inference in which a gp prior on the mixing weights of a two-component noise model augments the standard process over latent function values . <eos> this approach is a generalization of the mixture likelihood used in traditional robust gp regression , and a specialization of the gp mixture models suggested by tresp ( 2000 ) and rasmussen and ghahramani ( 2002 ) . <eos> the value of this restriction is in its tractable expectation propagation updates , which allow for faster inference and model selection , and better convergence than the standard mixture . <eos> an additional benefit over the latter method lies in our ability to incorporate knowledge of the noise domain to influence predictions , and to recover with the predictive distribution information about the outlier distribution via the gating process . <eos> the model has asymptotic complexity equal to that of conventional robust methods , but yields more confident predictions on benchmark problems than classical heavy-tailed models and exhibits improved stability for data with clustered corruptions , for which they fail altogether . <eos> we show further how our approach can be used without adjustment for more smoothly heteroscedastic data , and suggest how it could be extended to more general noise models . <eos> we also address similarities with the work of goldberg et al . ( 1998 ) , and the more recent contributions of tresp , and rasmussen and ghahramani .
adaptation to other initially unknown agents often requires computing an effective counter-strategy . <eos> in the bayesian paradigm , one must find a good counter-strategy to the inferred posterior of the other agents ' behavior . <eos> in the experts paradigm , one may want to choose experts that are good counter-strategies to the other agents ' expected behavior . <eos> in this paper we introduce a technique for computing robust counter-strategies for adaptation in multiagent scenarios under a variety of paradigms . <eos> the strategies can take advantage of a suspected tendency in the decisions of the other agents , while bounding the worst-case performance when the tendency is not observed . <eos> the technique involves solving a modified game , and therefore can make use of recently developed algorithms for solving very large extensive games . <eos> we demonstrate the effectiveness of the technique in two-player texas hold'em . <eos> we show that the computed poker strategies are substantially more robust than best response counter-strategies , while still exploiting a suspected tendency . <eos> we also compose the generated strategies in an experts algorithm showing a dramatic improvement in performance over using simple best responses .
we present a new and efficient semi-supervised training method for parameter estimation and feature selection in conditional random fields ( crfs ) . <eos> in real-world applications such as activity recognition , unlabeled sensor traces are relatively easy to obtain whereas labeled examples are expensive and tedious to collect . <eos> furthermore , the ability to automatically select a small subset of discriminatory features from a large pool can be advantageous in terms of computational speed as well as accuracy . <eos> in this paper , we introduce the semi-supervised virtual evidence boosting ( sveb ) algorithm for training crfs -- a semi-supervised extension to the recently developed virtual evidence boosting ( veb ) method for feature selection and parameter learning . <eos> semi-supervised veb takes advantage of the unlabeled data via minimum entropy regularization -- the objective function combines the unlabeled conditional entropy with labeled conditional pseudo-likelihood . <eos> the sveb algorithm reduces the overall system cost as well as the human labeling cost required during training , which are both important considerations in building real world inference systems . <eos> in a set of experiments on synthetic data and real activity traces collected from wearable sensors , we illustrate that our algorithm benefits from both the use of unlabeled data and automatic feature selection , and outperforms other semi-supervised training approaches .
reward-modulated spike-timing-dependent plasticity ( stdp ) has recently emerged as a candidate for a learning rule that could explain how local learning rules at single synapses support behaviorally relevant adaptive changes in complex networks of spiking neurons . <eos> however the potential and limitations of this learning rule could so far only be tested through computer simulations . <eos> this article provides tools for an analytic treatment of reward-modulated stdp , which allow us to predict under which conditions reward-modulated stdp will be able to achieve a desired learning effect . <eos> in particular , we can produce in this way a theoretical explanation and a computer model for a fundamental experimental finding on biofeedback in monkeys ( reported in [ 1 ] ) .
we combine two threads of research on approximate dynamic programming : random sampling of states and using local trajectory optimizers to globally optimize a policy and associated value function . <eos> this combination allows us to replace a dense multidimensional grid with a much sparser adaptive sampling of states . <eos> our focus is on finding steady state policies for the deterministic time invariant discrete time control problems with continuous states and actions often found in robotics . <eos> in this paper we show that we can now solve problems we could n't solve previously with regular grid-based approaches .
we present an efficient generalization of the sparse pseudo-input gaussian process ( spgp ) model developed by snelson and ghahramani [ 1 ] , applying it to binary classification problems . <eos> by taking advantage of the spgp prior covariance structure , we derive a numerically stable algorithm with o ( n m 2 ) training complexity ? asymptotically the same as related sparse methods such as the informative vector machine [ 2 ] , but which more faithfully represents the posterior . <eos> we present experimental results for several benchmark problems showing that in many cases this allows an exceptional degree of sparsity without compromising accuracy . <eos> following [ 1 ] , we locate pseudo-inputs by gradient ascent on the marginal likelihood , but exhibit occasions when this is likely to fail , for which we suggest alternative solutions .
we propose a randomized algorithm for large scale svm learning which solves the problem by iterating over random subsets of the data . <eos> crucial to the algorithm for scalability is the size of the subsets chosen . <eos> in the context of text classification we show that , by using ideas from random projections , a sample size of o ( log n ) can be used to obtain a solution which is close to the optimal with a high probability . <eos> experiments done on synthetic and real life data sets demonstrate that the algorithm scales up svm learners , without loss in accuracy .
clustering is often formulated as a discrete optimization problem . <eos> the objective is to find , among all partitions of the data set , the best one according to some quality measure . <eos> however , in the statistical setting where we assume that the finite data set has been sampled from some underlying space , the goal is not to find the best partition of the given sample , but to approximate the true partition of the underlying space . <eos> we argue that the discrete optimization approach usually does not achieve this goal . <eos> as an alternative , we suggest the paradigm of ? nearest neighbor clustering ? . <eos> instead of selecting the best out of all partitions of the sample , it only considers partitions in some restricted function class . <eos> using tools from statistical learning theory we prove that nearest neighbor clustering is statistically consistent . <eos> moreover , its worst case complexity is polynomial by construction , and it can be implemented with small average case complexity using branch and bound .
variational methods are frequently used to approximate or bound the partition or likelihood function of a markov random field . <eos> methods based on mean field theory are guaranteed to provide lower bounds , whereas certain types of convex relaxations provide upper bounds . <eos> in general , loopy belief propagation ( bp ) provides ( often accurate ) approximations , but not bounds . <eos> we prove that for a class of attractive binary models , the value specified by any fixed point of loopy bp always provides a lower bound on the true likelihood . <eos> empirically , this bound is much better than the naive mean field bound , and requires no further work than running bp . <eos> we establish these lower bounds using a loop series expansion due to chertkov and chernyak , which we show can be derived as a consequence of the tree reparameterization characterization of bp fixed points .
most models of decision-making in neuroscience assume an infinite horizon , which yields an optimal solution that integrates evidence up to a fixed decision threshold ; however , under most experimental as well as naturalistic behavioral settings , the decision has to be made before some finite deadline , which is often experienced as a stochastic quantity , either due to variable external constraints or internal timing uncertainty . <eos> in this work , we formulate this problem as sequential hypothesis testing under a stochastic horizon . <eos> we use dynamic programming tools to show that , for a large class of deadline distributions , the bayes-optimal solution requires integrating evidence up to a threshold that declines monotonically over time . <eos> we use numerical simulations to illustrate the optimal policy in the special cases of a fixed deadline and one that is drawn from a gamma distribution .
we consider the problem of support vector machine transduction , which involves a combinatorial problem with exponential computational complexity in the number of unlabeled examples . <eos> although several studies are devoted to transductive svm , they suffer either from the high computation complexity or from the solutions of local optimum . <eos> to address this problem , we propose solving transductive svm via a convex relaxation , which converts the np-hard problem to a semi-definite programming . <eos> compared with the other sdp relaxation for transductive svm , the proposed algorithm is computationally more efficient with the number of free parameters reduced from o ( n2 ) to o ( n ) where n is the number of examples . <eos> empirical study with several benchmark data sets shows the promising performance of the proposed algorithm in comparison with other state-of-the-art implementations of transductive svm .
can we leverage learning techniques to build a fast nearest-neighbor ( nn ) retrieval data structure ? <eos> we present a general learning framework for the nn problem in which sample queries are used to learn the parameters of a data structure that minimize the retrieval time and/or the miss rate . <eos> we explore the potential of this novel framework through two popular nn data structures : kd-trees and the rectilinear structures employed by locality sensitive hashing . <eos> we derive a generalization theory for these data structure classes and present simple learning algorithms for both . <eos> experimental results reveal that learning often improves on the already strong performance of these data structures .
stability is a desirable characteristic for linear dynamical systems , but it is often ignored by algorithms that learn these systems from data . <eos> we propose a novel method for learning stable linear dynamical systems : we formulate an approximation of the problem as a convex program , start with a solution to a relaxed version of the program , and incrementally add constraints to improve stability . <eos> rather than continuing to generate constraints until we reach a feasible solution , we test stability at each step ; because the convex program is only an approximation of the desired problem , this early stopping rule can yield a higher-quality solution . <eos> we apply our algorithm to the task of learning dynamic textures from image sequences as well as to modeling biosurveillance drug-sales data . <eos> the constraint generation approach leads to noticeable improvement in the quality of simulated sequences . <eos> we compare our method to those of lacy and bernstein [ 1 , 2 ] , with positive results in terms of accuracy , quality of simulated sequences , and efficiency .
in this paper , we consider collaborative filtering as a ranking problem . <eos> we present a method which uses maximum margin matrix factorization and optimizes ranking instead of rating . <eos> we employ structured output prediction to optimize directly for ranking scores . <eos> experimental results show that our method gives very good ranking scores and scales well on collaborative filtering tasks .
current computational models of bottom-up and top-down components of attention are predictive of eye movements across a range of stimuli and of simple , fixed visual tasks ( such as visual search for a target among distractors ) . <eos> however , to date there exists no computational framework which can reliably mimic human gaze behavior in more complex environments and tasks , such as driving a vehicle through traffic . <eos> here , we develop a hybrid computational/behavioral framework , combining simple models for bottom-up salience and top-down relevance , and looking for changes in the predictive power of these components at different critical event times during 4.7 hours ( 500,000 video frames ) of observers playing car racing and flight combat video games . <eos> this approach is motivated by our observation that the predictive strengths of the salience and relevance models exhibit reliable temporal signatures during critical event windows in the task sequence ? for example , when the game player directly engages an enemy plane in a flight combat game , the predictive strength of the salience model increases significantly , while that of the relevance model decreases significantly . <eos> our new framework combines these temporal signatures to implement several event detectors . <eos> critically , we find that an event detector based on fused behavioral and stimulus information ( in the form of the model ? s predictive strength ) is much stronger than detectors based on behavioral information alone ( eye position ) or image information alone ( model prediction maps ) . <eos> this approach to event detection , based on eye tracking combined with computational models applied to the visual input , may have useful applications as a less-invasive alternative to other event detection approaches based on neural signatures derived from eeg or fmri recordings .
this paper proposes constraint propagation relaxation ( cpr ) , a probabilistic approach to classical constraint propagation that provides another view on the whole parametric family of survey propagation algorithms sp ( & # 961 ; ) , ranging from belief propagation ( & # 961 ; = 0 ) to ( pure ) survey propagation ( & # 961 ; = 1 ) . <eos> more importantly , the approach elucidates the implicit , but fundamental assumptions underlying sp ( & # 961 ; ) , thus shedding some light on its effectiveness and leading to applications beyond k-sat .
we introduce a hierarchical bayesian model for the discovery of putative regulators from gene expression data only . <eos> the hierarchy incorporates the knowledge that there are just a few regulators that by themselves only regulate a handful of genes . <eos> this is implemented through a so-called spike-and-slab prior , a mixture of gaussians with different widths , with mixing weights from a hierarchical bernoulli model . <eos> for efficient inference we implemented expectation propagation . <eos> running the model on a malaria parasite data set , we found four genes with significant homology to transcription factors in an amoebe , one rna regulator and three genes of unknown function ( out of the top ten genes considered ) .
unsupervised learning algorithms aim to discover the structure hidden in the data , and to learn representations that are more suitable as input to a supervised machine than the raw input . <eos> many unsupervised methods are based on reconstructing the input from the representation , while constraining the representation to have certain desirable properties ( e.g . low dimension , sparsity , etc ) . <eos> others are based on approximating density by stochastically reconstructing the input from the representation . <eos> we describe a novel and efficient algorithm to learn sparse representations , and compare it theoretically and experimentally with a similar machines trained probabilistically , namely a restricted boltzmann machine . <eos> we propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation . <eos> we demonstrate this method by extracting features from a dataset of handwritten numerals , and from a dataset of natural image patches . <eos> we show that by stacking multiple levels of such machines and by training sequentially , high-order dependencies between the input variables can be captured .
a non ? linear dynamic system is called contracting if initial conditions are forgotten exponentially fast , so that all trajectories converge to a single trajectory . <eos> we use contraction theory to derive an upper bound for the strength of recurrent connections that guarantees contraction for complex neural networks . <eos> specifically , we apply this theory to a special class of recurrent networks , often called cooperative competitive networks ( ccns ) , which are an abstract representation of the cooperative-competitive connectivity observed in cortex . <eos> this specific type of network is believed to play a major role in shaping cortical responses and selecting the relevant signal among distractors and noise . <eos> in this paper , we analyze contraction of combined ccns of linear threshold units and verify the results of our analysis in a hybrid analog/digital vlsi ccn comprising spiking neurons and dynamic synapses .
we present a novel paradigm for statistical machine translation ( smt ) , based on joint modeling of word alignment and the topical aspects underlying bilingual document pairs via a hidden markov bilingual topic admixture ( hm-bitam ) . <eos> in this new paradigm , parallel sentence-pairs from a parallel document-pair are coupled via a certain semantic-flow , to ensure coherence of topical context in the alignment of matching words between languages , during likelihood-based training of topic-dependent translational lexicons , as well as topic representations in each language . <eos> the resulting trained hm-bitam can not only display topic patterns like other methods such as lda , but now for bilingual corpora ; it also offers a principled way of inferring optimal translation in a context-dependent way . <eos> our method integrates the conventional ibm models based on hmm -- - a key component for most of the state-of-the-art smt systems , with the recently proposed bitam model , and we report an extensive empirical analysis ( in many way complementary to the description-oriented of our method in three aspects : word alignment , bilingual topic representation , and translation .
natural sounds are structured on many time-scales . <eos> a typical segment of speech , for example , contains features that span four orders of magnitude : sentences ( ~1s ) ; phonemes ( ~0.1s ) ; glottal pulses ( ~0.01s ) ; and formants ( < 0.001s ) . <eos> the auditory system uses information from each of these time-scales to solve complicated tasks such as auditory scene analysis . <eos> one route toward understanding how auditory processing accomplishes this analysis is to build neuroscience-inspired algorithms which solve similar tasks and to compare the properties of these algorithms with properties of auditory processing . <eos> there is however a discord : current machine-audition algorithms largely concentrate on the shorter time-scale structures in sounds , and the longer structures are ignored . <eos> the reason for this is two-fold . <eos> firstly , it is a difficult technical problem to construct an algorithm that utilises both sorts of information . <eos> secondly , it is computationally demanding to simultaneously process data both at high resolution ( to extract short temporal information ) and for long duration ( to extract long temporal information ) . <eos> the contribution of this work is to develop a new statistical model for natural sounds that captures structure across a wide range of time-scales , and to provide efficient learning and inference algorithms . <eos> we demonstrate the success of this approach on a missing data task .
assessing similarity between features is a key step in object recognition and scene categorization tasks . <eos> we argue that knowledge on the distribution of distances generated by similarity functions is crucial in deciding whether features are similar or not . <eos> intuitively one would expect that similarities between features could arise from any distribution . <eos> in this paper , we will derive the contrary , and report the theoretical result that $ l_p $ -norms -- a class of commonly applied distance metrics -- from one feature vector to other vectors are weibull-distributed if the feature values are correlated and non-identically distributed . <eos> besides these assumptions being realistic for images , we experimentally show them to hold for various popular feature extraction algorithms , for a diverse range of images . <eos> this fundamental insight opens new directions in the assessment of feature similarity , with projected improvements in object and scene recognition algorithms . <eos> erratum : the authors of paper have declared that they have become convinced that the reasoning in the reference is too simple as a proof of their claims . <eos> as a consequence , they withdraw their theorems .
when we have several related tasks , solving them simultaneously is shown to be more effective than solving them individually . <eos> this approach is called multi-task learning ( mtl ) and has been studied extensively . <eos> existing approaches to mtl often treat all the tasks as \emph { uniformly related to each other and the relatedness of the tasks is controlled globally . <eos> for this reason , the existing methods can lead to undesired solutions when some tasks are not highly related to each other , and some pairs of related tasks can have significantly different solutions . <eos> in this paper , we propose a novel mtl algorithm that can overcome these problems . <eos> our method makes use of a task network , which describes the relation structure among tasks . <eos> this allows us to deal with intricate relation structures in a systematic way . <eos> furthermore , we control the relatedness of the tasks locally , so all pairs of related tasks are guaranteed to have similar solutions . <eos> we apply the above idea to support vector machines ( svms ) and show that the optimization problem can be cast as a second order cone program , which is convex and can be solved efficiently . <eos> the usefulness of our approach is demonstrated through simulations with protein super-family classification and ordinal regression problems .
social tags are user-generated keywords associated with some resource on the web . <eos> in the case of music , social tags have become an important component of web2.0 '' recommender systems , allowing users to generate playlists based on use-dependent terms such as `` chill '' or `` jogging '' that have been applied to particular songs . <eos> in this paper , we propose a method for predicting these social tags directly from mp3 files . <eos> using a set of boosted classifiers , we map audio features onto social tags collected from the web . <eos> the resulting automatic tags ( or `` autotags '' ) furnish information about music that is otherwise untagged or poorly tagged , allowing for insertion of previously unheard music into a social recommender . <eos> this avoids the `` cold-start problem '' common in such systems . <eos> autotags can also be used to smooth the tag space from which similarities and recommendations are made by providing a set of comparable baseline tags for all tracks in a recommender system . ''
automatic relevance determination ( ard ) , and the closely-related sparse bayesian learning ( sbl ) framework , are effective tools for pruning large numbers of irrelevant features . <eos> however , popular update rules used for this process are either prohibitively slow in practice and/or heuristic in nature without proven convergence properties . <eos> this paper furnishes an alternative means of optimizing a general ard cost function using an auxiliary function that can naturally be solved using a series of re-weighted l1 problems . <eos> the result is an efficient algorithm that can be implemented using standard convex programming toolboxes and is guaranteed to converge to a stationary point unlike existing methods . <eos> the analysis also leads to additional insights into the behavior of previous ard updates as well as the ard cost function . <eos> for example , the standard fixed-point updates of mackay ( 1992 ) are shown to be iteratively solving a particular min-max problem , although they are not guaranteed to lead to a stationary point . <eos> the analysis also reveals that ard is exactly equivalent to performing map estimation using a particular feature- and noise-dependent \textit { non-factorial } weight prior with several desirable properties over conventional priors with respect to feature selection . <eos> in particular , it provides a tighter approximation to the l0 quasi-norm sparsity measure than the l1 norm . <eos> overall these results suggests alternative cost functions and update procedures for selecting features and promoting sparse solutions .
the problem of obtaining the maximum a posteriori estimate of a general discrete random field ( i.e . a random field defined using a finite and discrete set of labels ) is known to be np-hard . <eos> however , due to its central importance in many applications , several approximate algorithms have been proposed in the literature . <eos> in this paper , we present an analysis of three such algorithms based on convex relaxations : ( i ) lp - s : the linear programming ( lp ) relaxation proposed by schlesinger [ 20 ] for a special case and independently in [ 4 , 12 , 23 ] for the general case ; ( ii ) qp - rl : the quadratic programming ( qp ) relaxation by ravikumar and lafferty [ 18 ] ; and ( iii ) socp - ms : the second order cone programming ( socp ) relaxation first proposed by muramatsu and suzuki [ 16 ] for two label problems and later extended in [ 14 ] for a general label set . <eos> we show that the socp - ms and the qp - rl relaxations are equivalent . <eos> furthermore , we prove that despite the flexibility in the form of the constraints/objective function offered by qp and socp , the lp - s relaxation strictly dominates ( i.e . provides a better approximation than ) qp - rl and socp - ms. we generalize these results by defining a large class of socp ( and equivalent qp ) relaxations which is dominated by the lp - s relaxation . <eos> based on these results we propose some novel socp relaxations which strictly dominate the previous approaches .
we present a novel boosting algorithm , called softboost , designed for sets of binary labeled examples that are not necessarily separable by convex combinations of base hypotheses . <eos> our algorithm achieves robustness by capping the distributions on the examples . <eos> our update of the distribution is motivated by minimizing a relative entropy subject to the capping constraints and constraints on the edges of the obtained base hypotheses . <eos> the capping constraints imply a soft margin in the dual optimization problem . <eos> our algorithm produces a convex combination of hypotheses whose soft margin is within of its maximum . <eos> we employ relative entropy projection methods to prove an o ( ln ? 2n ) iteration bound for our algorithm , where n is number of examples . <eos> we compare our algorithm with other approaches including lpboost , brownboost , and smoothboost . <eos> we show that there exist cases where the number of iterations required by lpboost grows linearly in n instead of the logarithmic growth for softboost . <eos> in simulation studies we show that our algorithm converges about as fast as lpboost , faster than brownboost , and much faster than smoothboost . <eos> in a benchmark comparison we illustrate the competitiveness of our approach .
in this paper , we show that classical survival analysis involving censored data can naturally be cast as a ranking problem . <eos> the concordance index ( ci ) , which quantifies the quality of rankings , is the standard performance measure for model \emph { assessment } in survival analysis . <eos> in contrast , the standard approach to \emph { learning } the popular proportional hazard ( ph ) model is based on cox 's partial likelihood . <eos> in this paper we devise two bounds on ci -- one of which emerges directly from the properties of ph models -- and optimize them \emph { directly } . <eos> our experimental results suggest that both methods perform about equally well , with our new approach giving slightly better results than the cox 's method . <eos> we also explain why a method designed to maximize the cox 's partial likelihood also ends up ( approximately ) maximizing the ci .
semi-supervised methods use unlabeled data in addition to labeled data to construct predictors . <eos> while existing semi-supervised methods have shown some promising empirical performance , their development has been based largely based on heuristics . <eos> in this paper we study semi-supervised learning from the viewpoint of minimax theory . <eos> our first result shows that some common methods based on regularization using graph laplacians do not lead to faster minimax rates of convergence . <eos> thus , the estimators that use the unlabeled data do not have smaller risk than the estimators that use only labeled data . <eos> we then develop several new approaches that provably lead to improved performance . <eos> the statistical tools of minimax analysis are thus used to offer some new perspective on the problem of semi-supervised learning .
brain-computer interfaces ( bcis ) , as any other interaction modality based on physiological signals and body channels ( e.g. , muscular activity , speech and gestures ) , are prone to errors in the recognition of subject 's intent . <eos> an elegant approach to improve the accuracy of bcis consists in a verification procedure directly based on the presence of error-related potentials ( errp ) in the eeg recorded right after the occurrence of an error . <eos> six healthy volunteer subjects with no prior bci experience participated in a new human-robot interaction experiment where they were asked to mentally move a cursor towards a target that can be reached within a few steps using motor imagination . <eos> this experiment confirms the previously reported presence of a new kind of errp . <eos> these interaction errp '' exhibit a first sharp negative peak followed by a positive peak and a second broader negative peak ( ~290 , ~350 and ~470 ms after the feedback , respectively ) . <eos> but in order to exploit these errp we need to detect them in each single trial using a short window following the feedback associated to the response of the classifier embedded in the bci . <eos> we have achieved an average recognition rate of correct and erroneous single trials of 81.8 % and 76.2 % , respectively . <eos> furthermore , we have achieved an average recognition rate of the subject 's intent while trying to mentally drive the cursor of 73.1 % . <eos> these results show that it 's possible to simultaneously extract useful information for mental control to operate a brain-actuated device as well as cognitive states such as error potentials to improve the quality of the brain-computer interaction . <eos> finally , using a well-known inverse model ( sloreta ) , we show that the main focus of activity at the occurrence of the errp are , as expected , in the pre-supplementary motor area and in the anterior cingulate cortex . ''
we describe a hierarchical , probabilistic model that learns to extract complex motion from movies of the natural environment . <eos> the model consists of two hidden layers : the first layer produces a sparse representation of the image that is expressed in terms of local amplitude and phase variables . <eos> the second layer learns the higher-order structure among the time-varying phase variables . <eos> after training on natural movies , the top layer units discover the structure of phase-shifts within the first layer . <eos> we show that the top layer units encode transformational invariants : they are selective for the speed and direction of a moving pattern , but are invariant to its spatial structure ( orientation/spatial-frequency ) . <eos> the diversity of units in both the intermediate and top layers of the model provides a set of testable predictions for representations that might be found in v1 and mt . <eos> in addition , the model demonstrates how feedback from higher levels can influence representations at lower levels as a by-product of inference in a graphical model .
gates are a new notation for representing mixture models and context-sensitive independence in factor graphs . <eos> factor graphs provide a natural representation for message-passing algorithms , such as expectation propagation . <eos> however , message passing in mixture models is not well captured by factor graphs unless the entire mixture is represented by one factor , because the message equations have a containment structure . <eos> gates capture this containment structure graphically , allowing both the independences and the message-passing equations for a model to be readily visualized . <eos> different variational approximations for mixture models can be understood as different ways of drawing the gates in a model . <eos> we present general equations for expectation propagation and variational message passing in the presence of gates .
we present a generative model for performing sparse probabilistic projections , which includes sparse principal component analysis and sparse canonical correlation analysis as special cases . <eos> sparsity is enforced by means of automatic relevance determination or by imposing appropriate prior distributions , such as generalised hyperbolic distributions . <eos> we derive a variational expectation-maximisation algorithm for the estimation of the hyperparameters and show that our novel probabilistic approach compares favourably to existing techniques . <eos> we illustrate how the proposed method can be applied in the context of cryptoanalysis as a pre-processing tool for the construction of template attacks .
is accurate classification possible in the absence of hand-labeled data ? <eos> this paper introduces the monotonic feature ( mf ) abstraction ? where the probability of class membership increases monotonically with the mf ? s value . <eos> the paper proves that when an mf is given , pac learning is possible with no hand-labeled data under certain assumptions . <eos> we argue that mfs arise naturally in a broad range of textual classification applications . <eos> on the classic ? 20 newsgroups ? <eos> data set , a learner given an mf and unlabeled data achieves classification accuracy equal to that of a state-of-the-art semi-supervised learner relying on 160 hand-labeled examples . <eos> even when mfs are not given as input , their presence or absence can be determined from a small amount of hand-labeled data , which yields a new semi-supervised learning method that reduces error by 15 % on the 20 newsgroups data .
before the age of 4 months , infants make inductive inferences about the motions of physical objects . <eos> developmental psychologists have provided verbal accounts of the knowledge that supports these inferences , but often these accounts focus on categorical rather than probabilistic principles . <eos> we propose that infant object perception is guided in part by probabilistic principles like persistence : things tend to remain the same , and when they change they do so gradually . <eos> to illustrate this idea , we develop an ideal observer model that includes probabilistic formulations of rigidity and inertia . <eos> like previous researchers , we suggest that rigid motions are expected from an early age , but we challenge the previous claim that expectations consistent with inertia are relatively slow to develop ( spelke et al. , 1992 ) . <eos> we support these arguments by modeling four experiments from the developmental literature .
semantic hashing seeks compact binary codes of datapoints so that the hamming distance between codewords correlates with semantic similarity . <eos> hinton et al . used a clever implementation of autoencoders to find such codes . <eos> in this paper , we show that the problem of finding a best code for a given dataset is closely related to the problem of graph partitioning and can be shown to be np hard . <eos> by relaxing the original problem , we obtain a spectral method whose solutions are simply a subset of thresh- olded eigenvectors of the graph laplacian . <eos> by utilizing recent results on convergence of graph laplacian eigenvectors to the laplace-beltrami eigen- functions of manifolds , we show how to efficiently calculate the code of a novel datapoint . <eos> taken together , both learning the code and applying it to a novel point are extremely simple . <eos> our experiments show that our codes significantly outperform the state-of-the art .
we address the challenge of assessing conservation of gene expression in complex , non-homogeneous datasets . <eos> recent studies have demonstrated the success of probabilistic models in studying the evolution of gene expression in simple eukaryotic organisms such as yeast , for which measurements are typically scalar and independent . <eos> models capable of studying expression evolution in much more complex organisms such as vertebrates are particularly important given the medical and scientific interest in species such as human and mouse . <eos> we present a statistical model that makes a number of significant extensions to previous models to enable characterization of changes in expression among highly complex organisms . <eos> we demonstrate the efficacy of our method on a microarray dataset containing diverse tissues from multiple vertebrate species . <eos> we anticipate that the model will be invaluable in the study of gene expression patterns in other diverse organisms as well , such as worms and insects .
the inverse dynamics problem for a robotic manipulator is to compute the torques needed at the joints to drive it along a given trajectory ; it is beneficial to be able to learn this function for adaptive control . <eos> a given robot manipulator will often need to be controlled while holding different loads in its end effector , giving rise to a multi-task learning problem . <eos> we show how the structure of the inverse dynamics problem gives rise to a multi-task gaussian process prior over functions , where the inter-task similarity depends on the underlying dynamic parameters . <eos> experiments demonstrate that this multi-task formulation generally improves performance over either learning only on single tasks or pooling the data over all tasks .
many popular optimization algorithms , like the levenberg-marquardt algorithm ( lma ) , use heuristic-based controllers '' that modulate the behavior of the optimizer during the optimization process . <eos> for example , in the lma a damping parameter is dynamically modified based on a set rules that were developed using various heuristic arguments . <eos> reinforcement learning ( rl ) is a machine learning approach to learn optimal controllers by examples and thus is an obvious candidate to improve the heuristic-based controllers implicit in the most popular and heavily used optimization algorithms . <eos> improving the performance of off-the-shelf optimizers is particularly important for time-constrained optimization problems . <eos> for example the lma algorithm has become popular for many real-time computer vision problems , including object tracking from video , where only a small amount of time can be allocated to the optimizer on each incoming video frame . <eos> here we show that a popular modern reinforcement learning technique using a very simply state space can dramatically improve the performance of general purpose optimizers , like the lma . <eos> most surprisingly the controllers learned for a particular domain appear to work very well also on very different optimization domains . <eos> for example we used rl methods to train a new controller for the damping parameter of the lma . <eos> this controller was trained on a collection of classic , relatively small , non-linear regression problems . <eos> the modified lma performed better than the standard lma on these problems . <eos> most surprisingly , it also dramatically outperformed the standard lma on a difficult large scale computer vision problem for which it had not been trained before . <eos> thus the controller appeared to have extracted control rules that were not just domain specific but generalized across a wide range of optimization domains . ''
we address the problem of estimating the ratio of two probability density functions ( a.k.a.~the importance ) . <eos> the importance values can be used for various succeeding tasks such as non-stationarity adaptation or outlier detection . <eos> in this paper , we propose a new importance estimation method that has a closed-form solution ; the leave-one-out cross-validation score can also be computed analytically . <eos> therefore , the proposed method is computationally very efficient and numerically stable . <eos> we also elucidate theoretical properties of the proposed method such as the convergence rate and approximation error bound . <eos> numerical experiments show that the proposed method is comparable to the best existing method in accuracy , while it is computationally more efficient than competing approaches .
we derive risk bounds for the randomized classifiers in sample compressions settings where the classifier-specification utilizes two sources of information viz . <eos> the compression set and the message string . <eos> by extending the recently proposed occam ? s hammer principle to the data-dependent settings , we derive point-wise versions of the bounds on the stochastic sample compressed classifiers and also recover the corresponding classical pac-bayes bound . <eos> we further show how these compare favorably to the existing results .
polysemy is a problem for methods that exploit image search engines to build object category models . <eos> existing unsupervised approaches do not take word sense into consideration . <eos> we propose a new method that uses a dictionary to learn models of visual word sense from a large collection of unlabeled web data . <eos> the use of lda to discover a latent sense space makes the model robust despite the very limited nature of dictionary definitions . <eos> the definitions are used to learn a distribution in the latent space that best represents a sense . <eos> the algorithm then uses the text surrounding image links to retrieve images with high probability of a particular dictionary sense . <eos> an object classifier is trained on the resulting sense-specific images . <eos> we evaluate our method on a dataset obtained by searching the web for polysemous words . <eos> category classification experiments show that our dictionary-based approach outperforms baseline methods .
we present polynomial-time algorithms for the exact computation of lowest- energy states , worst margin violators , partition functions , and marginals in binary undirected graphical models . <eos> our approach provides an interesting alternative to the well-known graph cut paradigm in that it does not impose any submodularity constraints ; instead we require planarity to establish a correspondence with perfect matchings in an expanded dual graph . <eos> maximum-margin parameter estimation for a boundary detection task shows our approach to be ef & # 64257 ; cient and effective .
uncertainty is omnipresent when we perceive or interact with our environment , and the bayesian framework provides computational methods for dealing with it . <eos> mathematical models for bayesian decision making typically require datastructures that are hard to implement in neural networks . <eos> this article shows that even the simplest and experimentally best supported type of synaptic plasticity , hebbian learning , in combination with a sparse , redundant neural code , can in principle learn to infer optimal bayesian decisions . <eos> we present a concrete hebbian learning rule operating on log-probability ratios . <eos> modulated by reward-signals , this hebbian plasticity rule also provides a new perspective for understanding how bayesian inference could support fast reinforcement learning in the brain . <eos> in particular we show that recent experimental results by yang and shadlen [ 1 ] on reinforcement learning of probabilistic inference in primates can be modeled in this way .
we consider the following instance of transfer learning : given a pair of regression problems , suppose that the regression coefficients share a partially common support , parameterized by the overlap fraction $ \overlap $ between the two supports . <eos> this set-up suggests the use of $ 1 , \infty $ -regularized linear regression for recovering the support sets of both regression vectors . <eos> our main contribution is to provide a sharp characterization of the sample complexity of this $ 1 , \infty $ relaxation , exactly pinning down the minimal sample size $ n $ required for joint support recovery as a function of the model dimension $ \pdim $ , support size $ \spindex $ and overlap $ \overlap \in [ 0,1 ] $ . <eos> for measurement matrices drawn from standard gaussian ensembles , we prove that the joint $ 1 , \infty $ -regularized method undergoes a phase transition characterized by order parameter $ \orpar ( \numobs , \pdim , \spindex , \overlap ) = \numobs { ( 4 - 3 \overlap ) s \log ( p- ( 2-\overlap ) s ) } $ . <eos> more precisely , the probability of successfully recovering both supports converges to $ 1 $ for scalings such that $ \orpar > 1 $ , and converges to $ 0 $ to scalings for which $ \orpar < 1 $ . <eos> an implication of this threshold is that use of $ 1 , \infty $ -regularization leads to gains in sample complexity if the overlap parameter is large enough ( $ \overlap > 2/3 $ ) , but performs worse than a naive approach if $ \overlap < 2/3 $ . <eos> we illustrate the close agreement between these theoretical predictions , and the actual behavior in simulations . <eos> thus , our results illustrate both the benefits and dangers associated with block- $ 1 , \infty $ regularization in high-dimensional inference .
in kernel-based regression learning , optimizing each kernel individually is useful when the data density , curvature of regression surfaces ( or decision boundaries ) or magnitude of output noise ( i.e. , heteroscedasticity ) varies spatially . <eos> unfortunately , it presents a complex computational problem as the danger of overfitting is high and the individual optimization of every kernel in a learning system may be overly expensive due to the introduction of too many open learning parameters . <eos> previous work has suggested gradient descent techniques or complex statistical hypothesis methods for local kernel shaping , typically requiring some amount of manual tuning of meta parameters . <eos> in this paper , we focus on nonparametric regression and introduce a bayesian formulation that , with the help of variational approximations , results in an em-like algorithm for simultaneous estimation of regression and kernel parameters . <eos> the algorithm is computationally efficient ( suitable for large data sets ) , requires no sampling , automatically rejects outliers and has only one prior to be specified . <eos> it can be used for nonparametric regression with local polynomials or as a novel method to achieve nonstationary regression with gaussian processes . <eos> our methods are particularly useful for learning control , where reliable estimation of local tangent planes is essential for adaptive controllers and reinforcement learning . <eos> we evaluate our methods on several synthetic data sets and on an actual robot which learns a task-level control law .
large networks of spiking neurons show abrupt changes in their collective dynamics resembling phase transitions studied in statistical physics . <eos> an example of this phenomenon is the transition from irregular , noise-driven dynamics to regular , self-sustained behavior observed in networks of integrate-and-fire neurons as the interaction strength between the neurons increases . <eos> in this work we show how a network of spiking neurons is able to self-organize towards a critical state for which the range of possible inter-spike-intervals ( dynamic range ) is maximized . <eos> self-organization occurs via synaptic dynamics that we analytically derive . <eos> the resulting plasticity rule is defined locally so that global homeostasis near the critical state is achieved by local regulation of individual synapses .
mixture of gaussian processes models extended a single gaussian process with ability of modeling multi-modal data and reduction of training complexity . <eos> previous inference algorithms for these models are mostly based on gibbs sampling , which can be very slow , particularly for large-scale data sets . <eos> we present a new generative mixture of experts model . <eos> each expert is still a gaussian process but is reformulated by a linear model . <eos> this breaks the dependency among training outputs and enables us to use a much faster variational bayesian algorithm for training . <eos> our gating network is more flexible than previous generative approaches as inputs for each expert are modeled by a gaussian mixture model . <eos> the number of experts and number of gaussian components for an expert are inferred automatically . <eos> a variety of tests show the advantages of our method .
embeddings of random variables in reproducing kernel hilbert spaces ( rkhss ) may be used to conduct statistical inference based on higher order moments . <eos> for sufficiently rich ( characteristic ) rkhss , each probability distribution has a unique embedding , allowing all statistical properties of the distribution to be taken into consideration . <eos> necessary and sufficient conditions for an rkhs to be characteristic exist for $ \r^n $ . <eos> in the present work , conditions are established for an rkhs to be characteristic on groups and semigroups . <eos> illustrative examples are provided , including characteristic kernels on periodic domains , rotation matrices , and $ \r^n_+ $ .
our setting is a partially observable markov decision process with continuous state , observation and action spaces . <eos> decisions are based on a particle filter for estimating the belief state given past observations . <eos> we consider a policy gradient approach for parameterized policy optimization . <eos> for that purpose , we investigate sensitivity analysis of the performance measure with respect to the parameters of the policy , focusing on finite difference ( fd ) techniques . <eos> we show that the naive fd is subject to variance explosion because of the non-smoothness of the resampling procedure . <eos> we propose a more sophisticated fd method which overcomes this problem and establish its consistency .
we develop \name\ ( stm ) , a nonparametric bayesian model of parsed documents . <eos> \shortname\ generates words that are both thematically and syntactically constrained , which combines the semantic insights of topic models with the syntactic information available from parse trees . <eos> each word of a sentence is generated by a distribution that combines document-specific topic weights and parse-tree specific syntactic transitions . <eos> words are assumed generated in an order that respects the parse tree . <eos> we derive an approximate posterior inference method based on variational methods for hierarchical dirichlet processes , and we report qualitative and quantitative results on both synthetic data and hand-parsed documents .
learning graphical models with hidden variables can offer semantic insights to complex data and lead to salient structured predictors without relying on expensive , sometime unattainable fully annotated training data . <eos> while likelihood-based methods have been extensively explored , to our knowledge , learning structured prediction models with latent variables based on the max-margin principle remains largely an open problem . <eos> in this paper , we present a partially observed maximum entropy discrimination markov network ( pomen ) model that attempts to combine the advantages of bayesian and margin based paradigms for learning markov networks from partially labeled data . <eos> pomen leads to an averaging prediction rule that resembles a bayes predictor that is more robust to overfitting , but is also built on the desirable discriminative laws resemble those of the m $ ^3 $ n . <eos> we develop an em-style algorithm utilizing existing convex optimization algorithms for m $ ^3 $ n as a subroutine . <eos> we demonstrate competent performance of pomen over existing methods on a real-world web data extraction task .
we show that the empirical minimizer of a stochastic strongly convex objective , where the stochastic component is linear , converges to the population minimizer with rate $ o ( 1/n ) $ . <eos> the result applies , in particular , to the svm objective . <eos> thus , we get a rate of $ o ( 1/n ) $ on the convergence of the svm objective to its infinite data limit . <eos> we demonstrate how this is essential for obtaining tight oracle inequalities for svms . <eos> the results extend also to strong convexity with respect to other $ \ellnorm_p $ norms , and so also to objectives regularized using other norms .
for undiscounted reinforcement learning in markov decision processes ( mdps ) we consider the total regret of a learning algorithm with respect to an optimal policy . <eos> in order to describe the transition structure of an mdp we propose a new parameter : an mdp has diameter d if for any pair of states s1 , s2 there is a policy which moves from s1 to s2 in at most d steps ( on average ) . <eos> we present a reinforcement learning algorithm with total regret o ( dsat ) after t steps for any unknown mdp with s states , a actions per state , and diameter d. this bound holds with high probability . <eos> we also present a corresponding lower bound of omega ( dsat ) on the total regret of any learning algorithm . <eos> both bounds demonstrate the utility of the diameter as structural parameter of the mdp .
this paper studies global ranking problem by learning to rank methods . <eos> conventional learning to rank methods are usually designed for ` local ranking ' , in the sense that the ranking model is defined on a single object , for example , a document in information retrieval . <eos> for many applications , this is a very loose approximation . <eos> relations always exist between objects and it is better to define the ranking model as a function on all the objects to be ranked ( i.e. , the relations are also included ) . <eos> this paper refers to the problem as global ranking and proposes employing a continuous conditional random fields ( crf ) for conducting the learning task . <eos> the continuous crf model is defined as a conditional probability distribution over ranking scores of objects conditioned on the objects . <eos> it can naturally represent the content information of objects as well as the relation information between objects , necessary for global ranking . <eos> taking two specific information retrieval tasks as examples , the paper shows how the continuous crf method can perform global ranking better than baselines .
learning in real-time applications , e.g. , online approximation of the inverse dynamics model for model-based robot control , requires fast online regression techniques . <eos> inspired by local learning , we propose a method to speed up standard gaussian process regression ( gpr ) with local gp models ( lgp ) . <eos> the training data is partitioned in local regions , for each an individual gp model is trained . <eos> the prediction for a query point is performed by weighted estimation using nearby local models . <eos> unlike other gp approximations , such as mixtures of experts , we use a distance based measure for partitioning of the data and weighted prediction . <eos> the proposed method achieves online learning and prediction in real-time . <eos> comparisons with other nonparametric regression methods show that lgp has higher accuracy than lwpr and close to the performance of standard gpr and nu-svr .
this paper is devoted to thoroughly investigating how to bootstrap the roc curve , a widely used visual tool for evaluating the accuracy of test/scoring statistics in the bipartite setup . <eos> the issue of confidence bands for the roc curve is considered and a resampling procedure based on a smooth version of the empirical distribution called the smoothed bootstrap '' is introduced . <eos> theoretical arguments and simulation results are presented to show that the `` smoothed bootstrap '' is preferable to a `` naive '' bootstrap in order to construct accurate confidence bands . ''
inspired by the hierarchical hidden markov models ( hhmm ) , we present the hierarchical semi-markov conditional random field ( hscrf ) , a generalisation of embedded undirected markov chains to model complex hierarchical , nested markov processes . <eos> it is parameterised in a discriminative framework and has polynomial time algorithms for learning and inference . <eos> importantly , we develop efficient algorithms for learning and constrained inference in a partially-supervised setting , which is important issue in practice where labels can only be obtained sparsely . <eos> we demonstrate the hscrf in two applications : ( i ) recognising human activities of daily living ( adls ) from indoor surveillance cameras , and ( ii ) noun-phrase chunking . <eos> we show that the hscrf is capable of learning rich hierarchical models with reasonable accuracy in both fully and partially observed data cases .
accurate and efficient inference in evolutionary trees is a central problem in computational biology . <eos> realistic models require tracking insertions and deletions along the phylogenetic tree , making inference challenging . <eos> we propose new sampling techniques that speed up inference and improve the quality of the samples . <eos> we compare our method to previous approaches and show performance improvement on metrics evaluating multiple sequence alignment and reconstruction of ancestral sequences .
we study the problem of domain transfer for a supervised classification task in mrna splicing . <eos> we consider a number of recent domain transfer methods from machine learning , including some that are novel , and evaluate them on genomic sequence data from model organisms of varying evolutionary distance . <eos> we find that in cases where the organisms are not closely related , the use of domain adaptation methods can help improve classification performance .
how does one extract unknown but stereotypical events that are linearly superimposed within a signal with variable latencies and variable amplitudes ? <eos> one could think of using template matching or matching pursuit to find the arbitrarily shifted linear components . <eos> however , traditional matching approaches require that the templates be known a priori . <eos> to overcome this restriction we use instead semi non-negative matrix factorization ( semi-nmf ) that we extend to allow for time shifts when matching the templates to the signal . <eos> the algorithm estimates templates directly from the data along with their non-negative amplitudes . <eos> the resulting method can be thought of as an adaptive template matching procedure . <eos> we demonstrate the procedure on the task of extracting spikes from single channel extracellular recordings . <eos> on these data the algorithm essentially performs spike detection and unsupervised spike clustering . <eos> results on simulated data and extracellular recordings indicate that the method performs well for signal-to-noise ratios of 6db or higher and that spike templates are recovered accurately provided they are sufficiently different .
covariance estimation for high dimensional vectors is a classically difficult problem in statistical analysis and machine learning due to limited sample size . <eos> in this paper , we propose a new approach to covariance estimation , which is based on constrained maximum likelihood ( ml ) estimation of the covariance . <eos> specifically , the covariance is constrained to have an eigen decomposition which can be represented as a sparse matrix transform ( smt ) . <eos> the smt is formed by a product of pairwise coordinate rotations known as givens rotations . <eos> using this framework , the covariance can be efficiently estimated using greedy minimization of the log likelihood function , and the number of givens rotations can be efficiently computed using a cross-validation procedure . <eos> the estimator obtained using this method is always positive definite and well-conditioned even with limited sample size . <eos> experiments on hyperspectral data show that smt covariance estimation results in consistently better estimates of the covariance for a variety of different classes and sample sizes compared to traditional shrinkage estimators .
we present the gaussian process density sampler ( gpds ) , an exchangeable generative model for use in nonparametric bayesian density estimation . <eos> samples drawn from the gpds are consistent with exact , independent samples from a fixed density function that is a transformation of a function drawn from a gaussian process prior . <eos> our formulation allows us to infer an unknown density from data using markov chain monte carlo , which gives samples from the posterior distribution over density functions and from the predictive distribution on data space . <eos> we can also infer the hyperparameters of the gaussian process . <eos> we compare this density modeling technique to several existing techniques on a toy problem and a skull-reconstruction task .
we present a characterization of a useful class of skills based on a graphical representation of an agent 's interaction with its environment . <eos> our characterization uses betweenness , a measure of centrality on graphs . <eos> it may be used directly to form a set of skills suitable for a given environment . <eos> more importantly , it serves as a useful guide for developing online , incremental skill discovery algorithms that do not rely on knowing or representing the environment graph in its entirety .
detecting underlying clusters from large-scale data plays a central role in machine learning research . <eos> in this paper , we attempt to tackle clustering problems for complex data of multiple distributions and large multi-scales . <eos> to this end , we develop an algorithm named zeta $ l $ -links , or zell which consists of two parts : zeta merging with a similarity graph and an initial set of small clusters derived from local $ l $ -links of the graph . <eos> more specifically , we propose to structurize a cluster using cycles in the associated subgraph . <eos> a mathematical tool , zeta function of a graph , is introduced for the integration of all cycles , leading to a structural descriptor of the cluster in determinantal form . <eos> the popularity character of the cluster is conceptualized as the global fusion of variations of the structural descriptor by means of the leave-one-out strategy in the cluster . <eos> zeta merging proceeds , in the agglomerative fashion , according to the maximum incremental popularity among all pairwise clusters . <eos> experiments on toy data , real imagery data , and real sensory data show the promising performance of zell . <eos> the $ 98.1\ % $ accuracy , in the sense of the normalized mutual information , is obtained on the frgc face data of 16028 samples and 466 facial clusters . <eos> the matlab codes of zell will be made publicly available for peer evaluation .
query expansion is a long-studied approach for improving retrieval effectiveness by enhancing the user ? s original query with additional related terms . <eos> current algorithms for automatic query expansion have been shown to consistently improve retrieval accuracy on average , but are highly unstable and have bad worst-case performance for individual queries . <eos> we introduce a novel risk framework that formulates query model estimation as a constrained metric labeling problem on a graph of term relations . <eos> themodel combines assignment costs based on a baseline feedback algorithm , edge weights based on term similarity , and simple constraints to enforce aspect balance , aspect coverage , and term centrality . <eos> results across multiple standard test collections show consistent and dramatic reductions in the number and magnitude of expansion failures , while retaining the strong positive gains of the baseline algorithm .
sampling functions in gaussian process ( gp ) models is challenging because of the highly correlated posterior distribution . <eos> we describe an efficient markov chain monte carlo algorithm for sampling from the posterior process of the gp model . <eos> this algorithm uses control variables which are auxiliary function values that provide a low dimensional representation of the function . <eos> at each iteration , the algorithm proposes new values for the control variables and generates the function from the conditional gp prior . <eos> the control variable input locations are found by continuously minimizing an objective function . <eos> we demonstrate the algorithm on regression and classification problems and we use it to estimate the parameters of a differential equation model of gene regulation .
we propose a novel bound on single-variable marginal probability distributions in factor graphs with discrete variables . <eos> the bound is obtained by propagating bounds ( convex sets of probability distributions ) over a subtree of the factor graph , rooted in the variable of interest . <eos> by construction , the method not only bounds the exact marginal probability distribution of a variable , but also its approximate belief propagation marginal ( `` belief '' ) . <eos> thus , apart from providing a practical means to calculate bounds on marginals , our contribution also lies in providing a better understanding of the error made by belief propagation . <eos> we show that our bound outperforms the state-of-the-art on some inference problems arising in medical diagnosis .
multi-level hierarchical models provide an attractive framework for incorporating correlations induced in a response variable organized in a hierarchy . <eos> model fitting is challenging , especially for hierarchies with large number of nodes . <eos> we provide a novel algorithm based on a multi-scale kalman filter that is both scalable and easy to implement . <eos> for non-gaussian responses , quadratic approximation to the log-likelihood results in biased estimates . <eos> we suggest a bootstrap strategy to correct such biases . <eos> our method is illustrated through simulation studies and analyses of real world data sets in health care and online advertising .
we analyze the estimation of information theoretic measures of continuous random variables such as : differential entropy , mutual information or kullback-leibler divergence . <eos> the objective of this paper is two-fold . <eos> first , we prove that the information theoretic measure estimates using the k-nearest-neighbor density estimation with fixed k converge almost surely , even though the k-nearest-neighbor density estimation with fixed k does not converge to its true measure . <eos> second , we show that the information theoretic measure estimates do not converge for k growing linearly with the number of samples . <eos> nevertheless , these nonconvergent estimates can be used for solving the two-sample problem and assessing if two random variables are independent . <eos> we show that the two-sample and independence tests based on these nonconvergent estimates compare favorably with the maximum mean discrepancy test and the hilbert schmidt independence criterion , respectively .
for supervised and unsupervised learning , positive definite kernels allow to use large and potentially infinite dimensional feature spaces with a computational cost that only depends on the number of observations . <eos> this is usually done through the penalization of predictor functions by euclidean or hilbertian norms . <eos> in this paper , we explore penalizing by sparsity-inducing norms such as the l1-norm or the block l1-norm . <eos> we assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph ; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework , in polynomial time in the number of selected kernels . <eos> this framework is naturally applied to non linear variable selection ; our extensive simulations on synthetic datasets and datasets from the uci repository show that efficiently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance .
in this theoretical contribution we provide mathematical proof that two of the most important classes of network learning - correlation-based differential hebbian learning and reward-based temporal difference learning - are asymptotically equivalent when timing the learning with a local modulatory signal . <eos> this opens the opportunity to consistently reformulate most of the abstract reinforcement learning framework from a correlation based perspective that is more closely related to the biophysics of neurons .
many machine learning algorithms require the summation of gaussian kernel functions , an expensive operation if implemented straightforwardly . <eos> several methods have been proposed to reduce the computational complexity of evaluating such sums , including tree and analysis based methods . <eos> these achieve varying speedups depending on the bandwidth , dimension , and prescribed error , making the choice between methods difficult for machine learning tasks . <eos> we provide an algorithm that combines tree methods with the improved fast gauss transform ( ifgt ) . <eos> as originally proposed the ifgt suffers from two problems : ( 1 ) the taylor series expansion does not perform well for very low bandwidths , and ( 2 ) parameter selection is not trivial and can drastically affect performance and ease of use . <eos> we address the first problem by employing a tree data structure , resulting in four evaluation methods whose performance varies based on the distribution of sources and targets and input parameters such as desired accuracy and bandwidth . <eos> to solve the second problem , we present an online tuning approach that results in a black box method that automatically chooses the evaluation method and its parameters to yield the best performance for the input data , desired accuracy , and bandwidth . <eos> in addition , the new ifgt parameter selection approach allows for tighter error bounds . <eos> our approach chooses the fastest method at negligible additional cost , and has superior performance in comparisons with previous approaches .
we propose a novel application of formal concept analysis ( fca ) to neural decoding : instead of just trying to figure out which stimulus was presented , we demonstrate how to explore the semantic relationships between the neural representation of large sets of stimuli . <eos> fca provides a way of displaying and interpreting such relationships via concept lattices . <eos> we explore the effects of neural code sparsity on the lattice . <eos> we then analyze neurophysiological data from high-level visual cortical area stsa , using an exact bayesian approach to construct the formal context needed by fca . <eos> prominent features of the resulting concept lattices are discussed , including indications for a product-of-experts code in real neurons .
we consider the problem of bounding from above the log-partition function corresponding to second-order ising models for binary distributions . <eos> we introduce a new bound , the cardinality bound , which can be computed via convex optimization . <eos> the corresponding error on the logpartition function is bounded above by twice the distance , in model parameter space , to a class of standardising models , for which variable inter-dependence is described via a simple mean field term . <eos> in the context of maximum-likelihood , using the new bound instead of the exact log-partition function , while constraining the distance to the class of standard ising models , leads not only to a good approximation to the log-partition function , but also to a model that is parsimonious , and easily interpretable . <eos> we compare our bound with the log-determinant bound introduced by wainwright and jordan ( 2006 ) , and show that when the l1 -norm of the model parameter vector is small enough , the latter is outperformed by the new bound .
we define a metric for measuring behavior similarity between states in a markov decision process ( mdp ) , in which action similarity is taken into account . <eos> we show that the kernel of our metric corresponds exactly to the classes of states defined by mdp homomorphisms ( ravindran \ & barto , 2003 ) . <eos> we prove that the difference in the optimal value function of different states can be upper-bounded by the value of this metric , and that the bound is tighter than that provided by bisimulation metrics ( ferns et al . 2004 , 2005 ) . <eos> our results hold both for discrete and for continuous actions . <eos> we provide an algorithm for constructing approximate homomorphisms , by using this metric to identify states that can be grouped together , as well as actions that can be matched . <eos> previous research on this topic is based mainly on heuristics .
this paper describes a recursive estimation procedure for multivariate binary densities using orthogonal expansions . <eos> for $ d $ covariates , there are $ 2^d $ basis coefficients to estimate , which renders conventional approaches computationally prohibitive when $ d $ is large . <eos> however , for a wide class of densities that satisfy a certain sparsity condition , our estimator runs in probabilistic polynomial time and adapts to the unknown sparsity of the underlying density in two key ways : ( 1 ) it attains near-minimax mean-squared error , and ( 2 ) the computational complexity is lower for sparser densities . <eos> our method also allows for flexible control of the trade-off between mean-squared error and computational complexity .
we provide statistical performance guarantees for a recently introduced kernel classifier that optimizes the $ l_2 $ or integrated squared error ( ise ) of a difference of densities . <eos> the classifier is similar to a support vector machine ( svm ) in that it is the solution of a quadratic program and yields a sparse classifier . <eos> unlike svms , however , the $ l_2 $ kernel classifier does not involve a regularization parameter . <eos> we prove a distribution free concentration inequality for a cross-validation based estimate of the ise , and apply this result to deduce an oracle inequality and consistency of the classifier on the sense of both ise and probability of error . <eos> our results can also be specialized to give performance guarantees for an existing method of $ l_2 $ kernel density estimation .
we propose an efficient sequential monte carlo inference scheme for the recently proposed coalescent clustering model ( teh et al , 2008 ) . <eos> our algorithm has a quadratic runtime while those in ( teh et al , 2008 ) is cubic . <eos> in experiments , we were surprised to find that in addition to being more efficient , it is also a better sequential monte carlo sampler than the best in ( teh et al , 2008 ) , when measured in terms of variance of estimated likelihood and effective sample size .
for many supervised learning problems , we possess prior knowledge about which features yield similar information about the target variable . <eos> in predicting the topic of a document , we might know that two words are synonyms , or when performing image recognition , we know which pixels are adjacent . <eos> such synonymous or neighboring features are near-duplicates and should therefore be expected to have similar weights in a good model . <eos> here we present a framework for regularized learning in settings where one has prior knowledge about which features are expected to have similar and dissimilar weights . <eos> this prior knowledge is encoded as a graph whose vertices represent features and whose edges represent similarities and dissimilarities between them . <eos> during learning , each feature 's weight is penalized by the amount it differs from the average weight of its neighbors . <eos> for text classification , regularization using graphs of word co-occurrences outperforms manifold learning and compares favorably to other recently proposed semi-supervised learning methods . <eos> for sentiment analysis , feature graphs constructed from declarative human knowledge , as well as from auxiliary task learning , significantly improve prediction accuracy .
we formulate the problem of bipartite graph inference as a supervised learning problem , and propose a new method to solve it from the viewpoint of distance metric learning . <eos> the method involves the learning of two mappings of the heterogeneous objects to a unified euclidean space representing the network topology of the bipartite graph , where the graph is easy to infer . <eos> the algorithm can be formulated as an optimization problem in a reproducing kernel hilbert space . <eos> we report encouraging results on the problem of compound-protein interaction network reconstruction from chemical structure data and genomic sequence data .
motor primitives or motion templates have become an important concept for both modeling human motor control as well as generating robot behaviors using imitation learning . <eos> recent impressive results range from humanoid robot movement generation to timing models of human motions . <eos> the automatic generation of skill libraries containing multiple motion templates is an important step in robot learning . <eos> such a skill learning system needs to cluster similar movements together and represent each resulting motion template as a generative model which is subsequently used for the execution of the behavior by a robot system . <eos> in this paper , we show how human trajectories captured as multidimensional time-series can be clustered using bayesian mixtures of linear gaussian state-space models based on the similarity of their dynamics . <eos> the appropriate number of templates is automatically determined by enforcing a parsimonious parametrization . <eos> as the resulting model is intractable , we introduce a novel approximation method based on variational bayes , which is especially designed to enable the use of efficient inference algorithms . <eos> on recorded human balero movements , this method is not only capable of finding reasonable motion templates but also yields a generative model which works well in the execution of this complex task on a simulated anthropomorphic sarcos arm .
we introduce a new interpretation of multiscale random fields ( msrfs ) that admits efficient optimization in the framework of regular ( single level ) random fields ( rfs ) . <eos> it is based on a new operator , called append , that combines sets of random variables ( rvs ) to single rvs . <eos> we assume that a msrf can be decomposed into disjoint trees that link rvs at different pyramid levels . <eos> the append operator is then applied to map rvs in each tree structure to a single rv . <eos> we demonstrate the usefulness of the proposed approach on a challenging task involving grouping contours of target shapes in images . <eos> msrfs provide a natural representation of multiscale contour models , which are needed in order to cope with unstable contour decompositions . <eos> the append operator allows us to find optimal image labels using the classical framework of relaxation labeling , alternative methods like markov chain monte carlo ( mcmc ) could also be used .
it has been shown that the problem of $ \ell_1 $ -penalized least-square regression commonly referred to as the lasso or basis pursuit denoising leads to solutions that are sparse and therefore achieves model selection . <eos> we propose in this paper an algorithm to solve the lasso with online observations . <eos> we introduce an optimization problem that allows us to compute an homotopy from the current solution to the solution after observing a new data point . <eos> we compare our method to lars and present an application to compressed sensing with sequential observations . <eos> our approach can also be easily extended to compute an homotopy from the current solution to the solution after removing a data point , which leads to an efficient algorithm for leave-one-out cross-validation .
we study the behavior of block ` 1 / ` 2 regularization for multivariate regression , where a k-dimensional response vector is regressed upon a fixed set of p covariates . <eos> the problem of support union recovery is to recover the subset of covariates that are active in at least one of the regression problems . <eos> studying this problem under high-dimensional scaling ( where the problem parameters as well as sample size n tend to infinity simultaneously ) , our main result is to show that exact recovery is possible once the order parameter given by ? ` 1 / ` 2 ( n , p , s ) : = n/ [ 2 ( b ? ) <eos> log ( p s ) ] exceeds a critical threshold . <eos> here n is the sample size , p is the ambient dimension of the regression model , s is the size of the union of supports , and ( b ? ) <eos> is a sparsity-overlap function that measures a combination of the sparsities and overlaps of the k-regression coefficient vectors that constitute the model . <eos> this sparsity-overlap function reveals that block ` 1 / ` 2 regularization for multivariate regression never harms performance relative to a naive ` 1 -approach , and can yield substantial improvements in sample complexity ( up to a factor of k ) when the regression vectors are suitably orthogonal relative to the design . <eos> we complement our theoretical results with simulations that demonstrate the sharpness of the result , even for relatively small problems .
subspace-based learning problems involve data whose elements are linear subspaces of a vector space . <eos> to handle such data structures , grassmann kernels have been proposed and used previously . <eos> in this paper , we analyze the relationship between grassmann kernels and probabilistic similarity measures . <eos> firstly , we show that the kl distance in the limit yields the projection kernel on the grassmann manifold , whereas the bhattacharyya kernel becomes trivial in the limit and is suboptimal for subspace-based problems . <eos> secondly , based on our analysis of the kl distance , we propose extensions of the projection kernel which can be extended to the set of affine as well as scaled subspaces . <eos> we demonstrate the advantages of these extended kernels for classification and recognition tasks with support vector machines and kernel discriminant analysis using synthetic and real image databases .
we describe an application of probabilistic modeling and inference technology to the problem of analyzing sensor data in the setting of an intensive care unit ( icu ) . <eos> in particular , we consider the arterial-line blood pressure sensor , which is subject to frequent data artifacts that cause false alarms in the icu and make the raw data almost useless for automated decision making . <eos> the problem is complicated by the fact that the sensor data are acquired at fixed intervals whereas the events causing data artifacts may occur at any time and have durations that may be significantly shorter than the data collection interval . <eos> we show that careful modeling of the sensor , combined with a general technique for detecting sub-interval events and estimating their duration , enables effective detection of artifacts and accurate estimation of the underlying blood pressure values .
we develop a statistical framework for the simultaneous , unsupervised segmentation and discovery of visual object categories from image databases . <eos> examining a large set of manually segmented scenes , we use chi -- square tests to show that object frequencies and segment sizes both follow power law distributions , which are well modeled by the pitman -- yor ( py ) process . <eos> this nonparametric prior distribution leads to learning algorithms which discover an unknown set of objects , and segmentation methods which automatically adapt their resolution to each image . <eos> generalizing previous applications of py processes , we use gaussian processes to discover spatially contiguous segments which respect image boundaries . <eos> using a novel family of variational approximations , our approach produces segmentations which compare favorably to state -- of -- the -- art methods , while simultaneously discovering categories shared among natural scenes .
we consider the problem of estimating the graph structure associated with a gaussian markov random field ( gmrf ) from i.i.d . <eos> samples . <eos> we study the performance of study the performance of the ? 1 -regularized maximum likelihood estimator in the high-dimensional setting , where the number of nodes in the graph p , the number of edges in the graph s and the maximum node degree d , are allowed to grow as a function of the number of samples n. our main result provides sufficient conditions on ( n , p , d ) for the ? 1 -regularized mle estimator to recover all the edges of the graph with high probability . <eos> under some conditions on the model covariance , we show that model selection can be achieved for sample sizes n = ( d2 log ( p ) ) , with the error decaying as o ( exp ( ? c log ( p ) ) ) for some constant c. we illustrate our theoretical results via simulations and show good correspondences between the theoretical predictions and behavior in simulations .
the roc curve is known to be the golden standard for measuring performance of a test/scoring statistic regarding its capacity of discrimination between two populations in a wide variety of applications , ranging from anomaly detection in signal processing to information retrieval , through medical diagnosis . <eos> most practical performance measures used in scoring applications such as the auc , the local auc , the p-norm push , the dcg and others , can be seen as summaries of the roc curve . <eos> this paper highlights the fact that many of these empirical criteria can be expressed as ( conditional ) linear rank statistics . <eos> we investigate the properties of empirical maximizers of such performance criteria and provide preliminary results for the concentration properties of a novel class of random variables that we will call a linear rank process .
the visual and auditory map alignment in the superior colliculus ( sc ) of barn owl is important for its accurate localization for prey behavior . <eos> prism learning or blindness may interfere this alignment and cause loss of the capability of accurate prey . <eos> however , juvenile barn owl could recover its sensory map alignment by shifting its auditory map . <eos> the adaptation of this map alignment is believed based on activity dependent axon developing in inferior colliculus ( ic ) . <eos> a model is built to explore this mechanism . <eos> in this model , axon growing process is instructed by an inhibitory network in sc while the strength of the inhibition adjusted by spike timing dependent plasticity ( stdp ) . <eos> we test and analyze this mechanism by application of the neural structures involved in spatial localization in a robotic system .
many machine learning algorithms can be formulated in the framework of statistical independence such as the hilbert schmidt independence criterion . <eos> in this paper , we extend this criterion to deal with with structured and interdependent observations . <eos> this is achieved by modeling the structures using undirected graphical models and comparing the hilbert space embeddings of distributions . <eos> we apply this new criterion to independent component analysis and sequence clustering .
we present a new family of linear time algorithms based on sufficient statistics for string comparison with mismatches under the string kernels framework . <eos> our algorithms improve theoretical complexity bounds of existing approaches while scaling well with respect to the sequence alphabet size , the number of allowed mismatches and the size of the dataset . <eos> in particular , on large alphabets with loose mismatch constraints our algorithms are several orders of magnitude faster than the existing algorithms for string comparison under the mismatch similarity measure . <eos> we evaluate our algorithms on synthetic data and real applications in music genre classification , protein remote homology detection and protein fold prediction . <eos> the scalability of the algorithms allows us to consider complex sequence transformations , modeled using longer string features and larger numbers of mismatches , leading to a state-of-the-art performance with significantly reduced running times .
recently , supervised dimensionality reduction has been gaining attention , owing to the realization that data labels are often available and strongly suggest important underlying structures in the data . <eos> in this paper , we present a novel convex supervised dimensionality reduction approach based on exponential family pca and provide a simple but novel form to project new testing data into the embedded space . <eos> this convex approach successfully avoids the local optima of the em learning . <eos> moreover , by introducing a sample-based multinomial approximation to exponential family models , it avoids the limitation of the prevailing gaussian assumptions of standard pca , and produces a kernelized formulation for nonlinear supervised dimensionality reduction . <eos> a training algorithm is then devised based on a subgradient bundle method , whose scalability can be gained through a coordinate descent procedure . <eos> the advantage of our global optimization approach is demonstrated by empirical results over both synthetic and real data .
cap tchas are computer-generated tests that humans can pass but current computer systems can not . <eos> cap tchas provide a method for automatically distinguishing a human from a computer program , and therefore can protect web services from abuse by so-called ? bots . ? <eos> most cap tchas consist of distorted images , usually text , for which a user must provide some description . <eos> unfortunately , visual cap tchas limit access to the millions of visually impaired people using the web . <eos> audio cap tchas were created to solve this accessibility issue ; however , the security of audio cap tchas was never formally tested . <eos> some visual cap tchas have been broken using machine learning techniques , and we propose using similar ideas to test the security of audio cap tchas . <eos> audio cap tchas are generally composed of a set of words to be identified , layered on top of noise . <eos> we analyzed the security of current audio cap tchas from popular web sites by using adaboost , svm , and k-nn , and achieved correct solutions for test samples with accuracy up to 71 % . <eos> such accuracy is enough to consider these captchas broken . <eos> training several different machine learning algorithms on different types of audio cap tchas allowed us to analyze the strengths and weaknesses of the algorithms so that we could suggest a design for a more robust audio captcha .
in this paper we present two transductive bounds on the risk of the majority vote estimated over partially labeled training sets . <eos> our first bound is tight when the additional unlabeled training data are used in the cases where the voted classifier makes its errors on low margin observations and where the errors of the associated gibbs classifier can accurately be estimated . <eos> in semi-supervised learning , considering the margin as an indicator of confidence constitutes the working hypothesis of algorithms which search the decision boundary on low density regions . <eos> in this case , we propose a second bound on the joint probability that the voted classifier makes an error over an example having its margin over a fixed threshold . <eos> as an application we are interested on self-learning algorithms which assign iteratively pseudo-labels to unlabeled training examples having margin above a threshold obtained from this bound . <eos> empirical results on different datasets show the effectiveness of our approach compared to the same algorithm and the tsvm in which the threshold is fixed manually .
in this paper we consider approximate policy-iteration-based reinforcement learning algorithms . <eos> in order to implement a flexible function approximation scheme we propose the use of non-parametric methods with regularization , providing a convenient way to control the complexity of the function approximator . <eos> we propose two novel regularized policy iteration algorithms by adding l2-regularization to two widely-used policy evaluation methods : bellman residual minimization ( brm ) and least-squares temporal difference learning ( lstd ) . <eos> we derive efficient implementation for our algorithms when the approximate value-functions belong to a reproducing kernel hilbert space . <eos> we also provide finite-sample performance bounds for our algorithms and show that they are able to achieve optimal rates of convergence under the studied conditions .
metric learning algorithms can provide useful distance functions for a variety of domains , and recent work has shown good accuracy for problems where the learner can access all distance constraints at once . <eos> however , in many real applications , constraints are only available incrementally , thus necessitating methods that can perform online updates to the learned metric . <eos> existing online algorithms offer bounds on worst-case performance , but typically do not perform well in practice as compared to their offline counterparts . <eos> we present a new online metric learning algorithm that updates a learned mahalanobis metric based on logdet regularization and gradient descent . <eos> we prove theoretical worst-case performance bounds , and empirically compare the proposed method against existing online metric learning algorithms . <eos> to further boost the practicality of our approach , we develop an online locality-sensitive hashing scheme which leads to efficient updates for approximate similarity search data structures . <eos> we demonstrate our algorithm on multiple datasets and show that it outperforms relevant baselines .
recent research suggests that neural systems employ sparse coding . <eos> however , there is limited theoretical understanding of fundamental resolution limits in such sparse coding . <eos> this paper considers a general sparse estimation problem of detecting the sparsity pattern of a $ k $ -sparse vector in $ \r^n $ from $ m $ random noisy measurements . <eos> our main results provide necessary and sufficient conditions on the problem dimensions , $ m $ , $ n $ and $ k $ , and the signal-to-noise ratio ( snr ) for asymptotically-reliable detection . <eos> we show a necessary condition for perfect recovery at any given snr for all algorithms , regardless of complexity , is $ m = \omega ( k\log ( n-k ) ) $ measurements . <eos> this is considerably stronger than all previous necessary conditions . <eos> we also show that the scaling of $ \omega ( k\log ( n-k ) ) $ measurements is sufficient for a trivial `` maximum correlation '' estimator to succeed . <eos> hence this scaling is optimal and does not require lasso , matching pursuit , or more sophisticated methods , and the optimal scaling can thus be biologically plausible .
it is now well established that sparse signal models are well suited to restoration tasks and can effectively be learned from audio , image , and video data . <eos> recent research has been aimed at learning discriminative sparse models instead of purely reconstructive ones . <eos> this paper proposes a new step in that direction with a novel sparse representation for signals belonging to different classes in terms of a shared dictionary and multiple decision functions . <eos> it is shown that the linear variant of the model admits a simple probabilistic interpretation , and that its most general variant also admits a simple interpretation in terms of kernels . <eos> an optimization framework for learning all the components of the proposed model is presented , along with experiments on standard handwritten digit and texture classification tasks .
offline handwriting recognition -- -the transcription of images of handwritten text -- -is an interesting task , in that it combines computer vision with sequence learning . <eos> in most systems the two elements are handled separately , with sophisticated preprocessing techniques used to extract the image features and sequential models such as hmms used to provide the transcriptions . <eos> by combining two recent innovations in neural networks -- -multidimensional recurrent neural networks and connectionist temporal classification -- -this paper introduces a globally trained offline handwriting recogniser that takes raw pixel data as input . <eos> unlike competing systems , it does not require any alphabet specific preprocessing , and can therefore be used unchanged for any language . <eos> evidence of its generality and power is provided by data from a recent international arabic recognition competition , where it outperformed all entries ( 91.4 % accuracy compared to 87.2 % for the competition winner ) despite the fact that neither author understands a word of arabic .
language and image understanding are two major goals of artificial intelligence which can both be conceptually formulated in terms of parsing the input signal into a hierarchical representation . <eos> natural language researchers have made great progress by exploiting the 1d structure of language to design efficient polynomialtime parsing algorithms . <eos> by contrast , the two-dimensional nature of images makes it much harder to design efficient image parsers and the form of the hierarchical representations is also unclear . <eos> attempts to adapt representations and algorithms from natural language have only been partially successful . <eos> in this paper , we propose a hierarchical image model ( him ) for 2d image parsing which outputs image segmentation and object recognition . <eos> this him is represented by recursive segmentation and recognition templates in multiple layers and has advantages for representation , inference , and learning . <eos> firstly , the him has a coarse-to-fine representation which is capable of capturing long-range dependency and exploiting different levels of contextual information . <eos> secondly , the structure of the him allows us to design a rapid inference algorithm , based on dynamic programming , which enables us to parse the image rapidly in polynomial time . <eos> thirdly , we can learn the him efficiently in a discriminative manner from a labeled dataset . <eos> we demonstrate that him outperforms other state-of-the-art methods by evaluation on the challenging public msrc image dataset . <eos> finally , we sketch how the him architecture can be extended to model more complex image phenomena .
large-margin structured estimation methods work by minimizing a convex upper bound of loss functions . <eos> while they allow for efficient optimization algorithms , these convex formulations are not tight and sacrifice the ability to accurately model the true loss . <eos> we present tighter non-convex bounds based on generalizing the notion of a ramp loss from binary classification to structured estimation . <eos> we show that a small modification of existing optimization algorithms suffices to solve this modified problem . <eos> on structured prediction tasks such as protein sequence alignment and web page ranking , our algorithm leads to improved accuracy .
we consider multi-armed bandit problems where the number of arms is larger than the possible number of experiments . <eos> we make a stochastic assumption on the mean-reward of a new selected arm which characterizes its probability of being a near-optimal arm . <eos> our assumption is weaker than in previous works . <eos> we describe algorithms based on upper-confidence-bounds applied to a restricted set of randomly selected arms and provide upper-bounds on the resulting expected regret . <eos> we also derive a lower-bound which matchs ( up to logarithmic factors ) the upper-bound in some cases .
we present a novel method for inducing synchronous context free grammars ( scfgs ) from a corpus of parallel string pairs . <eos> scfgs can model equivalence between strings in terms of substitutions , insertions and deletions , and the reordering of sub-strings . <eos> we develop a non-parametric bayesian model and apply it to a machine translation task , using priors to replace the various heuristics commonly used in this field . <eos> using a variational bayes training procedure , we learn the latent structure of translation equivalence through the induction of synchronous grammar categories for phrasal translations , showing improvements in translation performance over previously proposed maximum likelihood models .
we tackle the computational problem of query-conditioned search . <eos> given a machine-learned scoring rule and a query distribution , we build a predictive index by precomputing lists of potential results sorted based on an expected score of the result over future queries . <eos> the predictive index datastructure supports an anytime algorithm for approximate retrieval of the top elements . <eos> the general approach is applicable to webpage ranking , internet advertisement , and approximate nearest neighbor search . <eos> it is particularly effective in settings where standard techniques ( e.g. , inverted indices ) are intractable . <eos> we experimentally find substantial improvement over existing methods for internet advertisement and approximate nearest neighbors .
hierarchical probabilistic modeling of discrete data has emerged as a powerful tool for text analysis . <eos> posterior inference in such models is intractable , and practitioners rely on approximate posterior inference methods such as variational inference or gibbs sampling . <eos> there has been much research in designing better approximations , but there is yet little theoretical understanding of which of the available techniques are appropriate , and in which data analysis settings . <eos> in this paper we provide the beginnings of such understanding . <eos> we analyze the improvement that the recently proposed collapsed variational inference ( cvb ) provides over mean field variational inference ( vb ) in latent dirichlet allocation . <eos> we prove that the difference in the tightness of the bound on the likelihood of a document decreases as $ o ( k-1 ) + \log m /m $ , where $ k $ is the number of topics in the model and $ m $ is the number of words in a document . <eos> as a consequence , the advantage of cvb over vb is lost for long documents but increases with the number of topics . <eos> we demonstrate empirically that the theory holds , using simulated text data and two text corpora . <eos> we provide practical guidelines for choosing an approximation .
we investigate a topic at the interface of machine learning and cognitive science . <eos> human active learning , where learners can actively query the world for information , is contrasted with passive learning from random examples . <eos> furthermore , we compare human active learning performance with predictions from statistical learning theory . <eos> we conduct a series of human category learning experiments inspired by a machine learning task for which active and passive learning error bounds are well understood , and dramatically distinct . <eos> our results indicate that humans are capable of actively selecting informative queries , and in doing so learn better and faster than if they are given random training data , as predicted by learning theory . <eos> however , the improvement over passive learning is not as dramatic as that achieved by machine active learning algorithms . <eos> to the best of our knowledge , this is the first quantitative study comparing human category learning in active versus passive settings .
this paper examines the generalization properties of online convex programming algorithms when the loss function is lipschitz and strongly convex . <eos> our main result is a sharp bound , that holds with high probability , on the excess risk of the output of an online algorithm in terms of the average regret . <eos> this allows one to use recent algorithms with logarithmic cumulative regret guarantees to achieve fast convergence rates for the excess risk with high probability . <eos> the bound also solves an open problem regarding the convergence rate of { \pegasos } , a recently proposed method for solving the svm optimization problem .
psychophysical experiments show that humans are better at perceiving rotation and expansion than translation . <eos> these findings are inconsistent with standard models of motion integration which predict best performance for translation [ 6 ] . <eos> to explain this discrepancy , our theory formulates motion perception at two levels of inference : we first perform model selection between the competing models ( e.g . translation , rotation , and expansion ) and then estimate the velocity using the selected model . <eos> we define novel prior models for smooth rotation and expansion using techniques similar to those in the slow-and-smooth model [ 17 ] ( e.g . green functions of differential operators ) . <eos> the theory gives good agreement with the trends observed in human experiments .
in partially observable worlds with many agents , nested beliefs are formed when agents simultaneously reason about the unknown state of the world and the beliefs of the other agents . <eos> the multi-agent filtering problem is to efficiently represent and update these beliefs through time as the agents act in the world . <eos> in this paper , we formally define an infinite sequence of nested beliefs about the state of the world at the current time $ t $ and present a filtering algorithm that maintains a finite representation which can be used to generate these beliefs . <eos> in some cases , this representation can be updated exactly in constant time ; we also present a simple approximation scheme to compact beliefs if they become too complex . <eos> in experiments , we demonstrate efficient filtering in a range of multi-agent domains .
regularized least squares ( rls ) algorithms have the ability to avoid over-fitting problems and to express solutions as kernel expansions . <eos> however , we observe that the current rls algorithms can not provide a satisfactory interpretation even on a constant function . <eos> on the other hand , while kernel-based algorithms have been developed in such a tendency that almost all learning algorithms are kernelized or being kernelized , a basic fact is often ignored : the learned function from the data and the kernel fits the data well , but may not be consistent with the kernel . <eos> based on these considerations and on the intuition that a good kernel-based inductive function should be consistent with both the data and the kernel , a novel learning scheme is proposed . <eos> the advantages of this scheme lie in its corresponding representer theorem , its strong interpretation ability about what kind of functions should not be penalized , and its promising accuracy improvements shown in a number of experiments . <eos> furthermore , we provide a detailed technical description about heat kernels , which serves as an example for the readers to apply similar techniques for other kernels . <eos> our work provides a preliminary step in a new direction to explore the varying consistency between inductive functions and kernels under various distributions .
a crucial part of developing mathematical models of how the brain works is the quantification of their success . <eos> one of the most widely-used metrics yields the percentage of the variance in the data that is explained by the model . <eos> unfortunately , this metric is biased due to the intrinsic variability in the data . <eos> this variability is in principle unexplainable by the model . <eos> we derive a simple analytical modification of the traditional formula that significantly improves its accuracy ( as measured by bias ) with similar or better precision ( as measured by mean-square error ) in estimating the true underlying variance explained by the model class . <eos> our estimator advances on previous work by a ) accounting for the uncertainty in the noise estimate , b ) accounting for overfitting due to free model parameters mitigating the need for a separate validation data set and c ) adding a conditioning term . <eos> we apply our new estimator to binocular disparity tuning curves of a set of macaque v1 neurons and find that on a population level almost all of the variance unexplained by gabor functions is attributable to noise .
kernel supervised learning methods can be unified by utilizing the tools from regularization theory . <eos> the duality between regularization and prior leads to interpreting regularization methods in terms of maximum a posteriori estimation and has motivated bayesian interpretations of kernel methods . <eos> in this paper we pursue a bayesian interpretation of sparsity in the kernel setting by making use of a mixture of a point-mass distribution and prior that we refer to as `` silverman 's g-prior . '' <eos> we provide a theoretical analysis of the posterior consistency of a bayesian model choice procedure based on this prior . <eos> we also establish the asymptotic relationship between this procedure and the bayesian information criterion .
the problem of ranking arises ubiquitously in almost every aspect of life , and in particular in machine learning/information retrieval . <eos> a statistical model for ranking predicts how humans rank subsets v of some universe u . <eos> in this work we define a statistical model for ranking that satisfies certain desirable properties . <eos> the model automatically gives rise to a logistic regression based approach to learning how to rank , for which the score and comparison based approaches are dual views . <eos> this offers a new generative approach to ranking which can be used for ir . <eos> there are two main contexts for this work . <eos> the first is the theory of econometrics and study of statistical models explaining human choice of alternatives . <eos> in this context , we will compare our model with other well known models . <eos> the second context is the problem of ranking in machine learning , usually arising in the context of information retrieval . <eos> here , much work has been done in the discriminative setting , where different heuristics are used to define ranking risk functions . <eos> our model is built rigorously and axiomatically based on very simple desirable properties defined locally for comparisons , and automatically implies the existence of a global score function serving as a natural model parameter which can be efficiently fitted to pairwise comparison judgment data by solving a convex optimization problem .
models for near-rigid shape matching are typically based on distance-related features , in order to infer matches that are consistent with the isometric assumption . <eos> however , real shapes from image datasets , even when expected to be related by almost isometric '' transformations , are actually subject not only to noise but also , to some limited degree , to variations in appearance and scale . <eos> in this paper , we introduce a graphical model that parameterises appearance , distance , and angle features and we learn all of the involved parameters via structured prediction . <eos> the outcome is a model for near-rigid shape matching which is robust in the sense that it is able to capture the possibly limited but still important scale and appearance variations . <eos> our experimental results reveal substantial improvements upon recent successful models , while maintaining similar running times . ''
in most cognitive and motor tasks , speed-accuracy tradeoffs are observed : individuals can respond slowly and accurately , or quickly yet be prone to errors . <eos> control mechanisms governing the initiation of behavioral responses are sensitive not only to task instructions and the stimulus being processed , but also to the recent stimulus history . <eos> when stimuli can be characterized on an easy-hard dimension ( e.g. , word frequency in a naming task ) , items preceded by easy trials are responded to more quickly , and with more errors , than items preceded by hard trials . <eos> we propose a rationally motivated mathematical model of this sequential adaptation of control , based on a diffusion model of the decision process in which difficulty corresponds to the drift rate for the correct response . <eos> the model assumes that responding is based on the posterior distribution over which response is correct , conditioned on the accumulated evidence . <eos> we derive this posterior as a function of the drift rate , and show that higher estimates of the drift rate lead to ( normatively ) faster responding . <eos> trial-by-trial tracking of difficulty thus leads to sequential effects in speed and accuracy . <eos> simulations show the model explains a variety of phenomena in human speeded decision making . <eos> we argue this passive statistical mechanism provides a more elegant and parsimonious account than extant theories based on elaborate control structures .
in this paper lower and upper bounds for the number of support vectors are derived for support vector machines ( svms ) based on the epsilon-insensitive loss function . <eos> it turns out that these bounds are asymptotically tight under mild assumptions on the data generating distribution . <eos> finally , we briefly discuss a trade-off in epsilon between sparsity and accuracy if the svm is used to estimate the conditional median .
this paper presents a spike feature extraction algorithm that targets real-time spike sorting and facilitates miniaturized microchip implementation . <eos> the proposed algorithm has been evaluated on synthesized waveforms and experimentally recorded sequences . <eos> when compared with many spike sorting approaches our algorithm demonstrates improved speed , accuracy and allows unsupervised execution . <eos> a preliminary hardware implementation has been realized using an integrated microchip interfaced with a personal computer .
we explore a recently proposed mixture model approach to understanding interactions between conflicting sensory cues . <eos> alternative model formulations , differing in their sensory noise models and inference methods , are compared based on their fit to experimental data . <eos> heavy-tailed sensory likelihoods yield a better description of the subjects ? <eos> response behavior than standard gaussian noise models . <eos> we study the underlying cause for this result , and then present several testable predictions of these models .
roc curves are one of the most widely used displays to evaluate performance of scoring functions . <eos> in the paper , we propose a statistical method for directly optimizing the roc curve . <eos> the target is known to be the regression function up to an increasing transformation and this boils down to recovering the level sets of the latter . <eos> we propose to use classifiers obtained by empirical risk minimization of a weighted classification error and then to construct a scoring rule by overlaying these classifiers . <eos> we show the consistency and rate of convergence to the optimal roc curve of this procedure in terms of supremum norm and also , as a byproduct of the analysis , we derive an empirical estimate of the optimal roc curve .
we introduce a novel framework for estimating vector fields using sparse basis field expansions ( s-flex ) . <eos> the notion of basis fields , which are an extension of scalar basis functions , arises naturally in our framework from a rotational invariance requirement . <eos> we consider a regression setting as well as inverse problems . <eos> all variants discussed lead to second-order cone programming formulations . <eos> while our framework is generally applicable to any type of vector field , we focus in this paper on applying it to solving the eeg/meg inverse problem . <eos> it is shown that significantly more precise and neurophysiologically more plausible location and shape estimates of cerebral current sources from eeg/meg measurements become possible with our method when comparing to the state-of-the-art .
we study the convergence and the rate of convergence of a local manifold learning algorithm : ltsa [ 13 ] . <eos> the main technical tool is the perturbation analysis on the linear invariant subspace that corresponds to the solution of ltsa . <eos> we derive a worst-case upper bound of errors for ltsa which naturally leads to a convergence result . <eos> we then derive the rate of convergence for ltsa in a special case .
one of the original goals of computer vision was to fully understand a natural scene . <eos> this requires solving several problems simultaneously , including object detection , labeling of meaningful regions , and 3d reconstruction . <eos> while great progress has been made in tackling each of these problems in isolation , only recently have researchers again been considering the difficult task of assembling various methods to the mutual benefit of all . <eos> we consider learning a set of such classification models in such a way that they both solve their own problem and help each other . <eos> we develop a framework known as cascaded classification models ( ccm ) , where repeated instantiations of these classifiers are coupled by their input/output variables in a cascade that improves performance at each level . <eos> our method requires only a limited ? black box ?interface with the models , allowing us to use very sophisticated , state-of-the-art classifiers without having to look under the hood . <eos> we demonstrate the effectiveness of our method on a large set of natural images by combining the subtasks of scene categorization , object detection , multiclass image segmentation , and 3d scene reconstruction .
the singular value decomposition is a key operation in many machine learning methods . <eos> its computational cost , however , makes it unscalable and impractical for the massive-sized datasets becoming common in applications . <eos> we present a new method , quic-svd , for fast approximation of the full svd with automatic sample size minimization and empirical relative error control . <eos> previous monte carlo approaches have not addressed the full svd nor benefited from the efficiency of automatic , empirically-driven sample sizing . <eos> our empirical tests show speedups of several orders of magnitude over exact svd . <eos> such scalability should enable quic-svd to meet the needs of a wide array of methods and applications .
cognitive control refers to the flexible deployment of memory and attention in response to task demands and current goals . <eos> control is often studied experimentally by presenting sequences of stimuli , some demanding a response , and others modulating the stimulus-response mapping . <eos> in these tasks , participants must maintain information about the current stimulus-response mapping in working memory . <eos> prominent theories of cognitive control use recurrent neural nets to implement working memory , and optimize memory utilization via reinforcement learning . <eos> we present a novel perspective on cognitive control in which working memory representations are intrinsically probabilistic , and control operations that maintain and update working memory are dynamically determined via probabilistic inference . <eos> we show that our model provides a parsimonious account of behavioral and neuroimaging data , and suggest that it offers an elegant conceptualization of control in which behavior can be cast as optimal , subject to limitations on learning and the rate of information processing . <eos> moreover , our model provides insight into how task instructions can be directly translated into appropriate behavior and then efficiently refined with subsequent task experience .
current on-line learning algorithms for predicting the labelling of a graph have an important limitation in the case of large diameter graphs ; the number of mistakes made by such algorithms may be proportional to the square root of the number of vertices , even when tackling simple problems . <eos> we overcome this problem with an efficient algorithm which achieves a logarithmic mistake bound . <eos> furthermore , current algorithms are optimised for data which exhibits cluster-structure ; we give an additional algorithm which performs well locally in the presence of cluster structure and on large diameter graphs .
from an information-theoretic perspective , a noisy transmission system such as a visual brain-computer interface ( bci ) speller could benefit from the use of error-correcting codes . <eos> however , optimizing the code solely according to the maximal minimum-hamming-distance criterion tends to lead to an overall increase in target frequency of target stimuli , and hence a significantly reduced average target-to-target interval ( tti ) , leading to difficulties in classifying the individual event-related potentials ( erps ) due to overlap and refractory effects . <eos> clearly any change to the stimulus setup must also respect the possible psychophysiological consequences . <eos> here we report new eeg data from experiments in which we explore stimulus types and codebooks in a within-subject design , finding an interaction between the two factors . <eos> our data demonstrate that the traditional , row-column code has particular spatial properties that lead to better performance than one would expect from its ttis and hamming-distances alone , but nonetheless error-correcting codes can improve performance provided the right stimulus type is used .
we provide a new analysis of an efficient margin-based algorithm for selective sampling in classification problems . <eos> using the so-called tsybakov low noise condition to parametrize the instance distribution , we show bounds on the convergence rate to the bayes risk of both the fully supervised and the selective sampling versions of the basic algorithm . <eos> our analysis reveals that , excluding logarithmic factors , the average risk of the selective sampler converges to the bayes risk at rate $ n^ { - ( 1+\alpha ) / ( 3+\alpha ) } $ , with labels being sampled at the same rate ( here $ n $ denotes the sample size , and $ \alpha > 0 $ is the exponent in the low noise condition ) . <eos> we compare this convergence rate to the rate $ n^ { - ( 1+\alpha ) / ( 2+\alpha ) } $ achieved by the fully supervised algorithm using all labels . <eos> experiments on textual data reveal that simple variants of the proposed selective sampler perform much better than popular and similarly efficient competitors .
a novel center-based clustering algorithm is proposed in this paper . <eos> we first formulate clustering as an np-hard linear integer program and we then use linear programming and the duality theory to derive the solution of this optimization problem . <eos> this leads to an efficient and very general algorithm , which works in the dual domain , and can cluster data based on an arbitrary set of distances . <eos> despite its generality , it is independent of initialization ( unlike em-like methods such as k-means ) , has guaranteed convergence , and can also provide online optimality bounds about the quality of the estimated clustering solutions . <eos> to deal with the most critical issue in a center-based clustering algorithm ( selection of cluster centers ) , we also introduce the notion of stability of a cluster center , which is a well defined lp-based quantity that plays a key role to our algorithm 's success . <eos> furthermore , we also introduce , what we call , the margins ( another key ingredient in our algorithm ) , which can be roughly thought of as dual counterparts to stabilities and allow us to obtain computationally efficient approximations to the latter . <eos> promising experimental results demonstrate the potentials of our method .
we propose a multiplicative approximation scheme ( mas ) for inference problems in graphical models , which can be applied to various inference algorithms . <eos> the method uses $ \epsilon $ -decompositions which decompose functions used throughout the inference procedure into functions over smaller sets of variables with a known error $ \epsilon $ . <eos> mas translates these local approximations into bounds on the accuracy of the results . <eos> we show how to optimize $ \epsilon $ -decompositions and provide a fast closed-form solution for an $ l_2 $ approximation . <eos> applying mas to the variable elimination inference algorithm , we introduce an algorithm we call dynadecomp which is extremely fast in practice and provides guaranteed error bounds on the result . <eos> the superior accuracy and efficiency of dynadecomp is demonstrated .
spectral clustering is useful for a wide-ranging set of applications in areas such as biological data analysis , image processing and data mining . <eos> however , the computational and/or communication resources required by the method in processing large-scale data sets are often prohibitively high , and practitioners are often required to perturb the original data in various ways ( quantization , downsampling , etc ) before invoking a spectral algorithm . <eos> in this paper , we use stochastic perturbation theory to study the effects of data perturbation on the performance of spectral clustering . <eos> we show that the error under perturbation of spectral clustering is closely related to the perturbation of the eigenvectors of the laplacian matrix . <eos> from this result we derive approximate upper bounds on the clustering error . <eos> we show that this bound is tight empirically across a wide range of problems , suggesting that it can be used in practical settings to determine the amount of data reduction allowed in order to meet a specification of permitted loss in clustering performance .
we propose a novel hierarchical , nonlinear model that predicts brain activity in area v1 evoked by natural images . <eos> in the study reported here brain activity was measured by means of functional magnetic resonance imaging ( fmri ) , a noninvasive technique that provides an indirect measure of neural activity pooled over a small volume ( ~ 2mm cube ) of brain tissue . <eos> our model , which we call the spam v1 model , is based on the reasonable assumption that fmri measurements reflect the ( possibly nonlinearly ) pooled , rectified output of a large population of simple and complex cells in v1 . <eos> it has a hierarchical filtering stage that consists of three layers : model simple cells , model complex cells , and a third layer in which the complex cells are linearly pooled ( called ? ? ? pooled-complex ? ? cells ) . <eos> the pooling stage then obtains the measured fmri signals as a sparse additive model ( spam ) in which a sparse nonparametric ( nonlinear ) combination of model complex cell and model pooled-complex cell outputs are summed . <eos> our results show that the spam v1 model predicts fmri responses evoked by natural images better than a benchmark model that only provides linear pooling of model complex cells . <eos> furthermore , the spatial receptive fields , frequency tuning and orientation tuning curves of the spam v1 model estimated for each voxel appears to be consistent with the known properties of v1 , and with previous analyses of this data set . <eos> a visualization procedure applied to the spam v1 model shows that most of the nonlinear pooling consists of simple compressive or saturating nonlinearities .
we describe a way of learning matrix representations of objects and relationships . <eos> the goal of learning is to allow multiplication of matrices to represent symbolic relationships between objects and symbolic relationships between relationships , which is the main novelty of the method . <eos> we demonstrate that this leads to excellent generalization in two different domains : modular arithmetic and family relationships . <eos> we show that the same system can learn first-order propositions such as $ ( 2 , 5 ) \member +\ ! 3 $ or $ ( christopher , penelope ) \member has\_wife $ , and higher-order propositions such as $ ( 3 , +\ ! 3 ) \member plus $ and $ ( +\ ! 3 , -\ ! 3 ) \member inverse $ or $ ( has\_husband , has\_wife ) \in higher\_oppsex $ . <eos> we further demonstrate that the system understands how higher-order propositions are related to first-order ones by showing that it can correctly answer questions about first-order propositions involving the relations $ +\ ! 3 $ or $ has\_wife $ even though it has not been trained on any first-order examples involving these relations .
we present a new co-clustering problem of images and visual features . <eos> the problem involves a set of non-object images in addition to a set of object images and features to be co-clustered . <eos> co-clustering is performed in a way of maximising discrimination of object images from non-object images , thus emphasizing discriminative features . <eos> this provides a way of obtaining perceptual joint-clusters of object images and features . <eos> we tackle the problem by simultaneously boosting multiple strong classifiers which compete for images by their expertise . <eos> each boosting classifier is an aggregation of weak-learners , i.e . simple visual features . <eos> the obtained classifiers are useful for multi-category and multi-view object detection tasks . <eos> experiments on a set of pedestrian images and a face data set demonstrate that the method yields intuitive image clusters with associated features and is much superior to conventional boosting classifiers in object detection tasks .
we describe a primal-dual framework for the design and analysis of online strongly convex optimization algorithms . <eos> our framework yields the tightest known logarithmic regret bounds for follow-the-leader and for the gradient descent algorithm proposed in hazankakaag06 . <eos> we then show that one can interpolate between these two extreme cases . <eos> in particular , we derive a new algorithm that shares the computational simplicity of gradient descent but achieves lower regret in many practical situations . <eos> finally , we further extend our framework for generalized strongly convex functions .
bartlett et al ( 2006 ) recently proved that a ground condition for convex surrogates , classification calibration , ties up the minimization of the surrogates and classification risks , and left as an important problem the algorithmic questions about the minimization of these surrogates . <eos> in this paper , we propose an algorithm which provably minimizes any classification calibrated surrogate strictly convex and differentiable -- - a set whose losses span the exponential , logistic and squared losses -- - , with boosting-type guaranteed convergence rates under a weak learning assumption . <eos> a particular subclass of these surrogates , that we call balanced convex surrogates , has a key rationale that ties it to maximum likelihood estimation , zero-sum games and the set of losses that satisfy some of the most common requirements for losses in supervised learning . <eos> we report experiments on more than 50 readily available domains of 11 flavors of the algorithm , that shed light on new surrogates , and the potential of data dependent strategies to tune surrogates .
this paper addresses the important tradeoff between privacy and learnability , when designing algorithms for learning from private databases . <eos> first we apply an idea of dwork et al . to design a specific privacy-preserving machine learning algorithm , logistic regression . <eos> this involves bounding the sensitivity of logistic regression , and perturbing the learned classifier with noise proportional to the sensitivity . <eos> noting that the approach of dwork et al . has limitations when applied to other machine learning algorithms , we then present another privacy-preserving logistic regression algorithm . <eos> the algorithm is based on solving a perturbed objective , and does not depend on the sensitivity . <eos> we prove that our algorithm preserves privacy in the model due to dwork et al. , and we provide a learning performance guarantee . <eos> our work also reveals an interesting connection between regularization and privacy .
compressive sensing ( cs ) combines sampling and compression into a single sub-nyquist linear measurement process for sparse and compressible signals . <eos> in this paper , we extend the theory of cs to include signals that are concisely represented in terms of a graphical model . <eos> in particular , we use markov random fields ( mrfs ) to represent sparse signals whose nonzero coefficients are clustered . <eos> our new model-based reconstruction algorithm , dubbed lattice matching pursuit ( lamp ) , stably recovers mrf-modeled signals using many fewer measurements and computations than the current state-of-the-art algorithms .
the cluster assumption is exploited by most semi-supervised learning ( ssl ) methods . <eos> however , if the unlabeled data is merely weakly related to the target classes , it becomes questionable whether driving the decision boundary to the low density regions of the unlabeled data will help the classification . <eos> in such case , the cluster assumption may not be valid ; and consequently how to leverage this type of unlabeled data to enhance the classification accuracy becomes a challenge . <eos> we introduce semi-supervised learning with weakly-related unlabeled data '' ( sslw ) , an inductive method that builds upon the maximum-margin approach , towards a better usage of weakly-related unlabeled information . <eos> although the sslw could improve a wide range of classification tasks , in this paper , we focus on text categorization with a small training pool . <eos> the key assumption behind this work is that , even with different topics , the word usage patterns across different corpora tends to be consistent . <eos> to this end , sslw estimates the optimal word-correlation matrix that is consistent with both the co-occurrence information derived from the weakly-related unlabeled documents and the labeled documents . <eos> for empirical evaluation , we present a direct comparison with a number of state-of-the-art methods for inductive semi-supervised learning and text categorization ; and we show that sslw results in a significant improvement in categorization accuracy , equipped with a small training set and an unlabeled resource that is weakly related to the test beds . ''
this paper presents the first data-dependent generalization bounds for non-i.i.d . <eos> settings based on the notion of rademacher complexity . <eos> our bounds extend to the non-i.i.d . <eos> case existing rademacher complexity bounds derived for the i.i.d . <eos> setting . <eos> these bounds provide a strict generalization of the ones found in the i.i.d . <eos> case , and can also be used within the standard i.i.d . <eos> scenario . <eos> they apply to the standard scenario of beta-mixing stationary sequences examined in many previous studies of non-i.i.d . <eos> settings and benefit form the crucial advantages of rademacher complexity over other measures of the complexity of hypothesis classes . <eos> in particular , they are data-dependent and measure the complexity of a class of hypotheses based on the training sample . <eos> the empirical rademacher complexity can be estimated from finite samples and lead to tighter bounds .
in this paper , we address the question of what kind of knowledge is generally transferable from unlabeled text . <eos> we suggest and analyze the semantic correlation of words as a generally transferable structure of the language and propose a new method to learn this structure using an appropriately chosen latent variable model . <eos> this semantic correlation contains structural information of the language space and can be used to control the joint shrinkage of model parameters for any specific task in the same space through regularization . <eos> in an empirical study , we construct 190 different text classification tasks from a real-world benchmark , and the unlabeled documents are a mixture from all these tasks . <eos> we test the ability of various algorithms to use the mixed unlabeled text to enhance all classification tasks . <eos> empirical results show that the proposed approach is a reliable and scalable method for semi-supervised learning , regardless of the source of unlabeled data , the specific task to be enhanced , and the prediction model used .
aiming towards the development of a general clustering theory , we discuss abstract axiomatization for clustering . <eos> in this respect , we follow up on the work of kelinberg , ( kleinberg ) that showed an impossibility result for such axiomatization . <eos> we argue that an impossibility result is not an inherent feature of clustering , but rather , to a large extent , it is an artifact of the specific formalism used in kleinberg . <eos> as opposed to previous work focusing on clustering functions , we propose to address clustering quality measures as the primitive object to be axiomatized . <eos> we show that principles like those formulated in kleinberg 's axioms can be readily expressed in the latter framework without leading to inconsistency . <eos> a clustering-quality measure is a function that , given a data set and its partition into clusters , returns a non-negative real number representing how ` strong ' or ` conclusive ' the clustering is . <eos> we analyze what clustering-quality measures should look like and introduce a set of requirements ( ` axioms ' ) that express these requirement and extend the translation of kleinberg 's axioms to our framework . <eos> we propose several natural clustering quality measures , all satisfying the proposed axioms . <eos> in addition , we show that the proposed clustering quality can be computed in polynomial time .
this paper investigates a new machine learning strategy called translated learning . <eos> unlike many previous learning tasks , we focus on how to use labeled data from one feature space to enhance the classification of other entirely different learning spaces . <eos> for example , we might wish to use labeled text data to help learn a model for classifying image data , when the labeled images are difficult to obtain . <eos> an important aspect of translated learning is to build a bridgeto link one feature space ( known as the ? source space ? ) <eos> to another space ( known as the ? target space ? ) <eos> through a translator in order to migrate the knowledge from source to target . <eos> the translated learning solution uses a language model to link the class labels to the features in the source spaces , which in turn is translated to the features in the target spaces . <eos> finally , this chain of linkages is completed by tracing back to the instances in the target spaces . <eos> we show that this path of linkage can be modeled using a markov chain and risk minimization . <eos> through experiments on the text-aided image classification and cross-language classification tasks , we demonstrate that our translated learning framework can greatly outperform many state-of-the-art baseline methods .
existing approaches to nonrigid structure from motion assume that the instantaneous 3d shape of a deforming object is a linear combination of basis shapes , which have to be estimated anew for each video sequence . <eos> in contrast , we propose that the evolving 3d structure be described by a linear combination of basis trajectories . <eos> the principal advantage of this lateral approach is that we do not need to estimate any basis vectors during computation . <eos> instead , we show that generic bases over trajectories , such as the discrete cosine transform ( dct ) bases , can be used to effectively describe most real motions . <eos> this results in a significant reduction in unknowns , and corresponding stability , in estimation . <eos> we report empirical performance , quantitatively using motion capture data and qualitatively on several video sequences exhibiting nonrigid motions including piece-wise rigid motion , articulated motion , partially nonrigid motion ( such as a facial expression ) , and highly nonrigid motion ( such as a person dancing ) .
we consider the problem of extracting smooth low-dimensional `` neural trajectories '' that summarize the activity recorded simultaneously from tens to hundreds of neurons on individual experimental trials . <eos> beyond the benefit of visualizing the high-dimensional noisy spiking activity in a compact denoised form , such trajectories can offer insight into the dynamics of the neural circuitry underlying the recorded activity . <eos> current methods for extracting neural trajectories involve a two-stage process : the data are first `` denoised '' by smoothing over time , then a static dimensionality reduction technique is applied . <eos> we first describe extensions of the two-stage methods that allow the degree of smoothing to be chosen in a principled way , and account for spiking variability that may vary both across neurons and across time . <eos> we then present a novel method for extracting neural trajectories , gaussian-process factor analysis ( gpfa ) , which unifies the smoothing and dimensionality reduction operations in a common probabilistic framework . <eos> we applied these methods to the activity of 61 neurons recorded simultaneously in macaque premotor and motor cortices during reach planning and execution . <eos> by adopting a goodness-of-fit metric that measures how well the activity of each neuron can be predicted by all other recorded neurons , we found that gpfa provided a better characterization of the population activity than the two-stage methods . <eos> from the extracted single-trial neural trajectories , we directly observed a convergence in neural state during motor planning , an effect suggestive of attractor dynamics that was shown indirectly by previous studies .
randomized neural networks are immortalized in this ai koan : in the days when sussman was a novice , minsky once came to him as he sat hacking at the pdp-6 . <eos> `` what are you doing ? '' <eos> asked minsky . <eos> `` i am training a randomly wired neural net to play tic-tac-toe , '' sussman replied . <eos> `` why is the net wired randomly ? '' <eos> asked minsky . <eos> sussman replied , `` i do not want it to have any preconceptions of how to play . '' <eos> minsky then shut his eyes . <eos> `` why do you close your eyes ? '' <eos> sussman asked his teacher . <eos> `` so that the room will be empty , '' replied minsky . <eos> at that moment , sussman was enlightened . <eos> we analyze shallow random networks with the help of concentration of measure inequalities . <eos> specifically , we consider architectures that compute a weighted sum of their inputs after passing them through a bank of arbitrary randomized nonlinearities . <eos> we identify conditions under which these networks exhibit good classification performance , and bound their test error in terms of the size of the dataset and the number of random nonlinearities .
graph clustering methods such as spectral clustering are defined for general weighted graphs . <eos> in machine learning , however , data often is not given in form of a graph , but in terms of similarity ( or distance ) values between points . <eos> in this case , first a neighborhood graph is constructed using the similarities between the points and then a graph clustering algorithm is applied to this graph . <eos> in this paper we investigate the influence of the construction of the similarity graph on the clustering results . <eos> we first study the convergence of graph clustering criteria such as the normalized cut ( ncut ) as the sample size tends to infinity . <eos> we find that the limit expressions are different for different types of graph , for example the r-neighborhood graph or the k-nearest neighbor graph . <eos> in plain words : ncut on a knn graph does something systematically different than ncut on an r-neighborhood graph ! <eos> this finding shows that graph clustering criteria can not be studied independently of the kind of graph they are applied to . <eos> we also provide examples which show that these differences can be observed for toy and real data already for rather small sample sizes .
identification and comparison of nonlinear dynamical systems using noisy and sparse experimental data is a vital task in many fields , however current methods are computationally expensive and prone to error due in part to the nonlinear nature of the likelihood surfaces induced . <eos> we present an accelerated sampling procedure which enables bayesian inference of parameters in nonlinear ordinary and delay differential equations via the novel use of gaussian processes ( gp ) . <eos> our method involves gp regression over time-series data , and the resulting derivative and time delay estimates make parameter inference possible without solving the dynamical system explicitly , resulting in dramatic savings of computational time . <eos> we demonstrate the speed and statistical accuracy of our approach using examples of both ordinary and delay differential equations , and provide a comprehensive comparison with current state of the art methods .
metal binding is important for the structural and functional characterization of proteins . <eos> previous prediction efforts have only focused on bonding state , i.e . deciding which protein residues act as metal ligands in some binding site . <eos> identifying the geometry of metal-binding sites , i.e . deciding which residues are jointly involved in the coordination of a metal ion is a new prediction problem that has been never attempted before from protein sequence alone . <eos> in this paper , we formulate it in the framework of learning with structured outputs . <eos> our solution relies on the fact that , from a graph theoretical perspective , metal binding has the algebraic properties of a matroid , enabling the application of greedy algorithms for learning structured outputs . <eos> on a data set of 199 non-redundant metalloproteins , we obtained precision/recall levels of 75\ % /46\ % correct ligand-ion assignments , which improves to 88\ % /88\ % in the setting where the metal binding state is known .
in multi-task learning several related tasks are considered simultaneously , with the hope that by an appropriate sharing of information across tasks , each task may benefit from the others . <eos> in the context of learning linear functions for supervised classification or regression , this can be achieved by including a priori information about the weight vectors associated with the tasks , and how they are expected to be related to each other . <eos> in this paper , we assume that tasks are clustered into groups , which are unknown beforehand , and that tasks within a group have similar weight vectors . <eos> we design a new spectral norm that encodes this a priori assumption , without the prior knowledge of the partition of tasks into groups , resulting in a new convex optimization formulation for multi-task learning . <eos> we show in simulations on synthetic examples and on the iedb mhc-i binding dataset , that our approach outperforms well-known convex methods for multi-task learning , as well as related non convex methods dedicated to the same problem .
in this paper we introduce the meannn approach for estimation of main information theoretic measures such as differential entropy , mutual information and divergence . <eos> as opposed to other nonparametric approaches the meannn results in smooth differentiable functions of the data samples with clear geometrical interpretation . <eos> then we apply the proposed estimators to the ica problem and obtain a smooth expression for the mutual information that can be analytically optimized by gradient descent methods . <eos> the improved performance on the proposed ica algorithm is demonstrated on standard tests in comparison with state-of-the-art techniques .
recently , fitted q-iteration ( fqi ) based methods have become more popular due to their increased sample efficiency , a more stable learning process and the higher quality of the resulting policy . <eos> however , these methods remain hard to use for continuous action spaces which frequently occur in real-world tasks , e.g. , in robotics and other technical applications . <eos> the greedy action selection commonly used for the policy improvement step is particularly problematic as it is expensive for continuous actions , can cause an unstable learning process , introduces an optimization bias and results in highly non-smooth policies unsuitable for real-world systems . <eos> in this paper , we show that by using a soft-greedy action selection the policy improvement step used in fqi can be simplified to an inexpensive advantage-weighted regression . <eos> with this result , we are able to derive a new , computationally efficient fqi algorithm which can even deal with high dimensional action spaces .
ranking is at the heart of many information retrieval applications . <eos> unlike standard regression or classification , in which we predict outputs independently , in ranking , we are interested in predicting structured outputs so that misranking one object can significantly affect whether we correctly rank the other objects . <eos> in practice , the problem of ranking involves a large number of objects to be ranked and either approximate structured prediction methods are required , or assumptions of independence between object scores must be made in order to make the problem tractable . <eos> we present a probabilistic method for learning to rank using the graphical modelling framework of cumulative distribution networks ( cdns ) , where we can take into account the structure inherent to the problem of ranking by modelling the joint cumulative distribution functions ( cdfs ) over multiple pairwise preferences . <eos> we apply our framework to the problem of document retrieval in the case of the ohsumed benchmark dataset . <eos> we will show that the ranknet , listnet and listmle probabilistic models can be viewed as particular instances of cdns and that our proposed framework allows for the exploration of a broad class of flexible structured loss functionals for ranking learning .
we present a novel mathematical formalism for the idea of a local model , '' a model of a potentially complex dynamical system that makes only certain predictions in only certain situations . <eos> as a result of its restricted responsibilities , a local model may be far simpler than a complete model of the system . <eos> we then show how one might combine several local models to produce a more detailed model . <eos> we demonstrate our ability to learn a collection of local models on a large-scale example and do a preliminary empirical comparison of learning a collection of local models and some other model learning methods . ''
markov decision processes ( mdps ) have been extensively studied and used in the context of planning and decision-making , and many methods exist to find the optimal policy for problems modelled as mdps . <eos> although finding the optimal policy is sufficient in many domains , in certain applications such as decision support systems where the policy is executed by a human ( rather than a machine ) , finding all possible near-optimal policies might be useful as it provides more flexibility to the person executing the policy . <eos> in this paper we introduce the new concept of non-deterministic mdp policies , and address the question of finding near-optimal non-deterministic policies . <eos> we propose two solutions to this problem , one based on a mixed integer program and the other one based on a search algorithm . <eos> we include experimental results obtained from applying this framework to optimize treatment choices in the context of a medical decision support system .
eeg connectivity measures could provide a new type of feature space for inferring a subject 's intention in brain-computer interfaces ( bcis ) . <eos> however , very little is known on eeg connectivity patterns for bcis . <eos> in this study , eeg connectivity during motor imagery ( mi ) of the left and right is investigated in a broad frequency range across the whole scalp by combining beamforming with transfer entropy and taking into account possible volume conduction effects . <eos> observed connectivity patterns indicate that modulation intentionally induced by mi is strongest in the gamma-band , i.e. , above 35 hz . <eos> furthermore , modulation between mi and rest is found to be more pronounced than between mi of different hands . <eos> this is in contrast to results on mi obtained with bandpower features , and might provide an explanation for the so far only moderate success of connectivity features in bcis . <eos> it is concluded that future studies on connectivity based bcis should focus on high frequency bands and consider experimental paradigms that maximally vary cognitive demands between conditions .
we present an approach to low-level vision that combines two main ideas : the use of convolutional networks as an image processing architecture and an unsupervised learning procedure that synthesizes training samples from specific noise models . <eos> we demonstrate this approach on the challenging problem of natural image denoising . <eos> using a test set with a hundred natural images , we find that convolutional networks provide comparable and in some cases superior performance to state of the art wavelet and markov random field ( mrf ) methods . <eos> moreover , we find that a convolutional network offers similar performance in the blind denoising setting as compared to other techniques in the non-blind setting . <eos> we also show how convolutional networks are mathematically related to mrf approaches by presenting a mean field theory for an mrf specially designed for image denoising . <eos> although these approaches are related , convolutional networks avoid computational difficulties in mrf approaches that arise from probabilistic learning and inference . <eos> this makes it possible to learn image processing architectures that have a high degree of representational power ( we train models with over 15,000 parameters ) , but whose computational expense is significantly less than that associated with inference in mrf approaches with even hundreds of parameters .
compared to invasive brain-computer interfaces ( bci ) , non-invasive bci systems based on electroencephalogram ( eeg ) signals have not been applied successfully for complex control tasks . <eos> in the present study , however , we demonstrate this is possible and report on the interaction of a human subject with a complex real device : a pinball machine . <eos> first results in this single subject study clearly show that fast and well-timed control well beyond chance level is possible , even though the environment is extremely rich and requires complex predictive behavior . <eos> using machine learning methods for mental state decoding , bci-based pinball control is possible within the first session without the necessity to employ lengthy subject training . <eos> while the current study is still of anecdotal nature , it clearly shows that very compelling control with excellent timing and dynamics is possible for a non-invasive bci .
working memory is a central topic of cognitive neuroscience because it is critical for solving real world problems in which information from multiple temporally distant sources must be combined to generate appropriate behavior . <eos> however , an often neglected fact is that learning to use working memory effectively is itself a difficult problem . <eos> the gating '' framework is a collection of psychological models that show how dopamine can train the basal ganglia and prefrontal cortex to form useful working memory representations in certain types of problems . <eos> we bring together gating with ideas from machine learning about using finite memory systems in more general problems . <eos> thus we present a normative gating model that learns , by online temporal difference methods , to use working memory to maximize discounted future rewards in general partially observable settings . <eos> the model successfully solves a benchmark working memory problem , and exhibits limitations similar to those observed in human experiments . <eos> moreover , the model introduces a concise , normative definition of high level cognitive concepts such as working memory and cognitive control in terms of maximizing discounted future rewards . ''
sequential optimal design methods hold great promise for improving the efficiency of neurophysiology experiments . <eos> however , previous methods for optimal experimental design have incorporated only weak prior information about the underlying neural system ( e.g. , the sparseness or smoothness of the receptive field ) . <eos> here we describe how to use stronger prior information , in the form of parametric models of the receptive field , in order to construct optimal stimuli and further improve the efficiency of our experiments . <eos> for example , if we believe that the receptive field is well-approximated by a gabor function , then our method constructs stimuli that optimally constrain the gabor parameters ( orientation , spatial frequency , etc . ) <eos> using as few experimental trials as possible . <eos> more generally , we may believe a priori that the receptive field lies near a known sub-manifold of the full parameter space ; in this case , our method chooses stimuli in order to reduce the uncertainty along the tangent space of this sub-manifold as rapidly as possible . <eos> applications to simulated and real data indicate that these methods may in many cases improve the experimental efficiency .
we provide sharp bounds for rademacher and gaussian complexities of ( constrained ) linear classes . <eos> these bounds make short work of providing a number of corollaries including : risk bounds for linear prediction ( including settings where the weight vectors are constrained by either $ l_2 $ or $ l_1 $ constraints ) , margin bounds ( including both $ l_2 $ and $ l_1 $ margins , along with more general notions based on relative entropy ) , a proof of the pac-bayes theorem , and $ l_2 $ covering numbers ( with $ l_p $ norm constraints and relative entropy constraints ) . <eos> in addition to providing a unified analysis , the results herein provide some of the sharpest risk and margin bounds ( improving upon a number of previous results ) . <eos> interestingly , our results show that the uniform convergence rates of empirical risk minimization algorithms tightly match the regret bounds of online learning algorithms for linear prediction ( up to a constant factor of 2 ) .
policy gradient ( pg ) reinforcement learning algorithms have strong ( local ) convergence guarantees , but their learning performance is typically limited by a large variance in the estimate of the gradient . <eos> in this paper , we formulate the variance reduction problem by describing a signal-to-noise ratio ( snr ) for policy gradient algorithms , and evaluate this snr carefully for the popular weight perturbation ( wp ) algorithm . <eos> we confirm that snr is a good predictor of long-term learning performance , and that in our episodic formulation , the cost-to-go function is indeed the optimal baseline . <eos> we then propose two modifications to traditional model-free policy gradient algorithms in order to optimize the snr . <eos> first , we examine wp using anisotropic sampling distributions , which introduces a bias into the update but increases the snr ; this bias can be interpretted as following the natural gradient of the cost function . <eos> second , we show that non-gaussian distributions can also increase the snr , and argue that the optimal isotropic distribution is a ? ? ? shell ?distribution with a constant magnitude and uniform distribution in direction . <eos> we demonstrate that both modifications produce substantial improvements in learning performance in challenging policy gradient experiments .
we show that an important and computationally challenging solution space feature of the graph coloring problem ( col ) , namely the number of clusters of solutions , can be accurately estimated by a technique very similar to one for counting the number of solutions . <eos> this cluster counting approach can be naturally written in terms of a new factor graph derived from the factor graph representing the col instance . <eos> using a variant of the belief propagation inference framework , we can efficiently approximate cluster counts in random col problems over a large range of graph densities . <eos> we illustrate the algorithm on instances with up to 100 , 000 vertices . <eos> moreover , we supply a methodology for computing the number of clusters exactly using advanced techniques from the knowledge compilation literature . <eos> this methodology scales up to several hundred variables .
one major role of primary visual cortex ( v1 ) in vision is the encoding of the orientation of lines and contours . <eos> the role of the local recurrent network in these computations is , however , still a matter of debate . <eos> to address this issue , we analyze intracellular recording data of cat v1 , which combine measuring the tuning of a range of neuronal properties with a precise localization of the recording sites in the orientation preference map . <eos> for the analysis , we consider a network model of hodgkin-huxley type neurons arranged according to a biologically plausible two-dimensional topographic orientation preference map . <eos> we then systematically vary the strength of the recurrent excitation and inhibition relative to the strength of the afferent input . <eos> each parametrization gives rise to a different model instance for which the tuning of model neurons at different locations of the orientation map is compared to the experimentally measured orientation tuning of membrane potential , spike output , excitatory , and inhibitory conductances . <eos> a quantitative analysis shows that the data provides strong evidence for a network model in which the afferent input is dominated by strong , balanced contributions of recurrent excitation and inhibition . <eos> this recurrent regime is close to a regime of 'instability ' , where strong , self-sustained activity of the network occurs . <eos> the firing rate of neurons in the best-fitting network is particularly sensitive to small modulations of model parameters , which could be one of the functional benefits of a network operating in this particular regime .
we present cutoff averaging '' , a technique for converting any conservative online learning algorithm into a batch learning algorithm . <eos> most online-to-batch conversion techniques work well with certain types of online learning algorithms and not with others , whereas cutoff averaging explicitly tries to adapt to the characteristics of the online algorithm being converted . <eos> an attractive property of our technique is that it preserves the efficiency of the original online algorithm , making it approporiate for large-scale learning problems . <eos> we provide a statistical analysis of our technique and back our theoretical claims with experimental results . ''
we address the problem of learning classifiers for several related tasks that may differ in their joint distribution of input and output variables . <eos> for each task , small - possibly even empty - labeled samples and large unlabeled samples are available . <eos> while the unlabeled samples reflect the target distribution , the labeled samples may be biased . <eos> we derive a solution that produces resampling weights which match the pool of all examples to the target distribution of any given task . <eos> our work is motivated by the problem of predicting sociodemographic features for users of web portals , based on the content which they have accessed . <eos> here , questionnaires offered to a small portion of each portal 's users produce biased samples . <eos> transfer learning enables us to make predictions even for new portals with few or no training data and improves the overall prediction accuracy .
selective attention is a most intensively studied psychological phenomenon , rife with theoretical suggestions and schisms . <eos> a critical idea is that of limited capacity , the allocation of which has produced half a century 's worth of conflict about such phenomena as early and late selection . <eos> an influential resolution of this debate is based on the notion of perceptual load ( lavie , 2005 , tics , 9 : 75 ) , which suggests that low-load , easy tasks , because they underuse the total capacity of attention , mandatorily lead to the processing of stimuli that are irrelevant to the current attentional set ; whereas high-load , difficult tasks grab all resources for themselves , leaving distractors high and dry . <eos> we argue that this theory presents a challenge to bayesian theories of attention , and suggest an alternative , statistical , account of key supporting data .
actor-critic algorithms for reinforcement learning are achieving renewed popularity due to their good convergence properties in situations where other approaches often fail ( e.g. , when function approximation is involved ) . <eos> interestingly , there is growing evidence that actor-critic approaches based on phasic dopamine signals play a key role in biological learning through the cortical and basal ganglia . <eos> we derive a temporal difference based actor critic learning algorithm , for which convergence can be proved without assuming separate time scales for the actor and the critic . <eos> the approach is demonstrated by applying it to networks of spiking neurons . <eos> the established relation between phasic dopamine and the temporal difference signal lends support to the biological relevance of such algorithms .
we introduces a new probability distribution over a potentially infinite number of binary markov chains which we call the markov indian buffet process . <eos> this process extends the ibp to allow temporal dependencies in the hidden variables . <eos> we use this stochastic process to build a nonparametric extension of the factorial hidden markov model . <eos> after working out an inference scheme which combines slice sampling and dynamic programming we demonstrate how the infinite factorial hidden markov model can be used for blind source separation .
in a variety of behavioral tasks , subjects exhibit an automatic and apparently sub-optimal sequential effect : they respond more rapidly and accurately to a stimulus if it reinforces a local pattern in stimulus history , such as a string of repetitions or alternations , compared to when it violates such a pattern . <eos> this is often the case even if the local trends arise by chance in the context of a randomized design , such that stimulus history has no predictive power . <eos> in this work , we use a normative bayesian framework to examine the hypothesis that such idiosyncrasies may reflect the inadvertent engagement of fundamental mechanisms critical for adapting to changing statistics in the natural environment . <eos> we show that prior belief in non-stationarity can induce experimentally observed sequential effects in an otherwise bayes-optimal algorithm . <eos> the bayesian algorithm is shown to be well approximated by linear-exponential filtering of past observations , a feature also apparent in the behavioral data . <eos> we derive an explicit relationship between the parameters and computations of the exact bayesian algorithm and those of the approximate linear-exponential filter . <eos> since the latter is equivalent to a leaky-integration process , a commonly used model of neuronal dynamics underlying perceptual decision-making and trial-to-trial dependencies , our model provides a principled account of why such dynamics are useful . <eos> we also show that near-optimal tuning of the leaky-integration process is possible , using stochastic gradient descent based only on the noisy binary inputs . <eos> this is a proof of concept that not only can neurons implement near-optimal prediction based on standard neuronal dynamics , but that they can also learn to tune the processing parameters without explicitly representing probabilities .
we consider the problem of multiple kernel learning ( mkl ) , which can be formulated as a convex-concave problem . <eos> in the past , two efficient methods , i.e. , semi-infinite linear programming ( silp ) and subgradient descent ( sd ) , have been proposed for large-scale multiple kernel learning . <eos> despite their success , both methods have their own shortcomings : ( a ) the sd method utilizes the gradient of only the current solution , and ( b ) the silp method does not regularize the approximate solution obtained from the cutting plane model . <eos> in this work , we extend the level method , which was originally designed for optimizing non-smooth objective functions , to convex-concave optimization , and apply it to multiple kernel learning . <eos> the extended level method overcomes the drawbacks of silp and sd by exploiting all the gradients computed in past iterations and by regularizing the solution via a projection to a level set . <eos> empirical study with eight uci datasets shows that the extended level method can significantly improve efficiency by saving on average 91.9 % of computational time over the silp method and 70.3 % over the sd method .
we consider the problem of efficiently encoding a signal by transforming it to a new representation whose components are statistically independent . <eos> a widely studied linear solution , independent components analysis ( ica ) , exists for the case when the signal is generated as a linear transformation of independent non- gaussian sources . <eos> here , we examine a complementary case , in which the source is non-gaussian but elliptically symmetric . <eos> in this case , no linear transform suffices to properly decompose the signal into independent components , but we show that a simple nonlinear transformation , which we call radial gaussianization ( rg ) , is able to remove all dependencies . <eos> we then demonstrate this methodology in the context of natural signal statistics . <eos> we first show that the joint distributions of bandpass filter responses , for both sound and images , are better described as elliptical than linearly transformed independent sources . <eos> consistent with this , we demonstrate that the reduction in dependency achieved by applying rg to either pairs or blocks of bandpass filter responses is significantly greater than that achieved by pca or ica .
neural activity is non-stationary and varies across time . <eos> hidden markov models ( hmms ) have been used to track the state transition among quasi-stationary discrete neural states . <eos> within this context , independent poisson models have been used for the output distribution of hmms ; hence , the model is incapable of tracking the change in correlation without modulating the firing rate . <eos> to achieve this , we applied a multivariate poisson distribution with correlation terms for the output distribution of hmms . <eos> we formulated a variational bayes ( vb ) inference for the model . <eos> the vb could automatically determine the appropriate number of hidden states and correlation types while avoiding the overlearning problem . <eos> we developed an efficient algorithm for computing posteriors using the recursive relationship of a multivariate poisson distribution . <eos> we demonstrated the performance of our method on synthetic data and a real spike train recorded from a songbird .
most algorithms for solving markov decision processes rely on a discount factor , which ensures their convergence . <eos> in fact , it is often used in problems with is no intrinsic motivation . <eos> in this paper , we show that when used in approximate dynamic programming , an artificially low discount factor may significantly improve the performance on some problems , such as tetris . <eos> we propose two explanations for this phenomenon . <eos> our first justification follows directly from the standard approximation error bounds : using a lower discount factor may decrease the approximation error bounds . <eos> however , we also show that these bounds are loose , a thus their decrease does not entirely justify a better practical performance . <eos> we thus propose another justification : when the rewards are received only sporadically ( as it is the case in tetris ) , we can derive tighter bounds , which support a significant performance increase with a decrease in the discount factor .
distributed learning is a problem of fundamental interest in machine learning and cognitive science . <eos> in this paper , we present asynchronous distributed learning algorithms for two well-known unsupervised learning frameworks : latent dirichlet allocation ( lda ) and hierarchical dirichlet processes ( hdp ) . <eos> in the proposed approach , the data are distributed across p processors , and processors independently perform gibbs sampling on their local data and communicate their information in a local asynchronous manner with other processors . <eos> we demonstrate that our asynchronous algorithms are able to learn global topic models that are statistically as accurate as those learned by the standard lda and hdp samplers , but with significant improvements in computation time and memory . <eos> we show speedup results on a 730-million-word text corpus using 32 processors , and we provide perplexity results for up to 1500 virtual processors . <eos> as a stepping stone in the development of asynchronous hdp , a parallel hdp sampler is also introduced .
causal structure-discovery techniques usually assume that all causes of more than one variable are observed . <eos> this is the so-called causal sufficiency assumption . <eos> in practice , it is untestable , and often violated . <eos> in this paper , we present an efficient causal structure-learning algorithm , suited for causally insufficient data . <eos> similar to algorithms such as ic* and fci , the proposed approach drops the causal sufficiency assumption and learns a structure that indicates ( potential ) latent causes for pairs of observed variables . <eos> assuming a constant local density of the data-generating graph , our algorithm makes a quadratic number of conditional-independence tests w.r.t . <eos> the number of variables . <eos> we show with experiments that our algorithm is comparable to the state-of-the-art fci algorithm in accuracy , while being several orders of magnitude faster on large problems . <eos> we conclude that mbcs* makes a new range of causally insufficient problems computationally tractable .
we study learning formulations with non-convex regularizaton that are natural for sparse linear models . <eos> there are two approaches to this problem : ( 1 ) heuristic methods such as gradient descent that only find a local minimum . <eos> a drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution . <eos> ( 2 ) convex relaxation such as $ l_1 $ -regularization that solves the problem under some conditions . <eos> however it often leads to sub-optimal sparsity in reality . <eos> this paper tries to remedy the above gap between theory and practice . <eos> in particular , we investigate a multi-stage convex relaxation scheme for solving problems with non-convex regularization . <eos> theoretically , we analyze the behavior of a resulting two-stage relaxation scheme for the capped- $ l_1 $ regularization . <eos> our performance bound shows that the procedure is superior to the standard $ l_1 $ convex relaxation for learning sparse targets . <eos> experiments confirm the effectiveness of this method on some simulation and real data .
we present a new , massively parallel architecture for accelerating machine learning algorithms , based on arrays of variable-resolution arithmetic vector processing elements ( vpe ) . <eos> groups of vpes operate in simd ( single instruction multiple data ) mode , and each group is connected to an independent memory bank . <eos> in this way memory bandwidth scales with the number of vpe , and the main data flows are local , keeping power dissipation low . <eos> with 256 vpes , implemented on two fpga ( field programmable gate array ) chips , we obtain a sustained speed of 19 gmacs ( billion multiply-accumulate per sec . ) <eos> for svm training , and 86 gmacs for svm classification . <eos> this performance is more than an order of magnitude higher than that of any fpga implementation reported so far . <eos> the speed on one fpga is similar to the fastest speeds published on a graphics processor for the mnist problem , despite a clock rate of the fpga that is six times lower . <eos> high performance at low clock rates makes this massively parallel architecture particularly attractive for embedded applications , where low power dissipation is critical . <eos> tests with convolutional neural networks and other learning algorithms are under way now .
continuous attractor neural networks ( canns ) are emerging as promising models for describing the encoding of continuous stimuli in neural systems . <eos> due to the translational invariance of their neuronal interactions , canns can hold a continuous family of neutrally stable states . <eos> in this study , we systematically explore how neutral stability of a cann facilitates its tracking performance , a capacity believed to have wide applications in brain functions . <eos> we develop a perturbative approach that utilizes the dominant movement of the network stationary states in the state space . <eos> we quantify the distortions of the bump shape during tracking , and study their effects on the tracking performance . <eos> results are obtained on the maximum speed for a moving stimulus to be trackable , and the reaction time to catch up an abrupt change in stimulus .
accounts of how people learn functional relationships between continuous variables have tended to focus on two possibilities : that people are estimating explicit functions , or that they are simply performing associative learning supported by similarity . <eos> we provide a rational analysis of function learning , drawing on work on regression in machine learning and statistics . <eos> using the equivalence of bayesian linear regression and gaussian processes , we show that learning explicit rules and using similarity can be seen as two views of one solution to this problem . <eos> we use this insight to define a gaussian process model of human function learning that combines the strengths of both approaches .
bandpass filtering , orientation selectivity , and contrast gain control are prominent features of sensory coding at the level of v1 simple cells . <eos> while the effect of bandpass filtering and orientation selectivity can be assessed within a linear model , contrast gain control is an inherently nonlinear computation . <eos> here we employ the class of $ l_p $ elliptically contoured distributions to investigate the extent to which the two features -- -orientation selectivity and contrast gain control -- -are suited to model the statistics of natural images . <eos> within this framework we find that contrast gain control can play a significant role for the removal of redundancies in natural images . <eos> orientation selectivity , in contrast , has only a very limited potential for redundancy reduction .
a visual attention system should respond placidly when common stimuli are presented , while at the same time keep alert to anomalous visual inputs . <eos> in this paper , a dynamic visual attention model based on the rarity of features is proposed . <eos> we introduce the incremental coding length ( icl ) to measure the perspective entropy gain of each feature . <eos> the objective of our model is to maximize the entropy of the sampled visual features . <eos> in order to optimize energy consumption , the limit amount of energy of the system is re-distributed amongst features according to their incremental coding length . <eos> by selecting features with large coding length increments , the computational system can achieve attention selectivity in both static and dynamic scenes . <eos> we demonstrate that the proposed model achieves superior accuracy in comparison to mainstream approaches in static saliency map generation . <eos> moreover , we also show that our model captures several less-reported dynamic visual search behaviors , such as attentional swing and inhibition of return .
principal components analysis ( pca ) has become established as one of the key tools for dimensionality reduction when dealing with real valued data . <eos> approaches such as exponential family pca and non-negative matrix factorisation have successfully extended pca to non-gaussian data types , but these techniques fail to take advantage of bayesian inference and can suffer from problems of overfitting and poor generalisation . <eos> this paper presents a fully probabilistic approach to pca , which is generalised to the exponential family , based on hybrid monte carlo sampling . <eos> we describe the model which is based on a factorisation of the observed data matrix , and show performance of the model on both synthetic and real data .
integrating semantic and syntactic analysis is essential for document analysis . <eos> using an analogous reasoning , we present an approach that combines bag-of-words and spatial models to perform semantic and syntactic analysis for recognition of an object based on its internal appearance and its context . <eos> we argue that while object recognition requires modeling relative spatial locations of image features within the object , a bag-of-word is sufficient for representing context . <eos> learning such a model from weakly labeled data involves labeling of features into two classes : foreground ( object ) or `` informative '' background ( context ) . <eos> labeling . <eos> we present a `` shape-aware '' model which utilizes contour information for efficient and accurate labeling of features in the image . <eos> our approach iterates between an mcmc-based labeling and contour based labeling of features to integrate co-occurrence of features and shape similarity .
in classification problems , support vector machines maximize the margin of separation between two classes . <eos> while the paradigm has been successful , the solution obtained by svms is dominated by the directions with large data spread and biased to separate the classes by cutting along large spread directions . <eos> this article proposes a novel formulation to overcome such sensitivity and maximizes the margin relative to the spread of the data . <eos> the proposed formulation can be efficiently solved and experiments on digit datasets show drastic performance improvements over svms .
randomly connected recurrent neural circuits have proven to be very powerful models for online computations when a trained memoryless readout function is appended . <eos> such reservoir computing ( rc ) systems are commonly used in two flavors : with analog or binary ( spiking ) neurons in the recurrent circuits . <eos> previous work showed a fundamental difference between these two incarnations of the rc idea . <eos> the performance of a rc system built from binary neurons seems to depend strongly on the network connectivity structure . <eos> in networks of analog neurons such dependency has not been observed . <eos> in this article we investigate this apparent dichotomy in terms of the in-degree of the circuit nodes . <eos> our analyses based amongst others on the lyapunov exponent reveal that the phase transition between ordered and chaotic network behavior of binary circuits qualitatively differs from the one in analog circuits . <eos> this explains the observed decreased computational performance of binary circuits of high node in-degree . <eos> furthermore , a novel mean-field predictor for computational performance is introduced and shown to accurately predict the numerically obtained results .
we present a mixture model whose components are restricted boltzmann machines ( rbms ) . <eos> this possibility has not been considered before because computing the partition function of an rbm is intractable , which appears to make learning a mixture of rbms intractable as well . <eos> surprisingly , when formulated as a third-order boltzmann machine , such a mixture model can be learned tractably using contrastive divergence . <eos> the energy function of the model captures three-way interactions among visible units , hidden units , and a single hidden multinomial unit that represents the cluster labels . <eos> the distinguishing feature of this model is that , unlike other mixture models , the mixing proportions are not explicitly parameterized . <eos> instead , they are defined implicitly via the energy function and depend on all the parameters in the model . <eos> we present results for the mnist and norb datasets showing that the implicit mixture of rbms learns clusters that reflect the class structure in the data .
we propose a new class of consistency constraints for linear programming ( lp ) relaxations for finding the most probable ( map ) configuration in graphical models . <eos> usual cluster-based lp relaxations enforce joint consistency of the beliefs of a cluster of variables , with computational cost increasing exponentially with the size of the clusters . <eos> by partitioning the state space of a cluster and enforcing consistency only across partitions , we obtain a class of constraints which , although less tight , are computationally feasible for large clusters . <eos> we show how to solve the cluster selection and partitioning problem monotonically in the dual lp , using the current beliefs to guide these choices . <eos> we obtain a dual message-passing algorithm and apply it to protein design problems where the variables have large state spaces and the usual cluster-based relaxations are very costly .
prior work has shown that features which appear to be biologically plausible as well as empirically useful can be found by sparse coding with a prior such as a laplacian ( l1 ) that promotes sparsity . <eos> we show how smoother priors can preserve the benefits of these sparse priors while adding stability to the maximum a-posteriori ( map ) estimate that makes it more useful for prediction problems . <eos> additionally , we show how to calculate the derivative of the map estimate efficiently with implicit differentiation . <eos> one prior that can be differentiated this way is kl-regularization . <eos> we demonstrate its effectiveness on a wide variety of applications , and find that online optimization of the parameters of the kl-regularized model can significantly improve prediction performance .
we propose a new fast gaussian summation algorithm for high-dimensional datasets with high accuracy . <eos> first , we extend the original fast multipole-type methods to use approximation schemes with both hard and probabilistic error . <eos> second , we utilize a new data structure called subspace tree which maps each data point in the node to its lower dimensional mapping as determined by any linear dimension reduction method such as pca . <eos> this new data structure is suitable for reducing the cost of each pairwise distance computation , the most dominant cost in many kernel methods . <eos> our algorithm guarantees probabilistic relative error on each kernel sum , and can be applied to high-dimensional gaussian summations which are ubiquitous inside many kernel methods as the key computational bottleneck . <eos> we provide empirical speedup results on low to high-dimensional datasets up to 89 dimensions .
discriminative tasks , including object categorization and detection , are central components of high-level computer vision . <eos> sometimes , however , we are interested in more refined aspects of the object in an image , such as pose or particular regions . <eos> in this paper we develop a method ( loops ) for learning a shape and image feature model that can be trained on a particular object class , and used to outline instances of the class in novel images . <eos> furthermore , while the training data consists of uncorresponded outlines , the resulting loops model contains a set of landmark points that appear consistently across instances , and can be accurately localized in an image . <eos> our model achieves state-of-the-art results in precisely outlining objects that exhibit large deformations and articulations in cluttered natural images . <eos> these localizations can then be used to address a range of tasks , including descriptive classification , search , and clustering .
in this paper we focus on training deep neural networks for visual recognition tasks . <eos> one challenge is the lack of an informative regularization on the network parameters , to imply a meaningful control on the computed function . <eos> we propose a training strategy that takes advantage of kernel methods , where an existing kernel function represents useful prior knowledge about the learning task of interest . <eos> we derive an efficient algorithm using stochastic gradient descent , and demonstrate very positive results in a wide range of visual recognition tasks .
this paper introduces a new approach to constructing meaningful lower dimensional representations of sets of data points . <eos> we argue that constraining the mapping between the high and low dimensional spaces to be a diffeomorphism is a natural way of ensuring that pairwise distances are approximately preserved . <eos> accordingly we develop an algorithm which diffeomorphically maps the data near to a lower dimensional subspace and then projects onto that subspace . <eos> the problem of solving for the mapping is transformed into one of solving for an eulerian flow field which we compute using ideas from kernel methods . <eos> we demonstrate the efficacy of our approach on various real world data sets .
many interesting problems , including bayesian network structure-search , can be cast in terms of finding the optimum value of a function over the space of graphs . <eos> however , this function is often expensive to compute exactly . <eos> we here present a method derived from the study of reproducing-kernel hilbert spaces which takes advantage of the regular structure of the space of all graphs on a fixed number of nodes to obtain approximations to the desired function quickly and with reasonable accuracy . <eos> we then test this method on both a small testing set and a real-world bayesian network ; the results suggest that not only is this method reasonably accurate , but that the bde score itself varies quadratically over the space of all graphs .
motivated by applications like elections , web-page ranking , revenue maximization etc. , we consider the question of inferring popular rankings using constrained data . <eos> more specifically , we consider the problem of inferring a probability distribution over the group of permutations using its first order marginals . <eos> we first prove that it is not possible to recover more than o ( n ) permutations over n elements with the given information . <eos> we then provide a simple and novel algorithm that can recover up to o ( n ) permutations under a natural stochastic model ; in this sense , the algorithm is optimal . <eos> in certain applications , the interest is in recovering only the most popular ( or mode ) ranking . <eos> as a second result , we provide an algorithm based on the fourier transform over the symmetric group to recover the mode under a natural majority condition ; the algorithm turns out to be a maximum weight matching on an appropriately defined weighted bipartite graph . <eos> the questions considered are also thematically related to fourier transforms over the symmetric group and the currently popular topic of compressed sensing .
many motor skills in humanoid robotics can be learned using parametrized motor primitives as done in imitation learning . <eos> however , most interesting motor learning problems are high-dimensional reinforcement learning problems often beyond the reach of current methods . <eos> in this paper , we extend previous work on policy learning from the immediate reward case to episodic reinforcement learning . <eos> we show that this results into a general , common framework also connected to policy gradient methods and yielding a novel algorithm for policy learning by assuming a form of exploration that is particularly well-suited for dynamic motor primitives . <eos> the resulting algorithm is an em-inspired algorithm applicable in complex motor learning tasks . <eos> we compare this algorithm to alternative parametrized policy search methods and show that it outperforms previous methods . <eos> we apply it in the context of motor learning and show that it can learn a complex ball-in-a-cup task using a real barrett wam robot arm .
many nonlinear dynamical phenomena can be effectively modeled by a system that switches among a set of conditionally linear dynamical modes . <eos> we consider two such models : the switching linear dynamical system ( slds ) and the switching vector autoregressive ( var ) process . <eos> in this paper , we present a nonparametric approach to the learning of an unknown number of persistent , smooth dynamical modes by utilizing a hierarchical dirichlet process prior . <eos> we develop a sampling algorithm that combines a truncated approximation to the dirichlet process with an efficient joint sampling of the mode and state sequences . <eos> the utility and flexibility of our model are demonstrated on synthetic data , sequences of dancing honey bees , and the ibovespa stock index .
research in animal learning and behavioral neuroscience has distinguished between two forms of action control : a habit-based form , which relies on stored action values , and a goal-directed form , which forecasts and compares action outcomes based on a model of the environment . <eos> while habit-based control has been the subject of extensive computational research , the computational principles underlying goal-directed control in animals have so far received less attention . <eos> in the present paper , we advance a computational framework for goal-directed control in animals and humans . <eos> we take three empirically motivated points as founding premises : ( 1 ) neurons in dorsolateral prefrontal cortex represent action policies , ( 2 ) neurons in orbitofrontal cortex represent rewards , and ( 3 ) neural computation , across domains , can be appropriately understood as performing structured probabilistic inference . <eos> on a purely computational level , the resulting account relates closely to previous work using bayesian inference to solve markov decision problems , but extends this work by introducing a new algorithm , which provably converges on optimal plans . <eos> on a cognitive and neuroscientific level , the theory provides a unifying framework for several different forms of goal-directed action selection , placing emphasis on a novel form , within which orbitofrontal reward representations directly drive policy selection .
the discovery of causal relationships between a set of observed variables is a fundamental problem in science . <eos> for continuous-valued data linear acyclic causal models are often used because these models are well understood and there are well-known methods to fit them to data . <eos> in reality , of course , many causal relationships are more or less nonlinear , raising some doubts as to the applicability and usefulness of purely linear methods . <eos> in this contribution we show that in fact the basic linear framework can be generalized to nonlinear models with additive noise . <eos> in this extended framework , nonlinearities in the data-generating process are in fact a blessing rather than a curse , as they typically provide information on the underlying causal system and allow more aspects of the true data-generating mechanisms to be identified . <eos> in addition to theoretical results we show simulations and some simple real data experiments illustrating the identification power provided by nonlinearities .
given an $ n $ -vertex weighted tree with structural diameter $ s $ and a subset of $ m $ vertices , we present a technique to compute a corresponding $ m \times m $ gram matrix of the pseudoinverse of the graph laplacian in $ o ( n+ m^2 + m s ) $ time . <eos> we discuss the application of this technique to fast label prediction on a generic graph . <eos> we approximate the graph with a spanning tree and then we predict with the kernel perceptron . <eos> we address the approximation of the graph with either a minimum spanning tree or a shortest path tree . <eos> the fast computation of the pseudoinverse enables us to address prediction problems on large graphs . <eos> to this end we present experiments on two web-spam classification tasks , one of which includes a graph with 400,000 nodes and more than 10,000,000 edges . <eos> the results indicate that the accuracy of our technique is competitive with previous methods using the full graph information .
this paper presents a theoretical analysis of the problem of adaptation with multiple sources . <eos> for each source domain , the distribution over the input points as well as a hypothesis with error at most \epsilon are given . <eos> the problem consists of combining these hypotheses to derive a hypothesis with small error with respect to the target domain . <eos> we present several theoretical results relating to this problem . <eos> in particular , we prove that standard convex combinations of the source hypotheses may in fact perform very poorly and that , instead , combinations weighted by the source distributions benefit from favorable theoretical guarantees . <eos> our main result shows that , remarkably , for any fixed target function , there exists a distribution weighted combining rule that has a loss of at most \epsilon with respect to *any* target mixture of the source distributions . <eos> we further generalize the setting from a single target function to multiple consistent target functions and show the existence of a combining rule with error at most 3\epsilon . <eos> finally , we report empirical results for a multiple source adaptation problem with a real-world dataset .
empirical evidence shows that in favorable situations semi-supervised learning ( ssl ) algorithms can capitalize on the abundancy of unlabeled training data to improve the performance of a learning task , in the sense that fewer labeled training data are needed to achieve a target error bound . <eos> however , in other situations unlabeled data do not seem to help . <eos> recent attempts at theoretically characterizing the situations in which unlabeled data can help have met with little success , and sometimes appear to conflict with each other and intuition . <eos> in this paper , we attempt to bridge the gap between practice and theory of semi-supervised learning . <eos> we develop a rigorous framework for analyzing the situations in which unlabeled data can help and quantify the improvement possible using finite sample error bounds . <eos> we show that there are large classes of problems for which ssl can significantly outperform supervised learning , in finite sample regimes and sometimes also in terms of error convergence rates .
correlations between spike counts are often used to analyze neural coding . <eos> the noise is typically assumed to be gaussian . <eos> yet , this assumption is often inappropriate , especially for low spike counts . <eos> in this study , we present copulas as an alternative approach . <eos> with copulas it is possible to use arbitrary marginal distributions such as poisson or negative binomial that are better suited for modeling noise distributions of spike counts . <eos> furthermore , copulas place a wide range of dependence structures at the disposal and can be used to analyze higher order interactions . <eos> we develop a framework to analyze spike count data by means of copulas . <eos> methods for parameter inference based on maximum likelihood estimates and for computation of shannon entropy are provided . <eos> we apply the method to our data recorded from macaque prefrontal cortex . <eos> the data analysis leads to three significant findings : ( 1 ) copula-based distributions provide better fits than discretized multivariate normal distributions ; ( 2 ) negative binomial margins fit the data better than poisson margins ; and ( 3 ) a dependence model that includes only pairwise interactions overestimates the information entropy by at least 19 % compared to the model with higher order interactions .
we present a sparse approximation approach for dependent output gaussian processes ( gp ) . <eos> employing a latent function framework , we apply the convolution process formalism to establish dependencies between output variables , where each latent function is represented as a gp . <eos> based on these latent functions , we establish an approximation scheme using a conditional independence assumption between the output processes , leading to an approximation of the full covariance which is determined by the locations at which the latent functions are evaluated . <eos> we show results of the proposed methodology for synthetic data and real world applications on pollution prediction and a sensor network .
confidence-weighted ( cw ) learning [ 6 ] , an online learning method for linear classifiers , maintains a gaussian distributions over weight vectors , with a covariance matrix that represents uncertainty about weights and correlations . <eos> confidence constraints ensure that a weight vector drawn from the hypothesis distribution correctly classifies examples with a specified probability . <eos> within this framework , we derive a new convex form of the constraint and analyze it in the mistake bound model . <eos> empirical evaluation with both synthetic and text data shows our version of cw learning achieves lower cumulative and out-of-sample errors than commonly used first-order and second-order online methods .
the odor transduction process has a large time constant and is susceptible to various types of noise . <eos> therefore , the olfactory code at the sensor/receptor level is in general a slow and highly variable indicator of the input odor in both natural and artificial situations . <eos> insects overcome this problem by using a neuronal device in their antennal lobe ( al ) , which transforms the identity code of olfactory receptors to a spatio-temporal code . <eos> this transformation improves the decision of the mushroom bodies ( mbs ) , the subsequent classifier , in both speed and accuracy.here we propose a rate model based on two intrinsic mechanisms in the insect al , namely integration and inhibition . <eos> then we present a mb classifier model that resembles the sparse and random structure of insect mb . <eos> a local hebbian learning procedure governs the plasticity in the model . <eos> these formulations not only help to understand the signal conditioning and classification methods of insect olfactory systems , but also can be leveraged in synthetic problems . <eos> among them , we consider here the discrimination of odor mixtures from pure odors . <eos> we show on a set of records from metal-oxide gas sensors that the cascade of these two new models facilitates fast and accurate discrimination of even highly imbalanced mixtures from pure odors .
we introduce a kernel-based method for change-point analysis within a sequence of temporal observations . <eos> change-point analysis of an ( unlabelled ) sample of observations consists in , first , testing whether a change in the distribution occurs within the sample , and second , if a change occurs , estimating the change-point instant after which the distribution of the observations switches from one distribution to another different distribution . <eos> we propose a test statistics based upon the maximum kernel fisher discriminant ratio as a measure of homogeneity between segments . <eos> we derive its limiting distribution under the null hypothesis ( no change occurs ) , and establish the consistency under the alternative hypothesis ( a change occurs ) . <eos> this allows to build a statistical hypothesis testing procedure for testing the presence of change-point , with a prescribed false-alarm probability and detection probability tending to one in the large-sample setting . <eos> if a change actually occurs , the test statistics also yields an estimator of the change-point location . <eos> promising experimental results in temporal segmentation of mental tasks from bci data and pop song indexation are presented .
the essence of exploration is acting to try to decrease uncertainty . <eos> we propose a new methodology for representing uncertainty in continuous-state control problems . <eos> our approach , multi-resolution exploration ( mre ) , uses a hierarchical mapping to identify regions of the state space that would benefit from additional samples . <eos> we demonstrate mre 's broad utility by using it to speed up learning in a prototypical model-based and value-based reinforcement-learning method . <eos> empirical results show that mre improves upon state-of-the-art exploration approaches .
we show how improved sequences for magnetic resonance imaging can be found through automated optimization of bayesian design scores . <eos> combining recent advances in approximate bayesian inference and natural image statistics with high-performance numerical computation , we propose the first scalable bayesian experimental design framework for this problem of high relevance to clinical and brain research . <eos> our solution requires approximate inference for dense , non-gaussian models on a scale seldom addressed before . <eos> we propose a novel scalable variational inference algorithm , and show how powerful methods of numerical mathematics can be modified to compute primitives in our framework . <eos> our approach is evaluated on a realistic setup with raw data from a 3t mr scanner .
we explore a new bayesian model for probabilistic grammars , a family of distributions over discrete structures that includes hidden markov models and probabilistic context-free grammars . <eos> our model extends the correlated topic model framework to probabilistic grammars , exploiting the logistic normal distribution as a prior over the grammar parameters . <eos> we derive a variational em algorithm for that model , and then experiment with the task of unsupervised grammar induction for natural language dependency parsing . <eos> we show that our model achieves superior results over previous models that use different priors .
we describe a new content publishing system that selects articles to serve to a user , choosing from an editorially programmed pool that is frequently refreshed . <eos> it is now deployed on a major internet portal , and selects articles to serve to hundreds of millions of user visits per day , significantly increasing the number of user clicks over the original manual approach , in which editors periodically selected articles to display . <eos> some of the challenges we face include a dynamic content pool , short article lifetimes , non-stationary click-through rates , and extremely high traffic volumes . <eos> the fundamental problem we must solve is to quickly identify which items are popular ( perhaps within different user segments ) , and to exploit them while they remain current . <eos> we must also explore the underlying pool constantly to identify promising alternatives , quickly discarding poor performers . <eos> our approach is based on tracking per article performance in near real time through online models . <eos> we describe the characteristics and constraints of our application setting , discuss our design choices , and show the importance and effectiveness of coupling online models with a simple randomization procedure . <eos> we discuss the challenges encountered in a production online content-publishing environment and highlight issues that deserve careful attention . <eos> our analysis of this application also suggests a number of future research avenues .
this paper discusses non-parametric regression between riemannian manifolds . <eos> this learning problem arises frequently in many application areas ranging from signal processing , computer vision , over robotics to computer graphics . <eos> we present a new algorithmic scheme for the solution of this general learning problem based on regularized empirical risk minimization . <eos> the regularization functional takes into account the geometry of input and output manifold , and we show that it implements a prior which is particularly natural . <eos> moreover , we demonstrate that our algorithm performs well in a difficult surface registration problem .
by attempting to simultaneously partition both the rows ( examples ) and columns ( features ) of a data matrix , co-clustering algorithms often demonstrate surpris- ingly impressive performance improvements over traditional one-sided ( row ) clustering techniques . <eos> a good clustering of features may be seen as a combinatorial transformation of the data matrix , effectively enforcing a form of regularization that may lead to a better clustering of examples ( and vice-versa ) . <eos> in many applications , partial supervision in the form of a few row labels as well as column labels may be available to potentially assist co-clustering . <eos> in this paper , we develop two novel semi-supervised multi-class classification algorithms motivated respectively by spectral bipartite graph partitioning and matrix approximation ( e.g. , non-negative matrix factorization ) formulations for co-clustering . <eos> these algorithms ( i ) support dual supervision in the form of labels for both examples and/or features , ( ii ) provide principled predictive capability on out-of-sample test data , and ( iii ) arise naturally from the classical representer theorem applied to regularization problems posed on a collection of reproducing kernel hilbert spaces . <eos> empirical results demonstrate the effectiveness and utility of our algorithms .
we propose using correlated bigram lsa for unsupervised lm adaptation for automatic speech recognition . <eos> the model is trained using efficient variational em and smoothed using the proposed fractional kneser-ney smoothing which handles fractional counts . <eos> our approach can be scalable to large training corpora via bootstrapping of bigram lsa from unigram lsa . <eos> for lm adaptation , unigram and bigram lsa are integrated into the background n-gram lm via marginal adaptation and linear interpolation respectively . <eos> experimental results show that applying unigram and bigram lsa together yields 6 % -- 8 % relative perplexity reduction and 0.6 % absolute character error rates ( cer ) reduction compared to applying only unigram lsa on the mandarin rt04 test set . <eos> comparing with the unadapted baseline , our approach reduces the absolute cer by 1.2 % .
we present a discriminative part-based approach for human action recognition from video sequences using motion features . <eos> our model is based on the recently proposed hidden conditional random field~ ( hcrf ) for object recognition . <eos> similar to hcrf for object recognition , we model a human action by a flexible constellation of parts conditioned on image observations . <eos> different from object recognition , our model combines both large-scale global features and local patch features to distinguish various actions . <eos> our experimental results show that our model is comparable to other state-of-the-art approaches in action recognition . <eos> in particular , our experimental results demonstrate that combining large-scale global features and local patch features performs significantly better than directly applying hcrf on local patches alone .
kernel principal component analysis ( kpca ) is a popular generalization of linear pca that allows non-linear feature extraction . <eos> in kpca , data in the input space is mapped to higher ( usually ) dimensional feature space where the data can be linearly modeled . <eos> the feature space is typically induced implicitly by a kernel function , and linear pca in the feature space is performed via the kernel trick . <eos> however , due to the implicitness of the feature space , some extensions of pca such as robust pca can not be directly generalized to kpca . <eos> this paper presents a technique to overcome this problem , and extends it to a unified framework for treating noise , missing data , and outliers in kpca . <eos> our method is based on a novel cost function to perform inference in kpca . <eos> extensive experiments , in both synthetic and real data , show that our algorithm outperforms existing methods .
the temporal restricted boltzmann machine ( trbm ) is a probabilistic model for sequences that is able to successfully model ( i.e. , generate nice-looking samples of ) several very high dimensional sequences , such as motion capture data and the pixels of low resolution videos of balls bouncing in a box . <eos> the major disadvantage of the trbm is that exact inference is extremely hard , since even computing a gibbs update for a single variable of the posterior is exponentially expensive . <eos> this difficulty has necessitated the use of a heuristic inference procedure , that nonetheless was accurate enough for successful learning . <eos> in this paper we introduce the recurrent trbm , which is a very slight modification of the trbm for which exact inference is very easy and exact gradient learning is almost tractable . <eos> we demonstrate that the rtrbm is better than an analogous trbm at generating motion capture and videos of bouncing balls .
with the increased availability of data for complex domains , it is desirable to learn bayesian network structures that are sufficiently expressive for generalization while also allowing for tractable inference . <eos> while the method of thin junction trees can , in principle , be used for this purpose , its fully greedy nature makes it prone to overfitting , particularly when data is scarce . <eos> in this work we present a novel method for learning bayesian networks of bounded treewidth that employs global structure modifications and that is polynomial in the size of the graph and the treewidth bound . <eos> at the heart of our method is a triangulated graph that we dynamically update in a way that facilitates the addition of chain structures that increase the bound on the model 's treewidth by at most one . <eos> we demonstrate the effectiveness of our `` treewidth-friendly '' method on several real-life datasets . <eos> importantly , we also show that by using global operators , we are able to achieve better generalization even when learning bayesian networks of unbounded treewidth .
we present an algorithm for solving a broad class of online resource allocation problems . <eos> our online algorithm can be applied in environments where abstract jobs arrive one at a time , and one can complete the jobs by investing time in a number of abstract activities , according to some schedule . <eos> we assume that the fraction of jobs completed by a schedule is a monotone , submodular function of a set of pairs ( v , t ) , where t is the time invested in activity v. under this assumption , our online algorithm performs near-optimally according to two natural metrics : ( i ) the fraction of jobs completed within time t , for some fixed deadline t > 0 , and ( ii ) the average time required to complete each job . <eos> we evaluate our algorithm experimentally by using it to learn , online , a schedule for allocating cpu time among solvers entered in the 2007 sat solver competition .
we report a compact realization of short-term depression ( std ) in a vlsi stochastic synapse . <eos> the behavior of the circuit is based on a subtractive single release model of std . <eos> experimental results agree well with simulation and exhibit expected std behavior : the transmitted spike train has negative autocorrelation and lower power spectral density at low frequencies which can remove redundancy in the input spike train , and the mean transmission probability is inversely proportional to the input spike rate which has been suggested as an automatic gain control mechanism in neural systems . <eos> the dynamic stochastic synapse could potentially be a powerful addition to existing deterministic vlsi spiking neural systems .
a principled mechanism for identifying conditional dependencies in time-series data is provided through structure learning of dynamic bayesian networks ( dbns ) . <eos> an important assumption of dbn structure learning is that the data are generated by a stationary process ? an assumption that is not true in many important settings . <eos> in this paper , we introduce a new class of graphical models called non-stationary dynamic bayesian networks , in which the conditional dependence structure of the underlying data-generation process is permitted to change over time . <eos> non-stationary dynamic bayesian networks represent a new framework for studying problems in which the structure of a network is evolving over time . <eos> we define the non-stationary dbn model , present an mcmc sampling algorithm for learning the structure of the model from time-series data under different assumptions , and demonstrate the effectiveness of the algorithm on both simulated and biological data .
conditional random sampling ( crs ) was originally proposed for efficiently computing pairwise ( $ l_2 $ , $ l_1 $ ) distances , in static , large-scale , and sparse data sets such as text and web data . <eos> it was previously presented using a heuristic argument . <eos> this study extends crs to handle dynamic or streaming data , which much better reflect the real-world situation than assuming static data . <eos> compared with other known sketching algorithms for dimension reductions such as stable random projections , crs exhibits a significant advantage in that it is `` one-sketch-for-all . '' <eos> in particular , we demonstrate that crs can be applied to efficiently compute the $ l_p $ distance and the hilbertian metrics , both are popular in machine learning . <eos> although a fully rigorous analysis of crs is difficult , we prove that , with a simple modification , crs is rigorous at least for an important application of computing hamming norms . <eos> a generic estimator and an approximate variance formula are provided and tested on various applications , for computing hamming norms , hamming distances , and $ \chi^2 $ distances .
language comprehension in humans is significantly constrained by memory , yet rapid , highly incremental , and capable of utilizing a wide range of contextual information to resolve ambiguity and form expectations about future input . <eos> in contrast , most of the leading psycholinguistic models and fielded algorithms for natural language parsing are non-incremental , have run time superlinear in input length , and/or enforce structural locality constraints on probabilistic dependencies between events . <eos> we present a new limited-memory model of sentence comprehension which involves an adaptation of the particle filter , a sequential monte carlo method , to the problem of incremental parsing . <eos> we show that this model can reproduce classic results in online sentence comprehension , and that it naturally provides the first rational account of an outstanding problem in psycholinguistics , in which the preferred alternative in a syntactic ambiguity seems to grow more attractive over time even in the absence of strong disambiguating information .
we present a multi-label multiple kernel learning ( mkl ) formulation , in which the data are embedded into a low-dimensional space directed by the instance-label correlations encoded into a hypergraph . <eos> we formulate the problem in the kernel-induced feature space and propose to learn the kernel matrix as a linear combination of a given collection of kernel matrices in the mkl framework . <eos> the proposed learning formulation leads to a non-smooth min-max problem , and it can be cast into a semi-infinite linear program ( silp ) . <eos> we further propose an approximate formulation with a guaranteed error bound which involves an unconstrained and convex optimization problem . <eos> in addition , we show that the objective function of the approximate formulation is continuously differentiable with lipschitz gradient , and hence existing methods can be employed to compute the optimal solution efficiently . <eos> we apply the proposed formulation to the automated annotation of drosophila gene expression pattern images , and promising results have been reported in comparison with representative algorithms .
in analogy to the pca setting , the sparse pca problem is often solved by iteratively alternating between two subtasks : cardinality-constrained rank-one variance maximization and matrix deflation . <eos> while the former has received a great deal of attention in the literature , the latter is seldom analyzed and is typically borrowed without justification from the pca context . <eos> in this work , we demonstrate that the standard pca deflation procedure is seldom appropriate for the sparse pca setting . <eos> to rectify the situation , we first develop several heuristic deflation alternatives with more desirable properties . <eos> we then reformulate the sparse pca optimization problem to explicitly reflect the maximum additional variance objective on each round . <eos> the result is a generalized deflation procedure that typically outperforms more standard techniques on real-world datasets .
contours have been established in the biological and computer vision literatures as a compact yet descriptive representation of object shape . <eos> while individual contours provide structure , they lack the large spatial support of region segments ( which lack internal structure ) . <eos> we present a method for further grouping of contours in an image using their relationship to the contours of a second , related image . <eos> stereo , motion , and similarity all provide cues that can aid this task ; contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group . <eos> to find matches for contours , we rely only on shape , which applies directly to all three modalities without modification , in constrant to the specialized approaches developed for each independently . <eos> visually salient contours are extracted in each image , along with a set of candidate transformations for aligning subsets of them . <eos> for each transformation , groups of contours with matching shape across the two images are identified to provide a context for evaluating matches of individual contour points across the images . <eos> the resulting contexts of contours are used to perform a final grouping on contours in the original image while simultaneously finding matches in the related image , again by shape matching . <eos> we demonstrate grouping results on image pairs consisting of stereo , motion , and similar images . <eos> our method also produces qualitatively better results against a baseline method that does not use the inferred contexts .
neuroimaging datasets often have a very large number of voxels and a very small number of training cases , which means that overfitting of models for this data can become a very serious problem . <eos> working with a set of fmri images from a study on stroke recovery , we consider a classification task for which logistic regression performs poorly , even when l1- or l2- regularized . <eos> we show that much better discrimination can be achieved by fitting a generative model to each separate condition and then seeing which model is most likely to have generated the data . <eos> we compare discriminative training of exactly the same set of models , and we also consider convex blends of generative and discriminative training .
observations consisting of measurements on relationships for pairs of objects arise in many settings , such as protein interaction and gene regulatory networks , collections of author-recipient email , and social networks . <eos> analyzing such data with probabilisic models can be delicate because the simple exchangeability assumptions underlying many boilerplate models no longer hold . <eos> in this paper , we describe a class of latent variable models of such data called mixed membership stochastic blockmodels . <eos> this model extends blockmodels for relational data to ones which capture mixed membership latent relational structure , thus providing an object-specific low-dimensional representation . <eos> we develop a general variational inference algorithm for fast approximate posterior inference . <eos> we explore applications to social networks and protein interaction networks .
young children demonstrate the ability to make inferences about the preferences of other agents based on their choices . <eos> however , there exists no overarching account of what children are doing when they learn about preferences or how they use that knowledge . <eos> we use a rational model of preference learning , drawing on ideas from economics and computer science , to explain the behavior of children in several recent experiments . <eos> specifically , we show how a simple econometric model can be extended to capture two- to four-year-olds ?use of statistical information in inferring preferences , and their generalization of these preferences .
we formulate and study a new variant of the $ k $ -armed bandit problem , motivated by e-commerce applications . <eos> in our model , arms have ( stochastic ) lifetime after which they expire . <eos> in this setting an algorithm needs to continuously explore new arms , in contrast to the standard $ k $ -armed bandit model in which arms are available indefinitely and exploration is reduced once an optimal arm is identified with near-certainty . <eos> the main motivation for our setting is online-advertising , where ads have limited lifetime due to , for example , the nature of their content and their campaign budget . <eos> an algorithm needs to choose among a large collection of ads , more than can be fully explored within the ads ' lifetime . <eos> we present an optimal algorithm for the state-aware ( deterministic reward function ) case , and build on this technique to obtain an algorithm for the state-oblivious ( stochastic reward function ) case . <eos> empirical studies on various reward distributions , including one derived from a real-world ad serving application , show that the proposed algorithms significantly outperform the standard multi-armed bandit approaches applied to these settings .
unexpected stimuli are a challenge to any machine learning algorithm . <eos> here we identify distinct types of unexpected events , focusing on 'incongruent events ' - when 'general level ' and 'specific level ' classifiers give conflicting predictions . <eos> we define a formal framework for the representation and processing of incongruent events : starting from the notion of label hierarchy , we show how partial order on labels can be deduced from such hierarchies . <eos> for each event , we compute its probability in different ways , based on adjacent levels ( according to the partial order ) in the label hierarchy . <eos> an incongruent event is an event where the probability computed based on some more specific level ( in accordance with the partial order ) is much smaller than the probability computed based on some more general level , leading to conflicting predictions . <eos> we derive algorithms to detect incongruent events from different types of hierarchies , corresponding to class membership or part membership . <eos> respectively , we show promising results with real data on two specific problems : out of vocabulary words in speech recognition , and the identification of a new sub-class ( e.g. , the face of a new individual ) in audio-visual facial object recognition .
`` how is information decoded in the brain ? '' <eos> is one of the most difficult and important questions in neuroscience . <eos> whether neural correlation is important or not in decoding neural activities is of special interest . <eos> we have developed a general framework for investigating how far the decoding process in the brain can be simplified . <eos> first , we hierarchically construct simplified probabilistic models of neural responses that ignore more than $ k $ th-order correlations by using a maximum entropy principle . <eos> then , we compute how much information is lost when information is decoded using the simplified models , i.e. , `` mismatched decoders '' . <eos> we introduce an information theoretically correct quantity for evaluating the information obtained by mismatched decoders . <eos> we applied our proposed framework to spike data for vertebrate retina . <eos> we used 100-ms natural movies as stimuli and computed the information contained in neural activities about these movies . <eos> we found that the information loss is negligibly small in population activities of ganglion cells even if all orders of correlation are ignored in decoding . <eos> we also found that if we assume stationarity for long durations in the information analysis of dynamically changing stimuli like natural movies , pseudo correlations seem to carry a large portion of the information .
neural probabilistic language models ( nplms ) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models . <eos> the main drawback of nplms is their extremely long training and testing times . <eos> morin and bengio have proposed a hierarchical language model built around a binary tree of words that was two orders of magnitude faster than the non-hierarchical language model it was based on . <eos> however , it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge . <eos> we introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data . <eos> we then show that the resulting models can outperform non-hierarchical models and achieve state-of-the-art performance .
we present a simple new monte carlo algorithm for evaluating probabilities of observations in complex latent variable models , such as deep belief networks . <eos> while the method is based on markov chains , estimates based on short runs are formally unbiased . <eos> in expectation , the log probability of a test set will be underestimated , and this could form the basis of a probabilistic bound . <eos> the method is much cheaper than gold-standard annealing-based methods and only slightly more expensive than the cheapest monte carlo methods . <eos> we give examples of the new method substantially improving simple variational bounds at modest extra cost .
we propose a general method called truncated gradient to induce sparsity in the weights of online-learning algorithms with convex loss . <eos> this method has several essential properties . <eos> first , the degree of sparsity is continuous -- -a parameter controls the rate of sparsification from no sparsification to total sparsification . <eos> second , the approach is theoretically motivated , and an instance of it can be regarded as an online counterpart of the popular $ l_1 $ -regularization method in the batch setting . <eos> we prove that small rates of sparsification result in only small additional regret with respect to typical online-learning guarantees . <eos> finally , the approach works well empirically . <eos> we apply it to several datasets and find that for datasets with large numbers of features , substantial sparsity is discoverable .
consider linear prediction models where the target function is a sparse linear combination of a set of basis functions . <eos> we are interested in the problem of identifying those basis functions with non-zero coefficients and reconstructing the target function from noisy observations . <eos> two heuristics that are widely used in practice are forward and backward greedy algorithms . <eos> first , we show that neither idea is adequate . <eos> second , we propose a novel combination that is based on the forward greedy algorithm but takes backward steps adaptively whenever beneficial . <eos> we prove strong theoretical results showing that this procedure is effective in learning sparse representations . <eos> experimental results support our theory .
adaptation of visually guided reaching movements in novel visuomotor environments ( e.g . wearing prism goggles ) comprises not only motor adaptation but also substantial sensory adaptation , corresponding to shifts in the perceived spatial location of visual and proprioceptive cues . <eos> previous computational models of the sensory component of visuomotor adaptation have assumed that it is driven purely by the discrepancy introduced between visual and proprioceptive estimates of hand position and is independent of any motor component of adaptation . <eos> we instead propose a unified model in which sensory and motor adaptation are jointly driven by optimal bayesian estimation of the sensory and motor contributions to perceived errors . <eos> our model is able to account for patterns of performance errors during visuomotor adaptation as well as the subsequent perceptual aftereffects . <eos> this unified model also makes the surprising prediction that force field adaptation will elicit similar perceptual shifts , even though there is never any discrepancy between visual and proprioceptive observations . <eos> we confirm this prediction with an experiment .
many human interactions involve pieces of information being passed from one person to another , raising the question of how this process of information transmission is affected by the capacities of the agents involved . <eos> in the 1930s , sir frederic bartlett explored the influence of memory biases in ? serial reproduction ?of information , in which one person ? s reconstruction of a stimulus from memory becomes the stimulus seen by the next person . <eos> these experiments were done using relatively uncontrolled stimuli such as pictures and stories , but suggested that serial reproduction would transform information in a way that reflected the biases inherent in memory . <eos> we formally analyze serial reproduction using a bayesian model of reconstruction from memory , giving a general result characterizing the effect of memory biases on information transmission . <eos> we then test the predictions of this account in two experiments using simple one-dimensional stimuli . <eos> our results provide theoretical and empirical justification for the idea that serial reproduction reflects memory biases .
classical game theoretic approaches that make strong rationality assumptions have difficulty modeling observed behaviour in economic games of human subjects . <eos> we investigate the role of finite levels of iterated reasoning and non-selfish utility functions in a partially observable markov decision process model that incorporates game theoretic notions of interactivity . <eos> our generative model captures a broad class of characteristic behaviours in a multi-round investment game . <eos> we invert the generative process for a recognition model that is used to classify 200 subjects playing an investor-trustee game against randomly matched opponents .
continuously-adaptive discretization for message-passing ( cad-mp ) is a new message-passing algorithm employing adaptive discretization . <eos> most previous message-passing algorithms approximated arbitrary continuous probability distributions using either : a family of continuous distributions such as the exponential family ; a particle-set of discrete samples ; or a fixed , uniform discretization . <eos> in contrast , cad-mp uses a discretization that is ( i ) non-uniform , and ( ii ) adaptive . <eos> the non-uniformity allows cad-mp to localize interesting features ( such as sharp peaks ) in the marginal belief distributions with time complexity that scales logarithmically with precision , as opposed to uniform discretization which scales at best linearly . <eos> we give a principled method for altering the non-uniform discretization according to information-based measures . <eos> cad-mp is shown in experiments on simulated data to estimate marginal beliefs much more precisely than competing approaches for the same computational expense .
the machine learning problem of classifier design is studied from the perspective of probability elicitation , in statistics . <eos> this shows that the standard approach of proceeding from the specification of a loss , to the minimization of conditional risk is overly restrictive . <eos> it is shown that a better alternative is to start from the specification of a functional form for the minimum conditional risk , and derive the loss function . <eos> this has various consequences of practical interest , such as showing that 1 ) the widely adopted practice of relying on convex loss functions is unnecessary , and 2 ) many new losses can be derived for classification problems . <eos> these points are illustrated by the derivation of a new loss which is not convex , but does not compromise the computational tractability of classifier design , and is robust to the contamination of data with outliers . <eos> a new boosting algorithm , savageboost , is derived for the minimization of this loss . <eos> experimental results show that it is indeed less sensitive to outliers than conventional methods , such as ada , real , or logitboost , and converges in fewer iterations .
we introduce a family of unsupervised algorithms , numerical taxonomy clustering , to simultaneously cluster data , and to learn a taxonomy that encodes the relationship between the clusters . <eos> the algorithms work by maximizing the dependence between the taxonomy and the original data . <eos> the resulting taxonomy is a more informative visualization of complex data than simple clustering ; in addition , taking into account the relations between different clusters is shown to substantially improve the quality of the clustering , when compared with state-of-the-art algorithms in the literature ( both spectral clustering and a previous dependence maximization approach ) . <eos> we demonstrate our algorithm on image and text data .
the coding of information by neural populations depends critically on the statistical dependencies between neuronal responses . <eos> however , there is no simple model that combines the observations that ( 1 ) marginal distributions over single-neuron spike counts are often approximately poisson ; and ( 2 ) joint distributions over the responses of multiple neurons are often strongly dependent . <eos> here , we show that both marginal and joint properties of neural responses can be captured using poisson copula models . <eos> copulas are joint distributions that allow random variables with arbitrary marginals to be combined while incorporating arbitrary dependencies between them . <eos> different copulas capture different kinds of dependencies , allowing for a richer and more detailed description of dependencies than traditional summary statistics , such as correlation coefficients . <eos> we explore a variety of poisson copula models for joint neural response distributions , and derive an efficient maximum likelihood procedure for estimating them . <eos> we apply these models to neuronal data collected in and macaque motor cortex , and quantify the improvement in coding accuracy afforded by incorporating the dependency structure between pairs of neurons .
we consider the problem of binary classification where the classifier may abstain instead of classifying each observation . <eos> the bayes decision rule for this setup , known as chow 's rule , is defined by two thresholds on posterior probabilities . <eos> from simple desiderata , namely the consistency and the sparsity of the classifier , we derive the double hinge loss function that focuses on estimating conditional probabilities only in the vicinity of the threshold points of the optimal decision rule . <eos> we show that , for suitable kernel machines , our approach is universally consistent . <eos> we cast the problem of minimizing the double hinge loss as a quadratic program akin to the standard svm optimization problem and propose an active set method to solve it efficiently . <eos> we finally provide preliminary experimental results illustrating the interest of our constructive approach to devising loss functions .
we developed localized sliced inverse regression for supervised dimension reduction . <eos> it has the advantages of preventing degeneracy , increasing estimation accuracy , and automatic subclass discovery in classification problems . <eos> a semisupervised version is proposed for the use of unlabeled data . <eos> the utility is illustrated on simulated as well as real data sets .
we consider robust least-squares regression with feature-wise disturbance . <eos> we show that this formulation leads to tractable convex optimization problems , and we exhibit a particular uncertainty set for which the robust problem is equivalent to $ \ell_1 $ regularized regression ( lasso ) . <eos> this provides an interpretation of lasso from a robust optimization perspective . <eos> we generalize this robust formulation to consider more general uncertainty sets , which all lead to tractable convex optimization problems . <eos> therefore , we provide a new methodology for designing regression algorithms , which generalize known formulations . <eos> the advantage is that robustness to disturbance is a physical property that can be exploited : in addition to obtaining new formulations , we use it directly to show sparsity properties of lasso , as well as to prove a general consistency result for robust regression problems , including lasso , from a unified robustness perspective .
applications of multi-class classification , such as document categorization , often appear in cost-sensitive settings . <eos> recent work has significantly improved the state of the art by moving beyond `` flat '' classification through incorporation of class hierarchies [ cai and hoffman 04 ] . <eos> we present a novel algorithm that goes beyond hierarchical classification and estimates the latent semantic space that underlies the class hierarchy . <eos> in this space , each class is represented by a prototype and classification is done with the simple nearest neighbor rule . <eos> the optimization of the semantic space incorporates large margin constraints that ensure that for each instance the correct class prototype is closer than any other . <eos> we show that our optimization is convex and can be solved efficiently for large data sets . <eos> experiments on the ohsumed medical journal data base yield state-of-the-art results on topic categorization .
we introduce a framework for actively learning visual categories from a mixture of weakly and strongly labeled image examples . <eos> we propose to allow the category-learner to strategically choose what annotations it receives -- -based on both the expected reduction in uncertainty as well as the relative costs of obtaining each annotation . <eos> we construct a multiple-instance discriminative classifier based on the initial training data . <eos> then all remaining unlabeled and weakly labeled examples are surveyed to actively determine which annotation ought to be requested next . <eos> after each request , the current classifier is incrementally updated . <eos> unlike previous work , our approach accounts for the fact that the optimal use of manual annotation may call for a combination of labels at multiple levels of granularity ( e.g. , a full segmentation on some images and a present/absent flag on others ) . <eos> as a result , it is possible to learn more accurate category models with a lower total expenditure of manual annotation effort .
probabilistic topic models ( and their extensions ) have become popular as models of latent structures in collections of text documents or images . <eos> these models are usually treated as generative models and trained using maximum likelihood estimation , an approach which may be suboptimal in the context of an overall classification problem . <eos> in this paper , we describe disclda , a discriminative learning framework for such models as latent dirichlet allocation ( lda ) in the setting of dimensionality reduction with supervised side information . <eos> in disclda , a class-dependent linear transformation is introduced on the topic mixture proportions . <eos> this parameter is estimated by maximizing the conditional likelihood using monte carlo em . <eos> by using the transformed topic mixture proportions as a new representation of documents , we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classification . <eos> we compare the predictive power of the latent structure of disclda with unsupervised lda on the 20 newsgroup ocument classification task .
we study the profit-maximization problem of a monopolistic market-maker who sets two-sided prices in an asset market . <eos> the sequential decision problem is hard to solve because the state space is a function . <eos> we demonstrate that the belief state is well approximated by a gaussian distribution . <eos> we prove a key monotonicity property of the gaussian state update which makes the problem tractable , yielding the first optimal sequential market-making algorithm in an established model . <eos> the algorithm leads to a surprising insight : an optimal monopolist can provide more liquidity than perfectly competitive market-makers in periods of extreme uncertainty , because a monopolist is willing to absorb initial losses in order to learn a new valuation rapidly so she can extract higher profits later .
suppose we train an animal in a conditioning experiment . <eos> can one predict how a given animal , under given experimental conditions , would perform the task ? <eos> since various factors such as stress , motivation , genetic background , and previous errors in task performance can influence animal behaviour , this appears to be a very challenging aim . <eos> reinforcement learning ( rl ) models have been successful in modeling animal ( and human ) behaviour , but their success has been limited because of uncertainty as to how to set meta-parameters ( such as learning rate , exploitation-exploration balance and future reward discount factor ) that strongly influence model performance . <eos> we show that a simple rl model whose metaparameters are controlled by an artificial neural network , fed with inputs such as stress , affective phenotype , previous task performance , and even neuromodulatory manipulations , can successfully predict mouse behaviour in the hole-box- a simple conditioning task . <eos> our results also provide important insights on how stress and anxiety affect animal learning , performance accuracy , and discounting of future rewards , and on how noradrenergic systems can interact with these processes .
in solving complex visual learning tasks , adopting multiple descriptors to more precisely characterize the data has been a feasible way for improving performance . <eos> these representations are typically high dimensional and assume diverse forms . <eos> thus finding a way to transform them into a unified space of lower dimension generally facilitates the underlying tasks , such as object recognition or clustering . <eos> we describe an approach that incorporates multiple kernel learning with dimensionality reduction ( mkl-dr ) . <eos> while the proposed framework is flexible in simultaneously tackling data in various feature representations , the formulation itself is general in that it is established upon graph embedding . <eos> it follows that any dimensionality reduction techniques explainable by graph embedding can be generalized by our method to consider data in multiple feature representations .
statistical evolutionary models provide an important mechanism for describing and understanding the escape response of a viral population under a particular therapy . <eos> we present a new hierarchical model that incorporates spatially varying mutation and recombination rates at the nucleotide level . <eos> it also maintains sep- arate parameters for treatment and control groups , which allows us to estimate treatment effects explicitly . <eos> we use the model to investigate the sequence evolu- tion of hiv populations exposed to a recently developed antisense gene therapy , as well as a more conventional drug therapy . <eos> the detection of biologically rele- vant and plausible signals in both therapy studies demonstrates the effectiveness of the method .
in many domains , data are distributed among datasets that share only some variables ; other recorded variables may occur in only one dataset . <eos> there are several asymptotically correct , informative algorithms that search for causal information given a single dataset , even with missing values and hidden variables . <eos> there are , however , no such reliable procedures for distributed data with overlapping variables , and only a single heuristic procedure ( structural em ) . <eos> this paper describes an asymptotically correct procedure , ion , that provides all the information about structure obtainable from the marginal independence relations . <eos> using simulated and real data , the accuracy of ion is compared with that of structural em , and with inference on complete , unified data .
we consider a generalization of stochastic bandit problems where the set of arms , x , is allowed to be a generic topological space . <eos> we constraint the mean-payoff function with a dissimilarity function over x in a way that is more general than lipschitz . <eos> we construct an arm selection policy whose regret improves upon previous result for a large class of problems . <eos> in particular , our results imply that if x is the unit hypercube in a euclidean space and the mean-payoff function has a finite number of global maxima around which the behavior of the function is locally h ? lder with a known exponent , then the expected regret is bounded up to a logarithmic factor by $ n $ , i.e. , the rate of the growth of the regret is independent of the dimension of the space . <eos> moreover , we prove the minimax optimality of our algorithm for the class of mean-payoff functions we consider .
in this paper we propose a new incremental spike sorting model that automatically eliminates refractory period violations , accounts for action potential waveform drift , and can handle appearance '' and `` disappearance '' of neurons . <eos> our approach is to augment a known time-varying dirichlet process that ties together a sequence of infinite gaussian mixture models , one per action potential waveform observation , with an interspike-interval-dependent likelihood that prohibits refractory period violations . <eos> we demonstrate this model by showing results from sorting two publicly available neural data recordings for which the a partial ground truth labeling is known . ''
using machine learning algorithms to decode intended behavior from neural activity serves a dual purpose . <eos> first , these tools can be used to allow patients to interact with their environment through a brain-machine interface ( bmi ) . <eos> second , analysis of the characteristics of such methods can reveal the significance of various features of neural activity , stimuli and responses to the encoding-decoding task . <eos> in this study we adapted , implemented and tested a machine learning method , called kernel auto-regressive moving average ( karma ) , for the task of inferring movements from neural activity in primary motor cortex . <eos> our version of this algorithm is used in an on-line learning setting and is updated when feedback from the last inferred sequence become available . <eos> we first used it to track real hand movements executed by a monkey in a standard 3d motor control task . <eos> we then applied it in a closed-loop bmi setting to infer intended movement , while arms were restrained , allowing a monkey to perform the task using the bmi alone . <eos> karma is a recurrent method that learns a nonlinear model of output dynamics . <eos> it uses similarity functions ( termed kernels ) to compare between inputs . <eos> these kernels can be structured to incorporate domain knowledge into the method . <eos> we compare karma to various state-of-the-art methods by evaluating tracking performance and present results from the karma based bmi experiments .
object matching is a fundamental operation in data analysis . <eos> it typically requires the definition of a similarity measure between the classes of objects to be matched . <eos> instead , we develop an approach which is able to perform matching by requiring a similarity measure only within each of the classes . <eos> this is achieved by maximizing the dependency between matched pairs of observations by means of the hilbert schmidt independence criterion . <eos> this problem can be cast as one of maximizing a quadratic assignment problem with special structure and we present a simple algorithm for finding a locally optimal solution .
the synchronous brain activity measured via meg ( or eeg ) can be interpreted as arising from a collection ( possibly large ) of current dipoles or sources located throughout the cortex . <eos> estimating the number , location , and orientation of these sources remains a challenging task , one that is significantly compounded by the effects of source correlations and the presence of interference from spontaneous brain activity , sensor noise , and other artifacts . <eos> this paper derives an empirical bayesian method for addressing each of these issues in a principled fashion . <eos> the resulting algorithm guarantees descent of a cost function uniquely designed to handle unknown orientations and arbitrary correlations . <eos> robust interference suppression is also easily incorporated . <eos> in a restricted setting , the proposed method is shown to have theoretically zero bias estimating both the location and orientation of multi-component dipoles even in the presence of correlations , unlike a variety of existing bayesian localization methods or common signal processing techniques such as beamforming and sloreta . <eos> empirical results on both simulated and real data sets verify the efficacy of this approach .
cell assemblies exhibiting episodes of recurrent coherent activity have been observed in several brain regions including the striatum and hippocampus ca3 . <eos> here we address the question of how coherent dynamically switching assemblies appear in large networks of biologically realistic spiking neurons interacting deterministically . <eos> we show by numerical simulations of large asymmetric inhibitory networks with fixed external excitatory drive that if the network has intermediate to sparse connectivity , the individual cells are in the vicinity of a bifurcation between a quiescent and firing state and the network inhibition varies slowly on the spiking timescale , then cells form assemblies whose members show strong positive correlation , while members of different assemblies show strong negative correlation . <eos> we show that cells and assemblies switch between firing and quiescent states with time durations consistent with a power-law . <eos> our results are in good qualitative agreement with the experimental studies . <eos> the deterministic dynamical behaviour is related to winner-less competition shown in small closed loop inhibitory networks with heteroclinic cycles connecting saddle-points .
in this work , we consider the problem of learning a positive semidefinite matrix . <eos> the critical issue is how to preserve positive semidefiniteness during the course of learning . <eos> our algorithm is mainly inspired by lpboost [ 1 ] and the general greedy convex optimization framework of zhang [ 2 ] . <eos> we demonstrate the essence of the algorithm , termed psdboost ( positive semidefinite boosting ) , by focusing on a few different applications in machine learning . <eos> the proposed psdboost algorithm extends traditional boosting algorithms in that its parameter is a positive semidefinite matrix with trace being one instead of a classifier . <eos> psdboost is based on the observation that any trace-one positive semidefinitematrix can be decomposed into linear convex combinations of trace-one rank-one matrices , which serve as base learners of psdboost . <eos> numerical experiments are presented .
we analyse matching pursuit for kernel principal components analysis by proving that the sparse subspace it produces is a sample compression scheme . <eos> we show that this bound is tighter than the kpca bound of shawe-taylor et al swck-05 and highly predictive of the size of the subspace needed to capture most of the variance in the data . <eos> we analyse a second matching pursuit algorithm called kernel matching pursuit ( kmp ) which does not correspond to a sample compression scheme . <eos> however , we give a novel bound that views the choice of subspace of the kmp algorithm as a compression scheme and hence provide a vc bound to upper bound its future loss . <eos> finally we describe how the same bound can be applied to other matching pursuit related algorithms .
we develop as series of corrections to expectation propagation ( ep ) , which is one of the most popular methods for approximate probabilistic inference . <eos> these corrections can lead to improvements of the inference approximation or serve as a sanity check , indicating when ep yields unrealiable results .
the stochastic approximation method is behind the solution to many important , actively-studied problems in machine learning . <eos> despite its far-reaching application , there is almost no work on applying stochastic approximation to learning problems with constraints . <eos> the reason for this , we hypothesize , is that no robust , widely-applicable stochastic approximation method exists for handling such problems . <eos> we propose that interior-point methods are a natural solution . <eos> we establish the stability of a stochastic interior-point approximation method both analytically and empirically , and demonstrate its utility by deriving an on-line learning algorithm that also performs feature selection via l1 regularization .
we use graphical models and structure learning to explore how people learn policies in sequential decision making tasks . <eos> studies of sequential decision-making in humans frequently find suboptimal performance relative to an ideal actor that knows the graph model that generates reward in the environment . <eos> we argue that the learning problem humans face also involves learning the graph structure for reward generation in the environment . <eos> we formulate the structure learning problem using mixtures of reward models , and solve the optimal action selection problem using bayesian reinforcement learning . <eos> we show that structure learning in one and two armed bandit problems produces many of the qualitative behaviors deemed suboptimal in previous studies . <eos> our argument is supported by the results of experiments that demonstrate humans rapidly learn and exploit new reward structure .
we propose new families of models and algorithms for high-dimensional nonparametric learning with joint sparsity constraints . <eos> our approach is based on a regularization method that enforces common sparsity patterns across different function components in a nonparametric additive model . <eos> the algorithms employ a coordinate descent approach that is based on a functional soft-thresholding operator . <eos> the framework yields several new models , including multi-task sparse additive models , multi-response sparse additive models , and sparse additive multi-category logistic regression . <eos> the methods are illustrated with experiments on synthetic data and gene microarray data .
we develop new techniques for time series classification based on hierarchical bayesian generative models ( called mixed-effect models ) and the fisher kernel derived from them . <eos> a key advantage of the new formulation is that one can compute the fisher information matrix despite varying sequence lengths and sampling times . <eos> we therefore can avoid the ad hoc replacement of fisher information matrix with the identity matrix commonly used in literature , which destroys the geometrical grounding of the kernel construction . <eos> in contrast , our construction retains the proper geometric structure resulting in a kernel that is properly invariant under change of coordinates in the model parameter space . <eos> experiments on detecting cognitive decline show that classifiers based on the proposed kernel out-perform those based on generative models and other feature extraction routines .
we consider the problem of obtaining the approximate maximum a posteriori estimate of a discrete random field characterized by pairwise potentials that form a truncated convex model . <eos> for this problem , we propose an improved st-mincut based move making algorithm . <eos> unlike previous move making approaches , which either provide a loose bound or no bound on the quality of the solution ( in terms of the corresponding gibbs energy ) , our algorithm achieves the same guarantees as the standard linear programming ( lp ) relaxation . <eos> compared to previous approaches based on the lp relaxation , e.g . interior-point algorithms or tree-reweighted message passing ( trw ) , our method is faster as it uses only the efficient st-mincut algorithm in its design . <eos> furthermore , it directly provides us with a primal solution ( unlike trw and other related methods which attempt to solve the dual of the lp ) . <eos> we demonstrate the effectiveness of the proposed approach on both synthetic and standard real data problems . <eos> our analysis also opens up an interesting question regarding the relationship between move making algorithms ( such as $ \alpha $ -expansion and the algorithms presented in this paper ) and the randomized rounding schemes used with convex relaxations . <eos> we believe that further explorations in this direction would help design efficient algorithms for more complex relaxations .
we present a new reinforcement-learning model for the role of the hippocampus in classical conditioning , focusing on the differences between trace and delay conditioning . <eos> in the model , all stimuli are represented both as unindividuated wholes and as a series of temporal elements with varying delays . <eos> these two stimulus representations interact , producing different patterns of learning in trace and delay conditioning . <eos> the model proposes that hippocampal lesions eliminate long-latency temporal elements , but preserve short-latency temporal elements . <eos> for trace conditioning , with no contiguity between stimulus and reward , these long-latency temporal elements are vital to learning adaptively timed responses . <eos> for delay conditioning , in contrast , the continued presence of the stimulus supports conditioned responding , and the short-latency elements suppress responding early in the stimulus . <eos> in accord with the empirical data , simulated hippocampal damage impairs trace conditioning , but not delay conditioning , at medium-length intervals . <eos> with longer intervals , learning is impaired in both procedures , and , with shorter intervals , in neither . <eos> in addition , the model makes novel predictions about the response topography with extended stimuli or post-training lesions . <eos> these results demonstrate how temporal contiguity , as in delay conditioning , changes the timing problem faced by animals , rendering it both easier and less susceptible to disruption by hippocampal lesions .
extensive labeled data for image annotation systems , which learn to assign class labels to image regions , is difficult to obtain . <eos> we explore a hybrid model framework for utilizing partially labeled data that integrates a generative topic model for image appearance with discriminative label prediction . <eos> we propose three alternative formulations for imposing a spatial smoothness prior on the image labels . <eos> tests of the new models and some baseline approaches on two real image datasets demonstrate the effectiveness of incorporating the latent structure .
almost all successful machine learning algorithms and cognitive models require powerful representations capturing the features that are relevant to a particular problem . <eos> we draw on recent work in nonparametric bayesian statistics to define a rational model of human feature learning that forms a featural representation from raw sensory data without pre-specifying the number of features . <eos> by comparing how the human perceptual system and our rational model use distributional and category information to infer feature representations , we seek to identify some of the forces that govern the process by which people separate and combine sensory primitives to form features .
we describe a novel stochastic process that can be used to construct a multidimensional generalization of the stick-breaking process and which is related to the classic stick breaking process described by sethuraman1994 in one dimension . <eos> we describe how the process can be applied to relational data modeling using the de finetti representation for infinitely and partially exchangeable arrays .
in recent work long and servedio ls05short presented a `` martingale boosting '' algorithm that works by constructing a branching program over weak classifiers and has a simple analysis based on elementary properties of random walks . <eos> ls05short showed that this martingale booster can tolerate random classification noise when it is run with a noise-tolerant weak learner ; however , a drawback of the algorithm is that it is not adaptive , i.e . it can not effectively take advantage of variation in the quality of the weak classifiers it receives . <eos> in this paper we present a variant of the original martingale boosting algorithm and prove that it is adaptive . <eos> this adaptiveness is achieved by modifying the original algorithm so that the random walks that arise in its analysis have different step size depending on the quality of the weak learner at each stage . <eos> the new algorithm inherits the desirable properties of the original ls05short algorithm , such as random classification noise tolerance , and has several other advantages besides adaptiveness : it requires polynomially fewer calls to the weak learner than the original algorithm , and it can be used with confidence-rated weak hypotheses that output real values rather than boolean predictions .
we apply robust bayesian decision theory to improve both generative and discriminative learners under bias in class proportions in labeled training data , when the true class proportions are unknown . <eos> for the generative case , we derive an entropy-based weighting that maximizes expected log likelihood under the worst-case true class proportions . <eos> for the discriminative case , we derive a multinomial logistic model that minimizes worst-case conditional log loss . <eos> we apply our theory to the modeling of species geographic distributions from presence data , an extreme case of label bias since there is no absence data . <eos> on a benchmark dataset , we find that entropy-based weighting offers an improvement over constant estimates of class proportions , consistently reducing log loss on unbiased test data .
stochastic relational models provide a rich family of choices for learning and predicting dyadic data between two sets of entities . <eos> it generalizes matrix factorization to a supervised learning problem that utilizes attributes of objects in a hierarchical bayesian framework . <eos> previously empirical bayesian inference was applied , which is however not scalable when the size of either object sets becomes tens of thousands . <eos> in this paper , we introduce a markov chain monte carlo ( mcmc ) algorithm to scale the model to very large-scale dyadic data . <eos> both superior scalability and predictive accuracy are demonstrated on a collaborative filtering problem , which involves tens of thousands users and a half million items .
we introduce the first temporal-difference learning algorithm that is stable with linear function approximation and off-policy training , for any finite markov decision process , target policy , and exciting behavior policy , and whose complexity scales linearly in the number of parameters . <eos> we consider an i.i.d.\ policy-evaluation setting in which the data need not come from on-policy experience . <eos> the gradient temporal-difference ( gtd ) algorithm estimates the expected update vector of the td ( 0 ) algorithm and performs stochastic gradient descent on its l_2 norm . <eos> our analysis proves that its expected update is in the direction of the gradient , assuring convergence under the usual stochastic approximation conditions to the same least-squares solution as found by the lstd , but without its quadratic computational complexity . <eos> gtd is online and incremental , and does not involve multiplying by products of likelihood ratios as in importance-sampling methods .
we propose a nonparametric bayesian factor regression model that accounts for uncertainty in the number of factors , and the relationship between factors . <eos> to accomplish this , we propose a sparse variant of the indian buffet process and couple this with a hierarchical model over factors , based on kingman ? s coalescent . <eos> we apply this model to two problems ( factor analysis and factor regression ) in gene-expression data analysis .
we introduce a new family of positive-definite kernel functions that mimic the computation in large , multilayer neural nets . <eos> these kernel functions can be used in shallow architectures , such as support vector machines ( svms ) , or in deep kernel-based architectures that we call multilayer kernel machines ( mkms ) . <eos> we evaluate svms and mkms with these kernel functions on problems designed to illustrate the advantages of deep architectures . <eos> on several problems , we obtain better results than previous , leading benchmarks from both svms with gaussian kernels as well as deep belief nets .
while many advances have already been made on the topic of hierarchical classi- ? cation learning , we take a step back and examine how a hierarchical classi ? ca- tion problem should be formally de ? ned . <eos> we pay particular attention to the fact that many arbitrary decisions go into the design of the the label taxonomy that is provided with the training data , and that this taxonomy is often unbalanced . <eos> we correct this problem by using the data distribution to calibrate the hierarchical classi ? cation loss function . <eos> this distribution-based correction must be done with care , to avoid introducing unmanagable statstical dependencies into the learning problem . <eos> this leads us off the beaten path of binomial-type estimation and into the uncharted waters of geometric-type estimation . <eos> we present a new calibrated de ? nition of statistical risk for hierarchical classi ? cation , an unbiased geometric estimator for this risk , and a new algorithmic reduction from hierarchical classi ? - cation to cost-sensitive classi ? cation .
dependent dirichlet processes ( dps ) are dependent sets of random measures , each being marginally dirichlet process distributed . <eos> they are used in bayesian nonparametric models when the usual exchangebility assumption does not hold . <eos> we propose a simple and general framework to construct dependent dps by marginalizing and normalizing a single gamma process over an extended space . <eos> the result is a set of dps , each located at a point in a space such that neighboring dps are more dependent . <eos> we describe markov chain monte carlo inference , involving the typical gibbs sampling and three different metropolis-hastings proposals to speed up convergence . <eos> we report an empirical study of convergence speeds on a synthetic dataset and demonstrate an application of the model to topic modeling through time .
the relative merits of different population coding schemes have mostly been analyzed in the framework of stimulus reconstruction using fisher information . <eos> here , we consider the case of stimulus discrimination in a two alternative forced choice paradigm and compute neurometric functions in terms of the minimal discrimination error and the jensen-shannon information to study neural population codes . <eos> we first explore the relationship between minimum discrimination error , jensen-shannon information and fisher information and show that the discrimination framework is more informative about the coding accuracy than fisher information as it defines an error for any pair of possible stimuli . <eos> in particular , it includes fisher information as a special case . <eos> second , we use the framework to study population codes of angular variables . <eos> specifically , we assess the impact of different noise correlations structures on coding accuracy in long versus short decoding time windows . <eos> that is , for long time window we use the common gaussian noise approximation . <eos> to address the case of short time windows we analyze the ising model with identical noise correlation structure . <eos> in this way , we provide a new rigorous framework for assessing the functional consequences of noise correlation structures for the representational accuracy of neural population codes that is in particular applicable to short-time population coding .
we propose new methodologies to detect anomalies in discrete-time processes taking values in a set . <eos> the method is based on the inference of functionals whose evaluations on successive states visited by the process have low autocorrelations . <eos> deviations from this behavior are used to flag anomalies . <eos> the candidate functionals are estimated in a subset of a reproducing kernel hilbert space associated with the set where the process takes values . <eos> we provide experimental results which show that these techniques compare favorably with other algorithms .
with the advent of the internet it is now possible to collect hundreds of millions of images . <eos> these images come with varying degrees of label information . <eos> `` clean labels can be manually obtained on a small fraction , `` noisy labels may be extracted automatically from surrounding text , while for most images there are no labels at all . <eos> semi-supervised learning is a principled framework for combining these different label sources . <eos> however , it scales polynomially with the number of images , making it impractical for use on gigantic collections with hundreds of millions of images and thousands of classes . <eos> in this paper we show how to utilize recent results in machine learning to obtain highly efficient approximations for semi-supervised learning that are linear in the number of images . ? <eos> specifically , we use the convergence of the eigenvectors of the normalized graph laplacian to eigenfunctions of weighted laplace-beltrami operators . <eos> we combine this with a label sharing framework obtained from wordnet to propagate label information to classes lacking manual annotations . <eos> our algorithm enables us to apply semi-supervised learning to a database of 80 million images with 74 thousand classes .
extensive games are often used to model the interactions of multiple agents within an environment . <eos> much recent work has focused on increasing the size of an extensive game that can be feasibly solved . <eos> despite these improvements , many interesting games are still too large for such techniques . <eos> a common approach for computing strategies in these large games is to first employ an abstraction technique to reduce the original game to an abstract game that is of a manageable size . <eos> this abstract game is then solved and the resulting strategy is used in the original game . <eos> most top programs in recent aaai computer poker competitions use this approach . <eos> the trend in this competition has been that strategies found in larger abstract games tend to beat strategies found in smaller abstract games . <eos> these larger abstract games have more expressive strategy spaces and therefore contain better strategies . <eos> in this paper we present a new method for computing strategies in large games . <eos> this method allows us to compute more expressive strategies without increasing the size of abstract games that we are required to solve . <eos> we demonstrate the power of the approach experimentally in both small and large games , while also providing a theoretical justification for the resulting improvement .
the replica method is a non-rigorous but widely-used technique from statistical physics used in the asymptotic analysis of many large random nonlinear problems . <eos> this paper applies the replica method to non-gaussian map estimation . <eos> it is shown that with large random linear measurements and gaussian noise , the asymptotic behavior of the map estimate of an n-dimensional vector `` decouples as n scalar map estimators . <eos> the result is a counterpart to guo and verdus replica analysis on mmse estimation . <eos> the replica map analysis can be readily applied to many estimators used in compressed sensing , including basis pursuit , lasso , linear estimation with thresholding and zero-norm estimation . <eos> in the case of lasso estimation , the scalar estimator reduces to a soft-thresholding operator and for zero-norm estimation it reduces to a hard-threshold . <eos> among other benefits , the replica method provides a computationally tractable method for exactly computing various performance metrics including mse and sparsity recovery .
we study an encoding/decoding mechanism accounting for the relative spike timing of the signals propagating from peripheral nerve fibers to second-order somatosensory neurons in the cuneate nucleus ( cn ) . <eos> the cn is modeled as a population of spiking neurons receiving as inputs the spatiotemporal responses of real mechanoreceptors obtained via microneurography recordings in humans . <eos> the efficiency of the haptic discrimination process is quantified by a novel definition of entropy that takes into full account the metrical properties of the spike train space . <eos> this measure proves to be a suitable decoding scheme for generalizing the classical shannon entropy to spike-based neural codes . <eos> it permits an assessment of neurotransmission in the presence of a large output space ( i.e . hundreds of spike trains ) with 1 ms temporal precision . <eos> it is shown that the cn population code performs a complete discrimination of 81 distinct stimuli already within 35 ms of the first afferent spike , whereas a partial discrimination ( 80 % of the maximum information transmission ) is possible as rapidly as 15 ms . <eos> this study suggests that the cn may not constitute a mere synaptic relay along the somatosensory pathway but , rather , it may convey optimal contextual accounts ( in terms of fast and reliable information transfer ) of peripheral tactile inputs to downstream structures of the central nervous system .
we present a sequence of unsupervised , nonparametric bayesian models for clustering complex linguistic objects . <eos> in this approach , we consider a potentially infinite number of features and categorical outcomes . <eos> we evaluate these models for the task of within- and cross-document event coreference on two corpora . <eos> all the models we investigated show significant improvements when compared against an existing baseline for this task .
the indian buffet process ( ibp ) is an exchangeable distribution over binary matrices used in bayesian nonparametric featural models . <eos> in this paper we propose a three-parameter generalization of the ibp exhibiting power-law behavior . <eos> we achieve this by generalizing the beta process ( the de finetti measure of the ibp ) to the \emph { stable-beta process } and deriving the ibp corresponding to it . <eos> we find interesting relationships between the stable-beta process and the pitman-yor process ( another stochastic process used in bayesian nonparametric models with interesting power-law properties ) . <eos> we show that our power-law ibp is a good model for word occurrences in documents with improved performance over the normal ibp .
this paper tackles the problem of selecting among several linear estimators in non-parametric regression ; this includes model selection for linear regression , the choice of a regularization parameter in kernel ridge regression or spline smoothing , and the choice of a kernel in multiple kernel learning . <eos> we propose a new algorithm which first estimates consistently the variance of the noise , based upon the concept of minimal penalty which was previously introduced in the context of model selection . <eos> then , plugging our variance estimate in mallows $ c_l $ penalty is proved to lead to an algorithm satisfying an oracle inequality . <eos> simulation experiments with kernel ridge regression and multiple kernel learning show that the proposed algorithm often improves significantly existing calibration procedures such as 10-fold cross-validation or generalized cross-validation .
interesting real-world datasets often exhibit nonlinear , noisy , continuous-valued states that are unexplorable , are poorly described by first principles , and are only partially observable . <eos> if partial observability can be overcome , these constraints suggest the use of model-based reinforcement learning . <eos> we experiment with manifold embeddings as the reconstructed observable state-space of an off-line , model-based reinforcement learning approach to control . <eos> we demonstrate the embedding of a system changes as a result of learning and that the best performing embeddings well-represent the dynamics of both the uncontrolled and adaptively controlled system . <eos> we apply this approach in simulation to learn a neurostimulation policy that is more efficient in treating epilepsy than conventional policies . <eos> we then demonstrate the learned policy completely suppressing seizures in real-world neurostimulation experiments on actual animal brain slices .
the human brain can be described as containing a number of functional regions . <eos> for a given task , these regions , as well as the connections between them , play a key role in information processing in the brain . <eos> however , most existing multi-voxel pattern analysis approaches either treat multiple functional regions as one large uniform region or several independent regions , ignoring the connections between regions . <eos> in this paper , we propose to model such connections in an hidden conditional random field ( hcrf ) framework , where the classifier of one region of interest ( roi ) makes predictions based on not only its voxels but also the classifier predictions from rois that it connects to . <eos> furthermore , we propose a structural learning method in the hcrf framework to automatically uncover the connections between rois . <eos> experiments on fmri data acquired while human subjects viewing images of natural scenes show that our model can improve the top-level ( the classifier combining information from all rois ) and roi-level prediction accuracy , as well as uncover some meaningful connections between rois .
we introduce a new family of distributions , called $ l_p $ { \em -nested symmetric distributions } , whose densities access the data exclusively through a hierarchical cascade of $ l_p $ -norms . <eos> this class generalizes the family of spherically and $ l_p $ -spherically symmetric distributions which have recently been successfully used for natural image modeling . <eos> similar to those distributions it allows for a nonlinear mechanism to reduce the dependencies between its variables . <eos> with suitable choices of the parameters and norms , this family also includes the independent subspace analysis ( isa ) model , which has been proposed as a means of deriving filters that mimic complex cells found in mammalian primary visual cortex . <eos> $ l_p $ -nested distributions are easy to estimate and allow us to explore the variety of models between isa and the $ l_p $ -spherically symmetric models . <eos> our main findings are that , without a preprocessing step of contrast gain control , the independent subspaces of isa are in fact more dependent than the individual filter coefficients within a subspace and , with contrast gain control , where isa finds more than one subspace , the filter responses were almost independent anyway .
we develop a bayesian sequential model for category learning . <eos> the sequential model updates two category parameters , the mean and the variance , over time . <eos> we define conjugate temporal priors to enable closed form solutions to be obtained . <eos> this model can be easily extended to supervised and unsupervised learning involving multiple categories . <eos> to model the spacing effect , we introduce a generic prior in the temporal updating stage to capture a learning preference , namely , less change for repetition and more change for variation . <eos> finally , we show how this approach can be generalized to efficiently performmodel selection to decide whether observations are from one or multiple categories .
modern machine learning-based approaches to computer vision require very large databases of labeled images . <eos> some contemporary vision systems already require on the order of millions of images for training ( e.g. , omron face detector ) . <eos> while the collection of these large databases is becoming a bottleneck , new internet-based services that allow labelers from around the world to be easily hired and managed provide a promising solution . <eos> however , using these services to label large databases brings with it new theoretical and practical challenges : ( 1 ) the labelers may have wide ranging levels of expertise which are unknown a priori , and in some cases may be adversarial ; ( 2 ) images may vary in their level of difficulty ; and ( 3 ) multiple labels for the same image must be combined to provide an estimate of the actual label of the image . <eos> probabilistic approaches provide a principled way to approach these problems . <eos> in this paper we present a probabilistic model and use it to simultaneously infer the label of each image , the expertise of each labeler , and the difficulty of each image . <eos> on both simulated and real data , we demonstrate that the model outperforms the commonly used `` majority vote heuristic for inferring image labels , and is robust to both adversarial and noisy labelers .
imaging techniques such as optical imaging of intrinsic signals , 2-photon calcium imaging and voltage sensitive dye imaging can be used to measure the functional organization of visual cortex across different spatial scales . <eos> here , we present bayesian methods based on gaussian processes for extracting topographic maps from functional imaging data . <eos> in particular , we focus on the estimation of orientation preference maps ( opms ) from intrinsic signal imaging data . <eos> we model the underlying map as a bivariate gaussian process , with a prior covariance function that reflects known properties of opms , and a noise covariance adjusted to the data . <eos> the posterior mean can be interpreted as an optimally smoothed estimate of the map , and can be used for model based interpolations of the map from sparse measurements . <eos> by sampling from the posterior distribution , we can get error bars on statistical properties such as preferred orientations , pinwheel locations or -counts . <eos> finally , the use of an explicit probabilistic model facilitates interpretation of parameters and provides the basis for decoding studies . <eos> we demonstrate our model both on simulated data and on intrinsic signaling data from ferret visual cortex .
the concave-convex procedure ( cccp ) is a majorization-minimization algorithm that solves d.c. ( difference of convex functions ) programs as a sequence of convex programs . <eos> in machine learning , cccp is extensively used in many learning algorithms like sparse support vector machines ( svms ) , transductive svms , sparse principal component analysis , etc . <eos> though widely used in many applications , the convergence behavior of cccp has not gotten a lot of specific attention . <eos> yuille and rangarajan analyzed its convergence in their original paper , however , we believe the analysis is not complete . <eos> although the convergence of cccp can be derived from the convergence of the d.c. algorithm ( dca ) , their proof is more specialized and technical than actually required for the specific case of cccp . <eos> in this paper , we follow a different reasoning and show how zangwills global convergence theory of iterative algorithms provides a natural framework to prove the convergence of cccp , allowing a more elegant and simple proof . <eos> this underlines zangwills theory as a powerful and general framework to deal with the convergence issues of iterative algorithms , after also being used to prove the convergence of algorithms like expectation-maximization , generalized alternating minimization , etc . <eos> in this paper , we provide a rigorous analysis of the convergence of cccp by addressing these questions : ( i ) when does cccp find a local minimum or a stationary point of the d.c. program under consideration ? <eos> ( ii ) when does the sequence generated by cccp converge ? <eos> we also present an open problem on the issue of local convergence of cccp .
the use of context is critical for scene understanding in computer vision , where the recognition of an object is driven by both local appearance and the objects relationship to other elements of the scene ( context ) . <eos> most current approaches rely on modeling the relationships between object categories as a source of context . <eos> in this paper we seek to move beyond categories to provide a richer appearance-based model of context . <eos> we present an exemplar-based model of objects and their relationships , the visual memex , that encodes both local appearance and 2d spatial context between object instances . <eos> we evaluate our model on torralbas proposed context challenge against a baseline category-based system . <eos> our experiments suggest that moving beyond categories for context modeling appears to be quite beneficial , and may be the critical missing ingredient in scene understanding systems .
this paper considers a sensitivity analysis in hidden markov models with continuous state and observation spaces . <eos> we propose an infinitesimal perturbation analysis ( ipa ) on the filtering distribution with respect to some parameters of the model . <eos> we describe a methodology for using any algorithm that estimates the filtering density , such as sequential monte carlo methods , to design an algorithm that estimates its gradient . <eos> the resulting ipa estimator is proven to be asymptotically unbiased , consistent and has computational complexity linear in the number of particles . <eos> we consider an application of this analysis to the problem of identifying unknown parameters of the model given a sequence of observations . <eos> we derive an ipa estimator for the gradient of the log-likelihood , which may be used in a gradient method for the purpose of likelihood maximization . <eos> we illustrate the method with several numerical experiments .
motivated by recent developments in manifold-valued regression we propose a family of nonparametric kernel-smoothing estimators with metric-space valued output including a robust median type estimator and the classical frechet mean . <eos> depending on the choice of the output space and the chosen metric the estimator reduces to partially well-known procedures for multi-class classification , multivariate regression in euclidean space , regression with manifold-valued output and even some cases of structured output learning . <eos> in this paper we focus on the case of regression with manifold-valued input and output . <eos> we show pointwise and bayes consistency for all estimators in the family for the case of manifold-valued output and illustrate the robustness properties of the estimator with experiments .
we consider the problem of zero-shot learning , where the goal is to learn a classifier $ f : x \rightarrow y $ that must predict novel values of $ y $ that were omitted from the training set . <eos> to achieve this , we define the notion of a semantic output code classifier ( soc ) which utilizes a knowledge base of semantic properties of $ y $ to extrapolate to novel classes . <eos> we provide a formalism for this type of classifier and study its theoretical properties in a pac framework , showing conditions under which the classifier can accurately predict novel classes . <eos> as a case study , we build a soc classifier for a neural decoding task and show that it can often predict words that people are thinking about from functional magnetic resonance images ( fmri ) of their neural activity , even without training examples for those words .
recent advances in neuroimaging techniques provide great potentials for effective diagnosis of alzheimer ? s disease ( ad ) , the most common form of dementia . <eos> previous studies have shown that ad is closely related to alternation in the functional brain network , i.e. , the functional connectivity among different brain regions . <eos> in this paper , we consider the problem of learning functional brain connectivity from neuroimaging , which holds great promise for identifying image-based markers used to distinguish normal controls ( nc ) , patients with mild cognitive impairment ( mci ) , and patients with ad . <eos> more specifically , we study sparse inverse covariance estimation ( sice ) , also known as exploratory gaussian graphical models , for brain connectivity modeling . <eos> in particular , we apply sice to learn and analyze functional brain connectivity patterns from different subject groups , based on a key property of sice , called the ? monotone property ? <eos> we established in this paper . <eos> our experimental results on neuroimaging pet data of 42 ad , 116 mci , and 67 nc subjects reveal several interesting connectivity patterns consistent with literature findings , and also some new patterns that can help the knowledge discovery of ad .
we study the behavior of the popular laplacian regularization method for semi-supervised learning at the regime of a fixed number of labeled points but a large number of unlabeled points . <eos> we show that in $ \r^d $ , $ d \geq 2 $ , the method is actually not well-posed , and as the number of unlabeled points increases the solution degenerates to a noninformative function . <eos> we also contrast the method with the laplacian eigenvector method , and discuss the `` smoothness assumptions associated with this alternate method .
there has been a clear distinction between induction or training time and diagnosis time active information acquisition . <eos> while active learning during induction focuses on acquiring data that promises to provide the best classification model , the goal at diagnosis time focuses completely on next features to observe about the test case at hand in order to make better predictions about the case . <eos> we introduce a model and inferential methods that breaks this distinction . <eos> the methods can be used to extend case libraries under a budget but , more fundamentally , provide a framework for guiding agents to collect data under scarce resources , focused by diagnostic challenges . <eos> this extension to active learning leads to a new class of policies for real-time diagnosis , where recommended information-gathering sequences include actions that simultaneously seek new data for the case at hand and for cases in the training set .
discriminatively trained undirected graphical models have had wide empirical success , and there has been increasing interest in toolkits that ease their application to complex relational data . <eos> the power in relational models is in their repeated structure and tied parameters ; at issue is how to define these structures in a powerful and flexible way . <eos> rather than using a declarative language , such as sql or first-order logic , we advocate using an imperative language to express various aspects of model structure , inference , and learning . <eos> by combining the traditional , declarative , statistical semantics of factor graphs with imperative definitions of their construction and operation , we allow the user to mix declarative and procedural domain knowledge , and also gain significant efficiencies . <eos> we have implemented such imperatively defined factor graphs in a system we call factorie , a software library for an object-oriented , strongly-typed , functional language . <eos> in experimental comparisons to markov logic networks on joint segmentation and coreference , we find our approach to be 3-15 times faster while reducing error by 20-25 % -achieving a new state of the art .
a non-parametric bayesian model is proposed for processing multiple images . <eos> the analysis employs image features and , when present , the words associated with accompanying annotations . <eos> the model clusters the images into classes , and each image is segmented into a set of objects , also allowing the opportunity to assign a word to each object ( localized labeling ) . <eos> each object is assumed to be represented as a heterogeneous mix of components , with this realized via mixture models linking image features to object types . <eos> the number of image classes , number of object types , and the characteristics of the object-feature mixture models are inferred non-parametrically . <eos> to constitute spatially contiguous objects , a new logistic stick-breaking process is developed . <eos> inference is performed efficiently via variational bayesian analysis , with example results presented on two image databases .
the control of neuroprosthetic devices from the activity of motor cortex neurons benefits from learning effects where the function of these neurons is adapted to the control task . <eos> it was recently shown that tuning properties of neurons in monkey motor cortex are adapted selectively in order to compensate for an erroneous interpretation of their activity . <eos> in particular , it was shown that the tuning curves of those neurons whose preferred directions had been misinterpreted changed more than those of other neurons . <eos> in this article , we show that the experimentally observed self-tuning properties of the system can be explained on the basis of a simple learning rule . <eos> this learning rule utilizes neuronal noise for exploration and performs hebbian weight updates that are modulated by a global reward signal . <eos> in contrast to most previously proposed reward-modulated hebbian learning rules , this rule does not require extraneous knowledge about what is noise and what is signal . <eos> the learning rule is able to optimize the performance of the model system within biologically realistic periods of time and under high noise levels . <eos> when the neuronal noise is fitted to experimental data , the model produces learning effects similar to those found in monkey experiments .
we propose dirichlet-bernoulli alignment ( dba ) , a generative model for corpora in which each pattern ( e.g. , a document ) contains a set of instances ( e.g. , paragraphs in the document ) and belongs to multiple classes . <eos> by casting predefined classes as latent dirichlet variables ( i.e. , instance level labels ) , and modeling the multi-label of each pattern as bernoulli variables conditioned on the weighted empirical average of topic assignments , dba automatically aligns the latent topics discovered from data to human-defined classes . <eos> dba is useful for both pattern classification and instance disambiguation , which are tested on text classification and named entity disambiguation for web search queries respectively .
the learning of appropriate distance metrics is a critical problem in classification . <eos> in this work , we propose a boosting-based technique , termed boostmetric , for learning a mahalanobis distance metric . <eos> one of the primary difficulties in learning such a metric is to ensure that the mahalanobis matrix remains positive semidefinite . <eos> semidefinite programming is sometimes used to enforce this constraint , but does not scale well . <eos> boostmetric is instead based on a key observation that any positive semidefinite matrix can be decomposed into a linear positive combination of trace-one rank-one matrices . <eos> boostmetric thus uses rank-one positive semidefinite matrices as weak learners within an efficient and scalable boosting-based learning process . <eos> the resulting method is easy to implement , does not require tuning , and can accommodate various types of constraints . <eos> experiments on various datasets show that the proposed algorithm compares favorably to those state-of-the-art methods in terms of classification accuracy and running time .
many categories are better described by providing relational information than listing characteristic features . <eos> we present a hierarchical generative model that helps to explain how relational categories are learned and used . <eos> our model learns abstract schemata that specify the relational similarities shared by members of a category , and our emphasis on abstraction departs from previous theoretical proposals that focus instead on comparison of concrete instances . <eos> our first experiment suggests that our abstraction-based account can address some of the tasks that have previously been used to support comparison-based approaches . <eos> our second experiment focuses on one-shot schema learning , a problem that raises challenges for comparison-based approaches but is handled naturally by our abstraction-based account .
schizophrenia is a complex psychiatric disorder that has eluded a characterization in terms of local abnormalities of brain activity , and is hypothesized to affect the collective , `` emergent working of the brain . <eos> we propose a novel data-driven approach to capture emergent features using functional brain networks [ eguiluzet al ] extracted from fmri data , and demonstrate its advantage over traditional region-of-interest ( roi ) and local , task-specific linear activation analyzes . <eos> our results suggest that schizophrenia is indeed associated with disruption of global , emergent brain properties related to its functioning as a network , which can not be explained by alteration of local activation patterns . <eos> moreover , further exploitation of interactions by sparse markov random field classifiers shows clear gain over linear methods , such as gaussian naive bayes and svm , allowing to reach 86 % accuracy ( over 50 % baseline - random guess ) , which is quite remarkable given that it is based on a single fmri experiment using a simple auditory task .
the recent introduction of indefinite svm by luss and daspremont [ 15 ] has effectively demonstrated svm classification with a non-positive semi-definite kernel ( indefinite kernel ) . <eos> this paper studies the properties of the objective function introduced there . <eos> in particular , we show that the objective function is continuously differentiable and its gradient can be explicitly computed . <eos> indeed , we further show that its gradient is lipschitz continuous . <eos> the main idea behind our analysis is that the objective function is smoothed by the penalty term , in its saddle ( min-max ) representation , measuring the distance between the indefinite kernel matrix and the proxy positive semi-definite one . <eos> our elementary result greatly facilitates the application of gradient-based algorithms . <eos> based on our analysis , we further develop nesterovs smooth optimization approach [ 16,17 ] for indefinite svm which has an optimal convergence rate for smooth problems . <eos> experiments on various benchmark datasets validate our analysis and demonstrate the efficiency of our proposed algorithms .
the nested chinese restaurant process ( ncrp ) is a powerful nonparametric bayesian model for learning tree-based hierarchies from data . <eos> since its posterior distribution is intractable , current inference methods have all relied on mcmc sampling . <eos> in this paper , we develop an alternative inference technique based on variational methods . <eos> to employ variational methods , we derive a tree-based stick-breaking construction of the ncrp mixture model , and a novel variational algorithm that efficiently explores a posterior over a large set of combinatorial structures . <eos> we demonstrate the use of this approach for text and hand written digits modeling , where we show we can adapt the ncrp to continuous data as well .
in many domains , humans appear to combine perceptual cues in a near-optimal , probabilistic fashion : two noisy pieces of information tend to be combined linearly with weights proportional to the precision of each cue . <eos> here we present a case where structural information plays an important role . <eos> the presence of a background cue gives rise to the possibility of occlusion , and places a soft constraint on the location of a target ? <eos> in effect propelling it forward . <eos> we present an ideal observer model of depth estimation for this situation where structural or ordinal information is important and then fit the model to human data from a stereo-matching task . <eos> to test whether subjects are truly using ordinal cues in a probabilistic manner we then vary the uncertainty of the task . <eos> we find that the model accurately predicts shifts in subject ? s behavior . <eos> our results indicate that the nervous system estimates depth ordering in a probabilistic fashion and estimates the structure of the visual scene during depth perception .
we consider the reconstruction of sparse signals in the multiple measurement vector ( mmv ) model , in which the signal , represented as a matrix , consists of a set of jointly sparse vectors . <eos> mmv is an extension of the single measurement vector ( smv ) model employed in standard compressive sensing ( cs ) . <eos> recent theoretical studies focus on the convex relaxation of the mmv problem based on the $ ( 2,1 ) $ -norm minimization , which is an extension of the well-known $ 1 $ -norm minimization employed in smv . <eos> however , the resulting convex optimization problem in mmv is significantly much more difficult to solve than the one in smv . <eos> existing algorithms reformulate it as a second-order cone programming ( socp ) or semidefinite programming ( sdp ) , which is computationally expensive to solve for problems of moderate size . <eos> in this paper , we propose a new ( dual ) reformulation of the convex optimization problem in mmv and develop an efficient algorithm based on the prox-method . <eos> interestingly , our theoretical analysis reveals the close connection between the proposed reformulation and multiple kernel learning . <eos> our simulation studies demonstrate the scalability of the proposed algorithm .
there is a growing body of experimental evidence to suggest that the brain is capable of approximating optimal bayesian inference in the face of noisy input stimuli . <eos> despite this progress , the neural underpinnings of this computation are still poorly understood . <eos> in this paper we focus on the problem of bayesian filtering of stochastic time series . <eos> in particular we introduce a novel neural network , derived from a line attractor architecture , whose dynamics map directly onto those of the kalman filter in the limit where the prediction error is small . <eos> when the prediction error is large we show that the network responds robustly to change-points in a way that is qualitatively compatible with the optimal bayesian model . <eos> the model suggests ways in which probability distributions are encoded in the brain and makes a number of testable experimental predictions .
many transductive inference algorithms assume that distributions over training and test estimates should be related , e.g . by providing a large margin of separation on both sets . <eos> we use this idea to design a transduction algorithm which can be used without modification for classification , regression , and structured estimation . <eos> at its heart we exploit the fact that for a good learner the distributions over the outputs on training and test sets should match . <eos> this is a classical two-sample problem which can be solved efficiently in its most general form by using distance measures in hilbert space . <eos> it turns out that a number of existing heuristics can be viewed as special cases of our approach .
fast retrieval methods are increasingly critical for many large-scale analysis tasks , and there have been several recent methods that attempt to learn hash functions for fast and accurate nearest neighbor searches . <eos> in this paper , we develop an algorithm for learning hash functions based on explicitly minimizing the reconstruction error between the original distances and the hamming distances of the corresponding binary embeddings . <eos> we develop a scalable coordinate-descent algorithm for our proposed hashing objective that is able to efficiently learn hash functions in a variety of settings . <eos> unlike existing methods such as semantic hashing and spectral hashing , our method is easily kernelized and does not require restrictive assumptions about the underlying distribution of the data . <eos> we present results over several domains to demonstrate that our method outperforms existing state-of-the-art techniques .
in this paper we present an algorithm for separating mixed sounds from a monophonic recording . <eos> our approach makes use of training data which allows us to learn representations of the types of sounds that compose the mixture . <eos> in contrast to popular methods that attempt to extract com- pact generalizable models for each sound from training data , we employ the training data itself as a representation of the sources in the mixture . <eos> we show that mixtures of known sounds can be described as sparse com- binations of the training data itself , and in doing so produce signi ? cantly better separation results as compared to similar systems based on compact statistical models .
nonparametric bayesian models provide a framework for flexible probabilistic modelling of complex datasets . <eos> unfortunately , bayesian inference methods often require high-dimensional averages and can be slow to compute , especially with the potentially unbounded representations associated with nonparametric models . <eos> we address the challenge of scaling nonparametric bayesian inference to the increasingly large datasets found in real-world applications , focusing on the case of parallelising inference in the indian buffet process ( ibp ) . <eos> our approach divides a large data set between multiple processors . <eos> the processors use message passing to compute likelihoods in an asynchronous , distributed fashion and to propagate statistics about the global bayesian posterior . <eos> this novel mcmc sampler is the first parallel inference scheme for ibp-based models , scaling to datasets orders of magnitude larger than had previously been possible .
we extend dyna planning architecture for policy evaluation and control in two significant aspects . <eos> first , we introduce a multi-step dyna planning that projects the simulated state/feature many steps into the future . <eos> our multi-step dyna is based on a multi-step model , which we call the { \em $ \lambda $ -model } . <eos> the $ \lambda $ -model interpolates between the one-step model and an infinite-step model , and can be learned efficiently online . <eos> second , we use for dyna control a dynamic multi-step model that is able to predict the results of a sequence of greedy actions and track the optimal policy in the long run . <eos> experimental results show that dyna using the multi-step model evaluates a policy faster than using single-step models ; dyna control algorithms using the dynamic tracking model are much faster than model-free algorithms ; further , multi-step dyna control algorithms enable the policy and value function to converge much faster to their optima than single-step dyna algorithms .
in the quest to make brain computer interfacing ( bci ) more usable , dry electrodes have emerged that get rid of the initial 30 minutes required for placing an electrode cap . <eos> another time consuming step is the required individualized adaptation to the bci user , which involves another 30 minutes calibration for assessing a subjects brain signature . <eos> in this paper we aim to also remove this calibration proceedure from bci setup time by means of machine learning . <eos> in particular , we harvest a large database of eeg bci motor imagination recordings ( 83 subjects ) for constructing a library of subject-specific spatio-temporal filters and derive a subject independent bci classifier . <eos> our offline results indicate that bci-na\ { i } ve users could start real-time bci use with no prior calibration at only a very moderate performance loss . ''
many models for computations in recurrent networks of neurons assume that the network state moves from some initial state to some fixed point attractor or limit cycle that represents the output of the computation . <eos> however experimental data show that in response to a sensory stimulus the network state moves from its initial state through a trajectory of network states and eventually returns to the initial state , without reaching an attractor or limit cycle in between . <eos> this type of network response , where salient information about external stimuli is encoded in characteristic trajectories of continuously varying network states , raises the question how a neural system could compute with such code , and arrive for example at a temporally stable classification of the external stimulus . <eos> we show that a known unsupervised learning algorithm , slow feature analysis ( sfa ) , could be an important ingredient for extracting stable information from these network trajectories . <eos> in fact , if sensory stimuli are more often followed by another stimulus from the same class than by a stimulus from another class , sfa approaches the classification capability of fishers linear discriminant ( fld ) , a powerful algorithm for supervised learning . <eos> we apply this principle to simulated cortical microcircuits , and show that it enables readout neurons to learn discrimination of spoken digits and detection of repeating firing patterns within a stream of spike trains with the same firing statistics , without requiring any supervision for learning .
canonical correlation analysis ( cca ) is a useful technique for modeling dependencies between two ( or more ) sets of variables . <eos> building upon the recently suggested probabilistic interpretation of cca , we propose a nonparametric , fully bayesian framework that can automatically select the number of correlation components , and effectively capture the sparsity underlying the projections . <eos> in addition , given ( partially ) labeled data , our algorithm can also be used as a ( semi ) supervised dimensionality reduction technique , and can be applied to learn useful predictive features in the context of learning a set of related tasks . <eos> experimental results demonstrate the efficacy of the proposed approach for both cca as a stand-alone problem , and when applied to multi-label prediction .
in recent years , deep learning approaches have gained significant interest as a way of building hierarchical representations from unlabeled data . <eos> however , to our knowledge , these deep learning approaches have not been extensively studied for auditory data . <eos> in this paper , we apply convolutional deep belief networks to audio data and empirically evaluate them on various audio classification tasks . <eos> for the case of speech data , we show that the learned features correspond to phones/phonemes . <eos> in addition , our feature representations trained from unlabeled audio data show very good performance for multiple audio classification tasks . <eos> we hope that this paper will inspire more research on deep learning approaches applied to a wide range of audio recognition tasks .
learning linear combinations of multiple kernels is an appealing strategy when the right choice of features is unknown . <eos> previous approaches to multiple kernel learning ( mkl ) promote sparse kernel combinations and hence support interpretability . <eos> unfortunately , l1-norm mkl is hardly observed to outperform trivial baselines in practical applications . <eos> to allow for robust kernel mixtures , we generalize mkl to arbitrary lp-norms . <eos> we devise new insights on the connection between several existing mkl formulations and develop two efficient interleaved optimization strategies for arbitrary p > 1 . <eos> empirically , we demonstrate that the interleaved optimization strategies are much faster compared to the traditionally used wrapper approaches . <eos> finally , we apply lp-norm mkl to real-world problems from computational biology , showing that non-sparse mkl achieves accuracies that go beyond the state-of-the-art .
we prove strong noise-tolerance properties of a potential-based boosting algorithm , similar to madaboost ( domingo and watanabe , 2000 ) and smoothboost ( servedio , 2003 ) . <eos> our analysis is in the agnostic framework of kearns , schapire and sellie ( 1994 ) , giving polynomial-time guarantees in presence of arbitrary noise . <eos> a remarkable feature of our algorithm is that it can be implemented without reweighting examples , by randomly relabeling them instead . <eos> our boosting theorem gives , as easy corollaries , alternative derivations of two recent non-trivial results in computational learning theory : agnostically learning decision trees ( gopalan et al , 2008 ) and agnostically learning halfspaces ( kalai et al , 2005 ) . <eos> experiments suggest that the algorithm performs similarly to madaboost .
we propose a new approach to the problem of robust estimation in multiview geometry . <eos> inspired by recent advances in the sparse recovery problem of statistics , our estimator is defined as a bayesian maximum a posteriori with multivariate laplace prior on the vector describing the outliers . <eos> this leads to an estimator in which the fidelity to the data is measured by the $ l_\infty $ -norm while the regularization is done by the $ l_1 $ -norm . <eos> the proposed procedure is fairly fast since the outlier removal is done by solving one linear program ( lp ) . <eos> an important difference compared to existing algorithms is that for our estimator it is not necessary to specify neither the number nor the proportion of the outliers . <eos> the theoretical results , as well as the numerical example reported in this work , confirm the efficiency of the proposed approach .
learning distance functions with side information plays a key role in many machine learning and data mining applications . <eos> conventional approaches often assume a mahalanobis distance function . <eos> these approaches are limited in two aspects : ( i ) they are computationally expensive ( even infeasible ) for high dimensional data because the size of the metric is in the square of dimensionality ; ( ii ) they assume a fixed metric for the entire input space and therefore are unable to handle heterogeneous data . <eos> in this paper , we propose a novel scheme that learns nonlinear bregman distance functions from side information using a non-parametric approach that is similar to support vector machines . <eos> the proposed scheme avoids the assumption of fixed metric because its local distance metric is implicitly derived from the hessian matrix of a convex function that is used to generate the bregman distance function . <eos> we present an efficient learning algorithm for the proposed scheme for distance function learning . <eos> the extensive experiments with semi-supervised clustering show the proposed technique ( i ) outperforms the state-of-the-art approaches for distance function learning , and ( ii ) is computationally efficient for high dimensional data .
in this paper we address the problem of provably correct feature selection in arbitrary domains . <eos> an optimal solution to the problem is a markov boundary , which is a minimal set of features that make the probability distribution of a target variable conditionally invariant to the state of all other features in the domain . <eos> while numerous algorithms for this problem have been proposed , their theoretical correctness and practical behavior under arbitrary probability distributions is unclear . <eos> we address this by introducing the markov boundary theorem that precisely characterizes the properties of an ideal markov boundary , and use it to develop algorithms that learn a more general boundary that can capture complex interactions that only appear when the values of multiple features are considered together . <eos> we introduce two algorithms : an exact , provably correct one as well a more practical randomized anytime version , and show that they perform well on artificial as well as benchmark and real-world data sets . <eos> throughout the paper we make minimal assumptions that consist of only a general set of axioms that hold for every probability distribution , which gives these algorithms universal applicability .
this paper proposes a fast and scalable alternating optimization technique to detect regions of interest ( rois ) in cluttered web images without labels . <eos> the proposed approach discovers highly probable regions of object instances by iteratively repeating the following two functions : ( 1 ) choose the exemplar set ( i.e . small number of high ranked reference rois ) across the dataset and ( 2 ) refine the rois of each image with respect to the exemplar set . <eos> these two subproblems are formulated as ranking in two different similarity networks of roi hypotheses by link analysis . <eos> the experiments with the pascal 06 dataset show that our unsupervised localization performance is better than one of state-of-the-art techniques and comparable to supervised methods . <eos> also , we test the scalability of our approach with five objects in flickr dataset consisting of more than 200,000 images .
finding maximally sparse representations from overcomplete feature dictionaries frequently involves minimizing a cost function composed of a likelihood ( or data fit ) term and a prior ( or penalty function ) that favors sparsity . <eos> while typically the prior is factorial , here we examine non-factorial alternatives that have a number of desirable properties relevant to sparse estimation and are easily implemented using an efficient , globally-convergent reweighted $ \ell_1 $ minimization procedure . <eos> the first method under consideration arises from the sparse bayesian learning ( sbl ) framework . <eos> although based on a highly non-convex underlying cost function , in the context of canonical sparse estimation problems , we prove uniform superiority of this method over the lasso in that , ( i ) it can never do worse , and ( ii ) for any dictionary and sparsity profile , there will always exist cases where it does better . <eos> these results challenge the prevailing reliance on strictly convex penalty functions for finding sparse solutions . <eos> we then derive a new non-factorial variant with similar properties that exhibits further performance improvements in empirical tests . <eos> for both of these methods , as well as traditional factorial analogs , we demonstrate the effectiveness of reweighted $ \ell_1 $ -norm algorithms in handling more general sparse estimation problems involving classification , group feature selection , and non-negativity constraints . <eos> as a byproduct of this development , a rigorous reformulation of sparse bayesian classification ( e.g. , the relevance vector machine ) is derived that , unlike the original , involves no approximation steps and descends a well-defined objective function .
since the development of loopy belief propagation , there has been considerable work on advancing the state of the art for approximate inference over distributions defined on discrete random variables . <eos> improvements include guarantees of convergence , approximations that are provably more accurate , and bounds on the results of exact inference . <eos> however , extending these methods to continuous-valued systems has lagged behind . <eos> while several methods have been developed to use belief propagation on systems with continuous values , they have not as yet incorporated the recent advances for discrete variables . <eos> in this context we extend a recently proposed particle-based belief propagation algorithm to provide a general framework for adapting discrete message-passing algorithms to perform inference in continuous systems . <eos> the resulting algorithms behave similarly to their purely discrete counterparts , extending the benefits of these more advanced inference techniques to the continuous domain .
we introduce skill chaining , a skill discovery method for reinforcement learning agents in continuous domains , that builds chains of skills leading to an end-of-task reward . <eos> we demonstrate experimentally that it creates skills that result in performance benefits in a challenging continuous domain .
in this paper , we study the manifold regularization for the sliced inverse regression ( sir ) . <eos> the manifold regularization improves the standard sir in two aspects : 1 ) it encodes the local geometry for sir and 2 ) it enables sir to deal with transductive and semi-supervised learning problems . <eos> we prove that the proposed graph laplacian based regularization is convergent at rate root-n . <eos> the projection directions of the regularized sir are optimized by using a conjugate gradient method on the grassmann manifold . <eos> experimental results support our theory .
we propose a bayesian nonparametric approach to relating multiple time series via a set of latent , dynamical behaviors . <eos> using a beta process prior , we allow data-driven selection of the size of this set , as well as the pattern with which behaviors are shared among time series . <eos> via the indian buffet process representation of the beta process predictive distributions , we develop an exact markov chain monte carlo inference method . <eos> in particular , our approach uses the sum-product algorithm to efficiently compute metropolis-hastings acceptance probabilities , and explores new dynamical behaviors via birth/death proposals . <eos> we validate our sampling algorithm using several synthetic datasets , and also demonstrate promising unsupervised segmentation of visual motion capture data .
when used to guide decisions , linear regression analysis typically involves estimation of regression coefficients via ordinary least squares and their subsequent use to make decisions . <eos> when there are multiple response variables and features do not perfectly capture their relationships , it is beneficial to account for the decision objective when computing regression coefficients . <eos> empirical optimization does so but sacrifices performance when features are well-chosen or training data are insufficient . <eos> we propose directed regression , an efficient algorithm that combines merits of ordinary least squares and empirical optimization . <eos> we demonstrate through a computational study that directed regression can generate significant performance gains over either alternative . <eos> we also develop a theory that motivates the algorithm .
dynamic bayesian networks have been applied widely to reconstruct the structure of regulatory processes from time series data . <eos> the standard approach is based on the assumption of a homogeneous markov chain , which is not valid in many real-world scenarios . <eos> recent research efforts addressing this shortcoming have considered undirected graphs , directed graphs for discretized data , or over-flexible models that lack any information sharing between time series segments . <eos> in the present article , we propose a non-stationary dynamic bayesian network for continuous data , in which parameters are allowed to vary between segments , and in which a common network structure provides essential information sharing across segments . <eos> our model is based on a bayesian change-point process , and we apply a variant of the allocation sampler of nobile and fearnside to infer the number and location of the change-points .
this paper uses information-theoretic techniques to determine minimax rates for estimating nonparametric sparse additive regression models under high-dimensional scaling . <eos> we assume an additive decomposition of the form $ f^* ( x_1 , \ldots , x_p ) = \sum_ { j \in s } h_j ( x_j ) $ , where each component function $ h_j $ lies in some hilbert space $ \hilb $ and $ s \subset \ { 1 , \ldots , \pdim \ } $ is an unknown subset with cardinality $ \s = |s $ . <eos> given $ \numobs $ i.i.d . <eos> observations of $ f^* ( x ) $ corrupted with white gaussian noise where the covariate vectors $ ( x_1 , x_2 , x_3 , ... , x_ { \pdim } ) $ are drawn with i.i.d . <eos> components from some distribution $ \mp $ , we determine tight lower bounds on the minimax rate for estimating the regression function with respect to squared $ \ltp $ error . <eos> the main result shows that the minimax rates are $ \max { \big ( \frac { \s \log \pdim / \s } { n } , \lowerratesq \big ) } $ . <eos> the first term reflects the difficulty of performing \emph { subset selection } and is independent of the hilbert space $ \hilb $ ; the second term $ \lowerratesq $ is an \emph { \s-dimensional estimation } term , depending only on the low dimension $ \s $ but not the ambient dimension $ \pdim $ , that captures the difficulty of estimating a sum of $ \s $ univariate functions in the hilbert space $ \hilb $ . <eos> as a special case , if $ \hilb $ corresponds to the $ \m $ -th order sobolev space $ \sobm $ of functions that are $ m $ -times differentiable , the $ \s $ -dimensional estimation term takes the form $ \lowerratesq \asymp \s \ ; n^ { -2\m/ ( 2\m+1 ) } $ . <eos> the minimax rates are compared with rates achieved by an $ \ell_1 $ -penalty based approach , it can be shown that a certain $ \ell_1 $ -based approach achieves the minimax optimal rate .
despite the large amount of literature on upper bounds on complexity of convex analysis , surprisingly little is known about the fundamental hardness of these problems . <eos> the extensive use of convex optimization in machine learning and statistics makes such an understanding critical to understand fundamental computational limits of learning and estimation . <eos> in this paper , we study the complexity of stochastic convex optimization in an oracle model of computation . <eos> we improve upon known results and obtain tight minimax complexity estimates for some function classes . <eos> we also discuss implications of these results to the understanding the inherent complexity of large-scale learning and estimation problems .
we address the problem of learning classifiers when observations have multiple views , some of which may not be observed for all examples . <eos> we assume the existence of view generating functions which may complete the missing views in an approximate way . <eos> this situation corresponds for example to learning text classifiers from multilingual collections where documents are not available in all languages . <eos> in that case , machine translation ( mt ) systems may be used to translate each document in the missing languages . <eos> we derive a generalization error bound for classifiers learned on examples with multiple artificially created views . <eos> our result uncovers a trade-off between the size of the training set , the number of views , and the quality of the view generating functions . <eos> as a consequence , we identify situations where it is more interesting to use multiple views for learning instead of classical single view learning . <eos> an extension of this framework is a natural way to leverage unlabeled multi-view data in semi-supervised learning . <eos> experimental results on a subset of the reuters rcv1/rcv2 collections support our findings by showing that additional views obtained from mt may significantly improve the classification performance in the cases identified by our trade-off .
bag-of-words document representations are often used in text , image and video processing . <eos> while it is relatively easy to determine a suitable word dictionary for text documents , there is no simple mapping from raw images or videos to dictionary terms . <eos> the classical approach builds a dictionary using vector quantization over a large set of useful visual descriptors extracted from a training set , and uses a nearest-neighbor algorithm to count the number of occurrences of each dictionary word in documents to be encoded . <eos> more robust approaches have been proposed recently that represent each visual descriptor as a sparse weighted combination of dictionary words . <eos> while favoring a sparse representation at the level of visual descriptors , those methods however do not ensure that images have sparse representation . <eos> in this work , we use mixed-norm regularization to achieve sparsity at the image level as well as a small overall dictionary . <eos> this approach can also be used to encourage using the same dictionary words for all the images in a class , providing a discriminative signal in the construction of image representations . <eos> experimental results on a benchmark image classification dataset show that when compact image or dictionary representations are needed for computational efficiency , the proposed approach yields better mean average precision in classification .
this paper studies the general problem of learning kernels based on a polynomial combination of base kernels . <eos> it analyzes this problem in the case of regression and the kernel ridge regression algorithm . <eos> it examines the corresponding learning kernel optimization problem , shows how that minimax problem can be reduced to a simpler minimization problem , and proves that the global solution of this problem always lies on the boundary . <eos> we give a projection-based gradient descent algorithm for solving the optimization problem , shown empirically to converge in few iterations . <eos> finally , we report the results of extensive experiments with this algorithm using several publicly available datasets demonstrating the effectiveness of our technique .
many types of regularization schemes have been employed in statistical learning , each one motivated by some assumption about the problem domain . <eos> in this paper , we present a unified asymptotic analysis of smooth regularizers , which allows us to see how the validity of these assumptions impacts the success of a particular regularizer . <eos> in addition , our analysis motivates an algorithm for optimizing regularization parameters , which in turn can be analyzed within our framework . <eos> we apply our analysis to several examples , including hybrid generative-discriminative learning and multi-task learning .
we present a new empirical risk minimization framework for approximating functions from training samples for low-dimensional regression applications where a lattice ( look-up table ) is stored and interpolated at run-time for an efficient hardware implementation . <eos> rather than evaluating a fitted function at the lattice nodes without regard to the fact that samples will be interpolated , the proposed lattice regression approach estimates the lattice to minimize the interpolation error on the given training samples . <eos> experiments show that lattice regression can reduce mean test error compared to gaussian process regression for digital color management of printers , an application for which linearly interpolating a look-up table ( lut ) is standard . <eos> simulations confirm that lattice regression performs consistently better than the naive approach to learning the lattice , particularly when the density of training samples is low .
studying signal and noise properties of recorded neural data is critical in developing more efficient algorithms to recover the encoded information . <eos> important issues exist in this research including the variant spectrum spans of neural spikes that make it difficult to choose a global optimal bandpass filter . <eos> also , multiple sources produce aggregated noise that deviates from the conventional white gaussian noise . <eos> in this work , the spectrum variability of spikes is addressed , based on which the concept of adaptive bandpass filter that fits the spectrum of individual spikes is proposed . <eos> multiple noise sources have been studied through analytical models as well as empirical measurements . <eos> the dominant noise source is identified as neuron noise followed by interface noise of the electrode . <eos> this suggests that major efforts to reduce noise from electronics are not well spent . <eos> the measured noise from in vivo experiments shows a family of 1/f^ { x } ( x=1.5\pm 0.5 ) spectrum that can be reduced using noise shaping techniques . <eos> in summary , the methods of adaptive bandpass filtering and noise shaping together result in several db signal-to-noise ratio ( snr ) enhancement .
by adding a spatial regularization kernel to a standard loss function formulation of the boosting problem , we develop a framework for spatially informed boosting . <eos> from this regularized loss framework we derive an efficient boosting algorithm that uses additional weights/priors on the base classifiers . <eos> we prove that the proposed algorithm exhibits a `` grouping effect , which encourages the selection of all spatially local , discriminative base classifiers . <eos> the algorithms primary advantage is in applications where the trained classifier is used to identify the spatial pattern of discriminative information , e.g . the voxel selection problem in fmri . <eos> we demonstrate the algorithms performance on various data sets .
given $ n $ noisy samples with $ p $ dimensions , where $ n \ll p $ , we show that the multi-stage thresholding procedures can accurately estimate a sparse vector $ \beta \in \r^p $ in a linear model , under the restricted eigenvalue conditions ( bickel-ritov-tsybakov 09 ) . <eos> thus our conditions for model selection consistency are considerably weaker than what has been achieved in previous works . <eos> more importantly , this method allows very significant values of $ s $ , which is the number of non-zero elements in the true parameter $ \beta $ . <eos> for example , it works for cases where the ordinary lasso would have failed . <eos> finally , we show that if $ x $ obeys a uniform uncertainty principle and if the true parameter is sufficiently sparse , the gauss-dantzig selector ( cand\ { e } s-tao 07 ) achieves the $ \ell_2 $ loss within a logarithmic factor of the ideal mean square error one would achieve with an oracle which would supply perfect information about which coordinates are non-zero and which are above the noise level , while selecting a sufficiently sparse model .
we consider the problem of learning , from k input data , a regression function in a function space of high dimension n using projections onto a random subspace of lower dimension m. from any linear approximation algorithm using empirical risk minimization ( possibly penalized ) , we provide bounds on the excess risk of the estimate computed in the projected subspace ( compressed domain ) in terms of the excess risk of the estimate built in the high-dimensional space ( initial domain ) . <eos> we apply the analysis to the ordinary least-squares regression and show that by choosing m=o ( \sqrt { k } ) , the estimation error ( for the quadratic loss ) of the `` compressed least squares regression is o ( 1/\sqrt { k } ) up to logarithmic factors . <eos> we also discuss the numerical complexity of several algorithms ( both in initial and compressed domains ) as a function of n , k , and m .
the recently proposed \emph { additive noise model } has advantages over previous structure learning algorithms , when attempting to recover some true data generating mechanism , since it ( i ) does not assume linearity or gaussianity and ( ii ) can recover a unique dag rather than an equivalence class . <eos> however , its original extension to the multivariate case required enumerating all possible dags , and for some special distributions , e.g . linear gaussian , the model is invertible and thus can not be used for structure learning . <eos> we present a new approach which combines a pc style search using recent advances in kernel measures of conditional dependence with local searches for additive noise models in substructures of the equivalence class . <eos> this results in a more computationally efficient approach that is useful for arbitrary distributions even when additive noise models are invertible . <eos> experiments with synthetic and real data show that this method is more accurate than previous methods when data are nonlinear and/or non-gaussian .
probabilistic topic models are a popular tool for the unsupervised analysis of text , providing both a predictive model of future text and a latent topic representation of the corpus . <eos> practitioners typically assume that the latent space is semantically meaningful . <eos> it is used to check models , summarize the corpus , and guide exploration of its contents . <eos> however , whether the latent space is interpretable is in need of quantitative evaluation . <eos> in this paper , we present new quantitative methods for measuring semantic meaning in inferred topics . <eos> we back these measures with large-scale user studies , showing that they capture aspects of the model that are undetected by previous measures of model quality based on held-out likelihood . <eos> surprisingly , topic models which perform better on held-out likelihood may infer less semantically meaningful topics .
we study unsupervised learning in a probabilistic generative model for occlusion . <eos> the model uses two types of latent variables : one indicates which objects are present in the image , and the other how they are ordered in depth . <eos> this depth order then determines how the positions and appearances of the objects present , specified in the model parameters , combine to form the image . <eos> we show that the object parameters can be learnt from an unlabelled set of images in which objects occlude one another . <eos> exact maximum-likelihood learning is intractable . <eos> however , we show that tractable approximations to expectation maximization ( em ) can be found if the training images each contain only a small number of objects on average . <eos> in numerical experiments it is shown that these approximations recover the correct set of object parameters . <eos> experiments on a novel version of the bars test using colored bars , and experiments on more realistic data , show that the algorithm performs well in extracting the generating causes . <eos> experiments based on the standard bars benchmark test for object learning show that the algorithm performs well in comparison to other recent component extraction approaches . <eos> the model and the learning algorithm thus connect research on occlusion with the research field of multiple-cause component extraction methods .
it has been established that the second-order stochastic gradient descent ( 2sgd ) method can potentially achieve generalization performance as well as empirical optimum in a single pass ( i.e. , epoch ) through the training examples . <eos> however , 2sgd requires computing the inverse of the hessian matrix of the loss function , which is prohibitively expensive . <eos> this paper presents periodic step-size adaptation ( psa ) , which approximates the jacobian matrix of the mapping function and explores a linear relation between the jacobian and hessian to approximate the hessian periodically and achieve near-optimal results in experiments on a wide variety of models and tasks .
in this paper , we examine the generalization error of regularized distance metric learning . <eos> we show that with appropriate constraints , the generalization error of regularized distance metric learning could be independent from the dimensionality , making it suitable for handling high dimensional data . <eos> in addition , we present an efficient online learning algorithm for regularized distance metric learning . <eos> our empirical studies with data classification and face recognition show that the proposed algorithm is ( i ) effective for distance metric learning when compared to the state-of-the-art methods , and ( ii ) efficient and robust for high dimensional data .
principal component analysis is a fundamental operation in computational data analysis , with myriad applications ranging from web search to bioinformatics to computer vision and image analysis . <eos> however , its performance and applicability in real scenarios are limited by a lack of robustness to outlying or corrupted observations . <eos> this paper considers the idealized ? robust principal component analysis ? <eos> problem of recovering a low rank matrix a from corrupted observations d = a + e. here , the error entries e can be arbitrarily large ( modeling grossly corrupted observations common in visual and bioinformatic data ) , but are assumed to be sparse . <eos> we prove that most matrices a can be efficiently and exactly recovered from most error sign-and-support patterns , by solving a simple convex program . <eos> our result holds even when the rank of a grows nearly proportionally ( up to a logarithmic factor ) to the dimensionality of the observation space and the number of errors e grows in proportion to the total number of entries in the matrix . <eos> a by-product of our analysis is the first proportional growth results for the related problem of completing a low-rank matrix from a small fraction of its entries . <eos> simulations and real-data examples corroborate the theoretical results , and suggest potential applications in computer vision .
learning a measure of similarity between pairs of objects is a fundamental problem in machine learning . <eos> it stands in the core of classification methods like kernel machines , and is particularly useful for applications like searching for images that are similar to a given image or finding videos that are relevant to a given video . <eos> in these tasks , users look for objects that are not only visually similar but also semantically related to a given object . <eos> unfortunately , current approaches for learning similarity may not scale to large datasets with high dimensionality , especially when imposing metric constraints on the learned similarity . <eos> we describe oasis , a method for learning pairwise similarity that is fast and scales linearly with the number of objects and the number of non-zero features . <eos> scalability is achieved through online learning of a bilinear model over sparse representations using a large margin criterion and an efficient hinge loss cost . <eos> oasis is accurate at a wide range of scales : on a standard benchmark with thousands of images , it is more precise than state-of-the-art methods , and faster by orders of magnitude . <eos> on 2 million images collected from the web , oasis can be trained within 3 days on a single cpu . <eos> the non-metric similarities learned by oasis can be transformed into metric similarities , achieving higher precisions than similarities that are learned as metrics in the first place . <eos> this suggests an approach for learning a metric from data that is larger by an order of magnitude than was handled before .
multitask learning addressed the problem of learning related tasks whose information can be shared each other . <eos> traditional problem usually deal with homogeneous tasks such as regression , classification individually . <eos> in this paper we consider the problem learning multiple related tasks where tasks consist of both continuous and discrete outputs from a common set of input variables that lie in a high-dimensional space . <eos> all of the tasks are related in the sense that they share the same set of relevant input variables , but the amount of influence of each input on different outputs may vary . <eos> we formulate this problem as a combination of linear regression and logistic regression and model the joint sparsity as l1/linf and l1/l2-norm of the model parameters . <eos> among several possible applications , our approach addresses an important open problem in genetic association mapping , where we are interested in discovering genetic markers that influence multiple correlated traits jointly . <eos> in our experiments , we demonstrate our method in the scenario of association mapping , using simulated and asthma data , and show that the algorithm can effectively recover the relevant inputs with respect to all of the tasks .
the heavy-tailed distribution of gradients in natural scenes have proven effective priors for a range of problems such as denoising , deblurring and super-resolution . <eos> however , the use of sparse distributions makes the problem non-convex and impractically slow to solve for multi-megapixel images . <eos> in this paper we describe a deconvolution approach that is several orders of magnitude faster than existing techniques that use hyper-laplacian priors . <eos> we adopt an alternating minimization scheme where one of the two phases is a non-convex problem that is separable over pixels . <eos> this per-pixel sub-problem may be solved with a lookup table ( lut ) . <eos> alternatively , for two specific values of ? , 1/2 and 2/3 an analytic solution can be found , by finding the roots of a cubic and quartic polynomial , respectively . <eos> our approach ( using either luts or analytic formulae ) is able to deconvolve a 1 megapixel image in less than ? 3 seconds , achieving comparable quality to existing methods such as iteratively reweighted least squares ( irls ) that take ? 20 minutes . <eos> furthermore , our method is quite general and can easily be extended to related image processing problems , beyond the deconvolution application demonstrated .
learning to rank has become an important research topic in machine learning . <eos> while most learning-to-rank methods learn the ranking function by minimizing the loss functions , it is the ranking measures ( such as ndcg and map ) that are used to evaluate the performance of the learned ranking function . <eos> in this work , we reveal the relationship between ranking measures and loss functions in learning-to-rank methods , such as ranking svm , rankboost , ranknet , and listmle . <eos> we show that these loss functions are upper bounds of the measure-based ranking errors . <eos> as a result , the minimization of these loss functions will lead to the maximization of the ranking measures . <eos> the key to obtaining this result is to model ranking as a sequence of classification tasks , and define a so-called essential loss as the weighted sum of the classification errors of individual tasks in the sequence . <eos> we have proved that the essential loss is both an upper bound of the measure-based ranking errors , and a lower bound of the loss functions in the aforementioned methods . <eos> our proof technique also suggests a way to modify existing loss functions to make them tighter bounds of the measure-based ranking errors . <eos> experimental results on benchmark datasets show that the modifications can lead to better ranking performance , demonstrating the correctness of our analysis .
kernel learning is a powerful framework for nonlinear data modeling . <eos> using the kernel trick , a number of problems have been formulated as semidefinite programs ( sdps ) . <eos> these include maximum variance unfolding ( mvu ) ( weinberger et al. , 2004 ) in nonlinear dimensionality reduction , and pairwise constraint propagation ( pcp ) ( li et al. , 2008 ) in constrained clustering . <eos> although in theory sdps can be efficiently solved , the high computational complexity incurred in numerically processing the huge linear matrix inequality constraints has rendered the sdp approach unscalable . <eos> in this paper , we show that a large class of kernel learning problems can be reformulated as semidefinite-quadratic-linear programs ( sqlps ) , which only contain a simple positive semidefinite constraint , a second-order cone constraint and a number of linear constraints . <eos> these constraints are much easier to process numerically , and the gain in speedup over previous approaches is at least of the order $ m^ { 2.5 } $ , where m is the matrix dimension . <eos> experimental results are also presented to show the superb computational efficiency of our approach .
pruning can massively accelerate the computation of feature expectations in large models . <eos> however , any single pruning mask will introduce bias . <eos> we present a novel approach which employs a randomized sequence of pruning masks . <eos> formally , we apply auxiliary variable mcmc sampling to generate this sequence of masks , thereby gaining theoretical guarantees about convergence . <eos> because each mask is generally able to skip large portions of an underlying dynamic program , our approach is particularly compelling for high-degree algorithms . <eos> empirically , we demonstrate our method on bilingual parsing , showing decreasing bias as more masks are incorporated , and outperforming fixed tic-tac-toe pruning .
while many perceptual and cognitive phenomena are well described in terms of bayesian inference , the necessary computations are intractable at the scale of real-world tasks , and it remains unclear how the human mind approximates bayesian inference algorithmically . <eos> we explore the proposal that for some tasks , humans use a form of markov chain monte carlo to approximate the posterior distribution over hidden variables . <eos> as a case study , we show how several phenomena of perceptual multistability can be explained as mcmc inference in simple graphical models for low-level vision .
we show how to sequentially optimize magnetic resonance imaging measurement designs over stacks of neighbouring image slices , by performing convex variational inference on a large scale non-gaussian linear dynamical system , tracking dominating directions of posterior covariance without imposing any factorization constraints . <eos> our approach can be scaled up to high-resolution images by reductions to numerical mathematics primitives and parallelization on several levels . <eos> in a first study , designs are found that improve significantly on others chosen independently for each slice or drawn at random .
sequential decision-making with multiple agents and imperfect information is commonly modeled as an extensive game . <eos> one efficient method for computing nash equilibria in large , zero-sum , imperfect information games is counterfactual regret minimization ( cfr ) . <eos> in the domain of poker , cfr has proven effective , particularly when using a domain-specific augmentation involving chance outcome sampling . <eos> in this paper , we describe a general family of domain independent cfr sample-based algorithms called monte carlo counterfactual regret minimization ( mccfr ) of which the original and poker-specific versions are special cases . <eos> we start by showing that mccfr performs the same regret updates as cfr on expectation . <eos> then , we introduce two sampling schemes : { \it outcome sampling } and { \it external sampling } , showing that both have bounded overall regret with high probability . <eos> thus , they can compute an approximate equilibrium using self-play . <eos> finally , we prove a new tighter bound on the regret for the original cfr algorithm and relate this new bound to mccfrs bounds . <eos> we show empirically that , although the sample-based algorithms require more iterations , their lower cost per iteration can lead to dramatically faster convergence in various games .
hypergraph clustering refers to the process of extracting maximally coherent groups from a set of objects using high-order ( rather than pairwise ) similarities . <eos> traditional approaches to this problem are based on the idea of partitioning the input data into a user-defined number of classes , thereby obtaining the clusters as a by-product of the partitioning process . <eos> in this paper , we provide a radically different perspective to the problem . <eos> in contrast to the classical approach , we attempt to provide a meaningful formalization of the very notion of a cluster and we show that game theory offers an attractive and unexplored perspective that serves well our purpose . <eos> specifically , we show that the hypergraph clustering problem can be naturally cast into a non-cooperative multi-player `` clustering game , whereby the notion of a cluster is equivalent to a classical game-theoretic equilibrium concept . <eos> from the computational viewpoint , we show that the problem of finding the equilibria of our clustering game is equivalent to locally optimizing a polynomial function over the standard simplex , and we provide a discrete-time dynamics to perform this optimization . <eos> experiments are presented which show the superiority of our approach over state-of-the-art hypergraph clustering techniques .
we develop a structured output model for object category detection that explicitly accounts for alignment , multiple aspects and partial truncation in both training and inference . <eos> the model is formulated as large margin learning with latent variables and slack rescaling , and both training and inference are computationally efficient . <eos> we make the following contributions : ( i ) we note that extending the structured output regression formulation of blaschko and lampert ( eccv 2008 ) to include a bias term significantly improves performance ; ( ii ) that alignment ( to account for small rotations and anisotropic scalings ) can be included as a latent variable and efficiently determined and implemented ; ( iii ) that the latent variable extends to multiple aspects ( e.g . left facing , right facing , front ) with the same formulation ; and ( iv ) , most significantly for performance , that truncated and truncated instances can be included in both training and inference with an explicit truncation mask . <eos> we demonstrate the method by training and testing on the pascal voc 2007 data set -- training includes the truncated examples , and in testing object instances are detected at multiple scales , alignments , and with significant truncations .
directed graphical models such as bayesian networks are a favored formalism to model the dependency structures in complex multivariate systems such as those encountered in biology and neural sciences . <eos> when the system is undergoing dynamic transformation , often a temporally rewiring network is needed for capturing the dynamic causal influences between covariates . <eos> in this paper , we propose a time-varying dynamic bayesian network ( tv-dbn ) for modeling the structurally varying directed dependency structures underlying non-stationary biological/neural time series . <eos> this is a challenging problem due the non-stationarity and sample scarcity of the time series . <eos> we present a kernel reweighted $ \ell_1 $ regularized auto-regressive procedure for learning the tv-dbn model . <eos> our method enjoys nice properties such as computational efficiency and provable asymptotic consistency . <eos> applying tv-dbn to time series measurements during yeast cell cycle and brain response to visual stimuli reveals interesting dynamics underlying the respective biological systems .
markov random fields ( mrfs ) , or undirected graphical models , provide a powerful framework for modeling complex dependencies among random variables . <eos> maximum likelihood learning in mrfs is hard due to the presence of the global normalizing constant . <eos> in this paper we consider a class of stochastic approximation algorithms of robbins-monro type that uses markov chain monte carlo to do approximate maximum likelihood learning . <eos> we show that using mcmc operators based on tempered transitions enables the stochastic approximation algorithm to better explore highly multimodal distributions , which considerably improves parameter estimates in large densely-connected mrfs . <eos> our results on mnist and norb datasets demonstrate that we can successfully learn good generative models of high-dimensional , richly structured data and perform well on digit and object recognition tasks .
in this paper , we investigate how similar images sharing the same global description can help with unsupervised scene segmentation in an image . <eos> in contrast to recent work in semantic alignment of scenes , we allow an input image to be explained by partial matches of similar scenes . <eos> this allows for a better explanation of the input scenes . <eos> we perform mrf-based segmentation that optimizes over matches , while respecting boundary information . <eos> the recovered segments are then used to re-query a large database of images to retrieve better matches for the target region . <eos> we show improved performance in detecting occluding boundaries over previous methods on data gathered from the labelme database .
which ads should we display in sponsored search in order to maximize our revenue ? <eos> how should we dynamically rank information sources to maximize value of information ? <eos> these applications exhibit strong diminishing returns : selection of redundant ads and information sources decreases their marginal utility . <eos> we show that these and other problems can be formalized as repeatedly selecting an assignment of items to positions to maximize a sequence of monotone submodular functions that arrive one by one . <eos> we present an efficient algorithm for this general problem and analyze it in the no-regret model . <eos> our algorithm is equipped with strong theoretical guarantees , with a performance ratio that converges to the optimal constant of 1-1/e . <eos> we empirically evaluate our algorithms on two real-world online optimization problems on the web : ad allocation with submodular utilities , and dynamically ranking blogs to detect information cascades .
we develop a probabilistic model of human memory performance in free recall experiments . <eos> in these experiments , a subject first studies a list of words and then tries to recall them . <eos> to model these data , we draw on both previous psychological research and statistical topic models of text documents . <eos> we assume that memories are formed by assimilating the semantic meaning of studied words ( represented as a distribution over topics ) into a slowly changing latent context ( represented in the same space ) . <eos> during recall , this context is reinstated and used as a cue for retrieving studied words . <eos> by conceptualizing memory retrieval as a dynamic latent variable model , we are able to use bayesian inference to represent uncertainty and reason about the cognitive processes underlying memory . <eos> we present a particle filter algorithm for performing approximate posterior inference , and evaluate our model on the prediction of recalled words in experimental data . <eos> by specifying the model hierarchically , we are also able to capture inter-subject variability .
this paper addresses the problem of noisy generalized binary search ( gbs ) . <eos> gbs is a well-known greedy algorithm for determining a binary-valued hypothesis through a sequence of strategically selected queries . <eos> at each step , a query is selected that most evenly splits the hypotheses under consideration into two disjoint subsets , a natural generalization of the idea underlying classic binary search . <eos> gbs is used in many applications , including fault testing , machine diagnostics , disease diagnosis , job scheduling , image processing , computer vision , and active learning . <eos> in most of these cases , the responses to queries can be noisy . <eos> past work has provided a partial characterization of gbs , but existing noise-tolerant versions of gbs are suboptimal in terms of sample complexity . <eos> this paper presents the first optimal algorithm for noisy gbs and demonstrates its application to learning multidimensional threshold functions .
in this paper we introduce a new algorithm for updating the parameters of a heuristic evaluation function , by updating the heuristic towards the values computed by an alpha-beta search . <eos> our algorithm differs from previous approaches to learning from search , such as samuels checkers player and the td-leaf algorithm , in two key ways . <eos> first , we update all nodes in the search tree , rather than a single node . <eos> second , we use the outcome of a deep search , instead of the outcome of a subsequent search , as the training signal for the evaluation function . <eos> we implemented our algorithm in a chess program meep , using a linear heuristic function . <eos> after initialising its weight vector to small random values , meep was able to learn high quality weights from self-play alone . <eos> when tested online against human opponents , meep played at a master level , the best performance of any chess program with a heuristic learned entirely from self-play .
we propose a novel non-parametric adaptive anomaly detection algorithm for high dimensional data based on score functions derived from nearest neighbor graphs on n-point nominal data . <eos> anomalies are declared whenever the score of a test sample falls below q , which is supposed to be the desired false alarm level . <eos> the resulting anomaly detector is shown to be asymptotically optimal in that it is uniformly most powerful for the specified false alarm level , q , for the case when the anomaly density is a mixture of the nominal and a known density . <eos> our algorithm is computationally efficient , being linear in dimension and quadratic in data size . <eos> it does not require choosing complicated tuning parameters or function approximation classes and it can adapt to local structure such as local change in dimensionality . <eos> we demonstrate the algorithm on both artificial and real data sets in high dimensional feature spaces .
we present a novel feature selection algorithm for the $ k $ -means clustering problem . <eos> our algorithm is randomized and , assuming an accuracy parameter $ \epsilon \in ( 0,1 ) $ , selects and appropriately rescales in an unsupervised manner $ \theta ( k \log ( k / \epsilon ) / \epsilon^2 ) $ features from a dataset of arbitrary dimensions . <eos> we prove that , if we run any $ \gamma $ -approximate $ k $ -means algorithm ( $ \gamma \geq 1 $ ) on the features selected using our method , we can find a $ ( 1+ ( 1+\epsilon ) \gamma ) $ -approximate partition with high probability .
situations in which people with opposing prior beliefs observe the same evidence and then strengthen those existing beliefs are frequently offered as evidence of human irrationality . <eos> this phenomenon , termed belief polarization , is typically assumed to be non-normative . <eos> we demonstrate , however , that a variety of cases of belief polarization are consistent with a bayesian approach to belief revision . <eos> simulation results indicate that belief polarization is not only possible but relatively common within the class of bayesian models that we consider .
we extend the concept of phase tuning , a ubiquitous mechanism in sensory neurons including motion and disparity detection neurons , to the motion contrast detection . <eos> we demonstrate that motion contrast can be detected by phase shifts between motion neuronal responses in different spatial regions . <eos> by constructing the differential motion opponency in response to motions in two different spatial regions , varying motion contrasts can be detected , where similar motion is detected by zero phase shifts and differences in motion by non-zero phase shifts . <eos> the model can exhibit either enhancement or suppression of responses by either different or similar motion in the surrounding . <eos> a primary advantage of the model is that the responses are selective to relative motion instead of absolute motion , which could model neurons found in neurophysiological experiments responsible for motion pop-out detection .
when individuals independently recollect events or retrieve facts from memory , how can we aggregate these retrieved memories to reconstruct the actual set of events or facts ? <eos> in this research , we report the performance of individuals in a series of general knowledge tasks , where the goal is to reconstruct from memory the order of historic events , or the order of items along some physical dimension . <eos> we introduce two bayesian models for aggregating order information based on a thurstonian approach and mallows model . <eos> both models assume that each individuals reconstruction is based on either a random permutation of the unobserved ground truth , or by a pure guessing strategy . <eos> we apply mcmc to make inferences about the underlying truth and the strategies employed by individuals . <eos> the models demonstrate a wisdom of crowds '' effect , where the aggregated orderings are closer to the true ordering than the orderings of the best individual . ''
alignment of time series is an important problem to solve in many scientific disciplines . <eos> in particular , temporal alignment of two or more subjects performing similar activities is a challenging problem due to the large temporal scale difference between human actions as well as the inter/intra subject variability . <eos> in this paper we present canonical time warping ( ctw ) , an extension of canonical correlation analysis ( cca ) for spatio-temporal alignment of the behavior between two subjects . <eos> ctw extends previous work on cca in two ways : ( i ) it combines cca with dynamic time warping for temporal alignment ; and ( ii ) it extends cca to allow local spatial deformations . <eos> we show ctws effectiveness in three experiments : alignment of synthetic data , alignment of motion capture data of two subjects performing similar actions , and alignment of two people with similar facial expressions . <eos> our results demonstrate that ctw provides both visually and qualitatively better alignment than state-of-the-art techniques based on dynamic time warping .
we present a nonparametric bayesian method for texture learning and synthesis . <eos> a texture image is represented by a 2d-hidden markov model ( 2d-hmm ) where the hidden states correspond to the cluster labeling of textons and the transition matrix encodes their spatial layout ( the compatibility between adjacent textons ) . <eos> 2d-hmm is coupled with the hierarchical dirichlet process ( hdp ) which allows the number of textons and the complexity of transition matrix grow as the input texture becomes irregular . <eos> the hdp makes use of dirichlet process prior which favors regular textures by penalizing the model complexity . <eos> this framework ( hdp-2d-hmm ) learns the texton vocabulary and their spatial layout jointly and automatically . <eos> the hdp-2d-hmm results in a compact representation of textures which allows fast texture synthesis with comparable rendering quality over the state-of-the-art image-based rendering methods . <eos> we also show that hdp-2d-hmm can be applied to perform image segmentation and synthesis .
recent work has led to the ability to perform space ef ? cient , approximate counting over large vocabularies in a streaming context . <eos> motivated by the existence of data structures of this type , we explore the computation of associativity scores , other- wise known as pointwise mutual information ( pmi ) , in a streaming context . <eos> we give theoretical bounds showing the impracticality of perfect online pmi compu- tation , and detail an algorithm with high expected accuracy . <eos> experiments on news articles show our approach gives high accuracy on real world data .
when individuals learn facts ( e.g. , foreign language vocabulary ) over multiple study sessions , the temporal spacing of study has a significant impact on memory retention . <eos> behavioral experiments have shown a nonmonotonic relationship between spacing and retention : short or long intervals between study sessions yield lower cued-recall accuracy than intermediate intervals . <eos> appropriate spacing of study can double retention on educationally relevant time scales . <eos> we introduce a multiscale context model ( mcm ) that is able to predict the influence of a particular study schedule on retention for specific material . <eos> mcms prediction is based on empirical data characterizing forgetting of the material following a single study session . <eos> mcm is a synthesis of two existing memory models ( staddon , chelaru , & higa , 2002 ; raaijmakers , 2003 ) . <eos> on the surface , these models are unrelated and incompatible , but we show they share a core feature that allows them to be integrated . <eos> mcm can determine study schedules that maximize the durability of learning , and has implications for education and training . <eos> mcm can be cast either as a neural network with inputs that fluctuate over time , or as a cascade of leaky integrators . <eos> mcm is intriguingly similar to a bayesian multiscale model of memory ( kording , tenenbaum , shadmehr , 2007 ) , yet mcm is better able to account for human declarative memory .
a goal of central importance in the study of hierarchical models for object recognition -- and indeed the visual cortex -- is that of understanding quantitatively the trade-off between invariance and selectivity , and how invariance and discrimination properties contribute towards providing an improved representation useful for learning from data . <eos> in this work we provide a general group-theoretic framework for characterizing and understanding invariance in a family of hierarchical models . <eos> we show that by taking an algebraic perspective , one can provide a concise set of conditions which must be met to establish invariance , as well as a constructive prescription for meeting those conditions . <eos> analyses in specific cases of particular relevance to computer vision and text processing are given , yielding insight into how and when invariance can be achieved . <eos> we find that the minimal sets of transformations intrinsic to the hierarchical model needed to support a particular invariance can be clearly described , thereby encouraging efficient computational implementations .
although it is widely believed that reinforcement learning is a suitable tool for describing behavioral learning , the mechanisms by which it can be implemented in networks of spiking neurons are not fully understood . <eos> here , we show that different learning rules emerge from a policy gradient approach depending on which features of the spike trains are assumed to influence the reward signals , i.e. , depending on which neural code is in effect . <eos> we use the framework of williams ( 1992 ) to derive learning rules for arbitrary neural codes . <eos> for illustration , we present policy-gradient rules for three different example codes - a spike count code , a spike timing code and the most general `` full spike train code - and test them on simple model problems . <eos> in addition to classical synaptic learning , we derive learning rules for intrinsic parameters that control the excitability of the neuron . <eos> the spike count learning rule has structural similarities with established bienenstock-cooper-munro rules . <eos> if the distribution of the relevant spike train features belongs to the natural exponential family , the learning rules have a characteristic shape that raises interesting prediction problems .
we present a novel and highly effective approach for multi-body motion segmentation . <eos> drawing inspiration from robust statistical model fitting , we estimate putative subspace hypotheses from the data . <eos> however , instead of ranking them we encapsulate the hypotheses in a novel mercer kernel which elicits the potential of two point trajectories to have emerged from the same subspace . <eos> the kernel permits the application of well-established statistical learning methods for effective outlier rejection , automatic recovery of the number of motions and accurate segmentation of the point trajectories . <eos> the method operates well under severe outliers arising from spurious trajectories or mistracks . <eos> detailed experiments on a recent benchmark dataset ( hopkins 155 ) show that our method is superior to other state-of-the-art approaches in terms of recovering the number of motions , segmentation accuracy , robustness against gross outliers and computational efficiency .
in cognitive science , empirical data collected from participants are the arbiters in model selection . <eos> model discrimination thus depends on designing maximally informative experiments . <eos> it has been shown that adaptive design optimization ( ado ) allows one to discriminate models as efficiently as possible in simulation experiments . <eos> in this paper we use ado in a series of experiments with people to discriminate the power , exponential , and hyperbolic models of memory retention , which has been a long-standing problem in cognitive science , providing an ideal setting in which to test the application of ado for addressing questions about human cognition . <eos> using an optimality criterion based on mutual information , ado is able to find designs that are maximally likely to increase our certainty about the true model upon observation of the experiment outcomes . <eos> results demonstrate the usefulness of ado and also reveal some challenges in its implementation .
we prove an oracle inequality for generic regularized empirical risk minimization algorithms learning from $ \a $ -mixing processes . <eos> to illustrate this oracle inequality , we use it to derive learning rates for some learning methods including least squares svms . <eos> since the proof of the oracle inequality uses recent localization ideas developed for independent and identically distributed ( i.i.d . ) <eos> processes , it turns out that these learning rates are close to the optimal rates known in the i.i.d . <eos> case .
we consider the general problem of constructing nonparametric bayesian models on infinite-dimensional random objects , such as functions , infinite graphs or infinite permutations . <eos> the problem has generated much interest in machine learning , where it is treated heuristically , but has not been studied in full generality in nonparametric bayesian statistics , which tends to focus on models over probability distributions . <eos> our approach applies a standard tool of stochastic process theory , the construction of stochastic processes from their finite-dimensional marginal distributions . <eos> the main contribution of the paper is a generalization of the classic kolmogorov extension theorem to conditional probabilities . <eos> this extension allows a rigorous construction of nonparametric bayesian models from systems of finite-dimensional , parametric bayes equations . <eos> using this approach , we show ( i ) how existence of a conjugate posterior for the nonparametric model can be guaranteed by choosing conjugate finite-dimensional models in the construction , ( ii ) how the mapping to the posterior parameters of the nonparametric model can be explicitly determined , and ( iii ) that the construction of conjugate models in essence requires the finite-dimensional models to be in the exponential family . <eos> as an application of our constructive framework , we derive a model on infinite permutations , the nonparametric bayesian analogue of a model recently proposed for the analysis of rank data .
a kernel embedding of probability distributions into reproducing kernel hilbert spaces ( rkhs ) has recently been proposed , which allows the comparison of two probability measures p and q based on the distance between their respective embeddings : for a sufficiently rich rkhs , this distance is zero if and only if p and q coincide . <eos> in using this distance as a statistic for a test of whether two samples are from different distributions , a major difficulty arises in computing the significance threshold , since the empirical statistic has as its null distribution ( where p=q ) an infinite weighted sum of $ \chi^2 $ random variables . <eos> the main result of the present work is a novel , consistent estimate of this null distribution , computed from the eigenspectrum of the gram matrix on the aggregate sample from p and q . <eos> this estimate may be computed faster than a previous consistent estimate based on the bootstrap . <eos> another prior approach was to compute the null distribution based on fitting a parametric family with the low order moments of the test statistic : unlike the present work , this heuristic has no guarantee of being accurate or consistent . <eos> we verify the performance of our null distribution estimate on both an artificial example and on high dimensional multivariate data .
we prove certain theoretical properties of a graph-regularized transductive learning objective that is based on minimizing a kullback-leibler divergence based loss . <eos> these include showing that the iterative alternating minimization procedure used to minimize the objective converges to the correct solution and deriving a test for convergence . <eos> we also propose a graph node ordering algorithm that is cache cognizant and leads to a linear speedup in parallel computations . <eos> this ensures that the algorithm scales to large data sets . <eos> by making use of empirical evaluation on the timit and switchboard i corpora , we show this approach is able to out-perform other state-of-the-art ssl approaches . <eos> in one instance , we solve a problem on a 120 million node graph .
recent work on the statistical modeling of neural responses has focused on modulated renewal processes in which the spike rate is a function of the stimulus and recent spiking history . <eos> typically , these models incorporate spike-history dependencies via either : ( a ) a conditionally-poisson process with rate dependent on a linear projection of the spike train history ( e.g. , generalized linear model ) ; or ( b ) a modulated non-poisson renewal process ( e.g. , inhomogeneous gamma process ) . <eos> here we show that the two approaches can be combined , resulting in a { \it conditional renewal } ( cr ) model for neural spike trains . <eos> this model captures both real and rescaled-time effects , and can be fit by maximum likelihood using a simple application of the time-rescaling theorem [ 1 ] . <eos> we show that for any modulated renewal process model , the log-likelihood is concave in the linear filter parameters only under certain restrictive conditions on the renewal density ( ruling out many popular choices , e.g . gamma with $ \kappa \neq1 $ ) , suggesting that real-time history effects are easier to estimate than non-poisson renewal properties . <eos> moreover , we show that goodness-of-fit tests based on the time-rescaling theorem [ 1 ] quantify relative-time effects , but do not reliably assess accuracy in spike prediction or stimulus-response modeling . <eos> we illustrate the cr model with applications to both real and simulated neural data .
semi-supervised regression based on the graph laplacian suffers from the fact that the solution is biased towards a constant and the lack of extrapolating power . <eos> outgoing from these observations we propose to use the second-order hessian energy for semi-supervised regression which overcomes both of these problems , in particular , if the data lies on or close to a low-dimensional submanifold in the feature space , the hessian energy prefers functions which vary `` linearly with respect to the natural parameters in the data . <eos> this property makes it also particularly suited for the task of semi-supervised dimensionality reduction where the goal is to find the natural parameters in the data based on a few labeled points . <eos> the experimental result suggest that our method is superior to semi-supervised regression using laplacian regularization and standard supervised methods and is particularly suited for semi-supervised dimensionality reduction .
we present a class of nonlinear ( polynomial ) models that are discriminatively trained to directly map from the word content in a query-document or document-document pair to a ranking score . <eos> dealing with polynomial models on word features is computationally challenging . <eos> we propose a low rank ( but diagonal preserving ) representation of our polynomial models to induce feasible memory and computation requirements . <eos> we provide an empirical study on retrieval tasks based on wikipedia documents , where we obtain state-of-the-art performance while providing realistically scalable methods .
most of existing methods for dna motif discovery consider only a single set of sequences to find an over-represented motif . <eos> in contrast , we consider multiple sets of sequences where we group sets associated with the same motif into a cluster , assuming that each set involves a single motif . <eos> clustering sets of sequences yields clusters of coherent motifs , improving signal-to-noise ratio or enabling us to identify multiple motifs . <eos> we present a probabilistic model for dna motif discovery where we identify multiple motifs through searching for patterns which are shared across multiple sets of sequences . <eos> our model infers cluster-indicating latent variables and learns motifs simultaneously , where these two tasks interact with each other . <eos> we show that our model can handle various motif discovery problems , depending on how to construct multiple sets of sequences . <eos> experiments on three different problems for discovering dna motifs emphasize the useful behavior and confirm the substantial gains over existing methods where only single set of sequences is considered .
the principles by which spiking neurons contribute to the astounding computational power of generic cortical microcircuits , and how spike-timing-dependent plasticity ( stdp ) of synaptic weights could generate and maintain this computational function , are unknown . <eos> we show here that stdp , in conjunction with a stochastic soft winner-take-all ( wta ) circuit , induces spiking neurons to generate through their synaptic weights implicit internal models for subclasses ( or causes '' ) of the high-dimensional spike patterns of hundreds of pre-synaptic neurons . <eos> hence these neurons will fire after learning whenever the current input best matches their internal model . <eos> the resulting computational function of soft wta circuits , a common network motif of cortical microcircuits , could therefore be a drastic dimensionality reduction of information streams , together with the autonomous creation of internal models for the probability distributions of their input patterns . <eos> we show that the autonomous generation and maintenance of this computational function can be explained on the basis of rigorous mathematical principles . <eos> in particular , we show that stdp is able to approximate a stochastic online expectation-maximization ( em ) algorithm for modeling the input data . <eos> a corresponding result is shown for hebbian learning in artificial neural networks . ''
the minimum description length ( mdl ) principle selects the model that has the shortest code for data plus model . <eos> we show that for a countable class of models , mdl predictions are close to the true distribution in a strong sense . <eos> the result is completely general . <eos> no independence , ergodicity , stationarity , identifiability , or other assumption on the model class need to be made . <eos> more formally , we show that for any countable class of models , the distributions selected by mdl ( or map ) asymptotically predict ( merge with ) the true measure in the class in total variation distance . <eos> implications for non-i.i.d . <eos> domains like time-series forecasting , discriminative learning , and reinforcement learning are discussed .
everyday social interactions are heavily influenced by our snap judgments about others goals . <eos> even young infants can infer the goals of intentional agents from observing how they interact with objects and other agents in their environment : e.g. , that one agent is ` helping or ` hindering anothers attempt to get up a hill or open a box . <eos> we propose a model for how people can infer these social goals from actions , based on inverse planning in multiagent markov decision problems ( mdps ) . <eos> the model infers the goal most likely to be driving an agents behavior by assuming the agent acts approximately rationally given environmental constraints and its model of other agents present . <eos> we also present behavioral evidence in support of this model over a simpler , perceptual cue-based alternative .
resting state activity is brain activation that arises in the absence of any task , and is usually measured in awake subjects during prolonged fmri scanning sessions where the only instruction given is to close the eyes and do nothing . <eos> it has been recognized in recent years that resting state activity is implicated in a wide variety of brain function . <eos> while certain networks of brain areas have different levels of activation at rest and during a task , there is nevertheless significant similarity between activations in the two cases . <eos> this suggests that recordings of resting state activity can be used as a source of unlabeled data to augment discriminative regression techniques in a semi-supervised setting . <eos> we evaluate this setting empirically yielding three main results : ( i ) regression tends to be improved by the use of laplacian regularization even when no additional unlabeled data are available , ( ii ) resting state data may have a similar marginal distribution to that recorded during the execution of a visual processing task reinforcing the hypothesis that these conditions have similar types of activation , and ( iii ) this source of information can be broadly exploited to improve the robustness of empirical inference in fmri studies , an inherently data poor domain .
this paper addresses the problem of designing binary codes for high-dimensional data such that vectors that are similar in the original space map to similar binary strings . <eos> we introduce a simple distribution-free encoding scheme based on random projections , such that the expected hamming distance between the binary codes of two vectors is related to the value of a shift-invariant kernel ( e.g. , a gaussian kernel ) between the vectors . <eos> we present a full theoretical analysis of the convergence properties of the proposed scheme , and report favorable experimental performance as compared to a recent state-of-the-art method , spectral hashing .
embeddings of probability measures into reproducing kernel hilbert spaces have been proposed as a straightforward and practical means of representing and comparing probabilities . <eos> in particular , the distance between embeddings ( the maximum mean discrepancy , or mmd ) has several key advantages over many classical metrics on distributions , namely easy computability , fast convergence and low bias of finite sample estimates . <eos> an important requirement of the embedding rkhs is that it be characteristic : in this case , the mmd between two distributions is zero if and only if the distributions coincide . <eos> three new results on the mmd are introduced in the present study . <eos> first , it is established that mmd corresponds to the optimal risk of a kernel classifier , thus forming a natural link between the distance between distributions and their ease of classification . <eos> an important consequence is that a kernel must be characteristic to guarantee classifiability between distributions in the rkhs . <eos> second , the class of characteristic kernels is broadened to incorporate all strictly positive definite kernels : these include non-translation invariant kernels and kernels on non-compact domains . <eos> third , a generalization of the mmd is proposed for families of kernels , as the supremum over mmds on a class of kernels ( for instance the gaussian kernels with different bandwidths ) . <eos> this extension is necessary to obtain a single distance measure if a large selection or class of characteristic kernels is potentially appropriate . <eos> this generalization is reasonable , given that it corresponds to the problem of learning the kernel by minimizing the risk of the corresponding kernel classifier . <eos> the generalized mmd is shown to have consistent finite sample estimates , and its performance is demonstrated on a homogeneity testing example .
we introduce a novel multivariate laplace ( mvl ) distribution as a sparsity promoting prior for bayesian source localization that allows the specification of constraints between and within sources . <eos> we represent the mvl distribution as a scale mixture that induces a coupling between source variances instead of their means . <eos> approximation of the posterior marginals using expectation propagation is shown to be very efficient due to properties of the scale mixture representation . <eos> the computational bottleneck amounts to computing the diagonal elements of a sparse matrix inverse . <eos> our approach is illustrated using a mismatch negativity paradigm for which meg data and a structural mri have been acquired . <eos> we show that spatial coupling leads to sources which are active over larger cortical areas as compared with an uncoupled prior .
we investigate methods for selecting sets of labeled vertices for use in predicting the labels of vertices on a graph . <eos> we specifically study methods which choose a single batch of labeled vertices ( i.e . offline , non sequential methods ) . <eos> in this setting , we find common graph smoothness assumptions directly motivate simple label selection methods with interesting theoretical guarantees . <eos> these methods bound prediction error in terms of the smoothness of the true labels with respect to the graph . <eos> some of these bounds give new motivations for previously proposed algorithms , and some suggest new algorithms which we evaluate . <eos> we show improved performance over baseline methods on several real world data sets .
an algorithm is presented for online learning of rotations . <eos> the proposed algorithm involves matrix exponentiated gradient updates and is motivated by the von neumann divergence . <eos> the additive updates are skew-symmetric matrices with trace zero which comprise the lie algebra of the rotation group . <eos> the orthogonality and unit determinant of the matrix parameter are preserved using matrix logarithms and exponentials and the algorithm lends itself to interesting interpretations in terms of the computational topology of the compact lie groups . <eos> the stability and the computational complexity of the algorithm are discussed .
we propose a novel information theoretic approach for semi-supervised learning of conditional random fields . <eos> our approach defines a training objective that combines the conditional likelihood on labeled data and the mutual information on unlabeled data . <eos> different from previous minimum conditional entropy semi-supervised discriminative learning methods , our approach can be naturally cast into the rate distortion theory framework in information theory . <eos> we analyze the tractability of the framework for structured prediction and present a convergent variational training algorithm to defy the combinatorial explosion of terms in the sum over label configurations . <eos> our experimental results show that the rate distortion approach outperforms standard $ l_2 $ regularization and minimum conditional entropy regularization on both multi-class classification and sequence labeling problems .
the standard assumption of identically distributed training and test data can be violated when an adversary can exercise some control over the generation of the test data . <eos> in a prediction game , a learner produces a predictive model while an adversary may alter the distribution of input data . <eos> we study single-shot prediction games in which the cost functions of learner and adversary are not necessarily antagonistic . <eos> we identify conditions under which the prediction game has a unique nash equilibrium , and derive algorithms that will find the equilibrial prediction models . <eos> in a case study , we explore properties of nash-equilibrial prediction models for email spam filtering empirically .
graph matching and map inference are essential problems in computer vision and machine learning . <eos> we introduce a novel algorithm that can accommodate both problems and solve them efficiently . <eos> recent graph matching algorithms are based on a general quadratic programming formulation , that takes in consideration both unary and second-order terms reflecting the similarities in local appearance as well as in the pairwise geometric relationships between the matched features . <eos> in this case the problem is np-hard and a lot of effort has been spent in finding efficiently approximate solutions by relaxing the constraints of the original problem . <eos> most algorithms find optimal continuous solutions of the modified problem , ignoring during the optimization the original discrete constraints . <eos> the continuous solution is quickly binarized at the end , but very little attention is put into this final discretization step . <eos> in this paper we argue that the stage in which a discrete solution is found is crucial for good performance . <eos> we propose an efficient algorithm , with climbing and convergence properties , that optimizes in the discrete domain the quadratic score , and it gives excellent results either by itself or by starting from the solution returned by any graph matching algorithm . <eos> in practice it outperforms state-or-the art algorithms and it also significantly improves their performance if used in combination . <eos> when applied to map inference , the algorithm is a parallel extension of iterated conditional modes ( icm ) with climbing and convergence properties that make it a compelling alternative to the sequential icm . <eos> in our experiments on map inference our algorithm proved its effectiveness by outperforming icm and max-product belief propagation .
image representation based on image bases provides a framework for understanding neural representation of visual perception . <eos> a recent fmri study has shown that arbitrary contrast-defined visual images can be reconstructed from fmri activity patterns using a combination of multi-scale local image bases . <eos> in the reconstruction model , the mapping from an fmri activity pattern to the contrasts of the image bases was learned from measured fmri responses to visual images . <eos> but the shapes of the images bases were fixed , and thus may not be optimal for reconstruction . <eos> here , we propose a method to build a reconstruction model in which image bases are automatically extracted from the measured data . <eos> we constructed a probabilistic model that relates the fmri activity space to the visual image space via a set of latent variables . <eos> the mapping from the latent variables to the visual image space can be regarded as a set of image bases . <eos> we found that spatially localized , multi-scale image bases were estimated near the fovea , and that the model using the estimated image bases was able to accurately reconstruct novel visual images . <eos> the proposed method provides a means to discover a novel functional mapping between stimuli and brain activity patterns .
learning to rank is a relatively new field of study , aiming to learn a ranking function from a set of training data with relevancy labels . <eos> the ranking algorithms are often evaluated using information retrieval measures , such as normalized discounted cumulative gain [ 1 ] and mean average precision [ 2 ] . <eos> until recently , most learning to rank algorithms were not using a loss function related to the above mentioned evaluation measures . <eos> the main difficulty in direct optimization of these measures is that they depend on the ranks of documents , not the numerical values output by the ranking function . <eos> we propose a probabilistic framework that addresses this challenge by optimizing the expectation of ndcg over all the possible permutations of documents . <eos> a relaxation strategy is used to approximate the average of ndcg over the space of permutation , and a bound optimization approach is proposed to make the computation efficient . <eos> extensive experiments show that the proposed algorithm outperforms state-of-the-art ranking algorithms on several benchmark data sets .
given a corpus of news items consisting of images accompanied by text captions , we want to find out `` whos doing what , i.e . associate names and action verbs in the captions to the face and body pose of the persons in the images . <eos> we present a joint model for simultaneously solving the image-caption correspondences and learning visual appearance models for the face and pose classes occurring in the corpus . <eos> these models can then be used to recognize people and actions in novel images without captions . <eos> we demonstrate experimentally that our joint ` face and pose model solves the correspondence problem better than earlier models covering only the face , and that it can perform recognition of new uncaptioned images .
a classic debate in cognitive science revolves around understanding how children learn complex linguistic rules , such as those governing restrictions on verb alternations , without negative evidence . <eos> traditionally , formal learnability arguments have been used to claim that such learning is impossible without the aid of innate language-specific knowledge . <eos> however , recently , researchers have shown that statistical models are capable of learning complex rules from only positive evidence . <eos> these two kinds of learnability analyses differ in their assumptions about the role of the distribution from which linguistic input is generated . <eos> the former analyses assume that learners seek to identify grammatical sentences in a way that is robust to the distribution from which the sentences are generated , analogous to discriminative approaches in machine learning . <eos> the latter assume that learners are trying to estimate a generative model , with sentences being sampled from that model . <eos> we show that these two learning approaches differ in their use of implicit negative evidence -- the absence of a sentence -- when learning verb alternations , and demonstrate that human learners can produce results consistent with the predictions of both approaches , depending on the context in which the learning problem is presented .
existing value function approximation methods have been successfully used in many applications , but they often lack useful a priori error bounds . <eos> we propose approximate bilinear programming , a new formulation of value function approximation that provides strong a priori guarantees . <eos> in particular , it provably finds an approximate value function that minimizes the bellman residual . <eos> solving a bilinear program optimally is np hard , but this is unavoidable because the bellman-residual minimization itself is np hard . <eos> we , therefore , employ and analyze a common approximate algorithm for bilinear programs . <eos> the analysis shows that this algorithm offers a convergent generalization of approximate policy iteration . <eos> finally , we demonstrate that the proposed approach can consistently minimize the bellman residual on a simple benchmark problem .
several key problems in machine learning , such as feature selection and active learning , can be formulated as submodular set function maximization . <eos> we present herein a novel algorithm for maximizing a submodular set function under a cardinality constraint -- - the algorithm is based on a cutting-plane method and is implemented as an iterative small-scale binary-integer linear programming procedure . <eos> it is well known that this problem is np-hard , and the approximation factor achieved by the greedy algorithm is the theoretical limit for polynomial time . <eos> as for ( non-polynomial time ) exact algorithms that perform reasonably in practice , there has been very little in the literature although the problem is quite important for many applications . <eos> our algorithm is guaranteed to find the exact solution in finite iterations , and it converges fast in practice due to the efficiency of the cutting-plane mechanism . <eos> moreover , we also provide a method that produces successively decreasing upper-bounds of the optimal solution , while our algorithm provides successively increasing lower-bounds . <eos> thus , the accuracy of the current solution can be estimated at any point , and the algorithm can be stopped early once a desired degree of tolerance is met . <eos> we evaluate our algorithm on sensor placement and feature selection applications showing good performance .
the low-rank matrix completion problem is a fundamental problem with many important applications . <eos> recently , candes & recht , keshavan et al . and candes & tao obtained the first non-trivial theoretical results for the problem assuming that the observed entries are sampled uniformly at random . <eos> unfortunately , most real-world datasets do not satisfy this assumption , but instead exhibit power-law distributed samples . <eos> in this paper , we propose a graph theoretic approach to matrix completion that solves the problem for more realistic sampling models . <eos> our method is easier to analyze than previous methods with the analysis reducing to computing the threshold for complete cascades in random graphs , a problem of independent interest . <eos> by analyzing the graph theoretic problem , we show that our method achieves exact recovery when the observed entries are sampled from the chung-lu-vu model , which can generate power-law distributed graphs . <eos> we also hypothesize that our algorithm solves the matrix completion problem from an optimal number of entries for the popular preferential attachment model and provide strong empirical evidence for the claim . <eos> furthermore , our method is easier to implement and is substantially faster than existing methods . <eos> we demonstrate the effectiveness of our method on examples when the low-rank matrix is sampled according to the prevalent random graph models for complex networks and also on the netflix challenge dataset .
the inter-subject alignment of functional mri ( fmri ) data is important for improving the statistical power of fmri group analyses . <eos> in contrast to existing anatomically-based methods , we propose a novel multi-subject algorithm that derives a functional correspondence by aligning spatial patterns of functional connectivity across a set of subjects . <eos> we test our method on fmri data collected during a movie viewing experiment . <eos> by cross-validating the results of our algorithm , we show that the correspondence successfully generalizes to a secondary movie dataset not used to derive the alignment .
the estimation of high-dimensional parametric models requires imposing some structure on the models , for instance that they be sparse , or that matrix structured parameters have low rank . <eos> a general approach for such structured parametric model estimation is to use regularized m-estimation procedures , which regularize a loss function that measures goodness of fit of the parameters to the data with some regularization function that encourages the assumed structure . <eos> in this paper , we aim to provide a unified analysis of such regularized m-estimation procedures . <eos> in particular , we report the convergence rates of such estimators in any metric norm . <eos> using just our main theorem , we are able to rederive some of the many existing results , but also obtain a wide range of novel convergence rates results . <eos> our analysis also identifies key properties of loss and regularization functions such as restricted strong convexity , and decomposability , that ensure the corresponding regularized m-estimators have good convergence rates .
object detection and multi-class image segmentation are two closely related tasks that can be greatly improved when solved jointly by feeding information from one task to the other . <eos> however , current state-of-the-art models use a separate representation for each task making joint inference clumsy and leaving classification of many parts of the scene ambiguous . <eos> in this work , we propose a hierarchical region-based approach to joint object detection and image segmentation . <eos> our approach reasons about pixels , regions and objects in a coherent probabilistic model . <eos> importantly , our model gives a single unified description of the scene . <eos> we explain every pixel in the image and enforce global consistency between all variables in our model . <eos> we run experiments on challenging vision datasets and show significant improvement over state-of-the-art object detection accuracy .
policy gradient reinforcement learning ( rl ) algorithms have received much attention in seeking stochastic policies that maximize the average rewards . <eos> in addition , extensions based on the concept of the natural gradient ( ng ) show promising learning efficiency because these regard metrics for the task . <eos> though there are two candidate metrics , kakades fisher information matrix ( fim ) and morimuras fim , all rl algorithms with ng have followed the kakades approach . <eos> in this paper , we describe a generalized natural gradient ( gng ) by linearly interpolating the two fims and propose an efficient implementation for the gng learning based on a theory of the estimating function , generalized natural actor-critic ( gnac ) . <eos> the gnac algorithm involves a near optimal auxiliary function to reduce the variance of the gng estimates . <eos> interestingly , the gnac can be regarded as a natural extension of the current state-of-the-art nac algorithm , as long as the interpolating parameter is appropriately selected . <eos> numerical experiments showed that the proposed gnac algorithm can estimate gng efficiently and outperformed the nac algorithm .
we introduce a new perspective on approximations to the maximum a posteriori ( map ) task in probabilistic graphical models , that is based on simplifying a given instance , and then tightening the approximation . <eos> first , we start with a structural relaxation of the original model . <eos> we then infer from the relaxation its deficiencies , and compensate for them . <eos> this perspective allows us to identify two distinct classes of approximations . <eos> first , we find that max-product belief propagation can be viewed as a way to compensate for a relaxation , based on a particular idealized case for exactness . <eos> we identify a second approach to compensation that is based on a more refined idealized case , resulting in a new approximation with distinct properties . <eos> we go on to propose a new class of algorithms that , starting with a relaxation , iteratively yields tighter approximations .
it was recently shown that certain nonparametric regressors can escape the curse of dimensionality in the sense that their convergence rates adapt to the intrinsic dimension of data ( \cite { bl:65 , sk:77 } ) . <eos> we prove some stronger results in more general settings . <eos> in particular , we consider a regressor which , by combining aspects of both tree-based regression and kernel regression , operates on a general metric space , yields a smooth function , and evaluates in time $ o ( \log n ) $ . <eos> we derive a tight convergence rate of the form $ n^ { -2/ ( 2+d ) } $ where $ d $ is the assouad dimension of the input space .
stochastic neighbor embedding ( sne ) has shown to be quite promising for data visualization . <eos> currently , the most popular implementation , t-sne , is restricted to a particular student t-distribution as its embedding distribution . <eos> moreover , it uses a gradient descent algorithm that may require users to tune parameters such as the learning step size , momentum , etc. , in finding its optimum . <eos> in this paper , we propose the heavy-tailed symmetric stochastic neighbor embedding ( hssne ) method , which is a generalization of the t-sne to accommodate various heavy-tailed embedding similarity functions . <eos> with this generalization , we are presented with two difficulties . <eos> the first is how to select the best embedding similarity among all heavy-tailed functions and the second is how to optimize the objective function once the heave-tailed function has been selected . <eos> our contributions then are : ( 1 ) we point out that various heavy-tailed embedding similarities can be characterized by their negative score functions . <eos> based on this finding , we present a parameterized subset of similarity functions for choosing the best tail-heaviness for hssne ; ( 2 ) we present a fixed-point optimization algorithm that can be applied to all heavy-tailed functions and does not require the user to set any parameters ; and ( 3 ) we present two empirical studies , one for unsupervised visualization showing that our optimization algorithm runs as fast and as good as the best known t-sne implementation and the other for semi-supervised visualization showing quantitative superiority using the homogeneity measure as well as qualitative advantage in cluster separation over t-sne .
we propose to use rademacher complexity , originally developed in computational learning theory , as a measure of human learning capacity . <eos> rademacher complexity measures a learners ability to fit random data , and can be used to bound the learners true error based on the observed training sample error . <eos> we first review the definition of rademacher complexity and its generalization bound . <eos> we then describe a learning the noise '' procedure to experimentally measure human rademacher complexities . <eos> the results from empirical studies showed that : ( i ) human rademacher complexity can be successfully measured , ( ii ) the complexity depends on the domain and training sample size in intuitive ways , ( iii ) human learning respects the generalization bounds , ( iv ) the bounds can be useful in predicting the danger of overfitting in human learning . <eos> finally , we discuss the potential applications of human rademacher complexity in cognitive science . ''
one of the central problems in neuroscience is reconstructing synaptic connectivity in neural circuits . <eos> synapses onto a neuron can be probed by sequentially stimulating potentially pre-synaptic neurons while monitoring the membrane voltage of the post-synaptic neuron . <eos> reconstructing a large neural circuit using such a ? brute force ? <eos> approach is rather time-consuming and inefficient because the connectivity in neural circuits is sparse . <eos> instead , we propose to measure a post-synaptic neuron ? s voltage while stimulating simultaneously multiple randomly chosen potentially pre-synaptic neurons . <eos> to extract the weights of individual synaptic connections we apply a decoding algorithm recently developed for compressive sensing . <eos> compared to the brute force approach , our method promises significant time savings that grow with the size of the circuit . <eos> we use computer simulations to find optimal stimulation parameters and explore the feasibility of our reconstruction method under realistic experimental conditions including noise and non-linear synaptic integration . <eos> multiple-neuron stimulation allows reconstructing synaptic connectivity just from the spiking activity of post-synaptic neurons , even when sub-threshold voltage is unavailable . <eos> by using calcium indicators , voltage-sensitive dyes , or multi-electrode arrays one could monitor activity of multiple post-synaptic neurons simultaneously , thus mapping their synaptic inputs in parallel , potentially reconstructing a complete neural circuit .
we propose a probabilistic topic model for analyzing and extracting content-related annotations from noisy annotated discrete data such as web pages stored in social bookmarking services . <eos> in these services , since users can attach annotations freely , some annotations do not describe the semantics of the content , thus they are noisy , i.e . not content-related . <eos> the extraction of content-related annotations can be used as a preprocessing step in machine learning tasks such as text classification and image recognition , or can improve information retrieval performance . <eos> the proposed model is a generative model for content and annotations , in which the annotations are assumed to originate either from topics that generated the content or from a general distribution unrelated to the content . <eos> we demonstrate the effectiveness of the proposed method by using synthetic data and real social annotation data for text and images .
the proposal that cortical activity in the visual cortex is optimized for sparse neural activity is one of the most established ideas in computational neuroscience . <eos> however , direct experimental evidence for optimal sparse coding remains inconclusive , mostly due to the lack of reference values on which to judge the measured sparseness . <eos> here we analyze neural responses to natural movies in the primary visual cortex of ferrets at different stages of development , and of rats while awake and under different levels of anesthesia . <eos> in contrast with prediction from a sparse coding model , our data shows that population and lifetime sparseness decrease with visual experience , and increase from the awake to anesthetized state . <eos> these results suggest that the representation in the primary visual cortex is not actively optimized to maximize sparseness .
representing distributions over permutations can be a daunting task due to the fact that the number of permutations of n objects scales factorially in n. one recent way that has been used to reduce storage complexity has been to exploit probabilistic independence , but as we argue , full independence assumptions impose strong sparsity constraints on distributions and are unsuitable for modeling rankings . <eos> we identify a novel class of independence structures , called riffled independence , which encompasses a more expressive family of distributions while retaining many of the properties necessary for performing efficient inference and reducing sample complexity . <eos> in riffled independence , one draws two permutations independently , then performs the riffle shuffle , common in card games , to combine the two permutations to form a single permutation . <eos> in ranking , riffled independence corresponds to ranking disjoint sets of objects independently , then interleaving those rankings . <eos> we provide a formal introduction and present algorithms for using riffled independence within fourier-theoretic frameworks which have been explored by a number of recent papers .
humans are typically able to infer how many objects their environment contains and to recognize when the same object is encountered twice . <eos> we present a simple statistical model that helps to explain these abilities and evaluate it in three behavioral experiments . <eos> our first experiment suggests that humans rely on prior knowledge when deciding whether an object token has been previously encountered . <eos> our second and third experiments suggest that humans can infer how many objects they have seen and can learn about categories and their properties even when they are uncertain about which tokens are instances of the same object .
given a matrix m of low-rank , we consider the problem of reconstructing it from noisy observations of a small , random subset of its entries . <eos> the problem arises in a variety of applications , from collaborative filtering ( the ? netflix problem ? ) <eos> to structure-from-motion and positioning . <eos> we study a low complexity algorithm introduced in [ 1 ] , based on a combination of spectral techniques and manifold optimization , that we call here optspace . <eos> we prove performance guarantees that are order-optimal in a number of circumstances .
one crucial assumption made by both principal component analysis ( pca ) and probabilistic pca ( ppca ) is that the instances are independent and identically distributed ( i.i.d . ) . <eos> however , this common i.i.d . <eos> assumption is unreasonable for relational data . <eos> in this paper , by explicitly modeling covariance between instances as derived from the relational information , we propose a novel probabilistic dimensionality reduction method , called probabilistic relational pca ( prpca ) , for relational data analysis . <eos> although the i.i.d . <eos> assumption is no longer adopted in prpca , the learning algorithms for prpca can still be devised easily like those for ppca which makes explicit use of the i.i.d . <eos> assumption . <eos> experiments on real-world data sets show that prpca can effectively utilize the relational information to dramatically outperform pca and achieve state-of-the-art performance .
we propose a new approach to the analysis of loopy belief propagation ( lbp ) by establishing a formula that connects the hessian of the bethe free energy with the edge zeta function . <eos> the formula has a number of theoretical implications on lbp . <eos> it is applied to give a sufficient condition that the hessian of the bethe free energy is positive definite , which shows non-convexity for graphs with multiple cycles . <eos> the formula clarifies the relation between the local stability of a fixed point of lbp and local minima of the bethe free energy . <eos> we also propose a new approach to the uniqueness of lbp fixed point , and show various conditions of uniqueness .
the partially observable markov decision process ( pomdp ) framework has proven useful in planning domains that require balancing actions that increase an agents knowledge and actions that increase an agents reward . <eos> unfortunately , most pomdps are complex structures with a large number of parameters . <eos> in many realworld problems , both the structure and the parameters are difficult to specify from domain knowledge alone . <eos> recent work in bayesian reinforcement learning has made headway in learning pomdp models ; however , this work has largely focused on learning the parameters of the pomdp model . <eos> we define an infinite pomdp ( ipomdp ) model that does not require knowledge of the size of the state space ; instead , it assumes that the number of visited states will grow as the agent explores its world and explicitly models only visited states . <eos> we demonstrate the ipomdp utility on several standard problems .
search engines today present results that are often oblivious to recent shifts in intent . <eos> for example , the meaning of the query independence day shifts in early july to a us holiday and to a movie around the time of the box office release . <eos> while no studies exactly quantify the magnitude of intent-shifting traffic , studies suggest that news events , seasonal topics , pop culture , etc account for 1/2 the search queries . <eos> this paper shows that the signals a search engine receives can be used to both determine that a shift in intent happened , as well as find a result that is now more relevant . <eos> we present a meta-algorithm that marries a classifier with a bandit algorithm to achieve regret that depends logarithmically on the number of query impressions , under certain assumptions . <eos> we provide strong evidence that this regret is close to the best achievable . <eos> finally , via a series of experiments , we demonstrate that our algorithm outperforms prior approaches , particularly as the amount of intent-shifting traffic increases .
the goal of perception is to infer the hidden states in the hierarchical process by which sensory data are generated . <eos> human behavior is consistent with the optimal statistical solution to this problem in many tasks , including cue combination and orientation detection . <eos> understanding the neural mechanisms underlying this behavior is of particular importance , since probabilistic computations are notoriously challenging . <eos> here we propose a simple mechanism for bayesian inference which involves averaging over a few feature detection neurons which fire at a rate determined by their similarity to a sensory stimulus . <eos> this mechanism is based on a monte carlo method known as importance sampling , commonly used in computer science and statistics . <eos> moreover , a simple extension to recursive importance sampling can be used to perform hierarchical bayesian inference . <eos> we identify a scheme for implementing importance sampling with spiking neurons , and show that this scheme can account for human behavior in cue combination and oblique effect .
we present a general bayesian approach to probabilistic matrix factorization subject to linear constraints . <eos> the approach is based on a gaussian observation model and gaussian priors with bilinear equality and inequality constraints . <eos> we present an efficient markov chain monte carlo inference procedure based on gibbs sampling . <eos> special cases of the proposed model are bayesian formulations of non-negative matrix factorization and factor analysis . <eos> the method is evaluated on a blind source separation problem . <eos> we demonstrate that our algorithm can be used to extract meaningful and interpretable features that are remarkably different from features extracted using existing related matrix factorization techniques .
orthogonal matching pursuit ( omp ) is a widely used greedy algorithm for recovering sparse vectors from linear measurements . <eos> a well-known analysis of tropp and gilbert shows that omp can recover a k-sparse n-dimensional real vector from m = 4k log ( n ) noise-free random linear measurements with a probability that goes to one as n goes to infinity . <eos> this work shows strengthens this result by showing that a lower number of measurements , m = 2k log ( n-k ) , is in fact sufficient for asymptotic recovery . <eos> moreover , this number of measurements is also sufficient for detection of the sparsity pattern ( support ) of the vector with measurement errors provided the signal-to-noise ratio ( snr ) scales to infinity . <eos> the scaling m = 2k log ( n-k ) exactly matches the number of measurements required by the more complex lasso for signal recovery .
contrast statistics of the majority of natural images conform to a weibull distribution . <eos> this property of natural images may facilitate efficient and very rapid extraction of a scenes visual gist . <eos> here we investigate whether a neural response model based on the weibull contrast distribution captures visual information that humans use to rapidly identify natural scenes . <eos> in a learning phase , we measure eeg activity of 32 subjects viewing brief flashes of 800 natural scenes . <eos> from these neural measurements and the contrast statistics of the natural image stimuli , we derive an across subject weibull response model . <eos> we use this model to predict the responses to a large set of new scenes and estimate which scene the subject viewed by finding the best match between the model predictions and the observed eeg responses . <eos> in almost 90 percent of the cases our model accurately predicts the observed scene . <eos> moreover , in most failed cases , the scene mistaken for the observed scene is visually similar to the observed scene itself . <eos> these results suggest that weibull contrast statistics of natural images contain a considerable amount of scene gist information to warrant rapid identification of natural images .
we provide some insights into how task correlations in multi-task gaussian process ( gp ) regression affect the generalization error and the learning curve . <eos> we analyze the asymmetric two-task case , where a secondary task is to help the learning of a primary task . <eos> within this setting , we give bounds on the generalization error and the learning curve of the primary task . <eos> our approach admits intuitive understandings of the multi-task gp by relating it to single-task gps . <eos> for the case of one-dimensional input-space under optimal sampling with data only for the secondary task , the limitations of multi-task gp can be quantified explicitly .
in most online learning algorithms , the weights assigned to the misclassified examples ( or support vectors ) remain unchanged during the entire learning process . <eos> this is clearly insufficient since when a new misclassified example is added to the pool of support vectors , we generally expect it to affect the weights for the existing support vectors . <eos> in this paper , we propose a new online learning method , termed double updating online learning '' , or `` duol '' for short . <eos> instead of only assigning a fixed weight to the misclassified example received in current trial , the proposed online learning algorithm also tries to update the weight for one of the existing support vectors . <eos> we show that the mistake bound can be significantly improved by the proposed online learning method . <eos> encouraging experimental results show that the proposed technique is in general considerably more effective than the state-of-the-art online learning algorithms . ''
the recent emergence of graphics processing units ( gpus ) as general-purpose parallel computing devices provides us with new opportunities to develop scalable learning methods for massive data . <eos> in this work , we consider the problem of parallelizing two inference methods on gpus for latent dirichlet allocation ( lda ) models , collapsed gibbs sampling ( cgs ) and collapsed variational bayesian ( cvb ) . <eos> to address limited memory constraints on gpus , we propose a novel data partitioning scheme that effectively reduces the memory cost . <eos> furthermore , the partitioning scheme balances the computational cost on each multiprocessor and enables us to easily avoid memory access conflicts . <eos> we also use data streaming to handle extremely large datasets . <eos> extensive experiments showed that our parallel inference methods consistently produced lda models with the same predictive power as sequential training methods did but with 26x speedup for cgs and 196x speedup for cvb on a gpu with 30 multiprocessors ; actually the speedup is almost linearly scalable with the number of multiprocessors available . <eos> the proposed partitioning scheme and data streaming can be easily ported to many other models in machine learning .
we describe an algorithm for learning bilinear svms . <eos> bilinear classifiers are a discriminative variant of bilinear models , which capture the dependence of data on multiple factors . <eos> such models are particularly appropriate for visual data that is better represented as a matrix or tensor , rather than a vector . <eos> matrix encodings allow for more natural regularization through rank restriction . <eos> for example , a rank-one scanning-window classifier yields a separable filter . <eos> low-rank models have fewer parameters and so are easier to regularize and faster to score at run-time . <eos> we learn low-rank models with bilinear classifiers . <eos> we also use bilinear classifiers for transfer learning by sharing linear factors between different classification tasks . <eos> bilinear classifiers are trained with biconvex programs . <eos> such programs are optimized with coordinate descent , where each coordinate step requires solving a convex program - in our case , we use a standard off-the-shelf svm solver . <eos> we demonstrate bilinear svms on difficult problems of people detection in video sequences and action classification of video sequences , achieving state-of-the-art results in both .
for many computer vision applications , the ideal image feature would be invariant to multiple confounding image properties , such as illumination and viewing angle . <eos> recently , deep architectures trained in an unsupervised manner have been proposed as an automatic method for extracting useful features . <eos> however , outside of using these learning algorithms in a classi ? er , they can be sometimes dif ? cult to evaluate . <eos> in this paper , we propose a number of empirical tests that directly measure the degree to which these learned features are invariant to different image transforms . <eos> we ? nd that deep autoencoders become invariant to increasingly complex image transformations with depth . <eos> this further justi ? es the use of ? deep vs. ? shallower representations . <eos> our performance metrics agree with existing measures of invariance . <eos> our evaluation metrics can also be used to evaluate future work in unsupervised deep learning , and thus help the development of future algorithms .
we describe a method for learning a group of continuous transformation operators to traverse smooth nonlinear manifolds . <eos> the method is applied to model how natural images change over time and scale . <eos> the group of continuous transform operators is represented by a basis that is adapted to the statistics of the data so that the in ? nitesimal generator for a measurement orbit can be produced by a linear combination of a few basis elements . <eos> we illustrate how the method can be used to ef ? ciently code time-varying images by describing changes across time and scale in terms of the learned operators .
we devise a graphical model that supports the process of debugging software by guiding developers to code that is likely to contain defects . <eos> the model is trained using execution traces of passing test runs ; it reflects the distribution over transitional patterns of code positions . <eos> given a failing test case , the model determines the least likely transitional pattern in the execution trace . <eos> the model is designed such that bayesian inference has a closed-form solution . <eos> we evaluate the bernoulli graph model on data of the software projects aspectj and rhino .
we describe , analyze , and experiment with a new framework for empirical loss minimization with regularization . <eos> our algorithmic framework alternates between two phases . <eos> on each iteration we first perform an { \em unconstrained } gradient descent step . <eos> we then cast and solve an instantaneous optimization problem that trades off minimization of a regularization term while keeping close proximity to the result of the first phase . <eos> this yields a simple yet effective algorithm for both batch penalized risk minimization and online learning . <eos> furthermore , the two phase approach enables sparse solutions when used in conjunction with regularization functions that promote sparsity , such as $ \ell_1 $ . <eos> we derive concrete and very simple algorithms for minimization of loss functions with $ \ell_1 $ , $ \ell_2 $ , $ \ell_2^2 $ , and $ \ell_\infty $ regularization . <eos> we also show how to construct efficient algorithms for mixed-norm $ \ell_1/\ell_q $ regularization . <eos> we further extend the algorithms and give efficient implementations for very high-dimensional data with sparsity . <eos> we demonstrate the potential of the proposed framework in experiments with synthetic and natural datasets .
a central hypothesis about early visual processing is that it represents inputs in a coordinate system matched to the statistics of natural scenes . <eos> simple versions of this lead to gabor-like receptive fields and divisive gain modulation from local surrounds ; these have led to influential neural and psychological models of visual processing . <eos> however , these accounts are based on an incomplete view of the visual context surrounding each point . <eos> here , we consider an approximate model of linear and non-linear correlations between the responses of spatially distributed gabor-like receptive fields , which , when trained on an ensemble of natural scenes , unifies a range of spatial context effects . <eos> the full model accounts for neural surround data in primary visual cortex ( v1 ) , provides a statistical foundation for perceptual phenomena associated with lis ( 2002 ) hypothesis that v1 builds a saliency map , and fits data on the tilt illusion .
in practice , most investing is done assuming a probabilistic model of stock price returns known as the geometric brownian motion ( gbm ) . <eos> while it is often an acceptable approximation , the gbm model is not always valid empirically . <eos> this motivates a worst-case approach to investing , called universal portfolio management , where the objective is to maximize wealth relative to the wealth earned by the best fixed portfolio in hindsight . <eos> in this paper we tie the two approaches , and design an investment strategy which is universal in the worst-case , and yet capable of exploiting the mostly valid gbm model . <eos> our method is based on new and improved regret bounds for online convex optimization with exp-concave loss functions .
several key computational bottlenecks in machine learning involve pairwise distance computations , including all-nearest-neighbors ( finding the nearest neighbor ( s ) for each point , e.g . in manifold learning ) and kernel summations ( e.g . in kernel density estimation or kernel machines ) . <eos> we consider the general , bichromatic case for these problems , in addition to the scientific problem of n-body potential calculation . <eos> in this paper we show for the first time o ( n ) worst case runtimes for practical algorithms for these problems based on the cover tree data structure ( beygelzimer , kakade , langford , 2006 ) .
in this study , we present a method for estimating the mutual information for a localized pattern of fmri data . <eos> we show that taking a multivariate information approach to voxel selection leads to a decoding accuracy that surpasses an univariate inforamtion approach and other standard voxel selection methods . <eos> furthermore , we extend the multivariate mutual information theory to measure the functional connectivity between distributed brain regions . <eos> by jointly estimating the information shared by two sets of voxels we can reliably map out the connectivities in the human brain during experiment conditions . <eos> we validated our approach on a 6-way scene categorization fmri experiment . <eos> the multivariate information analysis is able to ? nd strong information ? ow between ppa and rsc , which con ? rms existing neuroscience studies on scenes . <eos> furthermore , by exploring over the whole brain , our method identifies other interesting rois that share information with the ppa , rsc scene network , suggesting interesting future work for neuroscientists .
multiple object class learning and detection is a challenging problem due to the large number of object classes and their high visual variability . <eos> specialized detectors usually excel in performance , while joint representations optimize sharing and reduce inference time -- - but are complex to train . <eos> conveniently , sequential learning of categories cuts down training time by transferring existing knowledge to novel classes , but can not fully exploit the richness of shareability and might depend on ordering in learning . <eos> in hierarchical frameworks these issues have been little explored . <eos> in this paper , we show how different types of multi-class learning can be done within one generative hierarchical framework and provide a rigorous experimental analysis of various object class learning strategies as the number of classes grows . <eos> specifically , we propose , evaluate and compare three important types of multi-class learning : 1 . ) <eos> independent training of individual categories , 2 . ) <eos> joint training of classes , 3 . ) <eos> sequential learning of classes . <eos> we explore and compare their computational behavior ( space and time ) and detection performance as a function of the number of learned classes on several recognition data sets .
we present a novel linear program for the approximation of the dynamic programming cost-to-go function in high-dimensional stochastic control problems . <eos> lp approaches to approximate dp naturally restrict attention to approximations that are lower bounds to the optimal cost-to-go function . <eos> our program -- the ` smoothed approximate linear program -- relaxes this restriction in an appropriate fashion while remaining computationally tractable . <eos> doing so appears to have several advantages : first , we demonstrate superior bounds on the quality of approximation to the optimal cost-to-go function afforded by our approach . <eos> second , experiments with our approach on a challenging problem ( the game of tetris ) show that the approach outperforms the existing lp approach ( which has previously been shown to be competitive with several adp algorithms ) by an order of magnitude .
locality information is crucial in datasets where each variable corresponds to a measurement in a manifold ( silhouettes , motion trajectories , 2d and 3d images ) . <eos> although these datasets are typically under-sampled and high-dimensional , they often need to be represented with low-complexity statistical models , which are comprised of only the important probabilistic dependencies in the datasets . <eos> most methods attempt to reduce model complexity by enforcing structure sparseness . <eos> however , sparseness can not describe inherent regularities in the structure . <eos> hence , in this paper we first propose a new class of gaussian graphical models which , together with sparseness , imposes local constancy through $ { \ell } _1 $ -norm penalization . <eos> second , we propose an efficient algorithm which decomposes the strictly convex maximum likelihood estimation into a sequence of problems with closed form solutions . <eos> through synthetic experiments , we evaluate the closeness of the recovered models to the ground truth . <eos> we also test the generalization performance of our method in a wide range of complex real-world datasets and demonstrate that it can capture useful structures such as the rotation and shrinking of a beating heart , motion correlations between body parts during walking and functional interactions of brain regions . <eos> our method outperforms the state-of-the-art structure learning techniques for gaussian graphical models both for small and large datasets .
speaker comparison , the process of finding the speaker similarity between two speech signals , occupies a central role in a variety of applications -- -speaker verification , clustering , and identification . <eos> speaker comparison can be placed in a geometric framework by casting the problem as a model comparison process . <eos> for a given speech signal , feature vectors are produced and used to adapt a gaussian mixture model ( gmm ) . <eos> speaker comparison can then be viewed as the process of compensating and finding metrics on the space of adapted models . <eos> we propose a framework , inner product discriminant functions ( ipdfs ) , which extends many common techniques for speaker comparison : support vector machines , joint factor analysis , and linear scoring . <eos> the framework uses inner products between the parameter vectors of gmm models motivated by several statistical methods . <eos> compensation of nuisances is performed via linear transforms on gmm parameter vectors . <eos> using the ipdf framework , we show that many current techniques are simple variations of each other . <eos> we demonstrate , on a 2006 nist speaker recognition evaluation task , new scoring methods using ipdfs which produce excellent error rates and require significantly less computation than current techniques .
we are often interested in casting classification and clustering problems in a regression framework , because it is feasible to achieve some statistical properties in this framework by imposing some penalty criteria . <eos> in this paper we illustrate optimal scoring , which was originally proposed for performing fisher linear discriminant analysis by regression , in the application of unsupervised learning . <eos> in particular , we devise a novel clustering algorithm that we call optimal discriminant clustering ( odc ) . <eos> we associate our algorithm with the existing unsupervised learning algorithms such as spectral clustering , discriminative clustering and sparse principal component analysis . <eos> thus , our work shows that optimal scoring provides a new approach to the implementation of unsupervised learning . <eos> this approach facilitates the development of new unsupervised learning algorithms .
we propose a multiple incremental decremental algorithm of support vector machine ( svm ) . <eos> conventional single cremental decremental svm can update the trained model efficiently when single data point is added to or removed from the training set . <eos> when we add and/or remove multiple data points , this algorithm is time-consuming because we need to repeatedly apply it to each data point . <eos> the roposed algorithm is computationally more efficient when multiple data points are added and/or removed simultaneously . <eos> the single incremental decremental algorithm is built on an optimization technique called parametric programming . <eos> we extend the idea and introduce multi-parametric programming for developing the proposed algorithm . <eos> experimental results on synthetic and real data sets indicate that the proposed algorithm can significantly reduce the computational cost of multiple incremental decremental operation . <eos> our approach is especially useful for online svm learning in which we need to remove old data points and add new data points in a short amount of time .
we present a probabilistic latent factor model which can be used for studying spatio-temporal datasets . <eos> the spatial and temporal structure is modeled by using gaussian process priors both for the loading matrix and the factors . <eos> the posterior distributions are approximated using the variational bayesian framework . <eos> high computational cost of gaussian process modeling is reduced by using sparse approximations . <eos> the model is used to compute the reconstructions of the global sea surface temperatures from a historical dataset . <eos> the results suggest that the proposed model can outperform the state-of-the-art reconstruction systems .
in the gaussian process regression the observation model is commonly assumed to be gaussian , which is convenient in computational perspective . <eos> however , the drawback is that the predictive accuracy of the model can be significantly compromised if the observations are contaminated by outliers . <eos> a robust observation model , such as the student-t distribution , reduces the influence of outlying observations and improves the predictions . <eos> the problem , however , is the analytically intractable inference . <eos> in this work , we discuss the properties of a gaussian process regression model with the student-t likelihood and utilize the laplace approximation for approximate inference . <eos> we compare our approach to a variational approximation and a markov chain monte carlo scheme , which utilize the commonly used scale mixture representation of the student-t distribution .
we formulate and address the problem of discovering dynamic malicious regions on the internet . <eos> we model this problem as one of adaptively pruning a known decision tree , but with additional challenges : ( 1 ) severe space requirements , since the underlying decision tree has over 4 billion leaves , and ( 2 ) a changing target function , since malicious activity on the internet is dynamic . <eos> we present a novel algorithm that addresses this problem , by putting together a number of different `` experts algorithms and online paging algorithms . <eos> we prove guarantees on our algorithms performance as a function of the best possible pruning of a similar size , and our experiments show that our algorithm achieves high accuracy on large real-world data sets , with significant improvements over existing approaches .
existing methods for recognition of object instances and categories based on quantized local features can perform poorly when local features exist on transparent surfaces , such as glass or plastic objects . <eos> there are characteristic patterns to the local appearance of transparent objects , but they may not be well captured by distances to individual examples or by a local pattern codebook obtained by vector quantization . <eos> the appearance of a transparent patch is determined in part by the refraction of a background pattern through a transparent medium : the energy from the background usually dominates the patch appearance . <eos> we model transparent local patch appearance using an additive model of latent factors : background factors due to scene content , and factors which capture a local edge energy distribution characteristic of the refraction . <eos> we implement our method using a novel lda-sift formulation which performs lda prior to any vector quantization step ; we discover latent topics which are characteristic of particular transparent patches and quantize the sift space into transparent visual words according to the latent topic dimensions . <eos> no knowledge of the background scene is required at test time ; we show examples recognizing transparent glasses in a domestic environment .
we introduce the first temporal-difference learning algorithms that converge with smooth value function approximators , such as neural networks . <eos> conventional temporal-difference ( td ) methods , such as td ( $ \lambda $ ) , q-learning and sarsa have been used successfully with function approximation in many applications . <eos> however , it is well known that off-policy sampling , as well as nonlinear function approximation , can cause these algorithms to become unstable ( i.e. , the parameters of the approximator may diverge ) . <eos> sutton et al ( 2009a , b ) solved the problem of off-policy learning with linear td algorithms by introducing a new objective function , related to the bellman-error , and algorithms that perform stochastic gradient-descent on this function . <eos> in this paper , we generalize their work to nonlinear function approximation . <eos> we present a bellman error objective function and two gradient-descent td algorithms that optimize it . <eos> we prove the asymptotic almost-sure convergence of both algorithms for any finite markov decision process and any smooth value function approximator , under usual stochastic approximation conditions . <eos> the computational complexity per iteration scales linearly with the number of parameters of the approximator . <eos> the algorithms are incremental and are guaranteed to converge to locally optimal solutions .
we consider the question of computing maximum a posteriori ( map ) assignment in an arbitrary pair-wise markov random field ( mrf ) . <eos> we present a randomized iterative algorithm based on simple local updates . <eos> the algorithm , starting with an arbitrary initial assignment , updates it in each iteration by first , picking a random node , then selecting an ( appropriately chosen ) random local neighborhood and optimizing over this local neighborhood . <eos> somewhat surprisingly , we show that this algorithm finds a near optimal assignment within $ 2n\ln n $ iterations on average and with high probability for { \em any } $ n $ node pair-wise mrf with { \em geometry } ( i.e . mrf graph with polynomial growth ) with the approximation error depending on ( in a reasonable manner ) the geometric growth rate of the graph and the average radius of the local neighborhood -- this allows for a graceful tradeoff between the complexity of the algorithm and the approximation error . <eos> through extensive simulations , we show that our algorithm finds extremely good approximate solutions for various kinds of mrfs with geometry .
we prove that linear projections between distribution families with fixed first and second moments are surjective , regardless of dimension . <eos> we further extend this result to families that respect additional constraints , such as symmetry , unimodality and log-concavity . <eos> by combining our results with classic univariate inequalities , we provide new worst-case analyses for natural risk criteria arising in different fields . <eos> one discovery is that portfolio selection under the worst-case value-at-risk and conditional value-at-risk criteria yields identical portfolios .
we provide a clustering algorithm that approximately optimizes the k-means objective , in the one-pass streaming setting . <eos> we make no assumptions about the data , and our algorithm is very light-weight in terms of memory , and computation . <eos> this setting is applicable to unsupervised learning on massive data sets , or resource-constrained devices . <eos> the two main ingredients of our theoretical work are : a derivation of an extremely simple pseudo-approximation batch algorithm for k-means , in which the algorithm is allowed to output more than k centers ( based on the recent k-means++ '' ) , and a streaming clustering algorithm in which batch clustering algorithms are performed on small inputs ( fitting in memory ) and combined in a hierarchical manner . <eos> empirical evaluations on real and simulated data reveal the practical utility of our method . ''
in this article , we propose fast subtree kernels on graphs . <eos> on graphs with n nodes and m edges and maximum degree d , these kernels comparing subtrees of height h can be computed in o ( mh ) , whereas the classic subtree kernel by ramon & g ? artner scales as o ( n2 4d h ) . <eos> key to this efficiency is the observation that the weisfeiler-lehman test of isomorphism from graph theory elegantly computes a subtree kernel as a byproduct . <eos> our fast subtree kernels can deal with labeled graphs , scale up easily to large graphs and outperform state-of-the-art graph kernels on several classification benchmark datasets in terms of accuracy and runtime .
a fundamental objective in reinforcement learning is the maintenance of a proper balance between exploration and exploitation . <eos> this problem becomes more challenging when the agent can only partially observe the states of its environment . <eos> in this paper we propose a dual-policy method for jointly learning the agent behavior and the balance between exploration exploitation , in partially observable environments . <eos> the method subsumes traditional exploration , in which the agent takes actions to gather information about the environment , and active learning , in which the agent queries an oracle for optimal actions ( with an associated cost for employing the oracle ) . <eos> the form of the employed exploration is dictated by the specific problem . <eos> theoretical guarantees are provided concerning the optimality of the balancing of exploration and exploitation . <eos> the effectiveness of the method is demonstrated by experimental results on benchmark problems .
dependencies among neighbouring labels in a sequence is an important source of information for sequence labeling problems . <eos> however , only dependencies between adjacent labels are commonly exploited in practice because of the high computational complexity of typical inference algorithms when longer distance dependencies are taken into account . <eos> in this paper , we show that it is possible to design efficient inference algorithms for a conditional random field using features that depend on long consecutive label sequences ( high-order features ) , as long as the number of distinct label sequences in the features used is small . <eos> this leads to efficient learning algorithms for these conditional random fields . <eos> we show experimentally that exploiting dependencies using high-order features can lead to substantial performance improvements for some problems and discuss conditions under which high-order features can be effective .
many researchers have suggested that the psychological complexity of a concept is related to the length of its representation in a language of thought . <eos> as yet , however , there are few concrete proposals about the nature of this language . <eos> this paper makes one such proposal : the language of thought allows first order quantification ( quantification over objects ) more readily than second-order quantification ( quantification over features ) . <eos> to support this proposal we present behavioral results from a concept learning study inspired by the work of shepard , hovland and jenkins . ''
regularized risk minimization often involves non-smooth optimization , either because of the loss function ( e.g. , hinge loss ) or the regularizer ( e.g. , $ \ell_1 $ -regularizer ) . <eos> gradient descent methods , though highly scalable and easy to implement , are known to converge slowly on these problems . <eos> in this paper , we develop novel accelerated gradient methods for stochastic optimization while still preserving their computational simplicity and scalability . <eos> the proposed algorithm , called sage ( stochastic accelerated gradient ) , exhibits fast convergence rates on stochastic optimization with both convex and strongly convex objectives . <eos> experimental results show that sage is faster than recent ( sub ) gradient methods including folos , smidas and scd . <eos> moreover , sage can also be extended for online learning , resulting in a simple but powerful algorithm .
we study pool-based active learning in the presence of noise , i.e . the agnostic setting . <eos> previous works have shown that the effectiveness of agnostic active learning depends on the learning problem and the hypothesis space . <eos> although there are many cases on which active learning is very useful , it is also easy to construct examples that no active learning algorithm can have advantage . <eos> in this paper , we propose intuitively reasonable sufficient conditions under which agnostic active learning algorithm is strictly superior to passive supervised learning . <eos> we show that under some noise condition , if the classification boundary and the underlying distribution are smooth to a finite order , active learning achieves polynomial improvement in the label complexity ; if the boundary and the distribution are infinitely smooth , the improvement is exponential .
we consider the problem of learning the structure of ising models ( pairwise binary markov random fields ) from i.i.d . <eos> samples . <eos> while several methods have been proposed to accomplish this task , their relative merits and limitations remain somewhat obscure . <eos> by analyzing a number of concrete examples , we show that low-complexity algorithms systematically fail when the markov random field develops long-range correlations . <eos> more precisely , this phenomenon appears to be related to the ising model phase transition ( although it does not coincide with it ) .
we present an approach for learning stochastic geometric models of object categories from single view images . <eos> we focus here on models expressible as a spatially contiguous assemblage of blocks . <eos> model topologies are learned across groups of images , and one or more such topologies is linked to an object category ( e.g . chairs ) . <eos> fitting learned topologies to an image can be used to identify the object class , as well as detail its geometry . <eos> the latter goes beyond labeling objects , as it provides the geometric structure of particular instances . <eos> we learn the models using joint statistical inference over structure parameters , camera parameters , and instance parameters . <eos> these produce an image likelihood through a statistical imaging model . <eos> we use trans-dimensional sampling to explore topology hypotheses , and alternate between metropolis-hastings and stochastic dynamics to explore instance parameters . <eos> experiments on images of furniture objects such as tables and chairs suggest that this is an effective approach for learning models that encode simple representations of category geometry and the statistics thereof , and support inferring both category and geometry on held out single view images .
we show that convex kl-regularized objective functions are obtained from a pac-bayes risk bound when using convex loss functions for the stochastic gibbs classifier that upper-bound the standard zero-one loss used for the weighted majority vote . <eos> by restricting ourselves to a class of posteriors , that we call quasi uniform , we propose a simple coordinate descent learning algorithm to minimize the proposed kl-regularized cost function . <eos> we show that standard ell_p-regularized objective functions currently used , such as ridge regression and ell_p-regularized boosting , are obtained from a relaxation of the kl divergence between the quasi uniform posterior and the uniform prior . <eos> we present numerical experiments where the proposed learning algorithm generally outperforms ridge regression and adaboost .
we develop a convex relaxation of maximum a posteriori estimation of a mixture of regression models . <eos> although our relaxation involves a semidefinite matrix variable , we reformulate the problem to eliminate the need for general semidefinite programming . <eos> in particular , we provide two reformulations that admit fast algorithms . <eos> the first is a max-min spectral reformulation exploiting quasi-newton descent . <eos> the second is a min-min reformulation consisting of fast alternating steps of closed-form updates . <eos> we evaluate the methods against expectation-maximization in a real problem of motion segmentation from video data .
we describe a new algorithmic framework for inference in probabilistic models , and apply it to inference for latent dirichlet allocation . <eos> our framework adopts the methodology of variational inference , but unlike existing variational methods such as mean field and expectation propagation it is not restricted to tractable classes of approximating distributions . <eos> our approach can also be viewed as a sequential monte carlo ( smc ) method , but unlike existing smc methods there is no need to design the artificial sequence of distributions . <eos> notably , our framework offers a principled means to exchange the variance of an importance sampling estimate for the bias incurred through variational approximation . <eos> experiments on a challenging inference problem in population genetics demonstrate improvements in stability and accuracy over existing methods , and at a comparable cost .
we consider multi-label prediction problems with large output spaces under the assumption of output sparsity ? <eos> that the target ( label ) vectors have small support . <eos> we develop a general theory for a variant of the popular error correcting output code scheme , using ideas from compressed sensing for exploiting this sparsity . <eos> the method can be regarded as a simple reduction from multi-label regression problems to binary regression problems . <eos> we show that the number of subproblems need only be logarithmic in the total number of possible labels , making this approach radically more efficient than others . <eos> we also state and prove robustness guarantees for this method in the form of regret transform bounds ( in general ) , and also provide a more detailed analysis for the linear prediction setting .
solving multi-agent reinforcement learning problems has proven difficult because of the lack of tractable algorithms . <eos> we provide the first approximation algorithm which solves stochastic games to within $ \epsilon $ relative error of the optimal game-theoretic solution , in time polynomial in $ 1/\epsilon $ . <eos> our algorithm extends murrays and gordon ? s ( 2007 ) modified bellman equation which determines the \emph { set } of all possible achievable utilities ; this provides us a truly general framework for multi-agent learning . <eos> further , we empirically validate our algorithm and find the computational cost to be orders of magnitude less than what the theory predicts .
kernel density estimation is the most widely-used practical method for accurate nonparametric density estimation . <eos> however , long-standing worst-case theoretical results showing that its performance worsens exponentially with the dimension of the data have quashed its application to modern high-dimensional datasets for decades . <eos> in practice , it has been recognized that often such data have a much lower-dimensional intrinsic structure . <eos> we propose a small modification to kernel density estimation for estimating probability density functions on riemannian submanifolds of euclidean space . <eos> using ideas from riemannian geometry , we prove the consistency of this modified estimator and show that the convergence rate is determined by the intrinsic dimension of the submanifold . <eos> we conclude with empirical results demonstrating the behavior predicted by our theory .
in this paper we make several contributions towards accelerating approximate bayesian structural inference for non-decomposable ggms . <eos> our first contribution is to show how to efficiently compute a bic or laplace approximation to the marginal likelihood of non-decomposable graphs using convex methods for precision matrix estimation . <eos> this optimization technique can be used as a fast scoring function inside standard stochastic local search ( sls ) for generating posterior samples . <eos> our second contribution is a novel framework for efficiently generating large sets of high-quality graph topologies without performing local search . <eos> this graph proposal method , which we call neighborhood fusion '' ( nf ) , samples candidate markov blankets at each node using sparse regression techniques . <eos> our final contribution is a hybrid method combining the complementary strengths of nf and sls . <eos> experimental results in structural recovery and prediction tasks demonstrate that nf and hybrid nf/sls out-perform state-of-the-art local search methods , on both synthetic and real-world datasets , when realistic computational limits are imposed . ''
multiple object tracking is a task commonly used to investigate the architecture of human visual attention . <eos> human participants show a distinctive pattern of successes and failures in tracking experiments that is often attributed to limits on an object system , a tracking module , or other specialized cognitive structures . <eos> here we use a computational analysis of the task of object tracking to ask which human failures arise from cognitive limitations and which are consequences of inevitable perceptual uncertainty in the tracking task . <eos> we find that many human performance phenomena , measured through novel behavioral experiments , are naturally produced by the operation of our ideal observer model ( a rao-blackwelized particle filter ) . <eos> the tradeoff between the speed and number of objects being tracked , however , can only arise from the allocation of a flexible cognitive resource , which can be formalized as either memory or attention .
this paper proposes a new algorithm for the linear least squares problem where the unknown variables are constrained to be in a finite set . <eos> the factor graph that corresponds to this problem is very loopy ; in fact , it is a complete graph . <eos> hence , applying the belief propagation ( bp ) algorithm yields very poor results . <eos> the algorithm described here is based on an optimal tree approximation of the gaussian density of the unconstrained linear system . <eos> it is shown that even though the approximation is not directly applied to the exact discrete distribution , applying the bp algorithm to the modified factor graph outperforms current methods in terms of both performance and complexity . <eos> the improved performance of the proposed algorithm is demonstrated on the problem of mimo detection .
score functions induced by generative models extract fixed-dimension feature vectors from different-length data observations by subsuming the process of data generation , projecting them in highly informative spaces called score spaces . <eos> in this way , standard discriminative classifiers are proved to achieve higher performances than a solely generative or discriminative approach . <eos> in this paper , we present a novel score space that exploits the free energy associated to a generative model through a score function . <eos> this function aims at capturing both the uncertainty of the model learning and `` local compliance of data observations with respect to the generative process . <eos> theoretical justifications and convincing comparative classification results on various generative models prove the goodness of the proposed strategy .
second-order maximum-entropy models have recently gained much interest for describing the statistics of binary spike trains . <eos> here , we extend this approach to take continuous stimuli into account as well . <eos> by constraining the joint second-order statistics , we obtain a joint gaussian-boltzmann distribution of continuous stimuli and binary neural firing patterns , for which we also compute marginal and conditional distributions . <eos> this model has the same computational complexity as pure binary models and fitting it to data is a convex problem . <eos> we show that the model can be seen as an extension to the classical spike-triggered average/covariance analysis and can be used as a non-linear method for extracting features which a neural population is sensitive to . <eos> further , by calculating the posterior distribution of stimuli given an observed neural response , the model can be used to decode stimuli and yields a natural spike-train metric . <eos> therefore , extending the framework of maximum-entropy models to continuous variables allows us to gain novel insights into the relationship between the firing patterns of neural ensembles and the stimuli they are processing .
large , relational factor graphs with structure defined by first-order logic or other languages give rise to notoriously difficult inference problems . <eos> because unrolling the structure necessary to represent distributions over all hypotheses has exponential blow-up , solutions are often derived from mcmc . <eos> however , because of limitations in the design and parameterization of the jump function , these sampling-based methods suffer from local minima|the system must transition through lower-scoring configurations before arriving at a better map solution . <eos> this paper presents a new method of explicitly selecting fruitful downward jumps by leveraging reinforcement learning ( rl ) . <eos> rather than setting parameters to maximize the likelihood of the training data , parameters of the factor graph are treated as a log-linear function approximator and learned with temporal difference ( td ) ; map inference is performed by executing the resulting policy on held out test data . <eos> our method allows efficient gradient updates since only factors in the neighborhood of variables affected by an action need to be computed|we bypass the need to compute marginals entirely . <eos> our method provides dramatic empirical success , producing new state-of-the-art results on a complex joint model of ontology alignment , with a 48\ % reduction in error over state-of-the-art in that domain .
the indian buffet process is a bayesian nonparametric approach that models objects as arising from an infinite number of latent factors . <eos> here we extend the latent factor model framework to two or more unbounded layers of latent factors . <eos> from a generative perspective , each layer defines a conditional \emph { factorial } prior distribution over the binary latent variables of the layer below via a noisy-or mechanism . <eos> we explore the properties of the model with two empirical studies , one digit recognition task and one music tag data experiment .
we propose a new model for natural image statistics . <eos> instead of minimizing dependency between components of natural images , we maximize a simple form of dependency in the form of tree-dependency . <eos> by learning filters and tree structures which are best suited for natural images we observe that the resulting filters are edge filters , similar to the famous ica on natural images results . <eos> calculating the likelihood of the model requires estimating the squared output of pairs of filters connected in the tree . <eos> we observe that after learning , these pairs of filters are predominantly of similar orientations but different phases , so their joint energy resembles models of complex cells .
we present a nonparametric hierarchical bayesian model of document collections that decouples sparsity and smoothness in the component distributions ( i.e. , the `` topics ) . <eos> in the sparse topic model ( stm ) , each topic is represented by a bank of selector variables that determine which terms appear in the topic . <eos> thus each topic is associated with a subset of the vocabulary , and topic smoothness is modeled on this subset . <eos> we develop an efficient gibbs sampler for the stm that includes a general-purpose method for sampling from a dirichlet mixture with a combinatorial number of components . <eos> we demonstrate the stm on four real-world datasets . <eos> compared to traditional approaches , the empirical results show that stms give better predictive performance with simpler inferred models .
to estimate the changing structure of a varying-coefficient varying-structure ( vcvs ) model remains an important and open problem in dynamic system modelling , which includes learning trajectories of stock prices , or uncovering the topology of an evolving gene network . <eos> in this paper , we investigate sparsistent learning of a sub-family of this model -- - piecewise constant vcvs models . <eos> we analyze two main issues in this problem : inferring time points where structural changes occur and estimating model structure ( i.e. , model selection ) on each of the constant segments . <eos> we propose a two-stage adaptive procedure , which first identifies jump points of structural changes and then identifies relevant covariates to a response on each of the segments . <eos> we provide an asymptotic analysis of the procedure , showing that with the increasing sample size , number of structural changes , and number of variables , the true model can be consistently selected . <eos> we demonstrate the performance of the method on synthetic data and apply it to the brain computer interface dataset . <eos> we also consider how this applies to structure estimation of time-varying probabilistic graphical models .
the method of common spatio-spectral patterns ( cssps ) is an extension of common spatial patterns ( csps ) by utilizing the technique of delay embedding to alleviate the adverse effects of noises and artifacts on the electroencephalogram ( eeg ) classification . <eos> although the cssps method has shown to be more powerful than the csps method in the eeg classification , this method is only suitable for two-class eeg classification problems . <eos> in this paper , we generalize the two-class cssps method to multi-class cases . <eos> to this end , we first develop a novel theory of multi-class bayes error estimation and then present the multi-class cssps ( mcssps ) method based on this bayes error theoretical framework . <eos> by minimizing the estimated closed-form bayes error , we obtain the optimal spatio-spectral filters of mcssps . <eos> to demonstrate the effectiveness of the proposed method , we conduct extensive experiments on the data set of bci competition 2005 . <eos> the experimental results show that our method significantly outperforms the previous multi-class csps ( mcsps ) methods in the eeg classification .
the purpose of the paper is to explore the connection between multivariate homogeneity tests and $ \auc $ optimization . <eos> the latter problem has recently received much attention in the statistical learning literature . <eos> from the elementary observation that , in the two-sample problem setup , the null assumption corresponds to the situation where the area under the optimal roc curve is equal to 1/2 , we propose a two-stage testing method based on data splitting . <eos> a nearly optimal scoring function in the auc sense is first learnt from one of the two half-samples . <eos> data from the remaining half-sample are then projected onto the real line and eventually ranked according to the scoring function computed at the first stage . <eos> the last step amounts to performing a standard mann-whitney wilcoxon test in the one-dimensional framework . <eos> we show that the learning step of the procedure does not affect the consistency of the test as well as its properties in terms of power , provided the ranking produced is accurate enough in the auc sense . <eos> the results of a numerical experiment are eventually displayed in order to show the efficiency of the method .
the linear correlation coefficient is typically used to characterize and analyze dependencies of neural spike counts . <eos> here , we show that the correlation coefficient is in general insufficient to characterize these dependencies . <eos> we construct two neuron spike count models with poisson-like marginals and vary their dependence structure using copulas . <eos> to this end , we construct a copula that allows to keep the spike counts uncorrelated while varying their dependence strength . <eos> moreover , we employ a network of leaky integrate-and-fire neurons to investigate whether weakly correlated spike counts with strong dependencies are likely to occur in real networks . <eos> we find that the entropy of uncorrelated but dependent spike count distributions can deviate from the corresponding distribution with independent components by more than 25 % and that weakly correlated but strongly dependent spike counts are very likely to occur in biological networks . <eos> finally , we introduce a test for deciding whether the dependence structure of distributions with poisson-like marginals is well characterized by the linear correlation coefficient and verify it for different copula-based models .
we investigate how well gaussian process regression can learn functions defined on graphs , using large regular random graphs as a paradigmatic example . <eos> random-walk based kernels are shown to have some surprising properties : within the standard approximation of a locally tree-like graph structure , the kernel does not become constant , i.e.neighbouring function values do not become fully correlated , when the lengthscale $ \sigma $ of the kernel is made large . <eos> instead the kernel attains a non-trivial limiting form , which we calculate . <eos> the fully correlated limit is reached only once loops become relevant , and we estimate where the crossover to this regime occurs . <eos> our main subject are learning curves of bayes error versus training set size . <eos> we show that these are qualitatively well predicted by a simple approximation using only the spectrum of a large tree as input , and generically scale with $ n/v $ , the number of training examples per vertex . <eos> we also explore how this behaviour changes once kernel lengthscales are large enough for loops to become important .
synapses exhibit an extraordinary degree of short-term malleability , with release probabilities and effective synaptic strengths changing markedly over multiple timescales . <eos> from the perspective of a fixed computational operation in a network , this seems like a most unacceptable degree of added noise . <eos> we suggest an alternative theory according to which short term synaptic plasticity plays a normatively-justifiable role . <eos> this theory starts from the commonplace observation that the spiking of a neuron is an incomplete , digital , report of the analog quantity that contains all the critical information , namely its membrane potential . <eos> we suggest that one key task for a synapse is to solve the inverse problem of estimating the pre-synaptic membrane potential from the spikes it receives and prior expectations , as in a recursive filter . <eos> we show that short-term synaptic depression has canonical dynamics which closely resemble those required for optimal estimation , and that it indeed supports high quality estimation . <eos> under this account , the local postsynaptic potential and the level of synaptic resources track the ( scaled ) mean and variance of the estimated presynaptic membrane potential . <eos> we make experimentally testable predictions for how the statistics of subthreshold membrane potential fluctuations and the form of spiking non-linearity should be related to the properties of short-term plasticity in any particular cell type .
we present a theory of compositionality in stochastic optimal control , showing how task-optimal controllers can be constructed from certain primitives . <eos> the primitives are themselves feedback controllers pursuing their own agendas . <eos> they are mixed in proportion to how much progress they are making towards their agendas and how compatible their agendas are with the present task . <eos> the resulting composite control law is provably optimal when the problem belongs to a certain class . <eos> this class is rather general and yet has a number of unique properties - one of which is that the bellman equation can be made linear even for non-linear or discrete dynamics . <eos> this gives rise to the compositionality developed here . <eos> in the special case of linear dynamics and gaussian noise our framework yields analytical solutions ( i.e . non-linear mixtures of linear-quadratic regulators ) without requiring the final cost to be quadratic . <eos> more generally , a natural set of control primitives can be constructed by applying svd to greens function of the bellman equation . <eos> we illustrate the theory in the context of human arm movements . <eos> the ideas of optimality and compositionality are both very prominent in the field of motor control , yet they are hard to reconcile . <eos> our work makes this possible .
we discuss the framework of transductive support vector machine ( tsvm ) from the perspective of the regularization strength induced by the unlabeled data . <eos> in this framework , svm and tsvm can be regarded as a learning machine without regularization and one with full regularization from the unlabeled data , respectively . <eos> therefore , to supplement this framework of the regularization strength , it is necessary to introduce data-dependant partial regularization . <eos> to this end , we reformulate tsvm into a form with controllable regularization strength , which includes svm and tsvm as special cases . <eos> furthermore , we introduce a method of adaptive regularization that is data dependant and is based on the smoothness assumption . <eos> experiments on a set of benchmark data sets indicate the promising results of the proposed work compared with state-of-the-art tsvm algorithms .
the problem of approximating a given probability distribution using a simpler distribution plays an important role in several areas of machine learning , e.g . variational inference and classification . <eos> within this context , we consider the task of learning a mixture of tree distributions . <eos> although mixtures of trees can be learned by minimizing the kl-divergence using an em algorithm , its success depends heavily on the initialization . <eos> we propose an efficient strategy for obtaining a good initial set of trees that attempts to cover the entire observed distribution by minimizing the $ \alpha $ -divergence with $ \alpha = \infty $ . <eos> we formulate the problem using the fractional covering framework and present a convergent sequential algorithm that only relies on solving a convex program at each iteration . <eos> compared to previous methods , our approach results in a significantly smaller mixture of trees that provides similar or better accuracies . <eos> we demonstrate the usefulness of our approach by learning pictorial structures for face recognition .
we consider the problem of using nearest neighbor methods to provide a conditional probability estimate , p ( y|a ) , when the number of labels y is large and the labels share some underlying structure . <eos> we propose a method for learning error-correcting output codes ( ecocs ) to model the similarity between labels within a nearest neighbor framework . <eos> the learned ecocs and nearest neighbor information are used to provide conditional probability estimates . <eos> we apply these estimates to the problem of acoustic modeling for speech recognition . <eos> we demonstrate an absolute reduction in word error rate ( wer ) of 0.9 % ( a 2.5 % relative reduction in wer ) on a lecture recognition task over a state-of-the-art baseline gmm model .
as the availability and importance of relational data -- such as the friendships summarized on a social networking website -- increases , it becomes increasingly important to have good models for such data . <eos> the kinds of latent structure that have been considered for use in predicting links in such networks have been relatively limited . <eos> in particular , the machine learning community has focused on latent class models , adapting nonparametric bayesian methods to jointly infer how many latent classes there are while learning which entities belong to each class . <eos> we pursue a similar approach with a richer kind of latent variable -- latent features -- using a nonparametric bayesian technique to simultaneously infer the number of features at the same time we learn which entities have each feature . <eos> the greater expressiveness of this approach allows us to improve link prediction on three datasets .
in this paper we study the problem of learning a low-dimensional ( sparse ) distance matrix . <eos> we propose a novel metric learning model which can simultaneously conduct dimension reduction and learn a distance matrix . <eos> the sparse representation involves a mixed-norm regularization which is non-convex . <eos> we then show that it can be equivalently formulated as a convex saddle ( min-max ) problem . <eos> from this saddle representation , we develop an efficient smooth optimization approach for sparse metric learning although the learning model is based on a non-differential loss function . <eos> this smooth optimization approach has an optimal convergence rate of $ o ( 1 /\ell^2 ) $ for smooth problems where $ \ell $ is the iteration number . <eos> finally , we run experiments to validate the effectiveness and efficiency of our sparse metric learning model on various datasets .
we present arow , a new online learning algorithm that combines several properties of successful : large margin training , confidence weighting , and the capacity to handle non-separable data . <eos> arow performs adaptive regularization of the prediction function upon seeing each new instance , allowing it to perform especially well in the presence of label noise . <eos> we derive a mistake bound , similar in form to the second order perceptron bound , which does not assume separability . <eos> we also relate our algorithm to recent confidence-weighted online learning techniques and empirically show that arow achieves state-of-the-art performance and notable robustness in the case of non-separable data .
existing models of categorization typically represent to-be-classified items as points in a multidimensional space . <eos> while from a mathematical point of view , an infinite number of basis sets can be used to represent points in this space , the choice of basis set is psychologically crucial . <eos> people generally choose the same basis dimensions , and have a strong preference to generalize along the axes of these dimensions , but not diagonally '' . <eos> what makes some choices of dimension special ? <eos> we explore the idea that the dimensions used by people echo the natural variation in the environment . <eos> specifically , we present a rational model that does not assume dimensions , but learns the same type of dimensional generalizations that people display . <eos> this bias is shaped by exposing the model to many categories with a structure hypothesized to be like those which children encounter . <eos> our model can be viewed as a type of transformed dirichlet process mixture model , where it is the learning of the base distribution of the dirichlet process which allows dimensional generalization.the learning behaviour of our model captures the developmental shift from roughly `` isotropic '' for children to the axis-aligned generalization that adults show . ''
a crucial technique for scaling kernel methods to very large data sets reaching or exceeding millions of instances is based on low-rank approximation of kernel matrices . <eos> we introduce a new family of algorithms based on mixtures of nystrom approximations , ensemble nystrom algorithms , that yield more accurate low-rank approximations than the standard nystrom method . <eos> we give a detailed study of multiple variants of these algorithms based on simple averaging , an exponential weight method , or regression-based methods . <eos> we also present a theoretical analysis of these algorithms , including novel error bounds guaranteeing a better convergence rate than the standard nystrom method . <eos> finally , we report the results of extensive experiments with several data sets containing up to 1m points demonstrating the signi ? cant performance improvements gained over the standard nystrom approximation .
non-parametric bayesian techniques are considered for learning dictionaries for sparse image representations , with applications in denoising , inpainting and compressive sensing ( cs ) . <eos> the beta process is employed as a prior for learning the dictionary , and this non-parametric method naturally infers an appropriate dictionary size . <eos> the dirichlet process and a probit stick-breaking process are also considered to exploit structure within an image . <eos> the proposed method can learn a sparse dictionary in situ ; training images may be exploited if available , but they are not required . <eos> further , the noise variance need not be known , and can be non-stationary . <eos> another virtue of the proposed method is that sequential inference can be readily employed , thereby allowing scaling to large images . <eos> several example results are presented , using both gibbs and variational bayesian inference , with comparisons to other state-of-the-art approaches .
we present a new framework for semi-supervised learning with sparse eigenfunction bases of kernel matrices . <eos> it turns out that when the \emph { cluster assumption } holds , that is , when the high density regions are sufficiently separated by low density valleys , each high density area corresponds to a unique representative eigenvector . <eos> linear combination of such eigenvectors ( or , more precisely , of their nystrom extensions ) provide good candidates for good classification functions . <eos> by first choosing an appropriate basis of these eigenvectors from unlabeled data and then using labeled data with lasso to select a classifier in the span of these eigenvectors , we obtain a classifier , which has a very sparse representation in this basis . <eos> importantly , the sparsity appears naturally from the cluster assumption . <eos> experimental results on a number of real-world data-sets show that our method is competitive with the state of the art semi-supervised learning algorithms and outperforms the natural base-line algorithm ( lasso in the kernel pca basis ) .
over recent years dirichlet processes and the associated chinese restaurant process ( crp ) have found many applications in clustering while the indian buffet process ( ibp ) is increasingly used to describe latent feature models . <eos> in the clustering case , we associate to each data point a latent allocation variable . <eos> these latent variables can share the same value and this induces a partition of the data set . <eos> the crp is a prior distribution on such partitions . <eos> in latent feature models , we associate to each data point a potentially infinite number of binary latent variables indicating the possession of some features and the ibp is a prior distribution on the associated infinite binary matrix . <eos> these prior distributions are attractive because they ensure exchangeability ( over samples ) . <eos> we propose here extensions of these models to decomposable graphs . <eos> these models have appealing properties and can be easily learned using monte carlo techniques .
implementations of topic models typically use symmetric dirichlet priors with fixed concentration parameters , with the implicit assumption that such smoothing parameters '' have little practical effect . <eos> in this paper , we explore several classes of structured priors for topic models . <eos> we find that an asymmetric dirichlet prior over the document-topic distributions has substantial advantages over a symmetric prior , while an asymmetric prior over the topic-word distributions provides no real benefit . <eos> approximation of this prior structure through simple , efficient hyperparameter optimization steps is sufficient to achieve these performance gains . <eos> the prior structure we advocate substantially increases the robustness of topic models to variations in the number of topics and to the highly skewed word frequency distributions common in natural language . <eos> since this prior structure can be implemented using efficient algorithms that add negligible cost beyond standard inference techniques , we recommend it as a new standard for topic modeling . ''
little work has been done to directly combine the outputs of multiple supervised and unsupervised models . <eos> however , it can increase the accuracy and applicability of ensemble methods . <eos> first , we can boost the diversity of classification ensemble by incorporating multiple clustering outputs , each of which provides grouping constraints for the joint label predictions of a set of related objects . <eos> secondly , ensemble of supervised models is limited in applications which have no access to raw data but to the meta-level model outputs . <eos> in this paper , we aim at calculating a consolidated classification solution for a set of objects by maximizing the consensus among both supervised predictions and unsupervised grouping constraints . <eos> we seek a global optimal label assignment for the target objects , which is different from the result of traditional majority voting and model combination approaches . <eos> we cast the problem into an optimization problem on a bipartite graph , where the objective function favors smoothness in the conditional probability estimates over the graph , as well as penalizes deviation from initial labeling of supervised models . <eos> we solve the problem through iterative propagation of conditional probability estimates among neighboring nodes , and interpret the method as conducting a constrained embedding in a transformed space , as well as a ranking on the graph . <eos> experimental results on three real applications demonstrate the benefits of the proposed method over existing alternatives .
we show how to model documents as bags of words using family of two-layer , undirected graphical models . <eos> each member of the family has the same number of binary hidden units but a different number of `` softmax visible units . <eos> all of the softmax units in all of the models in the family share the same weights to the binary hidden units . <eos> we describe efficient inference and learning procedures for such a family . <eos> each member of the family models the probability distribution of documents of a specific length as a product of topic-specific distributions rather than as a mixture and this gives much better generalization than latent dirichlet allocation for modeling the log probabilities of held-out documents . <eos> the low-dimensional topic vectors learned by the undirected family are also much better than lda topic vectors for retrieving documents that are similar to a query document . <eos> the learned topics are more general than those found by lda because precision is achieved by intersecting many general topics rather than by selecting a single precise topic to generate each word .
the worst-case complexity of general decentralized pomdps , which are equivalent to partially observable stochastic games ( posgs ) is very high , both for the cooperative and competitive cases . <eos> some reductions in complexity have been achieved by exploiting independence relations in some models . <eos> we show that these results are somewhat limited : when these independence assumptions are relaxed in very small ways , complexity returns to that of the general case .
in this paper , we develop an efficient moments-based permutation test approach to improve the system ? s efficiency by approximating the permutation distribution of the test statistic with pearson distribution series . <eos> this approach involves the calculation of the first four moments of the permutation distribution . <eos> we propose a novel recursive method to derive these moments theoretically and analytically without any permutation . <eos> experimental results using different test statistics are demonstrated using simulated data and real data . <eos> the proposed strategy takes advantage of nonparametric permutation tests and parametric pearson distribution approximation to achieve both accuracy and efficiency .
continuous-time markov chains are used to model systems in which transitions between states as well as the time the system spends in each state are random . <eos> many computational problems related to such chains have been solved , including determining state distributions as a function of time , parameter estimation , and control . <eos> however , the problem of inferring most likely trajectories , where a trajectory is a sequence of states as well as the amount of time spent in each state , appears unsolved . <eos> we study three versions of this problem : ( i ) an initial value problem , in which an initial state is given and we seek the most likely trajectory until a given final time , ( ii ) a boundary value problem , in which initial and final states and times are given , and we seek the most likely trajectory connecting them , and ( iii ) trajectory inference under partial observability , analogous to finding maximum likelihood trajectories for hidden markov models . <eos> we show that maximum likelihood trajectories are not always well-defined , and describe a polynomial time test for well-definedness . <eos> when well-definedness holds , we show that each of the three problems can be solved in polynomial time , and we develop efficient dynamic programming algorithms for doing so .
we propose an unsupervised method that , given a word , automatically selects non-abstract senses of that word from an online ontology and generates images depicting the corresponding entities . <eos> when faced with the task of learning a visual model based only on the name of an object , a common approach is to find images on the web that are associated with the object name , and then train a visual classifier from the search result . <eos> as words are generally polysemous , this approach can lead to relatively noisy models if many examples due to outlier senses are added to the model . <eos> we argue that images associated with an abstract word sense should be excluded when training a visual classifier to learn a model of a physical object . <eos> while image clustering can group together visually coherent sets of returned images , it can be difficult to distinguish whether an image cluster relates to a desired object or to an abstract sense of the word . <eos> we propose a method that uses both image features and the text associated with the images to relate latent topics to particular senses . <eos> our model does not require any human supervision , and takes as input only the name of an object category . <eos> we show results of retrieving concrete-sense images in two available multimodal , multi-sense databases , as well as experiment with object classifiers trained on concrete-sense images returned by our method for a set of ten common office objects .
we present a system which constructs a topological map of an environment given a sequence of images . <eos> this system includes a novel image similarity score which uses dynamic programming to match images using both the appearance and relative positions of local features simultaneously . <eos> additionally an mrf is constructed to model the probability of loop-closures . <eos> a locally optimal labeling is found using loopy-bp . <eos> finally we outline a method to generate a topological map from loop closure data . <eos> results are presented on four urban sequences and one indoor sequence .
we visit the following fundamental problem : for a ` generic model of consumer choice ( namely , distributions over preference lists ) and a limited amount of data on how consumers actually make decisions ( such as marginal preference information ) , how may one predict revenues from offering a particular assortment of choices ? <eos> this problem is central to areas within operations research , marketing and econometrics . <eos> we present a framework to answer such questions and design a number of tractable algorithms ( from a data and computational standpoint ) for the same .
we consider the problem of learning probabilistic models for complex relational structures between various types of objects . <eos> a model can help us `` understand a dataset of relational facts in at least two ways , by finding interpretable structure in the data , and by supporting predictions , or inferences about whether particular unobserved relations are likely to be true . <eos> often there is a tradeoff between these two aims : cluster-based models yield more easily interpretable representations , while factorization-based approaches have better predictive performance on large data sets . <eos> we introduce the bayesian clustered tensor factorization ( bctf ) model , which embeds a factorized representation of relations in a nonparametric bayesian clustering framework . <eos> inference is fully bayesian but scales well to large data sets . <eos> the model simultaneously discovers interpretable clusters and yields predictive performance that matches or beats previous probabilistic models for relational data .
the long-standing problem of efficient nearest-neighbor ( nn ) search has ubiquitous applications ranging from astrophysics to mp3 fingerprinting to bioinformatics to movie recommendations . <eos> as the dimensionality of the dataset increases , exact nn search becomes computationally prohibitive ; ( 1+eps ) -distance-approximate nn search can provide large speedups but risks losing the meaning of nn search present in the ranks ( ordering ) of the distances . <eos> this paper presents a simple , practical algorithm allowing the user to , for the first time , directly control the true accuracy of nn search ( in terms of ranks ) while still achieving the large speedups over exact nn . <eos> experiments with high-dimensional datasets show that it often achieves faster and more accurate results than the best-known distance-approximate method , with much more stable behavior .
in this paper we explore the problem of biasing unsupervised models to favor sparsity . <eos> we extend the posterior regularization framework [ 8 ] to encourage the model to achieve posterior sparsity on the unlabeled training data . <eos> we apply this new method to learn ? rst-order hmms for unsupervised part-of-speech ( pos ) tagging , and show that hmms learned this way consistently and signi ? cantly out-performs both em-trained hmms , and hmms with a sparsity-inducing dirichlet prior trained by variational em . <eos> we evaluate these hmms on three languages english , bulgarian and portuguese under four conditions . <eos> we ? nd that our method always improves performance with respect to both baselines , while variational bayes actually degrades performance in most cases . <eos> we increase accuracy with respect to em by 2.5 % -8.7 % absolute and we see improvements even in a semisupervised condition where a limited dictionary is provided .
we describe probability distributions , dubbed compressible priors , whose independent and identically distributed ( iid ) realizations result in compressible signals . <eos> a signal is compressible when sorted magnitudes of its coefficients exhibit a power-law decay so that the signal can be well-approximated by a sparse signal . <eos> since compressible signals live close to sparse signals , their intrinsic information can be stably embedded via simple non-adaptive linear projections into a much lower dimensional space whose dimension grows logarithmically with the ambient signal dimension . <eos> by using order statistics , we show that n-sample iid realizations of generalized pareto , student ? s t , log-normal , frechet , and log-logistic distributions are compressible , i.e. , they have a constant expected decay rate , which is independent of n. in contrast , we show that generalized gaussian distribution with shape parameter q is compressible only in restricted cases since the expected decay rate of its n-sample iid realizations decreases with n as 1/ [ q log ( n/q ) ] . <eos> we use compressible priors as a scaffold to build new iterative sparse signal recovery algorithms based on bayesian inference arguments . <eos> we show how tuning of these algorithms explicitly depends on the parameters of the compressible prior of the signal , and how to learn the parameters of the signal ? s compressible prior on the fly during recovery .
in this paper we present a novel approach to learn directed acyclic graphs ( dag ) and factor models within the same framework while also allowing for model comparison between them . <eos> for this purpose , we exploit the connection between factor models and dags to propose bayesian hierarchies based on spike and slab priors to promote sparsity , heavy-tailed priors to ensure identifiability and predictive densities to perform the model comparison . <eos> we require identifiability to be able to produce variable orderings leading to valid dags and sparsity to learn the structures . <eos> the effectiveness of our approach is demonstrated through extensive experiments on artificial and biological data showing that our approach outperform a number of state of the art methods .
we introduce a new type of neural network activation function based on recent physiological rate models for complex cells in visual area v1 . <eos> a single-hidden-layer neural network of this kind of model achieves 1.5 % error on mnist . <eos> we also introduce an existing criterion for learning slow , decorrelated features as a pretraining strategy for image models . <eos> this pretraining strategy results in orientation-selective features , similar to the receptive fields of complex cells . <eos> with this pretraining , the same single-hidden-layer model achieves better generalization error , even though the pretraining sample distribution is very different from the fine-tuning distribution . <eos> to implement this pretraining strategy , we derive a fast algorithm for online learning of decorrelated features such that each iteration of the algorithm runs in linear time with respect to the number of features .
conditional random fields ( crf ) are quite successful on sequence labeling tasks such as natural language processing and biological sequence analysis . <eos> crf models use linear potential functions to represent the relationship between input features and outputs . <eos> however , in many real-world applications such as protein structure prediction and handwriting recognition , the relationship between input features and outputs is highly complex and nonlinear , which can not be accurately modeled by a linear function . <eos> to model the nonlinear relationship between input features and outputs we propose conditional neural fields ( cnf ) , a new conditional probabilistic graphical model for sequence labeling . <eos> our cnf model extends crf by adding one ( or possibly several ) middle layer between input features and outputs . <eos> the middle layer consists of a number of hidden parameterized gates , each acting as a local neural network node or feature extractor to capture the nonlinear relationship between input features and outputs . <eos> therefore , conceptually this cnf model is much more expressive than the linear crf model . <eos> to better control the complexity of the cnf model , we also present a hyperparameter optimization procedure within the evidence framework . <eos> experiments on two widely-used benchmarks indicate that this cnf model performs significantly better than a number of popular methods . <eos> in particular , our cnf model is the best among about ten machine learning methods for protein secondary tructure prediction and also among a few of the best methods for handwriting recognition .
across a wide range of cognitive tasks , recent experience in ? uences behavior . <eos> for example , when individuals repeatedly perform a simple two-alternative forced-choice task ( 2afc ) , response latencies vary dramatically based on the immediately preceding trial sequence . <eos> these sequential effects have been interpreted as adaptation to the statistical structure of an uncertain , changing environment ( e.g . jones & sieck , 2003 ; mozer , kinoshita , & shettel , 2007 ; yu & cohen , 2008 ) . <eos> the dynamic belief model ( dbm ) ( yu & cohen , 2008 ) explains sequential effects in 2afc tasks as a rational consequence of a dynamic internal representation that tracks second-order statistics of the trial sequence ( repetition rates ) and predicts whether the upcoming trial will be a repetition or an alternation of the previous trial . <eos> experimental results suggest that ? rst-order statistics ( base rates ) also in ? uence sequential effects . <eos> we propose a model that learns both ? rst- and second-order sequence properties , each according to the basic principles of the dbm but under a uni ? ed inferential framework . <eos> this model , the dynamic belief mixture model ( dbm2 ) , obtains precise , parsimonious ? ts to data . <eos> furthermore , the model predicts dissociations in behavioral ( maloney , dal martello , sahm , & spillmann , 2005 ) and electrophysiological studies ( jentzsch & sommer , 2002 ) , supporting the psychological and neurobiological reality of its two components .
we consider an online decision problem over a discrete space in which the loss function is submodular . <eos> we give algorithms which are computationally efficient and are hannan-consistent in both the full information and bandit settings .
we introduce a new type of deep belief net and evaluate it on a 3d object recognition task . <eos> the top-level model is a third-order boltzmann machine , trained using a hybrid algorithm that combines both generative and discriminative gradients . <eos> performance is evaluated on the norb database ( normalized-uniform version ) , which contains stereo-pair images of objects under different lighting conditions and viewpoints . <eos> our model achieves 6.5 % error on the test set , which is close to the best published result for norb ( 5.9 % ) using a convolutional neural net that has built-in knowledge of translation invariance . <eos> it substantially outperforms shallow models such as svms ( 11.6 % ) . <eos> dbns are especially suited for semi-supervised learning , and to demonstrate this we consider a modified version of the norb recognition task in which additional unlabeled images are created by applying small translations to the images in the database . <eos> with the extra unlabeled data ( and the same amount of labeled data as before ) , our model achieves 5.2 % error , making it the current best result for norb .
we adapt a probabilistic latent variable model , namely gap ( gamma-poisson ) , to ad targeting in the contexts of sponsored search ( ss ) and behaviorally targeted ( bt ) display advertising . <eos> we also approach the important problem of ad positional bias by formulating a one-latent-dimension gap factorization . <eos> learning from click-through data is intrinsically large scale , even more so for ads . <eos> we scale up the algorithm to terabytes of real-world ss and bt data that contains hundreds of millions of users and hundreds of thousands of features , by leveraging the scalability characteristics of the algorithm and the inherent structure of the problem including data sparsity and locality . <eos> specifically , we demonstrate two somewhat orthogonal philosophies of scaling algorithms to large-scale problems , through the ss and bt implementations , respectively . <eos> finally , we report the experimental results using yahoos vast datasets , and show that our approach substantially outperform the state-of-the-art methods in prediction accuracy . <eos> for bt in particular , the roc area achieved by gap is exceeding 0.95 , while one prior approach using poisson regression yielded 0.83 . <eos> for computational performance , we compare a single-node sparse implementation with a parallel implementation using hadoop mapreduce , the results are counterintuitive yet quite interesting . <eos> we therefore provide insights into the underlying principles of large-scale learning .
in visual recognition , the images are frequently modeled as sets of local features ( bags ) . <eos> we show that bag of words , a common method to handle such cases , can be viewed as a special match kernel , which counts 1 if two local features fall into the same regions partitioned by visual words and 0 otherwise . <eos> despite its simplicity , this quantization is too coarse . <eos> it is , therefore , appealing to design match kernels that more accurately measure the similarity between local features . <eos> however , it is impractical to use such kernels on large datasets due to their significant computational cost . <eos> to address this problem , we propose an efficient match kernel ( emk ) , which maps local features to a low dimensional feature space , average the resulting feature vectors to form a set-level feature , then apply a linear classifier . <eos> the local feature maps are learned so that their inner products preserve , to the best possible , the values of the specified kernel function . <eos> emk is linear both in the number of images and in the number of local features . <eos> we demonstrate that emk is extremely efficient and achieves the current state of the art performance on three difficult real world datasets : scene-15 , caltech-101 and caltech-256 .
this paper introduces a new method for semi-supervised learning on high dimensional nonlinear manifolds , which includes a phase of unsupervised basis learning and a phase of supervised function learning . <eos> the learned bases provide a set of anchor points to form a local coordinate system , such that each data point x on the manifold can be locally approximated by a linear combination of its nearby anchor points , and the linear weights become its local coordinate coding . <eos> we show that a high dimensional nonlinear function can be approximated by a global linear function with respect to this coding scheme , and the approximation quality is ensured by the locality of such coding . <eos> the method turns a difficult nonlinear learning problem into a simple global linear learning problem , which overcomes some drawbacks of traditional local learning methods .
we present a general inference framework for inter-domain gaussian processes ( gps ) , focusing on its usefulness to build sparse gp models . <eos> the state-of-the-art sparse gp model introduced by snelson and ghahramani in [ 1 ] relies on finding a small , representative pseudo data set of m elements ( from the same domain as the n available data elements ) which is able to explain existing data well , and then uses it to perform inference . <eos> this reduces inference and model selection computation time from o ( n^3 ) to o ( m^2n ) , where m < < n. inter-domain gps can be used to find a ( possibly more compact ) representative set of features lying in a different domain , at the same computational cost . <eos> being able to specify a different domain for the representative features allows to incorporate prior knowledge about relevant characteristics of data and detaches the functional form of the covariance and basis functions . <eos> we will show how previously existing models fit into this framework and will use it to develop two new sparse gp models . <eos> tests on large , representative regression data sets suggest that significant improvement can be achieved , while retaining computational efficiency .
automated recovery from failures is a key component in the management of large data centers . <eos> such systems typically employ a hand-made controller created by an expert . <eos> while such controllers capture many important aspects of the recovery process , they are often not systematically optimized to reduce costs such as server downtime . <eos> in this paper we explain how to use data gathered from the interactions of the hand-made controller with the system , to create an optimized controller . <eos> we suggest learning an indefinite horizon partially observable markov decision process , a model for decision making under uncertainty , and solve it using a point-based algorithm . <eos> we describe the complete process , starting with data gathering , model learning , model checking procedures , and computing a policy . <eos> while our paper focuses on a specific domain , our method is applicable to other systems that use a hand-coded , imperfect controllers .
we consider the problem of variable group selection for least squares regression , namely , that of selecting groups of variables for best regression performance , leveraging and adhering to a natural grouping structure within the explanatory variables . <eos> we show that this problem can be efficiently addressed by using a certain greedy style algorithm . <eos> more precisely , we propose the group orthogonal matching pursuit algorithm ( group-omp ) , which extends the standard omp procedure ( also referred to as `` forward greedy feature selection algorithm for least squares regression ) to perform stage-wise group variable selection . <eos> we prove that under certain conditions group-omp can identify the correct ( groups of ) variables . <eos> we also provide an upperbound on the $ l_\infty $ norm of the difference between the estimated regression coefficients and the true coefficients . <eos> experimental results on simulated and real world datasets indicate that group-omp compares favorably to group lasso , omp and lasso , both in terms of variable selection and prediction accuracy .
this paper is concerned with the consistency analysis on listwise ranking methods . <eos> among various ranking methods , the listwise methods have competitive performances on benchmark datasets and are regarded as one of the state-of-the-art approaches . <eos> most listwise ranking methods manage to optimize ranking on the whole list ( permutation ) of objects , however , in practical applications such as information retrieval , correct ranking at the top k positions is much more important . <eos> this paper aims to analyze whether existing listwise ranking methods are statistically consistent in the top-k setting . <eos> for this purpose , we define a top-k ranking framework , where the true loss ( and thus the risks ) are defined on the basis of top-k subgroup of permutations . <eos> this framework can include the permutation-level ranking framework proposed in previous work as a special case . <eos> based on the new framework , we derive sufficient conditions for a listwise ranking method to be consistent with the top-k true loss , and show an effective way of modifying the surrogate loss functions in existing methods to satisfy these conditions . <eos> experimental results show that after the modifications , the methods can work significantly better than their original versions .
motivated from real world problems , like object categorization , we study a particular mixed-norm regularization for multiple kernel learning ( mkl ) . <eos> it is assumed that the given set of kernels are grouped into distinct components where each component is crucial for the learning task at hand . <eos> the formulation hence employs $ l_\infty $ regularization for promoting combinations at the component level and $ l_1 $ regularization for promoting sparsity among kernels in each component . <eos> while previous attempts have formulated this as a non-convex problem , the formulation given here is an instance of non-smooth convex optimization problem which admits an efficient mirror-descent ( md ) based procedure . <eos> the md procedure optimizes over product of simplexes , which is not a well-studied case in literature . <eos> results on real-world datasets show that the new mkl formulation is well-suited for object categorization tasks and that the md based algorithm outperforms state-of-the-art mkl solvers like \texttt { simplemkl } in terms of computational effort .
training conditional maximum entropy models on massive data requires significant time and computational resources . <eos> in this paper , we investigate three common distributed training strategies : distributed gradient , majority voting ensembles , and parameter mixtures . <eos> we analyze the worst-case runtime and resource costs of each and present a theoretical foundation for the convergence of parameters under parameter mixtures , the most efficient strategy . <eos> we present large-scale experiments comparing the different strategies and demonstrate that parameter mixtures over independent models use fewer resources and achieve comparable loss as compared to standard approaches .
we consider regularized stochastic learning and online optimization problems , where the objective function is the sum of two convex terms : one is the loss function of the learning task , and the other is a simple regularization term such as l1-norm for sparsity . <eos> we develop a new online algorithm , the regularized dual averaging method , that can explicitly exploit the regularization structure in an online setting . <eos> in particular , at each iteration , the learning variables are adjusted by solving a simple optimization problem that involves the running average of all past subgradients of the loss functions and the whole regularization term , not just its subgradient . <eos> this method achieves the optimal convergence rate and often enjoys a low complexity per iteration similar as the standard stochastic gradient method . <eos> computational experiments are presented for the special case of sparse online learning using l1-regularization .
we study the problem of decision-theoretic online learning ( dtol ) . <eos> motivated by practical applications , we focus on dtol when the number of actions is very large . <eos> previous algorithms for learning in this framework have a tunable learning rate parameter , and a major barrier to using online-learning in practical applications is that it is not understood how to set this parameter optimally , particularly when the number of actions is large . <eos> in this paper , we offer a clean solution by proposing a novel and completely parameter-free algorithm for dtol . <eos> in addition , we introduce a new notion of regret , which is more natural for applications with a large number of actions . <eos> we show that our algorithm achieves good performance with respect to this new notion of regret ; in addition , it also achieves performance close to that of the best bounds achieved by previous algorithms with optimally-tuned parameters , according to previous notions of regret .
we develop an algorithm for efficient range search when the notion of dissimilarity is given by a bregman divergence . <eos> the range search task is to return all points in a potentially large database that are within some specified distance of a query . <eos> it arises in many learning algorithms such as locally-weighted regression , kernel density estimation , neighborhood graph-based algorithms , and in tasks like outlier detection and information retrieval . <eos> in metric spaces , efficient range search-like algorithms based on spatial data structures have been deployed on a variety of statistical tasks . <eos> here we describe the first algorithm for range search for an arbitrary bregman divergence . <eos> this broad class of dissimilarity measures includes the relative entropy , mahalanobis distance , itakura-saito divergence , and a variety of matrix divergences . <eos> metric methods can not be directly applied since bregman divergences do not in general satisfy the triangle inequality . <eos> we derive geometric properties of bregman divergences that yield an efficient algorithm for range search based on a recently proposed space decomposition for bregman divergences .
we propose a new sketch recognition framework that combines a rich representation of low level visual appearance with a graphical model for capturing high level relationships between symbols . <eos> this joint model of appearance and context allows our framework to be less sensitive to noise and drawing variations , improving accuracy and robustness . <eos> the result is a recognizer that is better able to handle the wide range of drawing styles found in messy freehand sketches . <eos> we evaluate our work on two real-world domains , molecular diagrams and electrical circuit diagrams , and show that our combined approach significantly improves recognition performance .
in the last few decades , model complexity has received a lot of press . <eos> while many methods have been proposed that jointly measure a model ? s descriptive adequacy and its complexity , few measures exist that measure complexity in itself . <eos> moreover , existing measures ignore the parameter prior , which is an inherent part of the model and affects the complexity . <eos> this paper presents a stand alone measure for model complexity , that takes the number of parameters , the functional form , the range of the parameters and the parameter prior into account . <eos> this prior predictive complexity ( ppc ) is an intuitive and easy to compute measure . <eos> it starts from the observation that model complexity is the property of the model that enables it to fit a wide range of outcomes . <eos> the ppc then measures how wide this range exactly is .
images can be segmented by first using a classifier to predict an affinity graph that reflects the degree to which image pixels must be grouped together and then partitioning the graph to yield a segmentation . <eos> machine learning has been applied to the affinity classifier to produce affinity graphs that are good in the sense of minimizing edge misclassification rates . <eos> however , this error measure is only indirectly related to the quality of segmentations produced by ultimately partitioning the affinity graph . <eos> we present the first machine learning algorithm for training a classifier to produce affinity graphs that are good in the sense of producing segmentations that directly minimize the rand index , a well known segmentation performance measure . <eos> the rand index measures segmentation performance by quantifying the classification of the connectivity of image pixel pairs after segmentation . <eos> by using the simple graph partitioning algorithm of finding the connected components of the thresholded affinity graph , we are able to train an affinity classifier to directly minimize the rand index of segmentations resulting from the graph partitioning . <eos> our learning algorithm corresponds to the learning of maximin affinities between image pixel pairs , which are predictive of the pixel-pair connectivity .
online learning algorithms have impressive convergence properties when it comes to risk minimization and convex games on very large problems . <eos> however , they are inherently sequential in their design which prevents them from taking advantage of modern multi-core architectures . <eos> in this paper we prove that online learning with delayed updates converges well , thereby facilitating parallel online learning .
this paper studies the forward greedy strategy in sparse nonparametric regression . <eos> for additive models , we propose an algorithm called additive forward regression ; for general multivariate regression , we propose an algorithm called generalized forward regression . <eos> both of them simultaneously conduct estimation and variable selection in nonparametric settings for the high dimensional sparse learning problem . <eos> our main emphasis is empirical : on both simulated and real data , these two simple greedy methods can clearly outperform several state-of-the-art competitors , including the lasso , a nonparametric version of the lasso called the sparse additive model ( spam ) and a recently proposed adaptive parametric forward-backward algorithm called the foba . <eos> some theoretical justifications are also provided .
the cur decomposition provides an approximation of a matrix x that has low reconstruction error and that is sparse in the sense that the resulting approximation lies in the span of only a few columns of x . <eos> in this regard , it appears to be similar to many sparse pca methods . <eos> however , cur takes a randomized algorithmic approach whereas most sparse pca methods are framed as convex optimization problems . <eos> in this paper , we try to understand cur from a sparse optimization viewpoint . <eos> in particular , we show that cur is implicitly optimizing a sparse regression objective and , furthermore , can not be directly cast as a sparse pca method . <eos> we observe that the sparsity attained by cur possesses an interesting structure , which leads us to formulate a sparse pca method that achieves a cur-like sparsity .
the commute distance between two vertices in a graph is the expected time it takes a random walk to travel from the first to the second vertex and back . <eos> we study the behavior of the commute distance as the size of the underlying graph increases . <eos> we prove that the commute distance converges to an expression that does not take into account the structure of the graph at all and that is completely meaningless as a distance function on the graph . <eos> consequently , the use of the raw commute distance for machine learning purposes is strongly discouraged for large graphs and in high dimensions . <eos> as an alternative we introduce the amplified commute distance that corrects for the undesired large sample effects .
accurate short-term wind forecasts ( stwfs ) , with time horizons from 0.5 to 6 hours , are essential for efficient integration of wind power to the electrical power grid . <eos> physical models based on numerical weather predictions are currently not competitive , and research on machine learning approaches is ongoing . <eos> two major challenges confronting these efforts are missing observations and weather-regime induced dependency shifts among wind variables at geographically distributed sites . <eos> in this paper we introduce approaches that address both of these challenges . <eos> we describe a new regime-aware approach to stwf that use auto-regressive hidden markov models ( ar-hmm ) , a subclass of conditional linear gaussian ( clg ) models . <eos> although ar-hmms are a natural representation for weather regimes , as with clg models in general , exact inference is np-hard when observations are missing ( lerner and parr , 2001 ) . <eos> because of this high cost , we introduce a simple approximate inference method for ar-hmms , which we believe has applications to other sequential and temporal problem domains that involve continuous variables . <eos> in an empirical evaluation on publicly available wind data from two geographically distinct regions , our approach makes significantly more accurate predictions than baseline models , and uncovers meteorologically relevant regimes .
we introduce a new family of online learning algorithms based upon constraining the velocity flow over a distribution of weight vectors . <eos> in particular , we show how to effectively herd a gaussian weight vector distribution by trading off velocity constraints with a loss function . <eos> by uniformly bounding this loss function , we demonstrate how to solve the resulting optimization analytically . <eos> we compare the resulting algorithms on a variety of real world datasets , and demonstrate how these algorithms achieve state-of-the-art robust performance , especially with high label noise in the training data .
we establish an excess risk bound of o ( h r_n^2 + sqrt { h l* } r_n ) for erm with an h-smooth loss function and a hypothesis class with rademacher complexity r_n , where l* is the best risk achievable by the hypothesis class . <eos> for typical hypothesis classes where r_n = sqrt { r/n } , this translates to a learning rate of ? <eos> o ( rh/n ) in the separable ( l* = 0 ) case and o ( rh/n + sqrt { l* rh/n } ) more generally . <eos> we also provide similar guarantees for online and stochastic convex optimization of a smooth non-negative objective .
we describe a log-bilinear '' model that computes class probabilities by combining an input vector multiplicatively with a vector of binary latent variables . <eos> even though the latent variables can take on exponentially many possible combinations of values , we can efficiently compute the exact probability of each class by marginalizing over the latent variables . <eos> this makes it possible to get the exact gradient of the log likelihood . <eos> the bilinear score-functions are defined using a three-dimensional weight tensor , and we show that factorizing this tensor allows the model to encode invariances inherent in a task by learning a dictionary of invariant basis functions . <eos> experiments on a set of benchmark problems show that this fully probabilistic model can achieve classification performance that is competitive with ( kernel ) svms , backpropagation , and deep belief nets . ''
we consider the online binary classification problem , where we are given m classifiers . <eos> at each stage , the classifiers map the input to the probability that the input belongs to the positive class . <eos> an online classification meta-algorithm is an algorithm that combines the outputs of the classifiers in order to attain a certain goal , without having prior knowledge on the form and statistics of the input , and without prior knowledge on the performance of the given classifiers . <eos> in this paper , we use sensitivity and specificity as the performance metrics of the meta-algorithm . <eos> in particular , our goal is to design an algorithm which satisfies the following two properties ( asymptotically ) : ( i ) its average false positive rate ( fp-rate ) is under some given threshold , and ( ii ) its average true positive rate ( tp-rate ) is not worse than the tp-rate of the best convex combination of the m given classifiers that satisfies fp-rate constraint , in hindsight . <eos> we show that this problem is in fact a special case of the regret minimization problem with constraints , and therefore the above goal is not attainable . <eos> hence , we pose a relaxed goal and propose a corresponding practical online learning meta-algorithm that attains it . <eos> in the case of two classifiers , we show that this algorithm takes a very simple form . <eos> to our best knowledge , this is the first algorithm that addresses the problem of the average tp-rate maximization under average fp-rate constraints in the online setting .
we identify and investigate a strong connection between probabilistic inference and differential privacy , the latter being a recent privacy definition that permits only indirect observation of data through noisy measurement . <eos> previous research on differential privacy has focused on designing measurement processes whose output is likely to be useful on its own . <eos> we consider the potential of applying probabilistic inference to the measurements and measurement process to derive posterior distributions over the data sets and model parameters thereof . <eos> we find that probabilistic inference can improve accuracy , integrate multiple observations , measure uncertainty , and even provide posterior distributions over quantities that were not directly measured .
this paper explores links between basis construction methods in markov decision processes and power series expansions of value functions . <eos> this perspective provides a useful framework to analyze properties of existing bases , as well as provides insight into constructing more effective bases . <eos> krylov and bellman error bases are based on the neumann series expansion . <eos> these bases incur very large initial bellman errors , and can converge rather slowly as the discount factor approaches unity . <eos> the laurent series expansion , which relates discounted and average-reward formulations , provides both an explanation for this slow convergence as well as suggests a way to construct more efficient basis representations . <eos> the first two terms in the laurent series represent the scaled average-reward and the average-adjusted sum of rewards , and subsequent terms expand the discounted value function using powers of a generalized inverse called the drazin ( or group inverse ) of a singular matrix derived from the transition matrix . <eos> experiments show that drazin bases converge considerably more quickly than several other bases , particularly for large values of the discount factor . <eos> an incremental variant of drazin bases called bellman average-reward bases ( barbs ) is described , which provides some of the same benefits at lower computational cost .
probabilistic graphical models use local factors to represent dependence among sets of variables . <eos> for many problem domains , for instance climatology and epidemiology , in addition to local dependencies , we may also wish to model heavy-tailed statistics , where extreme deviations should not be treated as outliers . <eos> specifying such distributions using graphical models for probability density functions ( pdfs ) generally lead to intractable inference and learning . <eos> cumulative distribution networks ( cdns ) provide a means to tractably specify multivariate heavy-tailed models as a product of cumulative distribution functions ( cdfs ) . <eos> currently , algorithms for inference and learning , which correspond to computing mixed derivatives , are exact only for tree-structured graphs . <eos> for graphs of arbitrary topology , an efficient algorithm is needed that takes advantage of the sparse structure of the model , unlike symbolic differentiation programs such as mathematica and d* that do not . <eos> we present an algorithm for recursively decomposing the computation of derivatives for cdns of arbitrary topology , where the decomposition is naturally described using junction trees . <eos> we compare the performance of the resulting algorithm to mathematica and d* , and we apply our method to learning models for rainfall and h1n1 data , where we show that cdns with cycles are able to provide a significantly better fits to the data as compared to tree-structured and unstructured cdns and other heavy-tailed multivariate distributions such as the multivariate copula and logistic models .
many complex systems , ranging from neural cell assemblies to insect societies , involve and rely on some division of labor . <eos> how to enforce such a division in a decentralized and distributed way , is tackled in this paper , using a spiking neuron network architecture . <eos> specifically , a spatio-temporal model called spikeants is shown to enforce the emergence of synchronized activities in an ant colony . <eos> each ant is modelled from two spiking neurons ; the ant colony is a sparsely connected spiking neuron network . <eos> each ant makes its decision ( among foraging , sleeping and self-grooming ) from the competition between its two neurons , after the signals received from its neighbor ants . <eos> interestingly , three types of temporal patterns emerge in the ant colony : asynchronous , synchronous , and synchronous periodic foraging activities - similar to the actual behavior of some living ant colonies . <eos> a phase diagram of the emergent activity patterns with respect to two control parameters , respectively accounting for ant sociability and receptivity , is presented and discussed .
this paper discusses the topic of dimensionality reduction for $ k $ -means clustering . <eos> we prove that any set of $ n $ points in $ d $ dimensions ( rows in a matrix $ a \in \rr^ { n \times d } $ ) can be projected into $ t = \omega ( k / \eps^2 ) $ dimensions , for any $ \eps \in ( 0,1/3 ) $ , in $ o ( n d \lceil \eps^ { -2 } k/ \log ( d ) \rceil ) $ time , such that with constant probability the optimal $ k $ -partition of the point set is preserved within a factor of $ 2+\eps $ . <eos> the projection is done by post-multiplying $ a $ with a $ d \times t $ random matrix $ r $ having entries $ +1/\sqrt { t } $ or $ -1/\sqrt { t } $ with equal probability . <eos> a numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results .
we develop an online variational bayes ( vb ) algorithm for latent dirichlet allocation ( lda ) . <eos> online lda is based on online stochastic optimization with a natural gradient step , which we show converges to a local optimum of the vb objective function . <eos> it can handily analyze massive document collections , including those arriving in a stream . <eos> we study the performance of online lda in several ways , including by fitting a 100-topic topic model to 3.3m articles from wikipedia in a single pass . <eos> we demonstrate that online lda finds topic models as good or better than those found with batch vb , and in a fraction of the time .
we introduce cst , an algorithm for constructing skill trees from demonstration trajectories in continuous reinforcement learning domains . <eos> cst uses a changepoint detection method to segment each trajectory into a skill chain by detecting a change of appropriate abstraction , or that a segment is too complex to model as a single skill . <eos> the skill chains from each trajectory are then merged to form a skill tree . <eos> we demonstrate that cst constructs an appropriate skill tree that can be further refined through learning in a challenging continuous domain , and that it can be used to segment demonstration trajectories on a mobile manipulator into chains of skills where each skill is assigned an appropriate abstraction .
minimizing the rank of a matrix subject to affine constraints is a fundamental problem with many important applications in machine learning and statistics . <eos> in this paper we propose a simple and fast algorithm svp ( singular value projection ) for rank minimization under affine constraints armp and show that svp recovers the minimum rank solution for affine constraints that satisfy a restricted isometry property } ( rip ) . <eos> our method guarantees geometric convergence rate even in the presence of noise and requires strictly weaker assumptions on the rip constants than the existing methods . <eos> we also introduce a newton-step for our svp framework to speed-up the convergence with substantial empirical gains . <eos> next , we address a practically important application of armp - the problem of low-rank matrix completion , for which the defining affine constraints do not directly obey rip , hence the guarantees of svp do not hold . <eos> however , we provide partial progress towards a proof of exact recovery for our algorithm by showing a more restricted isometry property and observe empirically that our algorithm recovers low-rank incoherent matrices from an almost optimal number of uniformly sampled entries . <eos> we also demonstrate empirically that our algorithms outperform existing methods , such as those of \cite { caics2008 , leeb2009b , keshavanom2009 } , for armp and the matrix completion problem by an order of magnitude and are also more robust to noise and sampling schemes . <eos> in particular , results show that our svp-newton method is significantly robust to noise and performs impressively on a more realistic power-law sampling scheme for the matrix completion problem .
when the distribution of unlabeled data in feature space lies along a manifold , the information it provides may be used by a learner to assist classification in a semi-supervised setting . <eos> while manifold learning is well-known in machine learning , the use of manifolds in human learning is largely unstudied . <eos> we perform a set of experiments which test a human 's ability to use a manifold in a semi-supervised learning task , under varying conditions . <eos> we show that humans may be encouraged into using the manifold , overcoming the strong preference for a simple , axis-parallel linear boundary .
this paper is concerned with rank aggregation , which aims to combine multiple input rankings to get a better ranking . <eos> a popular approach to rank aggregation is based on probabilistic models on permutations , e.g. , the luce model and the mallows model . <eos> however , these models have their limitations in either poor expressiveness or high computational complexity . <eos> to avoid these limitations , in this paper , we propose a new model , which is defined with a coset-permutation distance , and models the generation of a permutation as a stagewise process . <eos> we refer to the new model as coset-permutation distance based stagewise ( cps ) model . <eos> the cps model has rich expressiveness and can therefore be used in versatile applications , because many different permutation distances can be used to induce the coset-permutation distance . <eos> the complexity of the cps model is low because of the stagewise decomposition of the permutation probability and the efficient computation of most coset-permutation distances . <eos> we apply the cps model to supervised rank aggregation , derive the learning and inference algorithms , and empirically study their effectiveness and efficiency . <eos> experiments on public datasets show that the derived algorithms based on the cps model can achieve state-of-the-art ranking accuracy , and are much more efficient than previous algorithms .
dimensionality reduction is commonly used in the setting of multi-label supervised classification to control the learning capacity and to provide a meaningful representation of the data . <eos> we introduce a simple forward probabilistic model which is a multinomial extension of reduced rank regression ; we show that this model provides a probabilistic interpretation of discriminative clustering methods with added benefits in terms of number of hyperparameters and optimization . <eos> while expectation-maximization ( em ) algorithm is commonly used to learn these models , its optimization usually leads to local minimum because it relies on a non-convex cost function with many such local minima . <eos> to avoid this problem , we introduce a local approximation of this cost function , which leads to a quadratic non-convex optimization problem over a product of simplices . <eos> in order to minimize such functions , we propose an efficient algorithm based on convex relaxation and low-rank representation of our data , which allows to deal with large instances . <eos> experiments on text document classification show that the new model outperforms other supervised dimensionality reduction methods , while simulations on unsupervised clustering show that our probabilistic formulation has better properties than existing discriminative clustering methods .
we define a data dependent permutation complexity for a hypothesis set \math { \hset } , which is similar to a rademacher complexity or maximum discrepancy . <eos> the permutation complexity is based like the maximum discrepancy on ( dependent ) sampling . <eos> we prove a uniform bound on the generalization error , as well as a concentration result which means that the permutation estimate can be efficiently estimated .
given an ensemble of distinct , low-level segmentations of an image , our goal is to identify visually meaningful '' segments in the ensemble . <eos> knowledge about any specific objects and surfaces present in the image is not available . <eos> the selection of image regions occupied by objects is formalized as the maximum-weight independent set ( mwis ) problem . <eos> mwis is the heaviest subset of mutually non-adjacent nodes of an attributed graph . <eos> we construct such a graph from all segments in the ensemble . <eos> then , mwis selects maximally distinctive segments that together partition the image . <eos> a new mwis algorithm is presented . <eos> the algorithm seeks a solution directly in the discrete domain , instead of relaxing mwis to a continuous problem , as common in previous work . <eos> it iteratively finds a candidate discrete solution of the taylor series expansion of the original mwis objective function around the previous solution . <eos> the algorithm is shown to converge to a maximum . <eos> our empirical evaluation on the benchmark berkeley segmentation dataset shows that the new algorithm eliminates the need for hand-picking optimal input parameters of the state-of-the-art segmenters , and outperforms their best , manually optimized results . ''
the human vision system is able to effortlessly perceive both short-range and long-range motion patterns in complex dynamic scenes . <eos> previous work has assumed that two different mechanisms are involved in processing these two types of motion . <eos> in this paper , we propose a hierarchical model as a unified framework for modeling both short-range and long-range motion perception . <eos> our model consists of two key components : a data likelihood that proposes multiple motion hypotheses using nonlinear matching , and a hierarchical prior that imposes slowness and spatial smoothness constraints on the motion field at multiple scales . <eos> we tested our model on two types of stimuli , random dot kinematograms and multiple-aperture stimuli , both commonly used in human vision research . <eos> we demonstrate that the hierarchical model adequately accounts for human performance in psychophysical experiments .
we consider the problem of discovering links of an evolving undirected graph given a series of past snapshots of that graph . <eos> the graph is observed through the time sequence of its adjacency matrix and only the presence of edges is observed . <eos> the absence of an edge on a certain snapshot can not be distinguished from a missing entry in the adjacency matrix . <eos> additional information can be provided by examining the dynamics of the graph through a set of topological features , such as the degrees of the vertices . <eos> we develop a novel methodology by building on both static matrix completion methods and the estimation of the future state of relevant graph features . <eos> our procedure relies on the formulation of an optimization problem which can be approximately solved by a fast alternating linearized algorithm whose properties are examined . <eos> we show experiments with both simulated and real data which reveal the interest of our methodology .
we present a model that describes the structure in the responses of different brain areas to a set of stimuli in terms of stimulus categories '' ( clusters of stimuli ) and `` functional units '' ( clusters of voxels ) . <eos> we assume that voxels within a unit respond similarly to all stimuli from the same category , and design a nonparametric hierarchical model to capture inter-subject variability among the units . <eos> the model explicitly captures the relationship between brain activations and fmri time courses . <eos> a variational inference algorithm derived based on the model can learn categories , units , and a set of unit-category activation probabilities from data . <eos> when applied to data from an fmri study of object recognition , the method finds meaningful and consistent clusterings of stimuli into categories and voxels into units . ''
in this paper we propose an approximated learning framework for large scale graphical models and derive message passing algorithms for learning their parameters efficiently . <eos> we first relate crfs and structured svms and show that in the crf 's primal a variant of the log-partition function , known as soft-max , smoothly approximates the hinge loss function of structured svms . <eos> we then propose an intuitive approximation for structured prediction problems using fenchel duality based on a local entropy approximation that computes the exact gradients of the approximated problem and is guaranteed to converge . <eos> unlike existing approaches , this allow us to learn graphical models with cycles and very large number of parameters efficiently . <eos> we demonstrate the effectiveness of our approach in an image denoising task . <eos> this task was previously solved by sharing parameters across cliques . <eos> in contrast , our algorithm is able to efficiently learn large number of parameters resulting in orders of magnitude better prediction .
support vector machines ( svm ) are increasingly used in brain image analyses since they allow capturing complex multivariate relationships in the data . <eos> moreover , when the kernel is linear , svms can be used to localize spatial patterns of discrimination between two groups of subjects . <eos> however , the features ' spatial distribution is not taken into account . <eos> as a consequence , the optimal margin hyperplane is often scattered and lacks spatial coherence , making its anatomical interpretation difficult . <eos> this paper introduces a framework to spatially regularize svm for brain image analysis . <eos> we show that laplacian regularization provides a flexible framework to integrate various types of constraints and can be applied to both cortical surfaces and 3d brain images . <eos> the proposed framework is applied to the classification of mr images based on gray matter concentration maps and cortical thickness measures from 30 patients with alzheimer 's disease and 30 elderly controls . <eos> the results demonstrate that the proposed method enables natural spatial and anatomical regularization of the classifier .
determining whether someone is talking has applications in many areas such as speech recognition , speaker diarization , social robotics , facial expression recognition , and human computer interaction . <eos> one popular approach to this problem is audio-visual synchrony detection . <eos> a candidate speaker is deemed to be talking if the visual signal around that speaker correlates with the auditory signal . <eos> here we show that with the proper visual features ( in this case movements of various facial muscle groups ) , a very accurate detector of speech can be created that does not use the audio signal at all . <eos> further we show that this person independent visual-only detector can be used to train very accurate audio-based person dependent voice models . <eos> the voice model has the advantage of being able to identify when a particular person is speaking even when they are not visible to the camera ( e.g . in the case of a mobile robot ) . <eos> moreover , we show that a simple sensory fusion scheme between the auditory and visual models improves performance on the task of talking detection . <eos> the work here provides dramatic evidence about the efficacy of two very different approaches to multimodal speech detection on a challenging database .
undirected graphical models encode in a graph $ g $ the dependency structure of a random vector $ y $ . <eos> in many applications , it is of interest to model $ y $ given another random vector $ x $ as input . <eos> we refer to the problem of estimating the graph $ g ( x ) $ of $ y $ conditioned on $ x=x $ as `` graph-valued regression '' . <eos> in this paper , we propose a semiparametric method for estimating $ g ( x ) $ that builds a tree on the $ x $ space just as in cart ( classification and regression trees ) , but at each leaf of the tree estimates a graph . <eos> we call the method `` graph-optimized cart '' , or go-cart . <eos> we study the theoretical properties of go-cart using dyadic partitioning trees , establishing oracle inequalities on risk minimization and tree partition consistency . <eos> we also demonstrate the application of go-cart to a meteorological dataset , showing how graph-valued regression can provide a useful tool for analyzing complex data .
when stimulated with complex action potential sequences synapses exhibit spike timing-dependent plasticity ( stdp ) with attenuated and enhanced pre- and postsynaptic contributions to long-term synaptic modifications . <eos> in order to investigate the functional consequences of these contribution dynamics ( cd ) we propose a minimal model formulated in terms of differential equations . <eos> we find that our model reproduces a wide range of experimental results with a small number of biophysically interpretable parameters . <eos> the model allows to investigate the susceptibility of stdp to arbitrary time courses of pre- and postsynaptic activities , i.e . its nonlinear filter properties . <eos> we demonstrate this for the simple example of small periodic modulations of pre- and postsynaptic firing rates for which our model can be solved . <eos> it predicts synaptic strengthening for synchronous rate modulations . <eos> for low baseline rates modifications are dominant in the theta frequency range , a result which underlines the well known relevance of theta activities in hippocampus and cortex for learning . <eos> we also find emphasis of low baseline spike rates and suppression for high baseline rates . <eos> the latter suggests a mechanism of network activity regulation inherent in stdp . <eos> furthermore , our novel formulation provides a general framework for investigating the joint dynamics of neuronal activity and the cd of stdp in both spike-based as well as rate-based neuronal network models .
the goal of inverse reinforcement learning is to find a reward function for a markov decision process , given example traces from its optimal policy . <eos> current irl techniques generally rely on user-supplied features that form a concise basis for the reward . <eos> we present an algorithm that instead constructs reward features from a large collection of component features , by building logical conjunctions of those component features that are relevant to the example policy . <eos> given example traces , the algorithm returns a reward function as well as the constructed features . <eos> the reward function can be used to recover a full , deterministic , stationary policy , and the features can be used to transplant the reward function into any novel environment on which the component features are well defined .
recently , batch-mode active learning has attracted a lot of attention . <eos> in this paper , we propose a novel batch-mode active learning approach that selects a batch of queries in each iteration by maximizing a natural form of mutual information criterion between the labeled and unlabeled instances . <eos> by employing a gaussian process framework , this mutual information based instance selection problem can be formulated as a matrix partition problem . <eos> although the matrix partition is an np-hard combinatorial optimization problem , we show a good local solution can be obtained by exploiting an effective local optimization technique on the relaxed continuous optimization problem . <eos> the proposed active learning approach is independent of employed classification models . <eos> our empirical studies show this approach can achieve comparable or superior performance to discriminative batch-mode active learning methods .
multi-label classification is the task of predicting potentially multiple labels for a given instance . <eos> this is common in several applications such as image annotation , document classification and gene function prediction . <eos> in this paper we present a formulation for this problem based on reverse prediction : we predict sets of instances given the labels . <eos> by viewing the problem from this perspective , the most popular quality measures for assessing the performance of multi-label classification admit relaxations that can be efficiently optimised . <eos> we optimise these relaxations with standard algorithms and compare our results with several state-of-the-art methods , showing excellent performance .
the problem of learning to predict structured labels is of key importance in many applications . <eos> however , for general graph structure both learning and inference in this setting are intractable . <eos> here we show that it is possible to circumvent this difficulty when the input distribution is rich enough via a method similar in spirit to pseudo-likelihood . <eos> we show how our new method achieves consistency , and illustrate empirically that it indeed performs as well as exact methods when sufficiently large training sets are used .
likelihood ratio policy gradient methods have been some of the most successful reinforcement learning algorithms , especially for learning on physical systems . <eos> we describe how the likelihood ratio policy gradient can be derived from an importance sampling perspective . <eos> this derivation highlights how likelihood ratio methods under-use past experience by ( a ) using the past experience to estimate { \em only } the gradient of the expected return $ u ( \theta ) $ at the current policy parameterization $ \theta $ , rather than to obtain a more complete estimate of $ u ( \theta ) $ , and ( b ) using past experience under the current policy { \em only } rather than using all past experience to improve the estimates . <eos> we present a new policy search method , which leverages both of these observations as well as generalized baselines -- -a new technique which generalizes commonly used baseline techniques for policy gradient methods . <eos> our algorithm outperforms standard likelihood ratio policy gradient algorithms on several testbeds .
latent variable models are a powerful tool for addressing several tasks in machine learning . <eos> however , the algorithms for learning the parameters of latent variable models are prone to getting stuck in a bad local optimum . <eos> to alleviate this problem , we build on the intuition that , rather than considering all samples simultaneously , the algorithm should be presented with the training data in a meaningful order that facilitates learning . <eos> the order of the samples is determined by how easy they are . <eos> the main challenge is that often we are not provided with a readily computable measure of the easiness of samples . <eos> we address this issue by proposing a novel , iterative self-paced learning algorithm where each iteration simultaneously selects easy samples and learns a new parameter vector . <eos> the number of samples selected is governed by a weight that is annealed until the entire training data has been considered . <eos> we empirically demonstrate that the self-paced learning algorithm outperforms the state of the art method for learning a latent structural svm on four applications : object localization , noun phrase coreference , motif finding and handwritten digit recognition .
remarkably easy implementation and guaranteed convergence has made the em algorithm one of the most used algorithms for mixture modeling . <eos> on the downside , the e-step is linear in both the sample size and the number of mixture components , making it impractical for large-scale data . <eos> based on the variational em framework , we propose a fast alternative that uses component-specific data partitions to obtain a sub-linear e-step in sample size , while the algorithm still maintains provable convergence . <eos> our approach builds on previous work , but is significantly faster and scales much better in the number of mixture components . <eos> we demonstrate this speedup by experiments on large-scale synthetic and real data .
we cast the problem of identifying basic blocks of code in a binary executable as learning a mapping from a byte sequence to a segmentation of the sequence . <eos> in general , inference in segmentation models , such as semi-crfs , can be cubic in the length of the sequence . <eos> by taking advantage of the structure of our problem , we derive a linear-time inference algorithm which makes our approach practical , given that even small programs are tens or hundreds of thousands bytes long . <eos> furthermore , we introduce two loss functions which are appropriate for our problem and show how to use structural svms to optimize the learned mapping for these losses . <eos> finally , we present experimental results that demonstrate the advantages of our method against a strong baseline .
multiple-instance learning has been long known as a hard non-convex problem . <eos> in this work , we propose an approach that recasts it as a convex likelihood ratio estimation problem . <eos> firstly , the constraint in multiple-instance learning is reformulated into a convex constraint on the likelihood ratio . <eos> then we show that a joint estimation of a likelihood ratio function and the likelihood on training instances can be learned convexly . <eos> theoretically , we prove a quantitative relationship between the risk estimated under the 0-1 classification loss , and under a loss function for likelihood ratio estimation . <eos> it is shown that our likelihood ratio estimation is generally a good surrogate for the 0-1 loss , and separates positive and negative instances well . <eos> however with the joint estimation it tends to underestimate the likelihood of an example to be positive . <eos> we propose to use these likelihood ratio estimates as features , and learn a linear combination on them to classify the bags . <eos> experiments on synthetic and real datasets show the superiority of the approach .
we consider markov decision processes where the values of the parameters are uncertain . <eos> this uncertainty is described by a sequence of nested sets ( that is , each set contains the previous one ) , each of which corresponds to a probabilistic guarantee for a different confidence level so that a set of admissible probability distributions of the unknown parameters is specified . <eos> this formulation models the case where the decision maker is aware of and wants to exploit some ( yet imprecise ) a-priori information of the distribution of parameters , and arises naturally in practice where methods to estimate the confidence region of parameters abound . <eos> we propose a decision criterion based on *distributional robustness* : the optimal policy maximizes the expected total reward under the most adversarial probability distribution over realizations of the uncertain parameters that is admissible ( i.e. , it agrees with the a-priori information ) . <eos> we show that finding the optimal distributionally robust policy can be reduced to a standard robust mdp where the parameters belong to a single uncertainty set , hence it can be computed in polynomial time under mild technical conditions .
this paper proposes a principled extension of the traditional single-layer flat sparse coding scheme , where a two-layer coding scheme is derived based on theoretical analysis of nonlinear functional approximation that extends recent results for local coordinate coding . <eos> the two-layer approach can be easily generalized to deeper structures in a hierarchical multiple-layer manner . <eos> empirically , it is shown that the deep coding approach yields improved performance in benchmark datasets .
sparse coding has recently become a popular approach in computer vision to learn dictionaries of natural images . <eos> in this paper we extend sparse coding to learn interpretable spatio-temporal primitives of human motion . <eos> we cast the problem of learning spatio-temporal primitives as a tensor factorization problem and introduce constraints to learn interpretable primitives . <eos> in particular , we use group norms over those tensors , diagonal constraints on the activations as well as smoothness constraints that are inherent to human motion . <eos> we demonstrate the effectiveness of our approach to learn interpretable representations of human motion from motion capture data , and show that our approach outperforms recently developed matching pursuit and sparse coding algorithms .
we consider the tree structured group lasso where the structure over the features can be represented as a tree with leaf nodes as features and internal nodes as clusters of the features . <eos> the structured regularization with a pre-defined tree structure is based on a group-lasso penalty , where one group is defined for each node in the tree . <eos> such a regularization can help uncover the structured sparsity , which is desirable for applications with some meaningful tree structures on the features . <eos> however , the tree structured group lasso is challenging to solve due to the complex regularization . <eos> in this paper , we develop an efficient algorithm for the tree structured group lasso . <eos> one of the key steps in the proposed algorithm is to solve the moreau-yosida regularization associated with the grouped tree structure . <eos> the main technical contributions of this paper include ( 1 ) we show that the associated moreau-yosida regularization admits an analytical solution , and ( 2 ) we develop an efficient algorithm for determining the effective interval for the regularization parameter . <eos> our experimental results on the ar and jaffe face data sets demonstrate the efficiency and effectiveness of the proposed algorithm .
we pose transductive classification as a matrix completion problem . <eos> by assuming the underlying matrix has a low rank , our formulation is able to handle three problems simultaneously : i ) multi-label learning , where each item has more than one label , ii ) transduction , where most of these labels are unspecified , and iii ) missing data , where a large number of features are missing . <eos> we obtained satisfactory results on several real-world tasks , suggesting that the low rank assumption may not be as restrictive as it seems . <eos> our method allows for different loss functions to apply on the feature and label entries of the matrix . <eos> the resulting nuclear norm minimization problem is solved with a modified fixed-point continuation method that is guaranteed to find the global optimum .
sparse methods for supervised learning aim at finding good linear predictors from as few variables as possible , i.e. , with small cardinality of their supports . <eos> this combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope ( tightest convex lower bound ) , in this case the l1-norm . <eos> in this paper , we investigate more general set-functions than the cardinality , that may incorporate prior knowledge or structural constraints which are common in many applications : namely , we show that for nondecreasing submodular set-functions , the corresponding convex envelope can be obtained from its lovasz extension , a common tool in submodular analysis . <eos> this defines a family of polyhedral norms , for which we provide generic algorithmic tools ( subgradients and proximal operators ) and theoretical results ( conditions for support recovery or high-dimensional inference ) . <eos> by selecting specific submodular functions , we can give a new interpretation to known norms , such as those based on rank-statistics or grouped norms with potentially overlapping groups ; we also define new norms , in particular ones that can be used as non-factorial priors for supervised learning .
although the dirichlet distribution is widely used , the independence structure of its components limits its accuracy as a model . <eos> the proposed shadow dirichlet distribution manipulates the support in order to model probability mass functions ( pmfs ) with dependencies or constraints that often arise in real world problems , such as regularized pmfs , monotonic pmfs , and pmfs with bounded variation . <eos> we describe some properties of this new class of distributions , provide maximum entropy constructions , give an expectation-maximization method for estimating the mean parameter , and illustrate with real data .
multi-task learning ( mtl ) improves the prediction performance on multiple , different but related , learning problems through shared parameters or representations . <eos> one of the most prominent multi-task learning algorithms is an extension to svms by evgeniou et al . although very elegant , multi-task svm is inherently restricted by the fact that support vector machines require each class to be addressed explicitly with its own weight vector which , in a multi-task setting , requires the different learning tasks to share the same set of classes . <eos> this paper proposes an alternative formulation for multi-task learning by extending the recently published large margin nearest neighbor ( lmnn ) algorithm to the mtl paradigm . <eos> instead of relying on separating hyperplanes , its decision function is based on the nearest neighbor rule which inherently extends to many classes and becomes a natural fit for multitask learning . <eos> we evaluate the resulting multi-task lmnn on real-world insurance data and speech classification problems and show that it consistently outperforms single-task knn under several metrics and state-of-the-art mtl classifiers .
robust regression and classification are often thought to require non-convex loss functions that prevent scalable , global training . <eos> however , such a view neglects the possibility of reformulated training methods that can yield practically solvable alternatives . <eos> a natural way to make a loss function more robust to outliers is to truncate loss values that exceed a maximum threshold . <eos> we demonstrate that a relaxation of this form of `` loss clipping '' can be made globally solvable and applicable to any standard loss while guaranteeing robustness against outliers . <eos> we present a generic procedure that can be applied to standard loss functions and demonstrate improved robustness in regression and classification problems .
intelligent agents are often faced with the need to choose actions with uncertain consequences , and to modify those actions according to ongoing sensory processing and changing task demands . <eos> the requisite ability to dynamically modify or cancel planned actions is known as inhibitory control in psychology . <eos> we formalize inhibitory control as a rational decision-making problem , and apply to it to the classical stop-signal task . <eos> using bayesian inference and stochastic control tools , we show that the optimal policy systematically depends on various parameters of the problem , such as the relative costs of different action choices , the noise level of sensory inputs , and the dynamics of changing environmental demands . <eos> our normative model accounts for a range of behavioral data in humans and animals in the stop-signal task , suggesting that the brain implements statistically optimal , dynamically adaptive , and reward-sensitive decision-making in the context of inhibitory control problems .
the sequence memoizer is a model for sequence data with state-of-the-art performance on language modeling and compression . <eos> we propose a number of improvements to the model and inference algorithm , including an enlarged range of hyperparameters , a memory-efficient representation , and inference algorithms operating on the new representation . <eos> our derivations are based on precise definitions of the various processes that will also allow us to provide an elementary proof of the mysterious '' coagulation and fragmentation properties used in the original paper on the sequence memoizer by wood et al . ( 2009 ) . <eos> we present some experimental results supporting our improvements . ''
neuronal connection weights exhibit short-term depression ( std ) . <eos> the present study investigates the impact of std on the dynamics of a continuous attractor neural network ( cann ) and its potential roles in neural information processing . <eos> we find that the network with std can generate both static and traveling bumps , and std enhances the performance of the network in tracking external inputs . <eos> in particular , we find that std endows the network with slow-decaying plateau behaviors , namely , the network being initially stimulated to an active state will decay to silence very slowly in the time scale of std rather than that of neural signaling . <eos> we argue that this provides a mechanism for neural systems to hold short-term memory easily and shut off persistent activities naturally .
we present a technique for exact simulation of gaussian markov random fields ( gmrfs ) , which can be interpreted as locally injecting noise to each gaussian factor independently , followed by computing the mean/mode of the perturbed gmrf . <eos> coupled with standard iterative techniques for the solution of symmetric positive definite systems , this yields a very efficient sampling algorithm with essentially linear complexity in terms of speed and memory requirements , well suited to extremely large scale probabilistic models . <eos> apart from synthesizing data under a gaussian model , the proposed technique directly leads to an efficient unbiased estimator of marginal variances . <eos> beyond gaussian models , the proposed algorithm is also very useful for handling highly non-gaussian continuously-valued mrfs such as those arising in statistical image modeling or in the first layer of deep belief networks describing real-valued data , where the non-quadratic potentials coupling different sites can be represented as finite or infinite mixtures of gaussians with the help of local or distributed latent mixture assignment variables . <eos> the bayesian treatment of such models most naturally involves a block gibbs sampler which alternately draws samples of the conditionally independent latent mixture assignments and the conditionally multivariate gaussian continuous vector and we show that it can directly benefit from the proposed methods .
in multi-instance learning , there are two kinds of prediction failure , i.e. , false negative and false positive . <eos> current research mainly focus on avoding the former . <eos> we attempt to utilize the geometric distribution of instances inside positive bags to avoid both the former and the latter . <eos> based on kernel principal component analysis , we define a projection constraint for each positive bag to classify its constituent instances far away from the separating hyperplane while place positive instances and negative instances at opposite sides . <eos> we apply the constrained concave-convex procedure to solve the resulted problem . <eos> empirical results demonstrate that our approach offers improved generalization performance .
continuous markov random fields are a general formalism to model joint probability distributions over events with continuous outcomes . <eos> we prove that marginal computation for constrained continuous mrfs is # p-hard in general and present a polynomial-time approximation scheme under mild assumptions on the structure of the random field . <eos> moreover , we introduce a sampling algorithm to compute marginal distributions and develop novel techniques to increase its efficiency . <eos> continuous mrfs are a general purpose probabilistic modeling tool and we demonstrate how they can be applied to statistical relational learning . <eos> on the problem of collective classification , we evaluate our algorithm and show that the standard deviation of marginals serves as a useful measure of confidence .
bayesian approaches to utility elicitation typically adopt ( myopic ) expected value of information ( evoi ) as a natural criterion for selecting queries . <eos> however , evoi-optimization is usually computationally prohibitive . <eos> in this paper , we examine evoi optimization using \emph { choice queries } , queries in which a user is ask to select her most preferred product from a set . <eos> we show that , under very general assumptions , the optimal choice query w.r.t.\ evoi coincides with \emph { optimal recommendation set } , that is , a set maximizing expected utility of the user selection . <eos> since recommendation set optimization is a simpler , submodular problem , this can greatly reduce the complexity of both exact and approximate ( greedy ) computation of optimal choice queries . <eos> we also examine the case where user responses to choice queries are error-prone ( using both constant and follow mixed multinomial logit noise models ) and provide worst-case guarantees . <eos> finally we present a local search technique that works well with large outcome spaces .
conventional dynamic bayesian networks ( dbns ) are based on the homogeneous markov assumption , which is too restrictive in many practical applications . <eos> various approaches to relax the homogeneity assumption have therefore been proposed in the last few years . <eos> the present paper aims to improve the flexibility of two recent versions of non-homogeneous dbns , which either ( i ) suffer from the need for data discretization , or ( ii ) assume a time-invariant network structure . <eos> allowing the network structure to be fully flexible leads to the risk of overfitting and inflated inference uncertainty though , especially in the highly topical field of systems biology , where independent measurements tend to be sparse . <eos> in the present paper we investigate three conceptually different regularization schemes based on inter-segment information sharing . <eos> we assess the performance in a comparative evaluation study based on simulated data . <eos> we compare the predicted segmentation of gene expression time series obtained during embryogenesis in drosophila melanogaster with other state-of-the-art techniques . <eos> we conclude our evaluation with an application to synthetic biology , where the objective is to predict a known regulatory network of five genes in saccharomyces cerevisiae .
a method for computing the rarity of latent fingerprints represented by minutiae is given . <eos> it allows determining the probability of finding a match for an evidence print in a database of n known prints . <eos> the probability of random correspondence between evidence and database is determined in three procedural steps . <eos> in the registration step the latent print is aligned by finding its core point ; which is done using a procedure based on a machine learning approach based on gaussian processes . <eos> in the evidence probability evaluation step a generative model based on bayesian networks is used to determine the probability of the evidence ; it takes into account both the dependency of each minutia on nearby minutiae and the confidence of their presence in the evidence . <eos> in the specific probability of random correspondence step the evidence probability is used to determine the probability of match among n for a given tolerance ; the last evaluation is similar to the birthday correspondence probability for a specific birthday . <eos> the generative model is validated using a goodness-of-fit test evaluated with a standard database of fingerprints . <eos> the probability of random correspondence for several latent fingerprints are evaluated for varying numbers of minutiae .
this paper presents bayesian non-parametric models that simultaneously learn to segment words from phoneme strings and learn the referents of some of those words , and shows that there is a synergistic interaction in the acquisition of these two kinds of linguistic information . <eos> the models themselves are novel kinds of adaptor grammars that are an extension of an embedding of topic models into pcfgs . <eos> these models simultaneously segment phoneme sequences into words and learn the relationship between non-linguistic objects to the words that refer to them . <eos> we show ( i ) that modelling inter-word dependencies not only improves the accuracy of the word segmentation but also of word-object relationships , and ( ii ) that a model that simultaneously learns word-object relationships and word segmentation segments more accurately than one that just learns word segmentation on its own . <eos> we argue that these results support an interactive view of language acquisition that can take advantage of synergies such as these .
we propose a new variational em algorithm for fitting factor analysis models with mixed continuous and categorical observations . <eos> the algorithm is based on a simple quadratic bound to the log-sum-exp function . <eos> in the special case of fully observed binary data , the bound we propose is significantly faster than previous variational methods . <eos> we show that em is significantly more robust in the presence of missing data compared to treating the latent factors as parameters , which is the approach used by exponential family pca and other related matrix-factorization methods . <eos> a further benefit of the variational approach is that it can easily be extended to the case of mixtures of factor analyzers , as we show . <eos> we present results on synthetic and real data sets demonstrating several desirable properties of our proposed method .
it has been speculated that the human motion system combines noisy measurements with prior expectations in an optimal , or rational , manner . <eos> the basic goal of our work is to discover experimentally which prior distribution is used . <eos> more specifically , we seek to infer the functional form of the motion prior from the performance of human subjects on motion estimation tasks . <eos> we restricted ourselves to priors which combine three terms for motion slowness , first-order smoothness , and second-order smoothness . <eos> we focused on two functional forms for prior distributions : l2-norm and l1-norm regularization corresponding to the gaussian and laplace distributions respectively . <eos> in our first experimental session we estimate the weights of the three terms for each functional form to maximize the fit to human performance . <eos> we then measured human performance for motion tasks and found that we obtained better fit for the l1-norm ( laplace ) than for the l2-norm ( gaussian ) . <eos> we note that the l1-norm is also a better fit to the statistics of motion in natural environments . <eos> in addition , we found large weights for the second-order smoothness term , indicating the importance of high-order smoothness compared to slowness and lower-order smoothness . <eos> to validate our results further , we used the best fit models using the l1-norm to predict human performance in a second session with different experimental setups . <eos> our results showed excellent agreement between human performance and model prediction -- ranging from 3\ % to 8\ % for five human subjects over ten experimental conditions -- and give further support that the human visual system uses an l1-norm ( laplace ) prior .
heavy-tailed distributions naturally occur in many real life problems . <eos> unfortunately , it is typically not possible to compute inference in closed-form in graphical models which involve such heavy tailed distributions . <eos> in this work , we propose a novel simple linear graphical model for independent latent random variables , called linear characteristic model ( lcm ) , defined in the characteristic function domain . <eos> using stable distributions , a heavy-tailed family of distributions which is a generalization of cauchy , l\'evy and gaussian distributions , we show for the first time , how to compute both exact and approximate inference in such a linear multivariate graphical model . <eos> lcms are not limited to only stable distributions , in fact lcms are always defined for any random variables ( discrete , continuous or a mixture of both ) . <eos> we provide a realistic problem from the field of computer networks to demonstrate the applicability of our construction . <eos> other potential application is iterative decoding of linear channels with non-gaussian noise .
the diffusion network ( dn ) is a stochastic recurrent network which has been shown capable of modeling the distributions of continuous-valued , continuous-time paths . <eos> however , the dynamics of the dn are governed by stochastic differential equations , making the dn unfavourable for simulation in a digital computer . <eos> this paper presents the implementation of the dn in analogue very large scale integration , enabling the dn to be simulated in real time . <eos> moreover , the log-domain representation is applied to the dn , allowing the supply voltage and thus the power consumption to be reduced without limiting the dynamic ranges for diffusion processes . <eos> a vlsi chip containing a dn with two stochastic units has been designed and fabricated . <eos> the design of component circuits will be described , so will the simulation of the full system be presented . <eos> the simulation results demonstrate that the dn in vlsi is able to regenerate various types of continuous paths in real-time .
communication between a speaker and hearer will be most efficient when both parties make accurate inferences about the other . <eos> we study inference and communication in a television game called password , where speakers must convey secret words to hearers by providing one-word clues . <eos> our working hypothesis is that human communication is relatively efficient , and we use game show data to examine three predictions . <eos> first , we predict that speakers and hearers are both considerate , and that both take the other ? s perspective into account . <eos> second , we predict that speakers and hearers are calibrated , and that both make accurate assumptions about the strategy used by the other . <eos> finally , we predict that speakers and hearers are collaborative , and that they tend to share the cognitive burden of communication equally . <eos> we find evidence in support of all three predictions , and demonstrate in addition that efficient communication tends to break down when speakers and hearers are placed under time pressure .
we propose a new approach to value function approximation which combines linear temporal difference reinforcement learning with subspace identification . <eos> in practical applications , reinforcement learning ( rl ) is complicated by the fact that state is either high-dimensional or partially observable . <eos> therefore , rl methods are designed to work with features of state rather than state itself , and the success or failure of learning is often determined by the suitability of the selected features . <eos> by comparison , subspace identification ( ssid ) methods are designed to select a feature set which preserves as much information as possible about state . <eos> in this paper we connect the two approaches , looking at the problem of reinforcement learning with a large set of features , each of which may only be marginally useful for value function approximation . <eos> we introduce a new algorithm for this situation , called predictive state temporal difference ( pstd ) learning . <eos> as in ssid for predictive state representations , pstd finds a linear compression operator that projects a large set of features down to a small set that preserves the maximum amount of predictive information . <eos> as in rl , pstd then uses a bellman recursion to estimate a value function . <eos> we discuss the connection between pstd and prior approaches in rl and ssid . <eos> we prove that pstd is statistically consistent , perform several experiments that illustrate its properties , and demonstrate its potential on a difficult optimal stopping problem .
recent approaches to multi-view learning have shown that factorizing the information into parts that are shared across all views and parts that are private to each view could effectively account for the dependencies and independencies between the different input modalities . <eos> unfortunately , these approaches involve minimizing non-convex objective functions . <eos> in this paper , we propose an approach to learning such factorized representations inspired by sparse coding techniques . <eos> in particular , we show that structured sparsity allows us to address the multi-view learning problem by alternately solving two convex optimization problems . <eos> furthermore , the resulting factorized latent spaces generalize over existing approaches in that they allow : having latent dimensions shared between any subset of the views instead of between all the views only . <eos> we show that our approach outperforms state-of-the-art methods on the task of human pose estimation .
in this paper , we point out that there exist scaling and initialization problems in most existing multiple kernel learning ( mkl ) approaches , which employ the large margin principle to jointly learn both a kernel and an svm classifier . <eos> the reason is that the margin itself can not well describe how good a kernel is due to the negligence of the scaling . <eos> we use the ratio between the margin and the radius of the minimum enclosing ball to measure the goodness of a kernel , and present a new minimization formulation for kernel learning . <eos> this formulation is invariant to scalings of learned kernels , and when learning linear combination of basis kernels it is also invariant to scalings of basis kernels and to the types ( e.g. , l1 or l2 ) of norm constraints on combination coefficients . <eos> we establish the differentiability of our formulation , and propose a gradient projection algorithm for kernel learning . <eos> experiments show that our method significantly outperforms both svm with the uniform combination of basis kernels and other state-of-art mkl approaches .
we study the problem of segmenting specific white matter structures of interest from diffusion tensor ( dt-mr ) images of the human brain . <eos> this is an important requirement in many neuroimaging studies : for instance , to evaluate whether a brain structure exhibits group level differences as a function of disease in a set of images . <eos> typically , interactive expert guided segmentation has been the method of choice for such applications , but this is tedious for large datasets common today . <eos> to address this problem , we endow an image segmentation algorithm with 'advice ' encoding some global characteristics of the region ( s ) we want to extract . <eos> this is accomplished by constructing ( using expert-segmented images ) an epitome of a specific region - as a histogram over a bag of 'words ' ( e.g. , suitable feature descriptors ) . <eos> now , given such a representation , the problem reduces to segmenting new brain image with additional constraints that enforce consistency between the segmented foreground and the pre-specified histogram over features . <eos> we present combinatorial approximation algorithms to incorporate such domain specific constraints for markov random field ( mrf ) segmentation . <eos> making use of recent results on image co-segmentation , we derive effective solution strategies for our problem . <eos> we provide an analysis of solution quality , and present promising experimental evidence showing that many structures of interest in neuroscience can be extracted reliably from 3-d brain image volumes using our algorithm .
we present the copula bayesian network model for representing multivariate continuous distributions . <eos> our approach builds on a novel copula-based parameterization of a conditional density that , joined with a graph that encodes independencies , offers great flexibility in modeling high-dimensional densities , while maintaining control over the form of the univariate marginals . <eos> we demonstrate the advantage of our framework for generalization over standard bayesian networks as well as tree structured copula models for varied real-life domains that are of substantially higher dimension than those typically considered in the copula literature .
network models are widely used to capture interactions among component of complex systems , such as social and biological . <eos> to understand their behavior , it is often necessary to analyze functionally related components of the system , corresponding to subsystems . <eos> therefore , the analysis of subnetworks may provide additional insight into the behavior of the system , not evident from individual components . <eos> we propose a novel approach for incorporating available network information into the analysis of arbitrary subnetworks . <eos> the proposed method offers an efficient dimension reduction strategy using laplacian eigenmaps with neumann boundary conditions , and provides a flexible inference framework for analysis of subnetworks , based on a group-penalized principal component regression model on graphs . <eos> asymptotic properties of the proposed inference method , as well as the choice of the tuning parameter for control of the false positive rate are discussed in high dimensional settings . <eos> the performance of the proposed methodology is illustrated using simulated and real data examples from biology .
the hypothesis that high dimensional data tends to lie in the vicinity of a low dimensional manifold is the basis of a collection of methodologies termed manifold learning . <eos> in this paper , we study statistical aspects of the question of fitting a manifold with a nearly optimal least squared error . <eos> given upper bounds on the dimension , volume , and curvature , we show that empirical risk minimization can produce a nearly optimal manifold using a number of random samples that is { \it independent } of the ambient dimension of the space in which data lie . <eos> we obtain an upper bound on the required number of samples that depends polynomially on the curvature , exponentially on the intrinsic dimension , and linearly on the intrinsic volume . <eos> for constant error , we prove a matching minimax lower bound on the sample complexity that shows that this dependence on intrinsic dimension , volume and curvature is unavoidable . <eos> whether the known lower bound of $ o ( \frac { k } { \eps^2 } + \frac { \log \frac { 1 } { \de } } { \eps^2 } ) $ for the sample complexity of empirical risk minimization on $ k- $ means applied to data in a unit ball of arbitrary dimension is tight , has been an open question since 1997 \cite { bart2 } . <eos> here $ \eps $ is the desired bound on the error and $ \de $ is a bound on the probability of failure . <eos> we improve the best currently known upper bound \cite { pontil } of $ o ( \frac { k^2 } { \eps^2 } + \frac { \log \frac { 1 } { \de } } { \eps^2 } ) $ to $ o\left ( \frac { k } { \eps^2 } \left ( \min\left ( k , \frac { \log^4 \frac { k } { \eps } } { \eps^2 } \right ) \right ) + \frac { \log \frac { 1 } { \de } } { \eps^2 } \right ) $ . <eos> based on these results , we devise a simple algorithm for $ k- $ means and another that uses a family of convex programs to fit a piecewise linear curve of a specified length to high dimensional data , where the sample complexity is independent of the ambient dimension .
we develop a deterministic single-pass algorithm for latent dirichlet allocation ( lda ) in order to process received documents one at a time and then discard them in an excess text stream . <eos> our algorithm does not need to store old statistics for all data . <eos> the proposed algorithm is much faster than a batch algorithm and is comparable to the batch algorithm in terms of perplexity in experiments .
in learning using privileged information ( lupi ) paradigm , along with the standard training data in the decision space , a teacher supplies a learner with the privileged information in the correcting space . <eos> the goal of the learner is to find a classifier with a low generalization error in the decision space . <eos> we consider a new version of empirical risk minimization algorithm , called privileged erm , that takes into account the privileged information in order to find a good function in the decision space . <eos> we outline the conditions on the correcting space that , if satisfied , allow privileged erm to have much faster learning rate in the decision space than the one of the regular empirical risk minimization .
a long-standing open research problem is how to use information from different experiments , including background knowledge , to infer causal relations . <eos> recent developments have shown ways to use multiple data sets , provided they originate from identical experiments . <eos> we present the mci-algorithm as the first method that can infer provably valid causal relations in the large sample limit from different experiments . <eos> it is fast , reliable and produces very clear and easily interpretable output . <eos> it is based on a result that shows that constraint-based causal discovery is decomposable into a candidate pair identification and subsequent elimination step that can be applied separately from different models . <eos> we test the algorithm on a variety of synthetic input model sets to assess its behavior and the quality of the output . <eos> the method shows promising signs that it can be adapted to suit causal discovery in real-world application areas as well , including large databases .
we consider bandit problems , motivated by applications in online advertising and news story selection , in which the learner must repeatedly select a slate , that is , a subset of size s from k possible actions , and then receives rewards for just the selected actions . <eos> the goal is to minimize the regret with respect to total reward of the best slate computed in hindsight . <eos> we consider unordered and ordered versions of the problem , and give efficient algorithms which have regret o ( sqrt ( t ) ) , where the constant depends on the specific nature of the problem . <eos> we also consider versions of the problem where we have access to a number of policies which make recommendations for slates in every round , and give algorithms with o ( sqrt ( t ) ) regret for competing with the best such policy as well . <eos> we make use of the technique of relative entropy projections combined with the usual multiplicative weight update algorithm to obtain our algorithms .
we present a generative probabilistic model for learning general graph structures , which we term concept graphs , from text . <eos> concept graphs provide a visual summary of the thematic content of a collection of documents-a task that is difficult to accomplish using only keyword search . <eos> the proposed model can learn different types of concept graph structures and is capable of utilizing partial prior knowledge about graph structure as well as labeled documents . <eos> we describe a generative model that is based on a stick-breaking process for graphs , and a markov chain monte carlo inference procedure . <eos> experiments on simulated data show that the model can recover known graph structure when learning in both unsupervised and semi-supervised modes . <eos> we also show that the proposed model is competitive in terms of empirical log likelihood with existing structure-based topic models ( such as hpam and hlda ) on real-world text data sets . <eos> finally , we illustrate the application of the model to the problem of updating wikipedia category graphs .
in some stochastic environments the well-known reinforcement learning algorithm q-learning performs very poorly . <eos> this poor performance is caused by large overestimations of action values . <eos> these overestimations result from a positive bias that is introduced because q-learning uses the maximum action value as an approximation for the maximum expected action value . <eos> we introduce an alternative way to approximate the maximum expected value for any set of random variables . <eos> the obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value . <eos> we apply the double estimator to q-learning to construct double q-learning , a new off-policy reinforcement learning algorithm . <eos> we show the new algorithm converges to the optimal policy and that it performs well in some settings in which q-learning performs poorly due to its overestimation .
we consider a class of learning problems that involve a structured sparsity-inducing norm defined as the sum of $ \ell_\infty $ -norms over groups of variables . <eos> whereas a lot of effort has been put in developing fast optimization methods when the groups are disjoint or embedded in a specific hierarchical structure , we address here the case of general overlapping groups . <eos> to this end , we show that the corresponding optimization problem is related to network flow optimization . <eos> more precisely , the proximal problem associated with the norm we consider is dual to a quadratic min-cost flow problem . <eos> we propose an efficient procedure which computes its solution exactly in polynomial time . <eos> our algorithm scales up to millions of groups and variables , and opens up a whole new range of applications for structured sparse models . <eos> we present several experiments on image and video data , demonstrating the applicability and scalability of our approach for various problems .
a challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way . <eos> the standard techniques include $ k $ -fold cross-validation ( $ k $ -cv ) , akaike information criterion ( aic ) , and bayesian information criterion ( bic ) . <eos> though these methods work well for low-dimensional problems , they are not suitable in high dimensional settings . <eos> in this paper , we present stars : a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs . <eos> the method has a clear interpretation : we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling . <eos> this interpretation requires essentially no conditions . <eos> under mild conditions , we show that stars is partially sparsistent in terms of graph estimation : i.e . with high probability , all the true edges will be included in the selected model even when the graph size asymptotically increases with the sample size . <eos> empirically , the performance of stars is compared with the state-of-the-art model selection procedures , including $ k $ -cv , aic , and bic , on both synthetic data and a real microarray dataset . <eos> stars outperforms all competing procedures .
generalized linear models ( glms ) are an increasingly popular framework for modeling neural spike trains . <eos> they have been linked to the theory of stochastic point processes and researchers have used this relation to assess goodness-of-fit using methods from point-process theory , e.g . the time-rescaling theorem . <eos> however , high neural firing rates or coarse discretization lead to a breakdown of the assumptions necessary for this connection . <eos> here , we show how goodness-of-fit tests from point-process theory can still be applied to glms by constructing equivalent surrogate point processes out of time-series observations . <eos> furthermore , two additional tests based on thinning and complementing point processes are introduced . <eos> they augment the instruments available for checking model adequacy of point processes as well as discretized models .
we propose a discriminative latent model for annotating images with unaligned object-level textual annotations . <eos> instead of using the bag-of-words image representation currently popular in the computer vision community , our model explicitly captures more intricate relationships underlying visual and textual information . <eos> in particular , we model the mapping that translates image regions to annotations . <eos> this mapping allows us to relate image regions to their corresponding annotation terms . <eos> we also model the overall scene label as latent information . <eos> this allows us to cluster test images . <eos> our training data consist of images and their associated annotations . <eos> but we do not have access to the ground-truth region-to-annotation mapping or the overall scene label . <eos> we develop a novel variant of the latent svm framework to model them as latent variables . <eos> our experimental results demonstrate the effectiveness of the proposed model compared with other baseline methods .
we present a novel probabilistic model for distributions over sets of structures -- for example , sets of sequences , trees , or graphs . <eos> the critical characteristic of our model is a preference for diversity : sets containing dissimilar structures are more likely . <eos> our model is a marriage of structured probabilistic models , like markov random fields and context free grammars , with determinantal point processes , which arise in quantum physics as models of particles with repulsive interactions . <eos> we extend the determinantal point process model to handle an exponentially-sized set of particles ( structures ) via a natural factorization of the model into parts . <eos> we show how this factorization leads to tractable algorithms for exact inference , including computing marginals , computing conditional probabilities , and sampling . <eos> our algorithms exploit a novel polynomially-sized dual representation of determinantal point processes , and use message passing over a special semiring to compute relevant quantities . <eos> we illustrate the advantages of the model on tracking and articulated pose estimation problems .
sodium entry during an action potential determines the energy efficiency of a neuron . <eos> the classic hodgkin-huxley model of action potential generation is notoriously inefficient in that regard with about 4 times more charges flowing through the membrane than the theoretical minimum required to achieve the observed depolarization . <eos> yet , recent experimental results show that mammalian neurons are close to the optimal metabolic efficiency and that the dynamics of their voltage-gated channels is significantly different than the one exhibited by the classic hodgkin-huxley model during the action potential . <eos> nevertheless , the original hodgkin-huxley model is still widely used and rarely to model the squid giant axon from which it was extracted . <eos> here , we introduce a novel family of hodgkin-huxley models that correctly account for sodium entry , action potential width and whose voltage-gated channels display a dynamics very similar to the most recent experimental observations in mammalian neurons . <eos> we speak here about a family of models because the model is parameterized by a unique parameter the variations of which allow to reproduce the entire range of experimental observations from cortical pyramidal neurons to purkinje cells , yielding a very economical framework to model a wide range of different central neurons . <eos> the present paper demonstrates the performances and discuss the properties of this new family of models .
partially observable markov decision processes ( pomdps ) model sequential decision-making problems under uncertainty and partial observability . <eos> unfortunately , some problems can not be modeled with state-dependent reward functions , e.g. , problems whose objective explicitly implies reducing the uncertainty on the state . <eos> to that end , we introduce rho-pomdps , an extension of pomdps where the reward function rho depends on the belief state . <eos> we show that , under the common assumption that rho is convex , the value function is also convex , what makes it possible to ( 1 ) approximate rho arbitrarily well with a piecewise linear and convex ( pwlc ) function , and ( 2 ) use state-of-the-art exact or approximate solving algorithms with limited changes .
we propose a computationally efficient random walk on a convex body which rapidly mixes to a time-varying gibbs distribution . <eos> in the setting of online convex optimization and repeated games , the algorithm yields low regret and presents a novel efficient method for implementing mixture forecasting strategies .
we consider least-squares regression using a randomly generated subspace g_p\subset f of finite dimension p , where f is a function space of infinite dimension , e.g.~l_2 ( [ 0,1 ] ^d ) . <eos> g_p is defined as the span of p random features that are linear combinations of the basis functions of f weighted by random gaussian i.i.d.~coefficients . <eos> in particular , we consider multi-resolution random combinations at all scales of a given mother function , such as a hat function or a wavelet . <eos> in this latter case , the resulting gaussian objects are called { \em scrambled wavelets } and we show that they enable to approximate functions in sobolev spaces h^s ( [ 0,1 ] ^d ) . <eos> as a result , given n data , the least-squares estimate \hat g built from p scrambled wavelets has excess risk ||f^* - \hat g||_\p^2 = o ( ||f^*||^2_ { h^s ( [ 0,1 ] ^d ) } ( \log n ) /p + p ( \log n ) /n ) for target functions f^*\in h^s ( [ 0,1 ] ^d ) of smoothness order s > d/2 . <eos> an interesting aspect of the resulting bounds is that they do not depend on the distribution \p from which the data are generated , which is important in a statistical regression setting considered here . <eos> randomization enables to adapt to any possible distribution . <eos> we conclude by describing an efficient numerical implementation using lazy expansions with numerical complexity \tilde o ( 2^d n^ { 3/2 } \log n + n^2 ) , where d is the dimension of the input space .
we deal with the problem of variable selection when variables must be selected group-wise , with possibly overlapping groups defined a priori . <eos> in particular we propose a new optimization procedure for solving the regularized algorithm presented in jacob et al . 09 , where the group lasso penalty is generalized to overlapping groups of variables . <eos> while in jacob et al . 09 the proposed implementation requires explicit replication of the variables belonging to more than one group , our iterative procedure is based on a combination of proximal methods in the primal space and constrained newton method in a reduced dual space , corresponding to the active groups . <eos> this procedure provides a scalable alternative with no need for data duplication , and allows to deal with high dimensional problems without pre-processing to reduce the dimensionality of the data . <eos> the computational advantages of our scheme with respect to state-of-the-art algorithms using data duplication are shown empirically with numerical simulations .
probabilistic grammars are generative statistical models that are useful for compositional and sequential structures . <eos> we present a framework , reminiscent of structural risk minimization , for empirical risk minimization of the parameters of a fixed probabilistic grammar using the log-loss . <eos> we derive sample complexity bounds in this framework that apply both to the supervised setting and the unsupervised setting .
this paper describes a probabilistic framework for studying associations between multiple genotypes , biomarkers , and phenotypic traits in the presence of noise and unobserved confounders for large genetic studies . <eos> the framework builds on sparse linear methods developed for regression and modified here for inferring causal structures of richer networks with latent variables . <eos> the method is motivated by the use of genotypes as `` instruments '' to infer causal associations between phenotypic biomarkers and outcomes , without making the common restrictive assumptions of instrumental variable methods . <eos> the method may be used for an effective screening of potentially interesting genotype phenotype and biomarker-phenotype associations in genome-wide studies , which may have important implications for validating biomarkers as possible proxy endpoints for early stage clinical trials . <eos> where the biomarkers are gene transcripts , the method can be used for fine mapping of quantitative trait loci ( qtls ) detected in genetic linkage studies . <eos> the method is applied for examining effects of gene transcript levels in the liver on plasma hdl cholesterol levels for a sample of sequenced mice from a heterogeneous stock , with $ \sim 10^5 $ genetic instruments and $ \sim 47 \times 10^3 $ gene transcripts .
we provide a sound and consistent foundation for the use of \emph { nonrandom } exploration data in `` contextual bandit '' or `` partially labeled '' settings where only the value of a chosen action is learned . <eos> the primary challenge in a variety of settings is that the exploration policy , in which `` offline '' data is logged , is not explicitly known . <eos> prior solutions here require either control of the actions during the learning process , recorded random exploration , or actions chosen obliviously in a repeated manner . <eos> the techniques reported here lift these restrictions , allowing the learning of a policy for choosing actions given features from historical data where no randomization occurred or was logged . <eos> we empirically verify our solution on two reasonably sized sets of real-world data obtained from an internet % online advertising company .
we study worst-case bounds on the quality of any fixed point assignment of the max-product algorithm for markov random fields ( mrf ) . <eos> we start proving a bound independent of the mrf structure and parameters . <eos> afterwards , we show how this bound can be improved for mrfs with particular structures such as bipartite graphs or grids . <eos> our results provide interesting insight into the behavior of max-product . <eos> for example , we prove that max-product provides very good results ( at least 90 % of the optimal ) on mrfs with large variable-disjoint cycles ( mrfs in which all cycles are variable-disjoint , namely that they do not share any edge and in which each cycle contains at least 20 variables ) .
motivated by an application to unsupervised part-of-speech tagging , we present an algorithm for the euclidean embedding of large sets of categorical data based on co-occurrence statistics . <eos> we use the code model of globerson et al . but constrain the embedding to lie on a high-dimensional unit sphere . <eos> this constraint allows for efficient optimization , even in the case of large datasets and high embedding dimensionality . <eos> using k-means clustering of the embedded data , our approach efficiently produces state-of-the-art results . <eos> we analyze the reasons why the sphere constraint is beneficial in this application , and conjecture that these reasons might apply quite generally to other large-scale tasks .
recent proposals suggest that large , generic neuronal networks could store memory traces of past input sequences in their instantaneous state . <eos> such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size , connectivity and signal statistics . <eos> prior work , in the case of gaussian input sequences and linear neuronal networks , shows that the duration of memory traces in a network can not exceed the number of neurons ( in units of the neuronal time constant ) , and that no network can out-perform an equivalent feedforward network . <eos> however a more ethologically relevant scenario is that of sparse input sequences . <eos> in this scenario , we show how linear neural networks can essentially perform compressed sensing ( cs ) of past inputs , thereby attaining a memory capacity that { \it exceeds } the number of neurons . <eos> this enhanced capacity is achieved by a class of `` orthogonal recurrent networks and not by feedforward networks or generic recurrent networks . <eos> we exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size , signal sparsity and integration time . <eos> alternately , viewed purely from the perspective of cs , this work introduces a new ensemble of measurement matrices derived from dynamical systems , and provides a theoretical analysis of their asymptotic performance . ''
we study learning curves for gaussian process regression which characterise performance in terms of the bayes error averaged over datasets of a given size . <eos> whilst learning curves are in general very difficult to calculate we show that for discrete input domains , where similarity between input points is characterised in terms of a graph , accurate predictions can be obtained . <eos> these should in fact become exact for large graphs drawn from a broad range of random graph ensembles with arbitrary degree distributions where each input ( node ) is connected only to a finite number of others . <eos> the method is based on translating the appropriate belief propagation equations to the graph ensemble . <eos> we demonstrate the accuracy of the predictions for poisson ( erdos-renyi ) and regular random graphs , and discuss when and why previous approximations to the learning curve fail .
the determination of dominant orientation at a given image location is formulated as a decision-theoretic question . <eos> this leads to a novel measure for the dominance of a given orientation $ \theta $ , which is similar to that used by sift . <eos> it is then shown that the new measure can be computed with a network that implements the sequence of operations of the standard neurophysiological model of v1 . <eos> the measure can thus be seen as a biologically plausible version of sift , and is denoted as biosift . <eos> the network units are shown to exhibit trademark properties of v1 neurons , such as cross-orientation suppression , sparseness and independence . <eos> the connection between sift and biological vision provides a justification for the success of sift-like features and reinforces the importance of contrast normalization in computer vision . <eos> we illustrate this by replacing the gabor units of an hmax network with the new biosift units . <eos> this is shown to lead to significant gains for classification tasks , leading to state-of-the-art performance among biologically inspired network models and performance competitive with the best non-biological object recognition systems .
recent experimental work has suggested that the neural firing rate can be interpreted as a fractional derivative , at least when signal variation induces neural adaptation . <eos> here , we show that the actual neural spike-train itself can be considered as the fractional derivative , provided that the neural signal is approximated by a sum of power-law kernels . <eos> a simple standard thresholding spiking neuron suffices to carry out such an approximation , given a suitable refractory response . <eos> empirically , we find that the online approximation of signals with a sum of power-law kernels is beneficial for encoding signals with slowly varying components , like long-memory self-similar signals . <eos> for such signals , the online power-law kernel approximation typically required less than half the number of spikes for similar snr as compared to sums of similar but exponentially decaying kernels . <eos> as power-law kernels can be accurately approximated using sums or cascades of weighted exponentials , we demonstrate that the corresponding decoding of spike-trains by a receiving neuron allows for natural and transparent temporal signal filtering by tuning the weights of the decoding kernel .
many statistical $ m $ -estimators are based on convex optimization problems formed by the weighted sum of a loss function with a norm-based regularizer . <eos> we analyze the convergence rates of first-order gradient methods for solving such problems within a high-dimensional framework that allows the data dimension $ d $ to grow with ( and possibly exceed ) the sample size $ n $ . <eos> this high-dimensional structure precludes the usual global assumptions -- -namely , strong convexity and smoothness conditions -- -that underlie classical optimization analysis . <eos> we define appropriately restricted versions of these conditions , and show that they are satisfied with high probability for various statistical models . <eos> under these conditions , our theory guarantees that nesterov 's first-order method~\cite { nesterov07 } has a globally geometric rate of convergence up to the statistical precision of the model , meaning the typical euclidean distance between the true unknown parameter $ \theta^* $ and the optimal solution $ \widehat { \theta } $ . <eos> this globally linear rate is substantially faster than previous analyses of global convergence for specific methods that yielded only sublinear rates . <eos> our analysis applies to a wide range of $ m $ -estimators and statistical models , including sparse linear regression using lasso ( $ \ell_1 $ -regularized regression ) , group lasso , block sparsity , and low-rank matrix recovery using nuclear norm regularization . <eos> overall , this result reveals an interesting connection between statistical precision and computational efficiency in high-dimensional estimation .
our objective is to train $ p $ -norm multiple kernel learning ( mkl ) and , more generally , linear mkl regularised by the bregman divergence , using the sequential minimal optimization ( smo ) algorithm . <eos> the smo algorithm is simple , easy to implement and adapt , and efficiently scales to large problems . <eos> as a result , it has gained widespread acceptance and svms are routinely trained using smo in diverse real world applications . <eos> training using smo has been a long standing goal in mkl for the very same reasons . <eos> unfortunately , the standard mkl dual is not differentiable , and therefore can not be optimised using smo style co-ordinate ascent . <eos> in this paper , we demonstrate that linear mkl regularised with the $ p $ -norm squared , or with certain bregman divergences , can indeed be trained using smo . <eos> the resulting algorithm retains both simplicity and efficiency and is significantly faster than the state-of-the-art specialised $ p $ -norm mkl solvers . <eos> we show that we can train on a hundred thousand kernels in approximately seven minutes and on fifty thousand points in less than half an hour on a single core .
we present a fast online solver for large scale maximum-flow problems as they occur in portfolio optimization , inventory management , computer vision , and logistics . <eos> our algorithm solves an integer linear program in an online fashion . <eos> it exploits total unimodularity of the constraint matrix and a lagrangian relaxation to solve the problem as a convex online game . <eos> the algorithm generates approximate solutions of max-flow problems by performing stochastic gradient descent on a set of flows . <eos> we apply the algorithm to optimize tier arrangement of over 80 million web pages on a layered set of caches to serve an incoming query stream optimally . <eos> we provide an empirical demonstration of the effectiveness of our method on real query-pages data .
in this paper , we propose an efficient algorithm for estimating the natural policy gradient with parameter-based exploration ; this algorithm samples directly in the parameter space . <eos> unlike previous methods based on natural gradients , our algorithm calculates the natural policy gradient using the inverse of the exact fisher information matrix . <eos> the computational cost of this algorithm is equal to that of conventional policy gradients whereas previous natural policy gradient methods have a prohibitive computational cost . <eos> experimental results show that the proposed method outperforms several policy gradient methods .
feature selection is an important component of many machine learning applications . <eos> especially in many bioinformatics tasks , efficient and robust feature selection methods are desired to extract meaningful features and eliminate noisy ones . <eos> in this paper , we propose a new robust feature selection method with emphasizing joint ? 2,1-norm minimization on both loss function and regularization . <eos> the ? 2,1-norm based loss function is robust to outliers in data points and the ? 2,1-norm regularization selects features across all data points with joint sparsity . <eos> an efficient algorithm is introduced with proved convergence . <eos> our regression based objective makes the feature selection process more efficient . <eos> our method has been applied into both genomic and proteomic biomarkers discovery . <eos> extensive empirical studies were performed on six data sets to demonstrate the effectiveness of our feature selection method .
applications of brain-machine-interfaces typically estimate user intent based on biological signals that are under voluntary control . <eos> for example , we might want to estimate how a patient with a paralyzed arm wants to move based on residual muscle activity . <eos> to solve such problems it is necessary to integrate obtained information over time . <eos> to do so , state of the art approaches typically use a probabilistic model of how the state , e.g . position and velocity of the arm , evolves over time a so-called trajectory model . <eos> we wanted to further develop this approach using two intuitive insights : ( 1 ) at any given point of time there may be a small set of likely movement targets , potentially identified by the location of objects in the workspace or by gaze information from the user . <eos> ( 2 ) the user may want to produce movements at varying speeds . <eos> we thus use a generative model with a trajectory model incorporating these insights . <eos> approximate inference on that generative model is implemented using a mixture of extended kalman filters . <eos> we find that the resulting algorithm allows us to decode arm movements dramatically better than when we use a trajectory model with linear dynamics .
we propose an algorithm to perform multitask learning where each task has potentially distinct label sets and label correspondences are not readily available . <eos> this is in contrast with existing methods which either assume that the label sets shared by different tasks are the same or that there exists a label mapping oracle . <eos> our method directly maximizes the mutual information among the labels , and we show that the resulting objective function can be efficiently optimized using existing algorithms . <eos> our proposed approach has a direct application for data integration with different label spaces for the purpose of classification , such as integrating yahoo ! <eos> and dmoz web directories .
the sample complexity of active learning under the realizability assumption has been well-studied . <eos> the realizability assumption , however , rarely holds in practice . <eos> in this paper , we theoretically characterize the sample complexity of active learning in the non-realizable case under multi-view setting . <eos> we prove that , with unbounded tsybakov noise , the sample complexity of multi-view active learning can be $ \widetilde { o } ( \log \frac { 1 } { \epsilon } ) $ , contrasting to single-view setting where the polynomial improvement is the best possible achievement . <eos> we also prove that in general multi-view setting the sample complexity of active learning with unbounded tsybakov noise is $ \widetilde { o } ( \frac { 1 } { \epsilon } ) $ , where the order of $ 1/\epsilon $ is independent of the parameter in tsybakov noise , contrasting to previous polynomial bounds where the order of $ 1/\epsilon $ is related to the parameter in tsybakov noise .
we consider reinforcement learning in partially observable domains where the agent can query an expert for demonstrations . <eos> our nonparametric bayesian approach combines model knowledge , inferred from expert information and independent exploration , with policy knowledge inferred from expert trajectories . <eos> we introduce priors that bias the agent towards models with both simple representations and simple policies , resulting in improved policy and model learning .
we consider multivariate regression problems involving high-dimensional predictor and response spaces . <eos> to efficiently address such problems , we propose a variable selection method , multivariate group orthogonal matching pursuit , which extends the standard orthogonal matching pursuit technique to account for arbitrary sparsity patterns induced by domain-specific groupings over both input and output variables , while also taking advantage of the correlation that may exist between the multiple outputs . <eos> we illustrate the utility of this framework for inferring causal relationships over a collection of high-dimensional time series variables . <eos> when applied to time-evolving social media content , our models yield a new family of causality-based influence measures that may be seen as an alternative to pagerank . <eos> theoretical guarantees , extensive simulations and empirical studies confirm the generality and value of our framework .
we consider the problem of reinforcement learning in high-dimensional spaces when the number of features is bigger than the number of samples . <eos> in particular , we study the least-squares temporal difference ( lstd ) learning algorithm when a space of low dimension is generated with a random projection from a high-dimensional space . <eos> we provide a thorough theoretical analysis of the lstd with random projections and derive performance bounds for the resulting algorithm . <eos> we also show how the error of lstd with random projections is propagated through the iterations of a policy iteration algorithm and provide a performance bound for the resulting least-squares policy iteration ( lspi ) algorithm .
we describe an accelerated hardware neuron being capable of emulating the adap-tive exponential integrate-and-fire neuron model . <eos> firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulation and in silicon on a prototype chip . <eos> the neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities . <eos> as the neuron is dedicated as a universal device for neuroscientific experiments , the focus lays on parameterizability and reproduction of the analytical model .
heavy-tailed distributions are often used to enhance the robustness of regression and classification methods to outliers in output space . <eos> often , however , we are confronted with `` outliers '' in input space , which are isolated observations in sparsely populated regions . <eos> we show that heavy-tailed process priors ( which we construct from gaussian processes via a copula ) , can be used to improve robustness of regression and classification estimators to such outliers by selectively shrinking them more strongly in sparse regions than in dense regions . <eos> we carry out a theoretical analysis to show that selective shrinkage occurs provided the marginals of the heavy-tailed process have sufficiently heavy tails . <eos> the analysis is complemented by experiments on biological data which indicate significant improvements of estimates in sparse regions while producing competitive results in dense regions .
we propose a class of sparse coding models that utilizes a laplacian scale mixture ( lsm ) prior to model dependencies among coefficients . <eos> each coefficient is modeled as a laplacian distribution with a variable scale parameter , with a gamma distribution prior over the scale parameter . <eos> we show that , due to the conjugacy of the gamma prior , it is possible to derive efficient inference procedures for both the coefficients and the scale parameter . <eos> when the scale parameters of a group of coefficients are combined into a single variable , it is possible to describe the dependencies that occur due to common amplitude fluctuations among coefficients , which have been shown to constitute a large fraction of the redundancy in natural images . <eos> we show that , as a consequence of this group sparse coding , the resulting inference of the coefficients follows a divisive normalization rule , and that this may be efficiently implemented a network architecture similar to that which has been proposed to occur in primary visual cortex . <eos> we also demonstrate improvements in image coding and compressive sensing recovery using the lsm model .
dimensionality reduction is often needed in many applications due to the high dimensionality of the data involved . <eos> in this paper , we first analyze the scatter measures used in the conventional linear discriminant analysis~ ( lda ) model and note that the formulation is based on the average-case view . <eos> based on this analysis , we then propose a new dimensionality reduction method called worst-case linear discriminant analysis~ ( wlda ) by defining new between-class and within-class scatter measures . <eos> this new model adopts the worst-case view which arguably is more suitable for applications such as classification . <eos> when the number of training data points or the number of features is not very large , we relax the optimization problem involved and formulate it as a metric learning problem . <eos> otherwise , we take a greedy approach by finding one direction of the transformation at a time . <eos> moreover , we also analyze a special case of wlda to show its relationship with conventional lda . <eos> experiments conducted on several benchmark datasets demonstrate the effectiveness of wlda when compared with some related dimensionality reduction methods .
we address the problem of estimating the f-measure of a given model as accurately as possible on a fixed labeling budget . <eos> this problem occurs whenever an estimate can not be obtained from held-out training data ; for instance , when data that have been used to train the model are held back for reasons of privacy or do not reflect the test distribution . <eos> in this case , new test instances have to be drawn and labeled at a cost . <eos> an active estimation procedure selects instances according to an instrumental sampling distribution . <eos> an analysis of the sources of estimation error leads to an optimal sampling distribution that minimizes estimator variance . <eos> we explore conditions under which active estimates of f-measures are more accurate than estimates based on instances sampled from the test distribution .
regularization technique has become a principle tool for statistics and machine learning research and practice . <eos> however , in most situations , these regularization terms are not well interpreted , especially on how they are related to the loss function and data . <eos> in this paper , we propose a robust minimax framework to interpret the relationship between data and regularization terms for a large class of loss functions . <eos> we show that various regularization terms are essentially corresponding to different distortions to the original data matrix . <eos> this minimax framework includes ridge regression , lasso , elastic net , fused lasso , group lasso , local coordinate coding , multiple kernel learning , etc. , as special cases . <eos> within this minimax framework , we further gave mathematically exact definition for a novel representation called sparse grouping representation ( sgr ) , and proved sufficient conditions for generating such group level sparsity . <eos> under these sufficient conditions , a large set of consistent regularization terms can be designed . <eos> this sgr is essentially different from group lasso in the way of using class or group information , and it outperforms group lasso when there appears group label noise . <eos> we also gave out some generalization bounds in a classification setting .
latent force models encode the interaction between multiple related dynamical systems in the form of a kernel or covariance function . <eos> each variable to be modeled is represented as the output of a differential equation and each differential equation is driven by a weighted sum of latent functions with uncertainty given by a gaussian process prior . <eos> in this paper we consider employing the latent force model framework for the problem of determining robot motor primitives . <eos> to deal with discontinuities in the dynamical systems or the latent driving force we introduce an extension of the basic latent force model , that switches between different latent functions and potentially different dynamical systems . <eos> this creates a versatile representation for robot movements that can capture discrete changes and non-linearities in the dynamics . <eos> we give illustrative examples on both synthetic data and for striking movements recorded using a barrett wam robot as haptic input device . <eos> our inspiration is robot motor primitives , but we expect our model to have wide application for dynamical systems including models for human motion capture data and systems biology .
we present a simple and effective approach to learning tractable conditional random fields with structure that depends on the evidence . <eos> our approach retains the advantages of tractable discriminative models , namely efficient exact inference and exact parameter learning . <eos> at the same time , our algorithm does not suffer a large expressive power penalty inherent to fixed tractable structures . <eos> on real-life relational datasets , our approach matches or exceeds state of the art accuracy of the dense models , and at the same time provides an order of magnitude speedup
in many machine learning domains ( such as scene understanding ) , several related sub-tasks ( such as scene categorization , depth estimation , object detection ) operate on the same raw data and provide correlated outputs . <eos> each of these tasks is often notoriously hard , and state-of-the-art classifiers already exist for many sub-tasks . <eos> it is desirable to have an algorithm that can capture such correlation without requiring to make any changes to the inner workings of any classifier . <eos> we propose feedback enabled cascaded classification models ( fe-ccm ) , that maximizes the joint likelihood of the sub-tasks , while requiring only a black-boxinterface to the original classifier for each sub-task . <eos> we use a two-layer cascade of classifiers , which are repeated instantiations of the original ones , with the output of the first layer fed into the second layer as input . <eos> our training method involves a feedback step that allows later classifiers to provide earlier classifiers information about what error modes to focus on . <eos> we show that our method significantly improves performance in all the sub-tasks in two different domains : ( i ) scene understanding , where we consider depth estimation , scene categorization , event categorization , object detection , geometric labeling and saliency detection , and ( ii ) robotic grasping , where we consider grasp point detection and object classification .
the paper develops a connection between traditional perceptron algorithms and recently introduced herding algorithms . <eos> it is shown that both algorithms can be viewed as an application of the perceptron cycling theorem . <eos> this connection strengthens some herding results and suggests new ( supervised ) herding algorithms that , like crfs or discriminative rbms , make predictions by conditioning on the input attributes . <eos> we develop and investigate variants of conditional herding , and show that conditional herding leads to practical algorithms that perform better than or on par with related classifiers such as the voted perceptron and the discriminative rbm .
singular value decomposition ( and principal component analysis ) is one of the most widely used techniques for dimensionality reduction : successful and efficiently computable , it is nevertheless plagued by a well-known , well-documented sensitivity to outliers . <eos> recent work has considered the setting where each point has a few arbitrarily corrupted components . <eos> yet , in applications of svd or pca such as robust collaborative filtering or bioinformatics , malicious agents , defective genes , or simply corrupted or contaminated experiments may effectively yield entire points that are completely corrupted . <eos> we present an efficient convex optimization-based algorithm we call outlier pursuit , that under some mild assumptions on the uncorrupted points ( satisfied , e.g. , by the standard generative assumption in pca problems ) recovers the *exact* optimal low-dimensional subspace , and identifies the corrupted points . <eos> such identification of corrupted points that do not conform to the low-dimensional approximation , is of paramount interest in bioinformatics and financial applications , and beyond . <eos> our techniques involve matrix decomposition using nuclear norm minimization , however , our results , setup , and approach , necessarily differ considerably from the existing line of work in matrix completion and matrix decomposition , since we develop an approach to recover the correct *column space* of the uncorrupted matrix , rather than the exact matrix itself .
with the increase in available data parallel machine learning has become an increasingly pressing problem . <eos> in this paper we present the first parallel stochastic gradient descent algorithm including a detailed analysis and experimental evidence . <eos> unlike prior work on parallel optimization algorithms our variant comes with parallel acceleration guarantees and it poses no overly tight latency constraints , which might only be available in the multicore setting . <eos> our analysis introduces a novel proof technique -- - contractive mappings to quantify the speed of convergence of parameter distributions to their asymptotic limits . <eos> as a side effect this answers the question of how quickly stochastic gradient descent algorithms reach the asymptotically normal regime .
modelling camera shake as a space-invariant convolution simplifies the problem of removing camera shake , but often insufficiently models actual motion blur such as those due to camera rotation and movements outside the sensor plane or when objects in the scene have different distances to the camera . <eos> in order to overcome such limitations we contribute threefold : ( i ) we introduce a taxonomy of camera shakes , ( ii ) we show how to combine a recently introduced framework for space-variant filtering based on overlap-add from hirsch et al.~and a fast algorithm for single image blind deconvolution for space-invariant filters from cho and lee to introduce a method for blind deconvolution for space-variant blur . <eos> and ( iii ) , we present an experimental setup for evaluation that allows us to take images with real camera shake while at the same time record the space-variant point spread function corresponding to that blur . <eos> finally , we demonstrate that our method is able to deblur images degraded by spatially-varying blur originating from real camera shake .
robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classification ; but pixels , or even local image patches , carry little semantic meanings . <eos> for high level visual tasks , such low-level image representations are potentially not enough . <eos> in this paper , we propose a high-level image representation , called the object bank , where an image is represented as a scale invariant response map of a large number of pre-trained generic object detectors , blind to the testing dataset or visual task . <eos> leveraging on the object bank representation , superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classifiers such as logistic regression and linear svm . <eos> sparsity algorithms make our representation more efficient and scalable for large scene datasets , and reveal semantically meaningful feature patterns .
this paper presents a co-regularization based approach to semi-supervised domain adaptation . <eos> our proposed approach ( ea++ ) builds on the notion of augmented space ( introduced in easyadapt ( ea ) [ 1 ] ) and harnesses unlabeled data in target domain to further enable the transfer of information from source to target . <eos> this semi-supervised approach to domain adaptation is extremely simple to implement and can be applied as a pre-processing step to any supervised learner . <eos> our theoretical analysis ( in terms of rademacher complexity ) of ea and ea++ show that the hypothesis class of ea++ has lower complexity ( compared to ea ) and hence results in tighter generalization bounds . <eos> experimental results on sentiment analysis tasks reinforce our theoretical findings and demonstrate the efficacy of the proposed method when compared to ea as well as a few other baseline approaches .
we present an algorithm for learning high-treewidth markov networks where inference is still tractable . <eos> this is made possible by exploiting context specific independence and determinism in the domain . <eos> the class of models our algorithm can learn has the same desirable properties as thin junction trees : polynomial inference , closed form weight learning , etc. , but is much broader . <eos> our algorithm searches for a feature that divides the state space into subspaces where the remaining variables decompose into independent subsets ( conditioned on the feature or its negation ) and recurses on each subspace/subset of variables until no useful new features can be found . <eos> we provide probabilistic performance guarantees for our algorithm under the assumption that the maximum feature length is k ( the treewidth can be much larger ) and dependences are of bounded strength . <eos> we also propose a greedy version of the algorithm that , while forgoing these guarantees , is much more efficient.experiments on a variety of domains show that our approach compares favorably with thin junction trees and other markov network structure learners .
arithmetic circuits ( acs ) exploit context-specific independence and determinism to allow exact inference even in networks with high treewidth . <eos> in this paper , we introduce the first ever approximate inference methods using acs , for domains where exact inference remains intractable . <eos> we propose and evaluate a variety of techniques based on exact compilation , forward sampling , ac structure learning , markov network parameter learning , variational inference , and gibbs sampling . <eos> in experiments on eight challenging real-world domains , we find that the methods based on sampling and learning work best : one such method ( ac2-f ) is faster and usually more accurate than loopy belief propagation , mean field , and gibbs sampling ; another ( ac2-g ) has a running time similar to gibbs sampling but is consistently more accurate than all baselines .
we present policy gradient results within the framework of linearly-solvable mdps . <eos> for the first time , compatible function approximators and natural policy gradients are obtained by estimating the cost-to-go function , rather than the ( much larger ) state-action advantage function as is necessary in traditional mdps . <eos> we also develop the first compatible function approximators and natural policy gradients for continuous-time stochastic systems .
we present and analyze an agnostic active learning algorithm that works without keeping a version space . <eos> this is unlike all previous approaches where a restricted set of candidate hypotheses is maintained throughout learning , and only hypotheses from this set are ever returned . <eos> by avoiding this version space approach , our algorithm sheds the computational burden and brittleness associated with maintaining version spaces , yet still allows for substantial improvements over supervised learning for classification .
we consider the following sparse signal recovery ( or feature selection ) problem : given a design matrix $ x\in \mathbb { r } ^ { n\times m } $ $ ( m\gg n ) $ and a noisy observation vector $ y\in \mathbb { r } ^ { n } $ satisfying $ y=x\beta^*+\epsilon $ where $ \epsilon $ is the noise vector following a gaussian distribution $ n ( 0 , \sigma^2i ) $ , how to recover the signal ( or parameter vector ) $ \beta^* $ when the signal is sparse ? <eos> the dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees . <eos> in this paper , we propose a multi-stage dantzig selector method , which iteratively refines the target signal $ \beta^* $ . <eos> we show that if $ x $ obeys a certain condition , then with a large probability the difference between the solution $ \hat\beta $ estimated by the proposed method and the true solution $ \beta^* $ measured in terms of the $ l_p $ norm ( $ p\geq 1 $ ) is bounded as \begin { equation* } \|\hat\beta-\beta^*\|_p\leq \left ( c ( s-n ) ^ { 1/p } \sqrt { \log m } +\delta\right ) \sigma , \end { equation* } $ c $ is a constant , $ s $ is the number of nonzero entries in $ \beta^* $ , $ \delta $ is independent of $ m $ and is much smaller than the first term , and $ n $ is the number of entries of $ \beta^* $ larger than a certain value in the order of $ \mathcal { o } ( \sigma\sqrt { \log m } ) $ . <eos> the proposed method improves the estimation bound of the standard dantzig selector approximately from $ cs^ { 1/p } \sqrt { \log m } \sigma $ to $ c ( s-n ) ^ { 1/p } \sqrt { \log m } \sigma $ where the value $ n $ depends on the number of large entries in $ \beta^* $ . <eos> when $ n=s $ , the proposed algorithm achieves the oracle solution with a high probability . <eos> in addition , with a large probability , the proposed method can select the same number of correct features under a milder condition than the dantzig selector .
functional segregation and integration are fundamental characteristics of the human brain . <eos> studying the connectivity among segregated regions and the dynamics of integrated brain networks has drawn increasing interest . <eos> a very controversial , yet fundamental issue in these studies is how to determine the best functional brain regions or rois ( regions of interests ) for individuals . <eos> essentially , the computed connectivity patterns and dynamics of brain networks are very sensitive to the locations , sizes , and shapes of the rois . <eos> this paper presents a novel methodology to optimize the locations of an individual 's rois in the working memory system . <eos> our strategy is to formulate the individual roi optimization as a group variance minimization problem , in which group-wise functional and structural connectivity patterns , and anatomic profiles are defined as optimization constraints . <eos> the optimization problem is solved via the simulated annealing approach . <eos> our experimental results show that the optimized rois have significantly improved consistency in structural and functional profiles across subjects , and have more reasonable localizations and more consistent morphological and anatomic profiles .
we propose a general framework to online learning for classification problems with time-varying potential functions in the adversarial setting . <eos> this framework allows to design and prove relative mistake bounds for any generic loss function . <eos> the mistake bounds can be specialized for the hinge loss , allowing to recover and improve the bounds of known online classification algorithms . <eos> by optimizing the general bound we derive a new online classification algorithm , called narow , that hybridly uses adaptive- and fixed- second order information . <eos> we analyze the properties of the algorithm and illustrate its performance using synthetic dataset .
algorithms based on iterative local approximations present a practical approach to optimal control in robotic systems . <eos> however , they generally require the temporal parameters ( for e.g . the movement duration or the time point of reaching an intermediate goal ) to be specified \textit { a priori } . <eos> here , we present a methodology that is capable of jointly optimising the temporal parameters in addition to the control command profiles . <eos> the presented approach is based on a bayesian canonical time formulation of the optimal control problem , with the temporal mapping from canonical to real time parametrised by an additional control variable . <eos> an approximate em algorithm is derived that efficiently optimises both the movement duration and control commands offering , for the first time , a practical approach to tackling generic via point problems in a systematic way under the optimal control framework . <eos> the proposed approach is evaluated on simulations of a redundant robotic plant .
this paper is concerned with the generalization analysis on learning to rank for information retrieval ( ir ) . <eos> in ir , data are hierarchically organized , i.e. , consisting of queries and documents per query . <eos> previous generalization analysis for ranking , however , has not fully considered this structure , and can not explain how the simultaneous change of query number and document number in the training data will affect the performance of algorithms . <eos> in this paper , we propose performing generalization analysis under the assumption of two-layer sampling , i.e. , the i.i.d . <eos> sampling of queries and the conditional i.i.d sampling of documents per query . <eos> such a sampling can better describe the generation mechanism of real data , and the corresponding generalization analysis can better explain the real behaviors of learning to rank algorithms . <eos> however , it is challenging to perform such analysis , because the documents associated with different queries are not identically distributed , and the documents associated with the same query become no longer independent if represented by features extracted from the matching between document and query . <eos> to tackle the challenge , we decompose the generalization error according to the two layers , and make use of the new concept of two-layer rademacher average . <eos> the generalization bounds we obtained are quite intuitive and are in accordance with previous empirical studies on the performance of ranking algorithms .
a striking aspect of cortical neural networks is the divergence of a relatively small number of input channels from the peripheral sensory apparatus into a large number of cortical neurons , an over-complete representation strategy . <eos> cortical neurons are then connected by a sparse network of lateral synapses . <eos> here we propose that such architecture may increase the persistence of the representation of an incoming stimulus , or a percept . <eos> we demonstrate that for a family of networks in which the receptive field of each neuron is re-expressed by its outgoing connections , a represented percept can remain constant despite changing activity . <eos> we term this choice of connectivity receptive field recombination ( refire ) networks . <eos> the sparse refire network may serve as a high-dimensional integrator and a biologically plausible model of the local cortical circuit .
consider a convex relaxation $ \hat f $ of a pseudo-boolean function $ f $ . <eos> we say that the relaxation is { \em totally half-integral } if $ \hat f ( \bx ) $ is a polyhedral function with half-integral extreme points $ \bx $ , and this property is preserved after adding an arbitrary combination of constraints of the form $ x_i=x_j $ , $ x_i=1-x_j $ , and $ x_i=\gamma $ where $ \gamma\in\ { 0,1 , \frac { 1 } { 2 } \ } $ is a constant . <eos> a well-known example is the { \em roof duality } relaxation for quadratic pseudo-boolean functions $ f $ . <eos> we argue that total half-integrality is a natural requirement for generalizations of roof duality to arbitrary pseudo-boolean functions . <eos> our contributions are as follows . <eos> first , we provide a complete characterization of totally half-integral relaxations $ \hat f $ by establishing a one-to-one correspondence with { \em bisubmodular functions } . <eos> second , we give a new characterization of bisubmodular functions . <eos> finally , we show some relationships between general totally half-integral relaxations and relaxations based on the roof duality .
when software developers modify one or more files in a large code base , they must also identify and update other related files . <eos> many file dependencies can be detected by mining the development history of the code base : in essence , groups of related files are revealed by the logs of previous workflows . <eos> from data of this form , we show how to detect dependent files by solving a problem in binary matrix completion . <eos> we explore different latent variable models ( lvms ) for this problem , including bernoulli mixture models , exponential family pca , restricted boltzmann machines , and fully bayesian approaches . <eos> we evaluate these models on the development histories of three large , open-source software systems : mozilla firefox , eclipse subversive , and gimp . <eos> in all of these applications , we find that lvms improve the performance of related file prediction over current leading methods .
we present a novel approach to inference in conditionally gaussian continuous time stochastic processes , where the latent process is a markovian jump process . <eos> we first consider the case of jump-diffusion processes , where the drift of a linear stochastic differential equation can jump at arbitrary time points . <eos> we derive partial differential equations for exact inference and present a very efficient mean field approximation . <eos> by introducing a novel lower bound on the free energy , we then generalise our approach to gaussian processes with arbitrary covariance , such as the non-markovian rbf covariance . <eos> we present results on both simulated and real data , showing that the approach is very accurate in capturing latent dynamics and can be useful in a number of real data modelling tasks .
the problem of controlling the margin of a classifier is studied . <eos> a detailed analytical study is presented on how properties of the classification risk , such as its optimal link and minimum risk functions , are related to the shape of the loss , and its margin enforcing properties . <eos> it is shown that for a class of risks , denoted canonical risks , asymptotic bayes consistency is compatible with simple analytical relationships between these functions . <eos> these enable a precise characterization of the loss for a popular class of link functions . <eos> it is shown that , when the risk is in canonical form and the link is inverse sigmoidal , the margin properties of the loss are determined by a single parameter . <eos> novel families of bayes consistent loss functions , of variable margin , are derived . <eos> these families are then used to design boosting style algorithms with explicit control of the classification margin . <eos> the new algorithms generalize well established approaches , such as logitboost . <eos> experimental results show that the proposed variable margin losses outperform the fixed margin counterparts used by existing algorithms . <eos> finally , it is shown that best performance can be achieved by cross-validating the margin parameter .
we study a setting in which poisson processes generate sequences of decision-making events . <eos> the optimization goal is allowed to depend on the rate of decision outcomes ; the rate may depend on a potentially long backlog of events and decisions . <eos> we model the problem as a poisson process with a throttling policy that enforces a data-dependent rate limit and reduce the learning problem to a convex optimization problem that can be solved efficiently . <eos> this problem setting matches applications in which damage caused by an attacker grows as a function of the rate of unsuppressed hostile events . <eos> we report on experiments on abuse detection for an email service .
we present a novel algorithm , random conic pursuit , that solves semidefinite programs ( sdps ) via repeated optimization over randomly selected two-dimensional subcones of the psd cone . <eos> this scheme is simple , easily implemented , applicable to very general sdps , scalable , and theoretically interesting . <eos> its advantages are realized at the expense of an ability to readily compute highly exact solutions , though useful approximate solutions are easily obtained . <eos> this property renders random conic pursuit of particular interest for machine learning applications , in which the relevant sdps are generally based upon random data and so exact minima are often not a priority . <eos> indeed , we present empirical results to this effect for various sdps encountered in machine learning ; these experiments demonstrate the potential practical usefulness of random conic pursuit . <eos> we also provide a preliminary analysis that yields insight into the theoretical properties and convergence of the algorithm .
multi-class classification becomes challenging at test time when the number of classes is very large and testing against every possible class can become computationally infeasible . <eos> this problem can be alleviated by imposing ( or learning ) a structure over the set of classes . <eos> we propose an algorithm for learning a tree-structure of classifiers which , by optimizing the overall tree loss , provides superior accuracy to existing tree labeling methods . <eos> we also propose a method that learns to embed labels in a low dimensional space that is faster than non-embedding approaches and has superior accuracy to existing embedding approaches . <eos> finally we combine the two ideas resulting in the label embedding tree that outperforms alternative methods including one-vs-rest while being orders of magnitude faster .
many combinatorial problems arising in machine learning can be reduced to the problem of minimizing a submodular function . <eos> submodular functions are a natural discrete analog of convex functions , and can be minimized in strongly polynomial time . <eos> unfortunately , state-of-the-art algorithms for general submodular minimization are intractable for practical problems . <eos> in this paper , we introduce a novel subclass of submodular minimization problems that we call decomposable . <eos> decomposable submodular functions are those that can be represented as sums of concave functions applied to linear functions . <eos> we develop an algorithm , slg , that can efficiently minimize decomposable submodular functions with tens of thousands of variables . <eos> our algorithm exploits recent results in smoothed convex minimization . <eos> we apply slg to synthetic benchmarks and a joint classification-and-segmentation task , and show that it outperforms the state-of-the-art general purpose submodular minimization algorithms by several orders of magnitude .
when animals repeatedly choose actions from multiple alternatives , they can allocate their choices stochastically depending on past actions and outcomes . <eos> it is commonly assumed that this ability is achieved by modifications in synaptic weights related to decision making . <eos> choice behavior has been empirically found to follow herrnstein ? s matching law . <eos> loewenstein & amp ; seung ( 2006 ) demonstrated that matching behavior is a steady state of learning in neural networks if the synaptic weights change proportionally to the covariance between reward and neural activities . <eos> however , their proof did not take into account the change in entire synaptic distributions . <eos> in this study , we show that matching behavior is not necessarily a steady state of the covariance-based learning rule when the synaptic strength is sufficiently strong so that the fluctuations in input from individual sensory neurons influence the net input to output neurons . <eos> this is caused by the increasing variance in the input potential due to the diffusion of synaptic weights . <eos> this effect causes an undermatching phenomenon , which has been observed in many behavioral experiments . <eos> we suggest that the synaptic diffusion effects provide a robust neural mechanism for stochastic choice behavior .
layered models are a powerful way of describing natural scenes containing smooth surfaces that may overlap and occlude each other . <eos> for image motion estimation , such models have a long history but have not achieved the wide use or accuracy of non-layered methods . <eos> we present a new probabilistic model of optical flow in layers that addresses many of the shortcomings of previous approaches . <eos> in particular , we define a probabilistic graphical model that explicitly captures : 1 ) occlusions and disocclusions ; 2 ) depth ordering of the layers ; 3 ) temporal consistency of the layer segmentation . <eos> additionally the optical flow in each layer is modeled by a combination of a parametric model and a smooth deviation based on an mrf with a robust spatial prior ; the resulting model allows roughness in layers . <eos> finally , a key contribution is the formulation of the layers using an image-dependent hidden field prior based on recent models for static scene segmentation . <eos> the method achieves state-of-the-art results on the middlebury benchmark and produces meaningful scene segmentations as well as detected occlusion regions .
this paper introduces a monte-carlo algorithm for online planning in large pomdps . <eos> the algorithm combines a monte-carlo update of the agent 's belief state with a monte-carlo tree search from the current belief state . <eos> the new algorithm , pomcp , has two important properties . <eos> first , monte-carlo sampling is used to break the curse of dimensionality both during belief state updates and during planning . <eos> second , only a black box simulator of the pomdp is required , rather than explicit probability distributions . <eos> these properties enable pomcp to plan effectively in significantly larger pomdps than has previously been possible . <eos> we demonstrate its effectiveness in three large pomdps . <eos> we scale up a well-known benchmark problem , rocksample , by several orders of magnitude . <eos> we also introduce two challenging new pomdps : 10x10 battleship and partially observable pacman , with approximately 10^18 and 10^56 states respectively . <eos> our monte-carlo planning algorithm achieved a high level of performance with no prior knowledge , and was also able to exploit simple domain knowledge to achieve better results with less search . <eos> pomcp is the first general purpose planner to achieve high performance in such large and unfactored pomdps .
we obtain a tight distribution-specific characterization of the sample complexity of large-margin classification with l2 regularization : we introduce the gamma-adapted-dimension , which is a simple function of the spectrum of a distribution 's covariance matrix , and show distribution-specific upper and lower bounds on the sample complexity , both governed by the gamma-adapted-dimension of the source distribution . <eos> we conclude that this new quantity tightly characterizes the true sample complexity of large-margin classification . <eos> the bounds hold for a rich family of sub-gaussian distributions .
the problem of optimal and automatic design of a detector cascade is considered . <eos> a novel mathematical model is introduced for a cascaded detector . <eos> this model is analytically tractable , leads to recursive computation , and accounts for both classification and complexity . <eos> a boosting algorithm , fcboost , is proposed for fully automated cascade design . <eos> it exploits the new cascade model , minimizes a lagrangian cost that accounts for both classification risk and complexity . <eos> it searches the space of cascade configurations to automatically determine the optimal number of stages and their predictors , and is compatible with bootstrapping of negative examples and cost sensitive learning . <eos> experiments show that the resulting cascades have state-of-the-art performance in various computer vision problems .
as increasing amounts of sensitive personal information finds its way into data repositories , it is important to develop analysis mechanisms that can derive aggregate information from these repositories without revealing information about individual data instances . <eos> though the differential privacy model provides a framework to analyze such mechanisms for databases belonging to a single party , this framework has not yet been considered in a multi-party setting . <eos> in this paper , we propose a privacy-preserving protocol for composing a differentially private aggregate classifier using classifiers trained locally by separate mutually untrusting parties . <eos> the protocol allows these parties to interact with an untrusted curator to construct additive shares of a perturbed aggregate classifier . <eos> we also present a detailed theoretical analysis containing a proof of differential privacy of the perturbed aggregate classifier and a bound on the excess risk introduced by the perturbation . <eos> we verify the bound with an experimental evaluation on a real dataset .
we address the problem of semi-supervised learning in an adversarial setting . <eos> instead of assuming that labels are missing at random , we analyze a less favorable scenario where the label information can be missing partially and arbitrarily , which is motivated by several practical examples . <eos> we present nearly matching upper and lower generalization bounds for learning in this setting under reasonable assumptions about available label information . <eos> motivated by the analysis , we formulate a convex optimization problem for parameter estimation , derive an efficient algorithm , and analyze its convergence . <eos> we provide experimental results on several standard data sets showing the robustness of our algorithm to the pattern of missing label information , outperforming several strong baselines .
since the discovery of sophisticated fully polynomial randomized algorithms for a range of # p problems ( karzanov et al. , 1991 ; jerrum et al. , 2001 ; wilson , 2004 ) , theoretical work on approximate inference in combinatorial spaces has focused on markov chain monte carlo methods . <eos> despite their strong theoretical guarantees , the slow running time of many of these randomized algorithms and the restrictive assumptions on the potentials have hindered the applicability of these algorithms to machine learning . <eos> because of this , in applications to combinatorial spaces simple exact models are often preferred to more complex models that require approximate inference ( siepel et al. , 2004 ) . <eos> variational inference would appear to provide an appealing alternative , given the success of variational methods for graphical models ( wainwright et al. , 2008 ) ; unfortunately , however , it is not obvious how to develop variational approximations for combinatorial objects such as matchings , partial orders , plane partitions and sequence alignments . <eos> we propose a new framework that extends variational inference to a wide range of combinatorial spaces . <eos> our method is based on a simple assumption : the existence of a tractable measure factorization , which we show holds in many examples . <eos> simulations on a range of matching models show that the algorithm is more general and empirically faster than a popular fully polynomial randomized algorithm . <eos> we also apply the framework to the problem of multiple alignment of protein sequences , obtaining state-of-the-art results on the balibase dataset ( thompson et al. , 1999 ) .
we present a new way of converting a reversible finite markov chain into a nonreversible one , with a theoretical guarantee that the asymptotic variance of the mcmc estimator based on the non-reversible chain is reduced . <eos> the method is applicable to any reversible chain whose states are not connected through a tree , and can be interpreted graphically as inserting vortices into the state transition graph . <eos> our result confirms that non-reversible chains are fundamentally better than reversible ones in terms of asymptotic performance , and suggests interesting directions for further improving mcmc .
to understand the relationship between genomic variations among population and complex diseases , it is essential to detect eqtls which are associated with phenotypic effects . <eos> however , detecting eqtls remains a challenge due to complex underlying mechanisms and the very large number of genetic loci involved compared to the number of samples . <eos> thus , to address the problem , it is desirable to take advantage of the structure of the data and prior information about genomic locations such as conservation scores and transcription factor binding sites . <eos> in this paper , we propose a novel regularized regression approach for detecting eqtls which takes into account related traits simultaneously while incorporating many regulatory features . <eos> we first present a bayesian network for a multi-task learning problem that includes priors on snps , making it possible to estimate the significance of each covariate adaptively . <eos> then we find the maximum a posteriori ( map ) estimation of regression coefficients and estimate weights of covariates jointly . <eos> this optimization procedure is efficient since it can be achieved by using convex optimization and a coordinate descent procedure iteratively . <eos> experimental results on simulated and real yeast datasets confirm that our model outperforms previous methods for finding eqtls .
the random projection tree ( rptree ) structures proposed in [ dasgupta-freund-stoc-08 ] are space partitioning data structures that automatically adapt to various notions of intrinsic dimensionality of data . <eos> we prove new results for both the rptree-max and the rptree-mean data structures . <eos> our result for rptree-max gives a near-optimal bound on the number of levels required by this data structure to reduce the size of its cells by a factor s > = 2 . <eos> we also prove a packing lemma for this data structure . <eos> our final result shows that low-dimensional manifolds possess bounded local covariance dimension . <eos> as a consequence we show that rptree-mean adapts to manifold dimension as well .
we consider the problem of learning a local metric to enhance the performance of nearest neighbor classification . <eos> conventional metric learning methods attempt to separate data distributions in a purely discriminative manner ; here we show how to take advantage of information from parametric generative models . <eos> we focus on the bias in the information-theoretic error arising from finite sampling effects , and find an appropriate local metric that maximally reduces the bias based upon knowledge from generative models . <eos> as a byproduct , the asymptotic theoretical analysis in this work relates metric learning with dimensionality reduction , which was not understood from previous discriminative approaches . <eos> empirical experiments show that this learned local metric enhances the discriminative nearest neighbor performance on various datasets using simple class conditional generative models .
in many real world applications we do not have access to fully-labeled training data , but only to a list of possible labels . <eos> this is the case , e.g. , when learning visual classifiers from images downloaded from the web , using just their text captions or tags as learning oracles . <eos> in general , these problems can be very difficult . <eos> however most of the time there exist different implicit sources of information , coming from the relations between instances and labels , which are usually dismissed . <eos> in this paper , we propose a semi-supervised framework to model this kind of problems . <eos> each training sample is a bag containing multi-instances , associated with a set of candidate labeling vectors . <eos> each labeling vector encodes the possible labels for the instances in the bag , with only one being fully correct . <eos> the use of the labeling vectors provides a principled way not to exclude any information . <eos> we propose a large margin discriminative formulation , and an efficient algorithm to solve it . <eos> experiments conducted on artificial datasets and a real-world images and captions dataset show that our approach achieves performance comparable to svm trained with the ground-truth labels , and outperforms other baselines .
steinwart was the ? rst to prove universal consistency of support vector machine classi ? cation . <eos> his proof analyzed the standardsupport vector machine classi ? er , which is restricted to binary classi ? cation problems . <eos> in contrast , recent analysis has resulted in the common belief that several extensions of svm classi ? cation to more than two classes are inconsistent . <eos> countering this belief , we proof the universal consistency of the multi-class support vector machine by crammer and singer . <eos> our proof extends steinwart ? s techniques to the multi-class case .
we propose a new supervised learning framework for visual object counting tasks , such as estimating the number of cells in a microscopic image or the number of humans in surveillance video frames . <eos> we focus on the practically-attractive case when the training images are annotated with dots ( one dot per object ) . <eos> our goal is to accurately estimate the count . <eos> however , we evade the hard task of learning to detect and localize individual object instances . <eos> instead , we cast the problem as that of estimating an image density whose integral over any image region gives the count of objects within that region . <eos> learning to infer such density can be formulated as a minimization of a regularized risk quadratic cost function . <eos> we introduce a new loss function , which is well-suited for such learning , and at the same time can be computed efficiently via a maximum subarray algorithm . <eos> the learning can then be posed as a convex quadratic program solvable with cutting-plane optimization . <eos> the proposed framework is very flexible as it can accept any domain-specific visual features . <eos> once trained , our system provides accurate object counts and requires a very small time overhead over the feature extraction step , making it a good candidate for applications involving real-time processing or dealing with huge amount of visual data .
when working with network datasets , the theoretical framework of detection theory for euclidean vector spaces no longer applies . <eos> nevertheless , it is desirable to determine the detectability of small , anomalous graphs embedded into background networks with known statistical properties . <eos> casting the problem of subgraph detection in a signal processing context , this article provides a framework and empirical results that elucidate a detection theory '' for graph-valued data . <eos> its focus is the detection of anomalies in unweighted , undirected graphs through l1 properties of the eigenvectors of the graph ? s so-called modularity matrix . <eos> this metric is observed to have relatively low variance for certain categories of randomly-generated graphs , and to reveal the presence of an anomalous subgraph with reasonable reliability when the anomaly is not well-correlated with stronger portions of the background graph . <eos> an analysis of subgraphs in real network datasets confirms the efficacy of this approach . ''
in this paper , we regard clustering as ensembles of k-ary affinity relations and clusters correspond to subsets of objects with maximal average affinity relations . <eos> the average affinity relation of a cluster is relaxed and well approximated by a constrained homogenous function . <eos> we present an efficient procedure to solve this optimization problem , and show that the underlying clusters can be robustly revealed by using priors systematically constructed from the data . <eos> our method can automatically select some points to form clusters , leaving other points un-grouped ; thus it is inherently robust to large numbers of outliers , which has seriously limited the applicability of classical methods . <eos> our method also provides a unified solution to clustering from k-ary affinity relations with k ? <eos> 2 , that is , it applies to both graph-based and hypergraph-based clustering problems . <eos> both theoretical analysis and experimental results show the superiority of our method over classical solutions to the clustering problem , especially when there exists a large number of outliers .
in the neural-network parameter space , an attractive field is likely to be induced by singularities . <eos> in such a singularity region , first-order gradient learning typically causes a long plateau with very little change in the objective function value e ( hence , a flat region ) . <eos> therefore , it may be confused with `` attractive '' local minima . <eos> our analysis shows that the hessian matrix of e tends to be indefinite in the vicinity of ( perturbed ) singular points , suggesting a promising strategy that exploits negative curvature so as to escape from the singularity plateaus . <eos> for numerical evidence , we limit the scope to small examples ( some of which are found in journal papers ) that allow us to confirm singularities and the eigenvalues of the hessian matrix , and for which computation using a descent direction of negative curvature encounters no plateau . <eos> even for those small problems , no efficient methods have been previously developed that avoided plateaus .
we present a new learning strategy for classification problems in which train and/or test data suffer from missing features . <eos> in previous work , instances are represented as vectors from some feature space and one is forced to impute missing values or to consider an instance-specific subspace . <eos> in contrast , our method considers instances as sets of ( feature , value ) pairs which naturally handle the missing value case . <eos> building onto this framework , we propose a classification strategy for sets . <eos> our proposal maps ( feature , value ) pairs into an embedding space and then non-linearly combines the set of embedded vectors . <eos> the embedding and the combination parameters are learned jointly on the final classification objective . <eos> this simple strategy allows great flexibility in encoding prior knowledge about the features in the embedding step and yields advantageous results compared to alternative solutions over several datasets .
we consider online learning in finite stochastic markovian environments where in each time step a new reward function is chosen by an oblivious adversary . <eos> the goal of the learning agent is to compete with the best stationary policy in terms of the total reward received . <eos> in each time step the agent observes the current state and the reward associated with the last transition , however , the agent does not observe the rewards associated with other state-action pairs . <eos> the agent is assumed to know the transition probabilities . <eos> the state of the art result for this setting is a no-regret algorithm . <eos> in this paper we propose a new learning algorithm and assuming that stationary policies mix uniformly fast , we show that after t time steps , the expected regret of the new algorithm is o ( t^ { 2/3 } ( ln t ) ^ { 1/3 } ) , giving the first rigorously proved convergence rate result for the problem .
identifying the features of objects becomes a challenge when those features can change in their appearance . <eos> we introduce the transformed indian buffet process ( tibp ) , and use it to define a nonparametric bayesian model that infers features that can transform across instantiations . <eos> we show that this model can identify features that are location invariant by modeling a previous experiment on human feature learning . <eos> however , allowing features to transform adds new kinds of ambiguity : are two parts of an object the same feature with different transformations or two unique features ? <eos> what transformations can features undergo ? <eos> we present two new experiments in which we explore how people resolve these questions , showing that the tibp model demonstrates a similar sensitivity to context to that shown by human learners when determining the invariant aspects of features .
activity of a neuron , even in the early sensory areas , is not simply a function of its local receptive field or tuning properties , but depends on global context of the stimulus , as well as the neural context . <eos> this suggests the activity of the surrounding neurons and global brain states can exert considerable influence on the activity of a neuron . <eos> in this paper we implemented an l1 regularized point process model to assess the contribution of multiple factors to the firing rate of many individual units recorded simultaneously from v1 with a 96-electrode utah '' array . <eos> we found that the spikes of surrounding neurons indeed provide strong predictions of a neuron 's response , in addition to the neuron 's receptive field transfer function . <eos> we also found that the same spikes could be accounted for with the local field potentials , a surrogate measure of global network states . <eos> this work shows that accounting for network fluctuations can improve estimates of single trial firing rate and stimulus-response transfer functions . ''
we study several classes of interactive assistants from the points of view of decision theory and computational complexity . <eos> we first introduce a class of pomdps called hidden-goal mdps ( hgmdps ) , which formalize the problem of interactively assisting an agent whose goal is hidden and whose actions are observable . <eos> in spite of its restricted nature , we show that optimal action selection in finite horizon hgmdps is pspace-complete even in domains with deterministic dynamics . <eos> we then introduce a more restricted model called helper action mdps ( hamdps ) , where the assistant 's action is accepted by the agent when it is helpful , and can be easily ignored by the agent otherwise . <eos> we show classes of hamdps that are complete for pspace and np along with a polynomial time class . <eos> furthermore , we show that for general hamdps a simple myopic policy achieves a regret , compared to an omniscient assistant , that is bounded by the entropy of the initial goal distribution . <eos> a variation of this policy is shown to achieve worst-case regret that is logarithmic in the number of goals for any goal distribution .
experts ( human or computer ) are often required to assess the probability of uncertain events . <eos> when a collection of experts independently assess events that are structurally interrelated , the resulting assessment may violate fundamental laws of probability . <eos> such an assessment is termed incoherent . <eos> in this work we investigate how the problem of incoherence may be affected by allowing experts to specify likelihood models and then update their assessments based on the realization of a globally-observable random sequence .
energy disaggregation is the task of taking a whole-home energy signal and separating it into its component appliances . <eos> studies have shown that having device-level energy information can cause users to conserve significant amounts of energy , but current electricity meters only report whole-home data . <eos> thus , developing algorithmic methods for disaggregation presents a key technical challenge in the effort to maximize energy conservation . <eos> in this paper , we examine a large scale energy disaggregation task , and apply a novel extension of sparse coding to this problem . <eos> in particular , we develop a method , based upon structured prediction , for discriminatively training sparse coding algorithms specifically to maximize disaggregation performance . <eos> we show that this significantly improves the performance of sparse coding algorithms on the energy task and illustrate how these disaggregation results can provide useful information about energy usage .
we consider linear models for stochastic dynamics . <eos> any such model can be associated a network ( namely a directed graph ) describing which degrees of freedom interact under the dynamics . <eos> we tackle the problem of learning such a network from observation of the system trajectory over a time interval t. we analyse the l1-regularized least squares algorithm and , in the setting in which the underlying network is sparse , we prove performance guarantees that are uniform in the sampling rate as long as this is sufficiently high . <eos> this result substantiates the notion of a well defined ? time complexity ? <eos> for the network inference problem .
automatic speech recognition has gradually improved over the years , but the reliable recognition of unconstrained speech is still not within reach . <eos> in order to achieve a breakthrough , many research groups are now investigating new methodologies that have potential to outperform the hidden markov model technology that is at the core of all present commercial systems . <eos> in this paper , it is shown that the recently introduced concept of reservoir computing might form the basis of such a methodology . <eos> in a limited amount of time , a reservoir system that can recognize the elementary sounds of continuous speech has been built . <eos> the system already achieves a state-of-the-art performance , and there is evidence that the margin for further improvements is still significant .
functional magnetic resonance imaging ( fmri ) can be applied to study the functional connectivity of the neural elements which form complex network at a whole brain level . <eos> most analyses of functional resting state networks ( rsn ) have been based on the analysis of correlation between the temporal dynamics of various regions of the brain . <eos> while these models can identify coherently behaving groups in terms of correlation they give little insight into how these groups interact . <eos> in this paper we take a different view on the analysis of functional resting state networks . <eos> starting from the definition of resting state as functional coherent groups we search for functional units of the brain that communicate with other parts of the brain in a coherent manner as measured by mutual information . <eos> we use the infinite relational model ( irm ) to quantify functional coherent groups of resting state networks and demonstrate how the extracted component interactions can be used to discriminate between functional resting state activity in multiple sclerosis and normal subjects .
figure/ground assignment , in which the visual image is divided into nearer ( figural ) and farther ( ground ) surfaces , is an essential step in visual processing , but its underlying computational mechanisms are poorly understood . <eos> figural assignment ( often referred to as border ownership ) can vary along a contour , suggesting a spatially distributed process whereby local and global cues are combined to yield local estimates of border ownership . <eos> in this paper we model figure/ground estimation in a bayesian belief network , attempting to capture the propagation of border ownership across the image as local cues ( contour curvature and t-junctions ) interact with more global cues to yield a figure/ground assignment . <eos> our network includes as a nonlocal factor skeletal ( medial axis ) structure , under the hypothesis that medial structure `` draws '' border ownership so that borders are owned by their interiors . <eos> we also briefly present a psychophysical experiment in which we measured local border ownership along a contour at various distances from an inducing cue ( a t-junction ) . <eos> both the human subjects and the network show similar patterns of performance , converging rapidly to a similar pattern of spatial variation in border ownership along contours .
matching functional brain regions across individuals is a challenging task , largely due to the variability in their location and extent . <eos> it is particularly difficult , but highly relevant , for patients with pathologies such as brain tumors , which can cause substantial reorganization of functional systems . <eos> in such cases spatial registration based on anatomical data is only of limited value if the goal is to establish correspondences of functional areas among different individuals , or to localize potentially displaced active regions . <eos> rather than rely on spatial alignment , we propose to perform registration in an alternative space whose geometry is governed by the functional interaction patterns in the brain . <eos> we first embed each brain into a functional map that reflects connectivity patterns during a fmri experiment . <eos> the resulting functional maps are then registered , and the obtained correspondences are propagated back to the two brains . <eos> in application to a language fmri experiment , our preliminary results suggest that the proposed method yields improved functional correspondences across subjects . <eos> this advantage is pronounced for subjects with tumors that affect the language areas and thus cause spatial reorganization of the functional regions .
score matching is a recently-proposed criterion for training high-dimensional density models for which maximum likelihood training is intractable . <eos> it has been applied to learning natural image statistics but has so-far been limited to simple models due to the difficulty of differentiating the loss with respect to the model parameters . <eos> we show how this differentiation can be automated with an extended version of the double-backpropagation algorithm . <eos> in addition , we introduce a regularization term for the score matching loss that enables its use for a broader range of problem by suppressing instabilities that occur with finite training sample sizes and quantized input values . <eos> results are reported for image denoising and super-resolution .
deep networks can potentially express a learning problem more efficiently than local learning machines . <eos> while deep networks outperform local learning machines on some problems , it is still unclear how their nice representation emerges from their complex structure . <eos> we present an analysis based on gaussian kernels that measures how the representation of the learning problem evolves layer after layer as the deep network builds higher-level abstract representations of the input . <eos> we use this analysis to show empirically that deep networks build progressively better representations of the learning problem and that the best representations are obtained when the deep network discriminates only in the last layers .
in this paper we consider the problem of learning from data the support of a probability distribution when the distribution { \em does not } have a density ( with respect to some reference measure ) . <eos> we propose a new class of regularized spectral estimators based on a new notion of reproducing kernel hilbert space , which we call { \em `` completely regular '' } . <eos> completely regular kernels allow to capture the relevant geometric and topological properties of an arbitrary probability space . <eos> in particular , they are the key ingredient to prove the universal consistency of the spectral estimators and in this respect they are the analogue of universal kernels for supervised problems . <eos> numerical experiments show that spectral estimators compare favorably to state of the art machine learning algorithms for density support estimation .
bayesian methods of matrix factorization ( mf ) have been actively explored recently as promising alternatives to classical singular value decomposition . <eos> in this paper , we show that , despite the fact that the optimization problem is non-convex , the global optimal solution of variational bayesian ( vb ) mf can be computed analytically by solving a quartic equation . <eos> this is highly advantageous over a popular vbmf algorithm based on iterated conditional modes since it can only find a local optimal solution after iterations . <eos> we further show that the global optimal solution of empirical vbmf ( hyperparameters are also learned from data ) can also be analytically computed . <eos> we illustrate the usefulness of our results through experiments .
most current image categorization methods require large collections of manually annotated training examples to learn accurate visual recognition models . <eos> the time-consuming human labeling effort effectively limits these approaches to recognition problems involving a small number of different object classes . <eos> in order to address this shortcoming , in recent years several authors have proposed to learn object classifiers from weakly-labeled internet images , such as photos retrieved by keyword-based image search engines . <eos> while this strategy eliminates the need for human supervision , the recognition accuracies of these methods are considerably lower than those obtained with fully-supervised approaches , because of the noisy nature of the labels associated to web data . <eos> in this paper we investigate and compare methods that learn image classifiers by combining very few manually annotated examples ( e.g. , 1-10 images per class ) and a large number of weakly-labeled web photos retrieved using keyword-based image search . <eos> we cast this as a domain adaptation problem : given a few strongly-labeled examples in a target domain ( the manually annotated examples ) and many source domain examples ( the weakly-labeled web photos ) , learn classifiers yielding small generalization error on the target domain . <eos> our experiments demonstrate that , for the same number of strongly-labeled examples , our domain adaptation approach produces significant recognition rate improvements over the best published results ( e.g. , 65 % better when using 5 labeled training examples per class ) and that our classifiers are one order of magnitude faster to learn and to evaluate than the best competing method , despite our use of large weakly-labeled data sets .
divisive normalization ( dn ) has been advocated as an effective nonlinear { \em efficient coding } transform for natural sensory signals with applications in biology and engineering . <eos> in this work , we aim to establish a connection between the dn transform and the statistical properties of natural sensory signals . <eos> our analysis is based on the use of multivariate { \em t } model to capture some important statistical properties of natural sensory signals . <eos> the multivariate { \em t } model justifies dn as an approximation to the transform that completely eliminates its statistical dependency . <eos> furthermore , using the multivariate { \em t } model and measuring statistical dependency with multi-information , we can precisely quantify the statistical dependency that is reduced by the dn transform . <eos> we compare this with the actual performance of the dn transform in reducing statistical dependencies of natural sensory signals . <eos> our theoretical analysis and quantitative evaluations confirm dn as an effective efficient coding transform for natural sensory signals . <eos> on the other hand , we also observe a previously unreported phenomenon that dn may increase statistical dependencies when the size of pooling is small .
many studies have explored the impact of response variability on the quality of sensory codes . <eos> the source of this variability is almost always assumed to be intrinsic to the brain . <eos> however , when inferring a particular stimulus property , variability associated with other stimulus attributes also effectively act as noise . <eos> here we study the impact of such stimulus-induced response variability for the case of binocular disparity inference . <eos> we characterize the response distribution for the binocular energy model in response to random dot stereograms and find it to be very different from the poisson-like noise usually assumed . <eos> we then compute the fisher information with respect to binocular disparity , present in the monocular inputs to the standard model of early binocular processing , and thereby obtain an upper bound on how much information a model could theoretically extract from them . <eos> then we analyze the information loss incurred by the different ways of combining those inputs to produce a scalar single-neuron response . <eos> we find that in the case of depth inference , monocular stimulus variability places a greater limit on the extractable information than intrinsic neuronal noise for typical spike counts . <eos> furthermore , the largest loss of information is incurred by the standard model for position disparity neurons ( tuned-excitatory ) , that are the most ubiquitous in monkey primary visual cortex , while more information from the inputs is preserved in phase-disparity neurons ( tuned-near or tuned-far ) primarily found in higher cortical regions .
lifted inference algorithms for representations that combine first-order logic and probabilistic graphical models have been the focus of much recent research . <eos> all lifted algorithms developed to date are based on the same underlying idea : take a standard probabilistic inference algorithm ( e.g. , variable elimination , belief propagation etc . ) <eos> and improve its efficiency by exploiting repeated structure in the first-order model . <eos> in this paper , we propose an approach from the other side in that we use techniques from logic for probabilistic inference . <eos> in particular , we define a set of rules that look only at the logical representation to identify models for which exact efficient inference is possible . <eos> we show that our rules yield several new tractable classes that can not be solved efficiently by any of the existing techniques .
for a density f on r^d , a high-density cluster is any connected component of { x : f ( x ) > = c } , for some c > 0 . <eos> the set of all high-density clusters form a hierarchy called the cluster tree of f. we present a procedure for estimating the cluster tree given samples from f. we give finite-sample convergence rates for our algorithm , as well as lower bounds on the sample complexity of this estimation problem .
in discriminative machine learning one is interested in training a system to optimize a certain desired measure of performance , or loss . <eos> in binary classification one typically tries to minimizes the error rate . <eos> but in structured prediction each task often has its own measure of performance such as the bleu score in machine translation or the intersection-over-union score in pascal segmentation . <eos> the most common approaches to structured prediction , structural svms and crfs , do not minimize the task loss : the former minimizes a surrogate loss with no guarantees for task loss and the latter minimizes log loss independent of task loss . <eos> the main contribution of this paper is a theorem stating that a certain perceptron-like learning rule , involving features vectors derived from loss-adjusted inference , directly corresponds to the gradient of task loss . <eos> we give empirical results on phonetic alignment of a standard test set from the timit corpus , which surpasses all previously reported results on this problem .
in system identification both the input and the output of a system are available to an observer and an algorithm is sought to identify parameters of a hypothesized model of that system . <eos> here we present a novel formal methodology for identifying dendritic processing in a neural circuit consisting of a linear dendritic processing filter in cascade with a spiking neuron model . <eos> the input to the circuit is an analog signal that belongs to the space of bandlimited functions . <eos> the output is a time sequence associated with the spike train . <eos> we derive an algorithm for identification of the dendritic processing filter and reconstruct its kernel with arbitrary precision .
we propose a new probabilistic model for analyzing dynamic evolutions of relational data , such as additions , deletions and split & merge , of relation clusters like communities in social networks . <eos> our proposed model abstracts observed time-varying object-object relationships into relationships between object clusters . <eos> we extend the infinite hidden markov model to follow dynamic and time-sensitive changes in the structure of the relational data and to estimate a number of clusters simultaneously . <eos> we show the usefulness of the model through experiments with synthetic and real-world data sets .
we present simple and computationally efficient nonparametric estimators of r\'enyi entropy and mutual information based on an i.i.d . <eos> sample drawn from an unknown , absolutely continuous distribution over $ \r^d $ . <eos> the estimators are calculated as the sum of $ p $ -th powers of the euclidean lengths of the edges of the ` generalized nearest-neighbor ' graph of the sample and the empirical copula of the sample respectively . <eos> for the first time , we prove the almost sure consistency of these estimators and upper bounds on their rates of convergence , the latter of which under the assumption that the density underlying the sample is lipschitz continuous . <eos> experiments demonstrate their usefulness in independent subspace analysis .
we tackle the fundamental problem of bayesian active learning with noise , where we need to adaptively select from a number of expensive tests in order to identify an unknown hypothesis sampled from a known prior distribution . <eos> in the case of noise-free observations , a greedy algorithm called generalized binary search ( gbs ) is known to perform near-optimally . <eos> we show that if the observations are noisy , perhaps surprisingly , gbs can perform very poorly . <eos> we develop ec2 , a novel , greedy active learning algorithm and prove that it is competitive with the optimal policy , thus obtaining the first competitiveness guarantees for bayesian active learning with noisy observations . <eos> our bounds rely on a recently discovered diminishing returns property called adaptive submodularity , generalizing the classical notion of submodular set functions to adaptive policies . <eos> our results hold even if the tests have non ? uniform cost and their noise is correlated . <eos> we also propose effecxtive , a particularly fast approximation of ec2 , and evaluate it on a bayesian experimental design problem involving human subjects , intended to tease apart competing economic theories of how people make decisions under uncertainty .
distributing labeling tasks among hundreds or thousands of annotators is an increasingly important method for annotating large datasets . <eos> we present a method for estimating the underlying value ( e.g . the class ) of each image from ( noisy ) annotations provided by multiple annotators . <eos> our method is based on a model of the image formation and annotation process . <eos> each image has different characteristics that are represented in an abstract euclidean space . <eos> each annotator is modeled as a multidimensional entity with variables representing competence , expertise and bias . <eos> this allows the model to discover and represent groups of annotators that have different sets of skills and knowledge , as well as groups of images that differ qualitatively . <eos> we find that our model predicts ground truth labels on both synthetic and real data more accurately than state of the art methods . <eos> experiments also show that our model , starting from a set of binary labels , may discover rich information , such as different ? schools of thought ? <eos> amongst the annotators , and can group together images belonging to separate categories .
we consider the problem of identifying an activation pattern in a complex , large-scale network that is embedded in very noisy measurements . <eos> this problem is relevant to several applications , such as identifying traces of a biochemical spread by a sensor network , expression levels of genes , and anomalous activity or congestion in the internet . <eos> extracting such patterns is a challenging task specially if the network is large ( pattern is very high-dimensional ) and the noise is so excessive that it masks the activity at any single node . <eos> however , typically there are statistical dependencies in the network activation process that can be leveraged to fuse the measurements of multiple nodes and enable reliable extraction of high-dimensional noisy patterns . <eos> in this paper , we analyze an estimator based on the graph laplacian eigenbasis , and establish the limits of mean square error recovery of noisy patterns arising from a probabilistic ( gaussian or ising ) model based on an arbitrary graph structure . <eos> we consider both deterministic and probabilistic network evolution models , and our results indicate that by leveraging the network interaction structure , it is possible to consistently recover high-dimensional patterns even when the noise variance increases with network size .
how much information does a neural population convey about a stimulus ? <eos> answers to this question are known to strongly depend on the correlation of response variability in neural populations . <eos> these noise correlations , however , are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size . <eos> here , we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix . <eos> our basic assumption is that noise correlations arise due to common inputs between neurons . <eos> on average , noise correlations will therefore reflect signal correlations , which can be measured in neural populations . <eos> we suggest an explicit parametric dependency between signal and noise correlations . <eos> we show how this dependency can be used to fill the gaps '' in noise correlations matrices using an iterative application of the wishart distribution over positive definitive matrices . <eos> we apply our method to data from the primary somatosensory cortex of monkeys performing a two-alternative-forced choice task . <eos> we compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler , average schemes of noise correlations . ''
we prove rates of convergence in the statistical sense for kernel-based least squares regression using a conjugate gradient algorithm , where regularization against overfitting is obtained by early stopping . <eos> this method is directly related to kernel partial least squares , a regression method that combines supervised dimensionality reduction with least squares projection . <eos> the rates depend on two key quantities : first , on the regularity of the target regression function and second , on the effective dimensionality of the data mapped into the kernel space . <eos> lower bounds on attainable rates depending on these two quantities were established in earlier literature , and we obtain upper bounds for the considered method that match these lower bounds ( up to a log factor ) if the true regression function belongs to the reproducing kernel hilbert space . <eos> if the latter assumption is not fulfilled , we obtain similar convergence rates provided additional unlabeled data are available . <eos> the order of the learning rates in these two situations match state-of-the-art results that were recently obtained for the least squares support vector machine and for linear regularization operators .
cardiovascular disease is the leading cause of death globally , resulting in 17 million deaths each year . <eos> despite the availability of various treatment options , existing techniques based upon conventional medical knowledge often fail to identify patients who might have benefited from more aggressive therapy . <eos> in this paper , we describe and evaluate a novel unsupervised machine learning approach for cardiac risk stratification . <eos> the key idea of our approach is to avoid specialized medical knowledge , and assess patient risk using symbolic mismatch , a new metric to assess similarity in long-term time-series activity . <eos> we hypothesize that high risk patients can be identified using symbolic mismatch , as individuals in a population with unusual long-term physiological activity . <eos> we describe related approaches that build on these ideas to provide improved medical decision making for patients who have recently suffered coronary attacks . <eos> we first describe how to compute the symbolic mismatch between pairs of long term electrocardiographic ( ecg ) signals . <eos> this algorithm maps the original signals into a symbolic domain , and provides a quantitative assessment of the difference between these symbolic representations of the original signals . <eos> we then show how this measure can be used with each of a one-class svm , a nearest neighbor classifier , and hierarchical clustering to improve risk stratification . <eos> we evaluated our methods on a population of 686 cardiac patients with available long-term electrocardiographic data . <eos> in a univariate analysis , all of the methods provided a statistically significant association with the occurrence of a major adverse cardiac event in the next 90 days . <eos> in a multivariate analysis that incorporated the most widely used clinical risk variables , the nearest neighbor and hierarchical clustering approaches were able to statistically significantly distinguish patients with a roughly two-fold risk of suffering a major adverse cardiac event in the next 90 days .
a new algorithm for isotonic regression is presented based on recursively partitioning the solution space . <eos> we develop efficient methods for each partitioning subproblem through an equivalent representation as a network flow problem , and prove that this sequence of partitions converges to the global solution . <eos> these network flow problems can further be decomposed in order to solve very large problems . <eos> success of isotonic regression in prediction and our algorithm 's favorable computational properties are demonstrated through simulated examples as large as 2x10^5 variables and 10^7 constraints .
spontaneous brain activity , as observed in functional neuroimaging , has been shown to display reproducible structure that expresses brain architecture and carries markers of brain pathologies . <eos> an important view of modern neuroscience is that such large-scale structure of coherent activity reflects modularity properties of brain connectivity graphs . <eos> however , to date , there has been no demonstration that the limited and noisy data available in spontaneous activity observations could be used to learn full-brain probabilistic models that generalize to new data . <eos> learning such models entails two main challenges : i ) modeling full brain connectivity is a difficult estimation problem that faces the curse of dimensionality and ii ) variability between subjects , coupled with the variability of functional signals between experimental runs , makes the use of multiple datasets challenging . <eos> we describe subject-level brain functional connectivity structure as a multivariate gaussian process and introduce a new strategy to estimate it from group data , by imposing a common structure on the graphical model in the population . <eos> we show that individual models learned from functional magnetic resonance imaging ( fmri ) data using this population prior generalize better to unseen data than models based on alternative regularization schemes . <eos> to our knowledge , this is the first report of a cross-validated model of spontaneous brain activity . <eos> finally , we use the estimated graphical model to explore the large-scale characteristics of functional architecture and show for the first time that known cognitive networks appear as the integrated communities of functional connectivity graph .
we present original empirical bernstein inequalities for u-statistics with bounded symmetric kernels q . <eos> they are expressed with respect to empirical estimates of either the variance of q or the conditional variance that appears in the bernstein-type inequality for u-statistics derived by arcones [ 2 ] . <eos> our result subsumes other existing empirical bernstein inequalities , as it reduces to them when u-statistics of order 1 are considered . <eos> in addition , it is based on a rather direct argument using two applications of the same ( non-empirical ) bernstein inequality for u-statistics . <eos> we discuss potential applications of our new inequalities , especially in the realm of learning ranking/scoring functions . <eos> in the process , we exhibit an efficient procedure to compute the variance estimates for the special case of bipartite ranking that rests on a sorting argument . <eos> we also argue that our results may provide test set bounds and particularly interesting empirical racing algorithms for the problem of online learning of scoring functions .
we define a copula process which describes the dependencies between arbitrarily many random variables independently of their marginal distributions . <eos> as an example , we develop a stochastic volatility model , gaussian copula process volatility ( gcpv ) , to predict the latent standard deviations of a sequence of random variables . <eos> to make predictions we use bayesian inference , with the laplace approximation , and with markov chain monte carlo as an alternative . <eos> we find our model can outperform garch on simulated and financial data . <eos> and unlike garch , gcpv can easily handle missing data , incorporate covariates other than time , and model a rich class of covariance structures .
bayesian optimization methods are often used to optimize unknown functions that are costly to evaluate . <eos> typically , these methods sequentially select inputs to be evaluated one at a time based on a posterior over the unknown function that is updated after each evaluation . <eos> there are a number of effective sequential policies for selecting the individual inputs . <eos> in many applications , however , it is desirable to perform multiple evaluations in parallel , which requires selecting batches of multiple inputs to evaluate at once . <eos> in this paper , we propose a novel approach to batch bayesian optimization , providing a policy for selecting batches of inputs with the goal of optimizing the function as efficiently as possible . <eos> the key idea is to exploit the availability of high-quality and efficient sequential policies , by using monte-carlo simulation to select input batches that closely match their expected behavior . <eos> to the best of our knowledge , this is the first batch selection policy for bayesian optimization . <eos> our experimental results on six benchmarks show that the proposed approach significantly outperforms two baselines and can lead to large advantages over a top sequential approach in terms of performance per unit time .
when learning models that are represented in matrix forms , enforcing a low-rank constraint can dramatically improve the memory and run time complexity , while providing a natural regularization of the model . <eos> however , naive approaches for minimizing functions over the set of low-rank matrices are either prohibitively time consuming ( repeated singular value decomposition of the matrix ) or numerically unstable ( optimizing a factored representation of the low rank matrix ) . <eos> we build on recent advances in optimization over manifolds , and describe an iterative online learning procedure , consisting of a gradient step , followed by a second-order retraction back to the manifold . <eos> while the ideal retraction is hard to compute , and so is the projection operator that approximates it , we describe another second-order retraction that can be computed efficiently , with run time and memory complexity of o ( ( n+m ) k ) for a rank-k matrix of dimension m x n , given rank one gradients . <eos> we use this algorithm , loreta , to learn a matrix-form similarity measure over pairs of documents represented as high dimensional vectors . <eos> loreta improves the mean average precision over a passive- aggressive approach in a factorized model , and also improves over a full model trained over pre-selected features using the same memory requirements . <eos> loreta also showed consistent improvement over standard methods in a large ( 1600 classes ) multi-label image classification task .
languages vary widely in many ways , including their canonical word order . <eos> a basic aspect of the observed variation is the fact that some word orders are much more common than others . <eos> although this regularity has been recognized for some time , it has not been well-explained . <eos> in this paper we offer an information-theoretic explanation for the observed word-order distribution across languages , based on the concept of uniform information density ( uid ) . <eos> we suggest that object-first languages are particularly disfavored because they are highly non-optimal if the goal is to distribute information content approximately evenly throughout a sentence , and that the rest of the observed word-order distribution is at least partially explainable in terms of uid . <eos> we support our theoretical analysis with data from child-directed speech and experimental work .
recent work in reinforcement learning has emphasized the power of l1 regularization to perform feature selection and prevent overfitting . <eos> we propose formulating the l1 regularized linear fixed point problem as a linear complementarity problem ( lcp ) . <eos> this formulation offers several advantages over the lars-inspired formulation , lars-td . <eos> the lcp formulation allows the use of efficient off-the-shelf solvers , leads to a new uniqueness result , and can be initialized with starting points from similar problems ( warm starts ) . <eos> we demonstrate that warm starts , as well as the efficiency of lcp solvers , can speed up policy iteration . <eos> moreover , warm starts permit a form of modified policy iteration that can be used to approximate a greedy '' homotopy path , a generalization of the lars-td homotopy path that combines policy evaluation and optimization . ''
gaussian graphical models with sparsity in the inverse covariance matrix are of significant interest in many modern applications . <eos> for the problem of recovering the graphical structure , information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso , which is a likelihood penalization technique . <eos> in this paper we establish the asymptotic consistency of an extended bayesian information criterion for gaussian graphical models in a scenario where both the number of variables p and the sample size n grow . <eos> compared to earlier work on the regression case , our treatment allows for growth in the number of non-zero parameters in the true model , which is necessary in order to cover connected graphs . <eos> we demonstrate the performance of this criterion on simulated data when used in conjuction with the graphical lasso , and verify that the criterion indeed performs better than either cross-validation or the ordinary bayesian information criterion when p and the number of non-zero parameters q both scale with n .
we consider the problem of retrieving the database points nearest to a given { \em hyperplane } query without exhaustively scanning the database . <eos> we propose two hashing-based solutions . <eos> our first approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point . <eos> our second approach embeds the data into a vector space where the euclidean norm reflects the desired distance between the original points and hyperplane query . <eos> both use hashing to retrieve near points in sub-linear time . <eos> our first method 's preprocessing stage is more efficient , while the second has stronger accuracy guarantees . <eos> we apply both to pool-based active learning : taking the current hyperplane classifier as a query , our algorithm identifies those points ( approximately ) satisfying the well-known minimal distance-to-hyperplane selection criterion . <eos> we empirically demonstrate our methods ' tradeoffs , and show that they make it practical to perform active selection with millions of unlabeled points .
we describe a model based on a boltzmann machine with third-order connections that can learn how to accumulate information about a shape over several fixations . <eos> the model uses a retina that only has enough high resolution pixels to cover a small area of the image , so it must decide on a sequence of fixations and it must combine the glimpse '' at each fixation with the location of the fixation before integrating the information with information from other glimpses of the same object . <eos> we evaluate this model on a synthetic dataset and two image classification datasets , showing that it can perform at least as well as a model trained on whole images . ''
the reinforcement learning community has explored many approaches to obtain- ing value estimates and models to guide decision making ; these approaches , how- ever , do not usually provide a measure of confidence in the estimate . <eos> accurate estimates of an agent ? s confidence are useful for many applications , such as bi- asing exploration and automatically adjusting parameters to reduce dependence on parameter-tuning . <eos> computing confidence intervals on reinforcement learning value estimates , however , is challenging because data generated by the agent- environment interaction rarely satisfies traditional assumptions . <eos> samples of value- estimates are dependent , likely non-normally distributed and often limited , partic- ularly in early learning when confidence estimates are pivotal . <eos> in this work , we investigate how to compute robust confidences for value estimates in continuous markov decision processes . <eos> we illustrate how to use bootstrapping to compute confidence intervals online under a changing policy ( previously not possible ) and prove validity under a few reasonable assumptions . <eos> we demonstrate the applica- bility of our confidence estimation algorithms with experiments on exploration , parameter estimation and tracking .
while clinicians can accurately identify different types of heartbeats in electrocardiograms ( ecgs ) from different patients , researchers have had limited success in applying supervised machine learning to the same task . <eos> the problem is made challenging by the variety of tasks , inter- and intra-patient differences , an often severe class imbalance , and the high cost of getting cardiologists to label data for individual patients . <eos> we address these difficulties using active learning to perform patient-adaptive and task-adaptive heartbeat classification . <eos> when tested on a benchmark database of cardiologist annotated ecg recordings , our method had considerably better performance than other recently proposed methods on the two primary classification tasks recommended by the association for the advancement of medical instrumentation . <eos> additionally , our method required over 90 % less patient-specific training data than the methods to which we compared it .
in order to study the properties of total visual input in humans , a single subject wore a camera for two weeks capturing , on average , an image every 20 seconds ( www.research.microsoft.com/~jojic/aihs ) . <eos> the resulting new dataset contains a mix of indoor and outdoor scenes as well as numerous foreground objects . <eos> our first analysis goal is to create a visual summary of the subject ? s two weeks of life using unsupervised algorithms that would automatically discover recurrent scenes , familiar faces or common actions . <eos> direct application of existing algorithms , such as panoramic stitching ( e.g . photosynth ) or appearance-based clustering models ( e.g . the epitome ) , is impractical due to either the large dataset size or the dramatic variation in the lighting conditions . <eos> as a remedy to these problems , we introduce a novel image representation , the ? stel epitome , ? <eos> and an associated efficient learning algorithm . <eos> in our model , each image or image patch is characterized by a hidden mapping t , which , as in previous epitome models , defines a mapping between the image-coordinates and the coordinates in the large all-i-have-seen '' epitome matrix . <eos> the limited epitome real-estate forces the mappings of different images to overlap , with this overlap indicating image similarity . <eos> however , in our model the image similarity does not depend on direct pixel-to-pixel intensity/color/feature comparisons as in previous epitome models , but on spatial configuration of scene or object parts , as the model is based on the palette-invariant stel models . <eos> as a result , stel epitomes capture structure that is invariant to non-structural changes , such as illumination , that tend to uniformly affect pixels belonging to a single scene or object part . ''
a new algorithm is proposed for a ) unsupervised learning of sparse representations from subsampled measurements and b ) estimating the parameters required for linearly reconstructing signals from the sparse codes . <eos> we verify that the new algorithm performs efficient data compression on par with the recent method of compressive sampling . <eos> further , we demonstrate that the algorithm performs robustly when stacked in several stages or when applied in undercomplete or overcomplete situations . <eos> the new algorithm can explain how neural populations in the brain that receive subsampled input through fiber bottlenecks are able to form coherent response properties .
we extend latent dirichlet allocation ( lda ) by explicitly allowing for the encoding of side information in the distribution over words . <eos> this results in a variety of new capabilities , such as improved estimates for infrequently occurring words , as well as the ability to leverage thesauri and dictionaries in order to boost topic cohesion within and across languages . <eos> we present experiments on multi-language topic synchronisation where dictionary information is used to bias corresponding words towards similar topics . <eos> results indicate that our model substantially improves topic cohesion when compared to the standard lda model .
in this paper , we propose a matrix-variate normal penalty with sparse inverse covariances to couple multiple tasks . <eos> learning multiple ( parametric ) models can be viewed as estimating a matrix of parameters , where rows and columns of the matrix correspond to tasks and features , respectively . <eos> following the matrix-variate normal density , we design a penalty that decomposes the full covariance of matrix elements into the kronecker product of row covariance and column covariance , which characterizes both task relatedness and feature representation . <eos> several recently proposed methods are variants of the special cases of this formulation . <eos> to address the overfitting issue and select meaningful task and feature structures , we include sparse covariance selection into our matrix-normal regularization via l-1 penalties on task and feature inverse covariances . <eos> we empirically study the proposed method and compare with related models in two real-world problems : detecting landmines in multiple fields and recognizing faces between different subjects . <eos> experimental results show that the proposed framework provides an effective and flexible way to model various different structures of multiple tasks .
we consider the problem of learning a coefficient vector x0 from noisy linear observation y=ax0+w . <eos> in many contexts ( ranging from model selection to image processing ) it is desirable to construct a sparse estimator . <eos> in this case , a popular approach consists in solving an l1-penalized least squares problem known as the lasso or bpdn . <eos> for sequences of matrices a of increasing dimensions , with iid gaussian entries , we prove that the normalized risk of the lasso converges to a limit , and we obtain an explicit expression for this limit . <eos> our result is the first rigorous derivation of an explicit formula for the asymptotic risk of the lasso for random instances . <eos> the proof technique is based on the analysis of amp , a recently developed efficient algorithm , that is inspired from graphical models ideas . <eos> through simulations on real data matrices ( gene expression data and hospital medical records ) we observe that these results can be relevant in a broad array of practical applications .
the charles bonnet syndrome ( cbs ) is characterized by complex vivid visual hallucinations in people with , primarily , eye diseases and no other neurological pathology . <eos> we present a deep boltzmann machine model of cbs , exploring two core hypotheses : first , that the visual cortex learns a generative or predictive model of sensory input , thus explaining its capability to generate internal imagery . <eos> and second , that homeostatic mechanisms stabilize neuronal activity levels , leading to hallucinations being formed when input is lacking . <eos> we reproduce a variety of qualitative findings in cbs . <eos> we also introduce a modification to the dbm that allows us to model a possible role of acetylcholine in cbs as mediating the balance of feed-forward and feed-back processing . <eos> our model might provide new insights into cbs and also demonstrates that generative frameworks are promising as hypothetical models of cortical learning and perception .
we study convex stochastic optimization problems where a noisy objective function value is observed after a decision is made . <eos> there are many stochastic optimization problems whose behavior depends on an exogenous state variable which affects the shape of the objective function . <eos> currently , there is no general purpose algorithm to solve this class of problems . <eos> we use nonparametric density estimation for the joint distribution of state-outcome pairs to create weights for previous observations . <eos> the weights effectively group similar states . <eos> those similar to the current state are used to create a convex , deterministic approximation of the objective function . <eos> we propose two solution methods that depend on the problem characteristics : function-based and gradient-based optimization . <eos> we offer two weighting schemes , kernel based weights and dirichlet process based weights , for use with the solution methods . <eos> the weights and solution methods are tested on a synthetic multi-product newsvendor problem and the hour ahead wind commitment problem . <eos> our results show dirichlet process weights can offer substantial benefits over kernel based weights and , more generally , that nonparametric estimation methods provide good solutions to otherwise intractable problems .
gaussian graphical models are of great interest in statistical learning . <eos> because the conditional independencies between different nodes correspond to zero entries in the inverse covariance matrix of the gaussian distribution , one can learn the structure of the graph by estimating a sparse inverse covariance matrix from sample data , by solving a convex maximum likelihood problem with an $ \ell_1 $ -regularization term . <eos> in this paper , we propose a first-order method based on an alternating linearization technique that exploits the problem 's special structure ; in particular , the subproblems solved in each iteration have closed-form solutions . <eos> moreover , our algorithm obtains an $ \epsilon $ -optimal solution in $ o ( 1/\epsilon ) $ iterations . <eos> numerical experiments on both synthetic and real data from gene association networks show that a practical version of this algorithm outperforms other competitive algorithms .
the international monitoring system ( ims ) is a global network of sensors whose purpose is to identify potential violations of the comprehensive nuclear-test-ban treaty ( ctbt ) , primarily through detection and localization of seismic events . <eos> we report on the first stage of a project to improve on the current automated software system with a bayesian inference system that computes the most likely global event history given the record of local sensor data . <eos> the new system , visa ( vertically integrated seismological analysis ) , is based on empirically calibrated , generative models of event occurrence , signal propagation , and signal detection . <eos> visa exhibits significantly improved precision and recall compared to the current operational system and is able to detect events that are missed even by the human analysts who post-process the ims output .
clustering is a basic data mining task with a wide variety of applications . <eos> not surprisingly , there exist many clustering algorithms . <eos> however , clustering is an ill defined problem - given a data set , it is not clear what a correctclustering for that set is . <eos> indeed , different algorithms may yield dramatically different outputs for the same input sets . <eos> faced with a concrete clustering task , a user needs to choose an appropriate clustering algorithm . <eos> currently , such decisions are often made in a very ad hoc , if not completely random , manner . <eos> given the crucial effect of the choice of a clustering algorithm on the resulting clustering , this state of affairs is truly regrettable . <eos> in this paper we address the major research challenge of developing tools for helping users make more informed decisions when they come to pick a clustering tool for their data . <eos> this is , of course , a very ambitious endeavor , and in this paper , we make some first steps towards this goal . <eos> we propose to address this problem by distilling abstract properties of the input-output behavior of different clustering paradigms . <eos> in this paper , we demonstrate how abstract , intuitive properties of clustering functions can be used to taxonomize a set of popular clustering algorithmic paradigms . <eos> on top of addressing deterministic clustering algorithms , we also propose similar properties for randomized algorithms and use them to highlight functional differences between different common implementations of k-means clustering . <eos> we also study relationships between the properties , independent of any particular algorithm . <eos> in particular , we strengthen kleinbergs famous impossibility result , while providing a simpler proof .
we show that matrix completion with trace-norm regularization can be significantly hurt when entries of the matrix are sampled non-uniformly , but that a properly weighted version of the trace-norm regularizer works well with non-uniform sampling . <eos> we show that the weighted trace-norm regularization indeed yields significant gains on the highly non-uniformly sampled netflix dataset .
generalized binary search ( gbs ) is a well known greedy algorithm for identifying an unknown object while minimizing the number of yes '' or `` no '' questions posed about that object , and arises in problems such as active learning and active diagnosis . <eos> here , we provide a coding-theoretic interpretation for gbs and show that gbs can be viewed as a top-down algorithm that greedily minimizes the expected number of queries required to identify an object . <eos> this interpretation is then used to extend gbs in two ways . <eos> first , we consider the case where the objects are partitioned into groups , and the objective is to identify only the group to which the object belongs . <eos> then , we consider the case where the cost of identifying an object grows exponentially in the number of queries . <eos> in each case , we present an exact formula for the objective function involving shannon or renyi entropy , and develop a greedy algorithm for minimizing it . ''
metric constraints are known to be highly discriminative for many objects , but if training is limited to data captured from a particular 3-d sensor the quantity of training data may be severly limited . <eos> in this paper , we show how a crucial aspect of 3-d information ? object and feature absolute size ? can be added to models learned from commonly available online imagery , without use of any 3-d sensing or re- construction at training time . <eos> such models can be utilized at test time together with explicit 3-d sensing to perform robust search . <eos> our model uses a 2.1dlocal feature , which combines traditional appearance gradient statistics with an estimate of average absolute depth within the local window . <eos> we show how category size information can be obtained from online images by exploiting relatively unbiquitous metadata fields specifying camera intrinstics . <eos> we develop an efficient metric branch-and-bound algorithm for our search task , imposing 3-d size constraints as part of an optimal search for a set of features which indicate the presence of a category . <eos> experiments on test scenes captured with a traditional stereo rig are shown , exploiting training data from from purely monocular sources with associated exif metadata .
a standard approach to learning object category detectors is to provide strong supervision in the form of a region of interest ( roi ) specifying each instance of the object in the training images . <eos> in this work are goal is to learn from heterogeneous labels , in which some images are only weakly supervised , specifying only the presence or absence of the object or a weak indication of object location , whilst others are fully annotated . <eos> to this end we develop a discriminative learning approach and make two contributions : ( i ) we propose a structured output formulation for weakly annotated images where full annotations are treated as latent variables ; and ( ii ) we propose to optimize a ranking objective function , allowing our method to more effectively use negatively labeled images to improve detection average precision performance . <eos> the method is demonstrated on the benchmark inria pedestrian detection dataset of dalal and triggs and the pascal voc dataset , and it is shown that for a significant proportion of weakly supervised images the performance achieved is very similar to the fully supervised ( state of the art ) results .
a number of objective functions in clustering problems can be described with submodular functions . <eos> in this paper , we introduce the minimum average cost criterion , and show that the theory of intersecting submodular functions can be used for clustering with submodular objective functions . <eos> the proposed algorithm does not require the number of clusters in advance , and it will be determined by the property of a given set of data points . <eos> the minimum average cost clustering problem is parameterized with a real variable , and surprisingly , we show that all information about optimal clusterings for all parameters can be computed in polynomial time in total . <eos> additionally , we evaluate the performance of the proposed algorithm through computational experiments .
this paper proposes a simple and efficient finite difference method for implicit differentiation of marginal inference results in discrete graphical models . <eos> given an arbitrary loss function , defined on marginals , we show that the derivatives of this loss with respect to model parameters can be obtained by running the inference procedure twice , on slightly perturbed model parameters . <eos> this method can be used with approximate inference , with a loss function over approximate marginals . <eos> convenient choices of loss functions make it practical to fit graphical models with hidden variables , high treewidth and/or model misspecification .
many data are naturally modeled by an unobserved hierarchical structure . <eos> in this paper we propose a flexible nonparametric prior over unknown data hierarchies . <eos> the approach uses nested stick-breaking processes to allow for trees of unbounded width and depth , where data can live at any node and are infinitely exchangeable . <eos> one can view our model as providing infinite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree . <eos> by using a stick-breaking approach , we can apply markov chain monte carlo methods based on slice sampling to perform bayesian inference and simulate from the posterior distribution on trees . <eos> we apply our method to hierarchical clustering of images and topic modeling of text data .
many time-series such as human movement data consist of a sequence of basic actions , e.g. , forehands and backhands in tennis . <eos> automatically extracting and characterizing such actions is an important problem for a variety of different applications . <eos> in this paper , we present a probabilistic segmentation approach in which an observed time-series is modeled as a concatenation of segments corresponding to different basic actions . <eos> each segment is generated through a noisy transformation of one of a few hidden trajectories representing different types of movement , with possible time re-scaling . <eos> we analyze three different approximation methods for dealing with model intractability , and demonstrate how the proposed approach can successfully segment table tennis movements recorded using a robot arm as haptic input device .
many problems in machine learning and statistics can be formulated as ( generalized ) eigenproblems . <eos> in terms of the associated optimization problem , computing linear eigenvectors amounts to finding critical points of a quadratic function subject to quadratic constraints . <eos> in this paper we show that a certain class of constrained optimization problems with nonquadratic objective and constraints can be understood as nonlinear eigenproblems . <eos> we derive a generalization of the inverse power method which is guaranteed to converge to a nonlinear eigenvector . <eos> we apply the inverse power method to 1-spectral clustering and sparse pca which can naturally be formulated as nonlinear eigenproblems . <eos> in both applications we achieve state-of-the-art results in terms of solution quality and runtime . <eos> moving beyond the standard eigenproblem should be useful also in many other applications and our inverse power method can be easily adapted to new problems .
matrix factorization in the presence of missing data is at the core of many computer vision problems such as structure from motion ( sfm ) , non-rigid sfm and photometric stereo . <eos> we formulate the problem of matrix factorization with missing data as a low-rank semidefinite program ( lrsdp ) with the advantage that : $ 1 ) $ an efficient quasi-newton implementation of the lrsdp enables us to solve large-scale factorization problems , and $ 2 ) $ additional constraints such as ortho-normality , required in orthographic sfm , can be directly incorporated in the new formulation . <eos> our empirical evaluations suggest that , under the conditions of matrix completion theory , the proposed algorithm finds the optimal solution , and also requires fewer observations compared to the current state-of-the-art algorithms . <eos> we further demonstrate the effectiveness of the proposed algorithm in solving the affine sfm problem , non-rigid sfm and photometric stereo problems .
size , color , and orientation have long been considered elementary features whose attributes are extracted in parallel and available to guide the deployment of attention . <eos> if each is processed in the same fashion with simply a different set of local detectors , one would expect similar search behaviours on localizing an equivalent flickering change among identically laid out disks . <eos> we analyze feature transitions associated with saccadic search and find out that size , color , and orientation are not alike in dynamic attribute processing over time . <eos> the markovian feature transition is attractive for size , repulsive for color , and largely reversible for orientation .
in many real-world scenarios , it is nearly impossible to collect explicit social network data . <eos> in such cases , whole networks must be inferred from underlying observations . <eos> here , we formulate the problem of inferring latent social networks based on network diffusion or disease propagation data . <eos> we consider contagions propagating over the edges of an unobserved social network , where we only observe the times when nodes became infected , but not who infected them . <eos> given such node infection times , we then identify the optimal network that best explains the observed data . <eos> we present a maximum likelihood approach based on convex programming with a l1-like penalty term that encourages sparsity . <eos> experiments on real and synthetic data reveal that our method near-perfectly recovers the underlying network structure as well as the parameters of the contagion propagation model . <eos> moreover , our approach scales well as it can infer optimal networks on thousands of nodes in a matter of minutes .
the gaussian process ( gp ) is a popular way to specify dependencies between random variables in a probabilistic model . <eos> in the bayesian framework the covariance structure can be specified using unknown hyperparameters . <eos> integrating over these hyperparameters considers different possible explanations for the data when making predictions . <eos> this integration is often performed using markov chain monte carlo ( mcmc ) sampling . <eos> however , with non-gaussian observations standard hyperparameter sampling approaches require careful tuning and may converge slowly . <eos> in this paper we present a slice sampling approach that requires little tuning while mixing well in both strong- and weak-data regimes .
despite the ubiquity of clustering as a tool in unsupervised learning , there is not yet a consensus on a formal theory , and the vast majority of work in this direction has focused on unsupervised clustering . <eos> we study a recently proposed framework for supervised clustering where there is access to a teacher . <eos> we give an improved generic algorithm to cluster any concept class in that model . <eos> our algorithm is query-efficient in the sense that it involves only a small amount of interaction with the teacher . <eos> we also present and study two natural generalizations of the model . <eos> the model assumes that the teacher response to the algorithm is perfect . <eos> we eliminate this limitation by proposing a noisy model and give an algorithm for clustering the class of intervals in this noisy model . <eos> we also propose a dynamic model where the teacher sees a random subset of the points . <eos> finally , for datasets satisfying a spectrum of weak to strong properties , we give query bounds , and show that a class of clustering functions containing single-linkage will find the target clustering under the strongest property .
we develop a theory of online learning by defining several complexity measures . <eos> among them are analogues of rademacher complexity , covering numbers and fat-shattering dimension from statistical learning theory . <eos> relationship among these complexity measures , their connection to online learning , and tools for bounding them are provided . <eos> we apply these results to various learning problems . <eos> we provide a complete characterization of online learnability in the supervised setting .
this paper introduces the first set of pac-bayesian bounds for the batch reinforcement learning problem in finite state spaces . <eos> these bounds hold regardless of the correctness of the prior distribution . <eos> we demonstrate how such bounds can be used for model-selection in control problems where prior information is available either on the dynamics of the environment , or on the value of actions . <eos> our empirical results confirm that pac-bayesian model-selection is able to leverage prior distributions when they are informative and , unlike standard bayesian rl approaches , ignores them when they are misleading .
we tackle the problem of simultaneously detecting occlusions and estimating optical flow . <eos> we show that , under standard assumptions of lambertian reflection and static illumination , the task can be posed as a convex minimization problem . <eos> therefore , the solution , computed using efficient algorithms , is guaranteed to be globally optimal , for any number of independently moving objects , and any number of occlusion layers . <eos> we test the proposed algorithm on benchmark datasets , expanded to enable evaluation of occlusion detection performance .
there has been a recent push in extraction of 3d spatial layout of scenes . <eos> however , none of these approaches model the 3d interaction between objects and the spatial layout . <eos> in this paper , we argue for a parametric representation of objects in 3d , which allows us to incorporate volumetric constraints of the physical world . <eos> we show that augmenting current structured prediction techniques with volumetric reasoning significantly improves the performance of the state-of-the-art .
several motor related brain computer interfaces ( bcis ) have been developed over the years that use activity decoded from the contralateral hemisphere to operate devices . <eos> many recent studies have also talked about the importance of ipsilateral activity in planning of motor movements . <eos> for successful upper limb bcis , it is important to decode finger movements from brain activity . <eos> this study uses ipsilateral cortical signals from humans ( using ecog ) to decode finger movements . <eos> we demonstrate , for the first time , successful finger movement detection using machine learning algorithms . <eos> our results show high decoding accuracies in all cases which are always above chance . <eos> we also show that significant accuracies can be achieved with the use of only a fraction of all the features recorded and that these core features also make sense physiologically . <eos> the results of this study have a great potential in the emerging world of motor neuroprosthetics and other bcis .
we apply the framework of kernel dimension reduction , originally designed for supervised problems , to unsupervised dimensionality reduction . <eos> in this framework , kernel-based measures of independence are used to derive low-dimensional representations that maximally capture information in covariates in order to predict responses . <eos> we extend this idea and develop similarly motivated measures for unsupervised problems where covariates and responses are the same . <eos> our empirical studies show that the resulting compact representation yields meaningful and appealing visualization and clustering of data . <eos> furthermore , when used in conjunction with supervised learners for classification , our methods lead to lower classification errors than state-of-the-art methods , especially when embedding data in spaces of very few dimensions .
we study repeated zero-sum games against an adversary on a budget . <eos> given that an adversary has some constraint on the sequence of actions that he plays , we consider what ought to be the player 's best mixed strategy with knowledge of this budget . <eos> we show that , for a general class of normal-form games , the minimax strategy is indeed efficiently computable and relies on a random playout '' technique . <eos> we give three diverse applications of this algorithmic template : a cost-sensitive `` hedge '' setting , a particular problem in metrical task systems , and the design of combinatorial prediction markets . ''
the max-norm was proposed as a convex matrix regularizer by srebro et al ( 2004 ) and was shown to be empirically superior to the trace-norm for collaborative filtering problems . <eos> although the max-norm can be computed in polynomial time , there are currently no practical algorithms for solving large-scale optimization problems that incorporate the max-norm . <eos> the present work uses a factorization technique of burer and monteiro ( 2003 ) to devise scalable first-order algorithms for convex programs involving the max-norm . <eos> these algorithms are applied to solve huge collaborative filtering , graph cut , and clustering problems . <eos> empirically , the new methods outperform mature techniques from all three areas .
we consider the multiple linear regression problem , in a setting where some of the set of relevant features could be shared across the tasks . <eos> a lot of recent research has studied the use of $ \ell_1/\ell_q $ norm block-regularizations with $ q > 1 $ for such ( possibly ) block-structured problems , establishing strong guarantees on recovery even under high-dimensional scaling where the number of features scale with the number of observations . <eos> however , these papers also caution that the performance of such block-regularized methods are very dependent on the { \em extent } to which the features are shared across tasks . <eos> indeed they show~\citep { nwjoint } that if the extent of overlap is less than a threshold , or even if parameter { \em values } in the shared features are highly uneven , then block $ \ell_1/\ell_q $ regularization could actually perform { \em worse } than simple separate elementwise $ \ell_1 $ regularization . <eos> we are far away from a realistic multi-task setting : not only do the set of relevant features have to be exactly the same across tasks , but their values have to as well . <eos> here , we ask the question : can we leverage support and parameter overlap when it exists , but not pay a penalty when it does not ? <eos> indeed , this falls under a more general question of whether we can model such \emph { dirty data } which may not fall into a single neat structural bracket ( all block-sparse , or all low-rank and so on ) . <eos> here , we take a first step , focusing on developing a dirty model for the multiple regression problem . <eos> our method uses a very simple idea : we decompose the parameters into two components and { \em regularize these differently . } <eos> we show both theoretically and empirically , our method strictly and noticeably outperforms both $ \ell_1 $ and $ \ell_1/\ell_q $ methods , over the entire range of possible overlaps . <eos> we also provide theoretical guarantees that the method performs well under high-dimensional scaling .
hypothesis testing on point processes has several applications such as model fitting , plasticity detection , and non-stationarity detection . <eos> standard tools for hypothesis testing include tests on mean firing rate and time varying rate function . <eos> however , these statistics do not fully describe a point process and thus the tests can be misleading . <eos> in this paper , we introduce a family of non-parametric divergence measures for hypothesis testing . <eos> we extend the traditional kolmogorov -- smirnov and cramer -- von-mises tests for point process via stratification . <eos> the proposed divergence measures compare the underlying probability structure and , thus , is zero if and only if the point processes are the same . <eos> this leads to a more robust test of hypothesis . <eos> we prove consistency and show that these measures can be efficiently estimated from data . <eos> we demonstrate an application of using the proposed divergence as a cost function to find optimally matched spike trains .
to localise the source of a sound , we use location-specific properties of the signals received at the two ears caused by the asymmetric filtering of the original sound by our head and pinnae , the head-related transfer functions ( hrtfs ) . <eos> these hrtfs change throughout an organism 's lifetime , during development for example , and so the required neural circuitry can not be entirely hardwired . <eos> since hrtfs are not directly accessible from perceptual experience , they can only be inferred from filtered sounds . <eos> we present a spiking neural network model of sound localisation based on extracting location-specific synchrony patterns , and a simple supervised algorithm to learn the mapping between synchrony patterns and locations from a set of example sounds , with no previous knowledge of hrtfs . <eos> after learning , our model was able to accurately localise new sounds in both azimuth and elevation , including the difficult task of distinguishing sounds coming from the front and back .
learning from multi-view data is important in many applications , such as image classification and annotation . <eos> in this paper , we present a large-margin learning framework to discover a predictive latent subspace representation shared by multiple views . <eos> our approach is based on an undirected latent space markov network that fulfills a weak conditional independence assumption that multi-view observations and response variables are independent given a set of latent variables . <eos> we provide efficient inference and parameter estimation methods for the latent subspace model . <eos> finally , we demonstrate the advantages of large-margin learning on real video and web image data for discovering predictive latent representations and improving the performance on image classification , annotation and retrieval .
to cope with concept drift , we placed a probability distribution over the location of the most-recent drift point . <eos> we used bayesian model comparison to update this distribution from the predictions of models trained on blocks of consecutive observations and pruned potential drift points with low probability . <eos> we compare our approach to a non-probabilistic method for drift and a probabilistic method for change-point detection . <eos> in our experiments , our approach generally yielded improved accuracy and/or speed over these other methods .
in this paper we consider the problem of learning an n x n kernel matrix from m similarity matrices under general convex loss . <eos> past research have extensively studied the m =1 case and have derived several algorithms which require sophisticated techniques like accp , socp , etc . <eos> the existing algorithms do not apply if one uses arbitrary losses and often can not handle m > 1 case . <eos> we present several provably convergent iterative algorithms , where each iteration requires either an svm or a multiple kernel learning ( mkl ) solver for m > 1 case . <eos> one of the major contributions of the paper is to extend the well known mirror descent ( md ) framework to handle cartesian product of psd matrices . <eos> this novel extension leads to an algorithm , called emkl , which solves the problem in o ( m^2 log n ) iterations ; in each iteration one solves an mkl involving m kernels and m eigen-decomposition of n x n matrices . <eos> by suitably defining a restriction on the objective function , a faster version of emkl is proposed , called rekl , which avoids the eigen-decomposition . <eos> an alternative to both emkl and rekl is also suggested which requires only an svm solver . <eos> experimental results on real world protein data set involving several similarity matrices illustrate the efficacy of the proposed algorithms .
we study the application of a strongly non-linear generative model to image patches . <eos> as in standard approaches such as sparse coding or independent component analysis , the model assumes a sparse prior with independent hidden variables . <eos> however , in the place where standard approaches use the sum to combine basis functions we use the maximum . <eos> to derive tractable approximations for parameter estimation we apply a novel approach based on variational expectation maximization . <eos> the derived learning algorithm can be applied to large-scale problems with hundreds of observed and hidden variables . <eos> furthermore , we can infer all model parameters including observation noise and the degree of sparseness . <eos> in applications to image patches we find that gabor-like basis functions are obtained . <eos> gabor-like functions are thus not a feature exclusive to approaches assuming linear superposition . <eos> quantitatively , the inferred basis functions show a large diversity of shapes with many strongly elongated and many circular symmetric functions . <eos> the distribution of basis function shapes reflects properties of simple cell receptive fields that are not reproduced by standard linear approaches . <eos> in the study of natural image statistics , the implications of using different superposition assumptions have so far not been investigated systematically because models with strong non-linearities have been found analytically and computationally challenging . <eos> the presented algorithm represents the first large-scale application of such an approach .
we propose an unsupervised method for learning multi-stage hierarchies of sparse convolutional features . <eos> while sparse coding has become an increasingly popular method for learning visual features , it is most often trained at the patch level . <eos> applying the resulting filters convolutionally results in highly redundant codes because overlapping patches are encoded in isolation . <eos> by training convolutionally over large image windows , our method reduces the redudancy between feature vectors at neighboring locations and improves the efficiency of the overall representation . <eos> in addition to a linear decoder that reconstructs the image from sparse features , our method trains an efficient feed-forward encoder that predicts quasi-sparse features from the input . <eos> while patch-based training rarely produces anything but oriented edge detectors , we show that convolutional training produces highly diverse filters , including center-surround filters , corner detectors , cross detectors , and oriented grating detectors . <eos> we show that using these filters in multi-stage convolutional network architecture improves performance on a number of visual recognition and detection tasks .
for many structured prediction problems , complex models often require adopting approximate inference techniques such as variational methods or sampling , which generally provide no satisfactory accuracy guarantees . <eos> in this work , we propose sidestepping intractable inference altogether by learning ensembles of tractable sub-models as part of a structured prediction cascade . <eos> we focus in particular on problems with high-treewidth and large state-spaces , which occur in many computer vision tasks . <eos> unlike other variational methods , our ensembles do not enforce agreement between sub-models , but filter the space of possible outputs by simply adding and thresholding the max-marginals of each constituent model . <eos> our framework jointly estimates parameters for all models in the ensemble for each level of the cascade by minimizing a novel , convex loss function , yet requires only a linear increase in computation over learning or inference in a single tractable sub-model . <eos> we provide a generalization bound on the filtering loss of the ensemble as a theoretical justification of our approach , and we evaluate our method on both synthetic data and the task of estimating articulated human pose from challenging videos . <eos> we find that our approach significantly outperforms loopy belief propagation on the synthetic data and a state-of-the-art model on the pose estimation/tracking problem .
boosting combines weak classifiers to form highly accurate predictors . <eos> although the case of binary classification is well understood , in the multiclass setting , the correctrequirements on the weak classifier , or the notion of the most efficient boosting algorithms are missing . <eos> in this paper , we create a broad and general framework , within which we make precise and identify the optimal requirements on the weak-classifier , as well as design the most effective , in a certain sense , boosting algorithms that assume such requirements .
convolutional neural networks ( cnns ) have been successfully applied to many tasks such as digit and object recognition . <eos> using convolutional ( tied ) weights signi ? cantly reduces the number of parameters that have to be learned , and also allows translational invariance to be hard-coded into the architecture . <eos> in this paper , we consider the problem of learning invariances , rather than relying on hard-coding . <eos> we propose tiled convolution neural networks ( tiled cnns ) , which use a regular tiledpattern of tied weights that does not require that adjacent hidden units share identical weights , but instead requires only that hidden units k steps away from each other to have tied weights . <eos> by pooling over neighboring units , this architecture is able to learn complex invariances ( such as scale and rotational invariance ) beyond translational invariance . <eos> further , it also enjoys much of cnns ? <eos> advantage of having a relatively small number of learned parameters ( such as ease of learning and greater scalability ) . <eos> we provide an efficient learning algorithm for tiled cnns based on topographic ica , and show that learning complex invariant features allows us to achieve highly competitive results for both the norb and cifar-10 datasets .
we study the problem of learning a sparse linear regression vector under additional conditions on the structure of its sparsity pattern . <eos> we present a family of convex penalty functions , which encode this prior knowledge by means of a set of constraints on the absolute values of the regression coefficients . <eos> this family subsumes the $ \ell_1 $ norm and is flexible enough to include different models of sparsity patterns , which are of practical and theoretical importance . <eos> we establish some important properties of these functions and discuss some examples where they can be computed explicitly . <eos> moreover , we present a convergent optimization algorithm for solving regularized least squares with these penalty functions . <eos> numerical simulations highlight the benefit of structured sparsity and the advantage offered by our approach over the lasso and other related methods .
probabilistic models of natural images are usually evaluated by measuring performance on rather indirect tasks , such as denoising and inpainting . <eos> a more direct way to evaluate a generative model is to draw samples from it and to check whether statistical properties of the samples match the statistics of natural images . <eos> this method is seldom used with high-resolution images , because current models produce samples that are very different from natural images , as assessed by even simple visual inspection . <eos> we investigate the reasons for this failure and we show that by augmenting existing models so that there are two sets of latent variables , one set modelling pixel intensities and the other set modelling image-specific pixel covariances , we are able to generate high-resolution images that look much more realistic than before . <eos> the overall model can be interpreted as a gated mrf where both pair-wise dependencies and mean intensities of pixels are modulated by the states of latent variables . <eos> finally , we confirm that if we disallow weight-sharing between receptive fields that overlap each other , the gated mrf learns more efficient internal representations , as demonstrated in several recognition tasks .
this paper outlines a hierarchical bayesian model for human category learning that learns both the organization of objects into categories , and the context in which this knowledge should be applied . <eos> the model is fit to multiple data sets , and provides a parsimonious method for describing how humans learn context specific conceptual representations .
we combine random forest ( rf ) and conditional random field ( crf ) into a new computational framework , called random forest random field ( rf ) ^2 . <eos> inference of ( rf ) ^2 uses the swendsen-wang cut algorithm , characterized by metropolis-hastings jumps . <eos> a jump from one state to another depends on the ratio of the proposal distributions , and on the ratio of the posterior distributions of the two states . <eos> prior work typically resorts to a parametric estimation of these four distributions , and then computes their ratio . <eos> our key idea is to instead directly estimate these ratios using rf . <eos> rf collects in leaf nodes of each decision tree the class histograms of training examples . <eos> we use these class histograms for a non-parametric estimation of the distribution ratios . <eos> we derive the theoretical error bounds of a two-class ( rf ) ^2 . <eos> ( rf ) ^2 is applied to a challenging task of multiclass object recognition and segmentation over a random field of input image regions . <eos> in our empirical evaluation , we use only the visual information provided by image regions ( e.g. , color , texture , spatial layout ) , whereas the competing methods additionally use higher-level cues about the horizon location and 3d layout of surfaces in the scene . <eos> nevertheless , ( rf ) ^2 outperforms the state of the art on benchmark datasets , in terms of accuracy and computation time .
bayesian approaches to preference elicitation ( pe ) are particularly attractive due to their ability to explicitly model uncertainty in users ' latent utility functions . <eos> however , previous approaches to bayesian pe have ignored the important problem of generalizing from previous users to an unseen user in order to reduce the elicitation burden on new users . <eos> in this paper , we address this deficiency by introducing a gaussian process ( gp ) prior over users ' latent utility functions on the joint space of user and item features . <eos> we learn the hyper-parameters of this gp on a set of preferences of previous users and use it to aid in the elicitation process for a new user . <eos> this approach provides a flexible model of a multi-user utility function , facilitates an efficient value of information ( voi ) heuristic query selection strategy , and provides a principled way to incorporate the elicitations of multiple users back into the model . <eos> we show the effectiveness of our method in comparison to previous work on a real dataset of user preferences over sushi types .
we discuss an online learning framework in which the agent is allowed to say `` i do n't know '' as well as making incorrect predictions on given examples . <eos> we analyze the trade off between saying `` i do n't know '' and making mistakes . <eos> if the number of do n't know predictions is forced to be zero , the model reduces to the well-known mistake-bound model introduced by littlestone [ lit88 ] . <eos> on the other hand , if no mistakes are allowed , the model reduces to kwik framework introduced by li et . <eos> al . <eos> [ llw08 ] . <eos> we propose a general , though inefficient , algorithm for general finite concept classes that minimizes the number of don't-know predictions if a certain number of mistakes are allowed . <eos> we then present specific polynomial-time algorithms for the concept classes of monotone disjunctions and linear separators .
this paper tackles the complex problem of visually matching people in similar pose but with different clothes , background , and other appearance changes . <eos> we achieve this with a novel method for learning a nonlinear embedding based on several extensions to the neighborhood component analysis ( nca ) framework . <eos> our method is convolutional , enabling it to scale to realistically-sized images . <eos> by cheaply labeling the head and hands in large video databases through amazon mechanical turk ( a crowd-sourcing service ) , we can use the task of localizing the head and hands as a proxy for determining body pose . <eos> we apply our method to challenging real-world data and show that it can generalize beyond hand localization to infer a more general notion of body pose . <eos> we evaluate our method quantitatively against other embedding methods . <eos> we also demonstrate that real-world performance can be improved through the use of synthetic data .
in a recent paper joachims ( 2006 ) presented svm-perf , a cutting plane method ( cpm ) for training linear support vector machines ( svms ) which converges to an $ \epsilon $ accurate solution in $ o ( 1/\epsilon^ { 2 } ) $ iterations . <eos> by tightening the analysis , teo et al . ( 2010 ) showed that $ o ( 1/\epsilon ) $ iterations suffice . <eos> given the impressive convergence speed of cpm on a number of practical problems , it was conjectured that these rates could be further improved . <eos> in this paper we disprove this conjecture . <eos> we present counter examples which are not only applicable for training linear svms with hinge loss , but also hold for support vector methods which optimize a \emph { multivariate } performance score . <eos> however , surprisingly , these problems are not inherently hard . <eos> by exploiting the structure of the objective function we can devise an algorithm that converges in $ o ( 1/\sqrt { \epsilon } ) $ iterations .
predicting the execution time of computer programs is an important but challenging problem in the community of computer systems . <eos> existing methods require experts to perform detailed analysis of program code in order to construct predictors or select important features . <eos> we recently developed a new system to automatically extract a large number of features from program execution on sample inputs , on which prediction models can be constructed without expert knowledge . <eos> in this paper we study the construction of predictive models for this problem . <eos> we propose the spore ( sparse polynomial regression ) methodology to build accurate prediction models of program performance using feature data collected from program execution on sample inputs . <eos> our two spore algorithms are able to build relationships between responses ( e.g. , the execution time of a computer program ) and features , and select a few from hundreds of the retrieved features to construct an explicitly sparse and non-linear model to predict the response variable . <eos> the compact and explicitly polynomial form of the estimated model could reveal important insights into the computer program ( e.g. , features and their non-linear combinations that dominate the execution time ) , enabling a better understanding of the program ? s behavior . <eos> our evaluation on three widely used computer programs shows that spore methods can give accurate prediction with relative error less than 7 % by using a moderate number of training data samples . <eos> in addition , we compare spore algorithms to state-of-the-art sparse regression algorithms , and show that spore methods , motivated by real applications , outperform the other methods in terms of both interpretability and prediction accuracy .
recent work has demonstrated that when artificial agents are limited in their ability to achieve their goals , the agent designer can benefit by making the agent 's goals different from the designer 's . <eos> this gives rise to the optimization problem of designing the artificial agent 's goals -- -in the rl framework , designing the agent 's reward function . <eos> existing attempts at solving this optimal reward problem do not leverage experience gained online during the agent 's lifetime nor do they take advantage of knowledge about the agent 's structure . <eos> in this work , we develop a gradient ascent approach with formal convergence guarantees for approximately solving the optimal reward problem online during an agent 's lifetime . <eos> we show that our method generalizes a standard policy gradient approach , and we demonstrate its ability to improve reward functions in agents with various forms of limitations .
the design of low-level image features is critical for computer vision algorithms . <eos> orientation histograms , such as those in sift~\cite { lowe2004distinctive } and hog~\cite { dalal2005histograms } , are the most successful and popular features for visual object and scene recognition . <eos> we highlight the kernel view of orientation histograms , and show that they are equivalent to a certain type of match kernels over image patches . <eos> this novel view allows us to design a family of kernel descriptors which provide a unified and principled framework to turn pixel attributes ( gradient , color , local binary pattern , \etc ) into compact patch-level features . <eos> in particular , we introduce three types of match kernels to measure similarities between image patches , and construct compact low-dimensional kernel descriptors from these match kernels using kernel principal component analysis ( kpca ) ~\cite { scholkopf1998nonlinear } . <eos> kernel descriptors are easy to design and can turn any type of pixel attribute into patch-level features . <eos> they outperform carefully tuned and sophisticated features including sift and deep belief networks . <eos> we report superior performance on standard image classification benchmarks : scene-15 , caltech-101 , cifar10 and cifar10-imagenet .
the standard strategy for efficient object detection consists of building a cascade composed of several binary classifiers . <eos> the detection process takes the form of a lazy evaluation of the conjunction of the responses of these classifiers , and concentrates the computation on difficult parts of the image which can not be trivially rejected . <eos> we introduce a novel algorithm to construct jointly the classifiers of such a cascade . <eos> we interpret the response of a classifier as a probability of a positive prediction , and the overall response of the cascade as the probability that all the predictions are positive . <eos> from this noisy-and model , we derive a consistent loss and a boosting procedure to optimize that global probability on the training set . <eos> such a joint learning allows the individual predictors to focus on a more restricted modeling problem , and improves the performance compared to a standard cascade . <eos> we demonstrate the efficiency of this approach on face and pedestrian detection with standard data-sets and comparisons with reference baselines .
upstream supervised topic models have been widely used for complicated scene understanding . <eos> however , existing maximum likelihood estimation ( mle ) schemes can make the prediction model learning independent of latent topic discovery and result in an imbalanced prediction rule for scene classification . <eos> this paper presents a joint max-margin and max-likelihood learning method for upstream scene understanding models , in which latent topic discovery and prediction model estimation are closely coupled and well-balanced . <eos> the optimization problem is efficiently solved with a variational em procedure , which iteratively solves an online loss-augmented svm . <eos> we demonstrate the advantages of the large-margin approach on both an 8-category sports dataset and the 67-class mit indoor scene dataset for scene categorization .
recently , some variants of the $ l_1 $ norm , particularly matrix norms such as the $ l_ { 1,2 } $ and $ l_ { 1 , \infty } $ norms , have been widely used in multi-task learning , compressed sensing and other related areas to enforce sparsity via joint regularization . <eos> in this paper , we unify the $ l_ { 1,2 } $ and $ l_ { 1 , \infty } $ norms by considering a family of $ l_ { 1 , q } $ norms for $ 1 < q\le\infty $ and study the problem of determining the most appropriate sparsity enforcing norm to use in the context of multi-task feature selection . <eos> using the generalized normal distribution , we provide a probabilistic interpretation of the general multi-task feature selection problem using the $ l_ { 1 , q } $ norm . <eos> based on this probabilistic interpretation , we develop a probabilistic model using the noninformative jeffreys prior . <eos> we also extend the model to learn and exploit more general types of pairwise relationships between tasks . <eos> for both versions of the model , we devise expectation-maximization~ ( em ) algorithms to learn all model parameters , including $ q $ , automatically . <eos> experiments have been conducted on two cancer classification applications using microarray gene expression data .
we present a novel method for constructing dependent dirichlet processes . <eos> the approach exploits the intrinsic relationship between dirichlet and poisson processes in order to create a markov chain of dirichlet processes suitable for use as a prior over evolving mixture models . <eos> the method allows for the creation , removal , and location variation of component models over time while maintaining the property that the random measures are marginally dp distributed . <eos> additionally , we derive a gibbs sampling algorithm for model inference and test it on both synthetic and real data . <eos> empirical results demonstrate that the approach is effective in estimating dynamically varying mixture models .
we consider problems for which one has incomplete binary matrices that evolve with time ( e.g. , the votes of legislators on particular legislation , with each year characterized by a different such matrix ) . <eos> an objective of such analysis is to infer structure and inter-relationships underlying the matrices , here defined by latent features associated with each axis of the matrix . <eos> in addition , it is assumed that documents are available for the entities associated with at least one of the matrix axes . <eos> by jointly analyzing the matrices and documents , one may be used to inform the other within the analysis , and the model offers the opportunity to predict matrix values ( e.g. , votes ) based only on an associated document ( e.g. , legislation ) . <eos> the research presented here merges two areas of machine-learning that have previously been investigated separately : incomplete-matrix analysis and topic modeling . <eos> the analysis is performed from a bayesian perspective , with efficient inference constituted via gibbs sampling . <eos> the framework is demonstrated by considering all voting data and available documents ( legislation ) during the 220-year lifetime of the united states senate and house of representatives .
recent studies compare gene expression data across species to identify core and species specific genes in biological systems . <eos> to perform such comparisons researchers need to match genes across species . <eos> this is a challenging task since the correct matches ( orthologs ) are not known for most genes . <eos> previous work in this area used deterministic matchings or reduced multidimensional expression data to binary representation . <eos> here we develop a new method that can utilize soft matches ( given as priors ) to infer both , unique and similar expression patterns across species and a matching for the genes in both species . <eos> our method uses a dirichlet process mixture model which includes a latent data matching variable . <eos> we present learning and inference algorithms based on variational methods for this model . <eos> applying our method to immune response data we show that it can accurately identify common and unique response patterns by improving the matchings between human and mouse genes .
is there a principled way to learn a probabilistic discriminative classifier from an unlabeled data set ? <eos> we present a framework that simultaneously clusters the data and trains a discriminative classifier . <eos> we call it regularized information maximization ( rim ) . <eos> rim optimizes an intuitive information-theoretic objective function which balances class separation , class balance and classifier complexity . <eos> the approach can flexibly incorporate different likelihood functions , express prior assumptions about the relative size of different classes and incorporate partial labels for semi-supervised learning . <eos> in particular , we instantiate the framework to unsupervised , multi-class kernelized logistic regression . <eos> our empirical evaluation indicates that rim outperforms existing methods on several real data sets , and demonstrates that rim is an effective model selection method .
from a functional viewpoint , a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon . <eos> we demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone . <eos> we begin by posing the problem in a classification based framework . <eos> we then derive a novel kernel for an srm0 model that is based on psp and ahp like functions . <eos> with the kernel we demonstrate how the learning problem can be posed as a quadratic program . <eos> experimental results demonstrate the strength of our approach .
this paper presents an analysis of importance weighting for learning from finite samples and gives a series of theoretical and algorithmic results . <eos> we point out simple cases where importance weighting can fail , which suggests the need for an analysis of the properties of this technique . <eos> we then give both upper and lower bounds for generalization with bounded importance weights and , more significantly , give learning guarantees for the more common case of unbounded importance weights under the weak assumption that the second moment is bounded , a condition related to the renyi divergence of the training and test distributions . <eos> these results are based on a series of novel and general bounds we derive for unbounded loss functions , which are of independent interest . <eos> we use these bounds to guide the definition of an alternative reweighting algorithm and report the results of experiments demonstrating its benefits . <eos> finally , we analyze the properties of normalized importance weights which are also commonly used .
we present a fast algorithm for the detection of multiple change-points when each is frequently shared by members of a set of co-occurring one-dimensional signals . <eos> we give conditions on consistency of the method when the number of signals increases , and provide empirical evidence to support the consistency results .
we propose a discriminative model for recognizing group activities . <eos> our model jointly captures the group activity , the individual person actions , and the interactions among them . <eos> two new types of contextual information , group-person interaction and person-person interaction , are explored in a latent variable framework . <eos> different from most of the previous latent structured models which assume a predefined structure for the hidden layer , e.g . a tree structure , we treat the structure of the hidden layer as a latent variable and implicitly infer it during learning and inference . <eos> our experimental results demonstrate that by inferring this contextual information together with adaptive structures , the proposed model can significantly improve activity recognition performance .
in this paper we consider the fundamental problem of semi-supervised kernel function learning . <eos> we propose a general regularized framework for learning a kernel matrix , and then demonstrate an equivalence between our proposed kernel matrix learning framework and a general linear transformation learning problem . <eos> our result shows that the learned kernel matrices parameterize a linear transformation kernel function and can be applied inductively to new data points . <eos> furthermore , our result gives a constructive method for kernelizing most existing mahalanobis metric learning formulations . <eos> to make our results practical for large-scale data , we modify our framework to limit the number of parameters in the optimization process . <eos> we also consider the problem of kernelized inductive dimensionality reduction in the semi-supervised setting . <eos> we introduce a novel method for this problem by considering a special case of our general kernel learning framework where we select the trace norm function as the regularizer . <eos> we empirically demonstrate that our framework learns useful kernel functions , improving the $ k $ -nn classification accuracy significantly in a variety of domains . <eos> furthermore , our kernelized dimensionality reduction technique significantly reduces the dimensionality of the feature space while achieving competitive classification accuracies .
we consider the problem of apprenticeship learning where the examples , demonstrated by an expert , cover only a small part of a large state space . <eos> inverse reinforcement learning ( irl ) provides an efficient tool for generalizing the demonstration , based on the assumption that the expert is maximizing a utility function that is a linear combination of state-action features . <eos> most irl algorithms use a simple monte carlo estimation to approximate the expected feature counts under the expert 's policy . <eos> in this paper , we show that the quality of the learned policies is highly sensitive to the error in estimating the feature counts . <eos> to reduce this error , we introduce a novel approach for bootstrapping the demonstration by assuming that : ( i ) , the expert is ( near- ) optimal , and ( ii ) , the dynamics of the system is known . <eos> empirical results on gridworlds and car racing problems show that our approach is able to learn good policies from a small number of demonstrations .
we propose a novel bayesian nonparametric approach to learning with probabilistic deterministic finite automata ( pdfa ) . <eos> we define and develop and sampler for a pdfa with an infinite number of states which we call the probabilistic deterministic infinite automata ( pdia ) . <eos> posterior predictive inference in this model , given a finite training sequence , can be interpreted as averaging over multiple pdfas of varying structure , where each pdfa is biased towards having few states . <eos> we suggest that our method for averaging over pdfas is a novel approach to predictive distribution smoothing . <eos> we test pdia inference both on pdfa structure learning and on both natural language and dna data prediction tasks . <eos> the results suggest that the pdia presents an attractive compromise between the computational cost of hidden markov models and the storage requirements of hierarchically smoothed markov models .
for over half a century , psychologists have been struck by how poor people are at expressing their internal sensations , impressions , and evaluations via rating scales . <eos> when individuals make judgments , they are incapable of using an absolute rating scale , and instead rely on reference points from recent experience . <eos> this relativity of judgment limits the usefulness of responses provided by individuals to surveys , questionnaires , and evaluation forms . <eos> fortunately , the cognitive processes that transform internal states to responses are not simply noisy , but rather are influenced by recent experience in a lawful manner . <eos> we explore techniques to remove sequential dependencies , and thereby decontaminate a series of ratings to obtain more meaningful human judgments . <eos> in our formulation , decontamination is fundamentally a problem of inferring latent states ( internal sensations ) which , because of the relativity of judgment , have temporal dependencies . <eos> we propose a decontamination solution using a conditional random field with constraints motivated by psychological theories of relative judgment . <eos> our exploration of decontamination models is supported by two experiments we conducted to obtain ground-truth rating data on a simple length estimation task . <eos> our decontamination techniques yield an over 20 % reduction in the error of human judgments .
we present a novel method for multitask learning ( mtl ) based on { \it manifold regularization } : assume that all task parameters lie on a manifold . <eos> this is the generalization of a common assumption made in the existing literature : task parameters share a common { \it linear } subspace . <eos> one proposed method uses the projection distance from the manifold to regularize the task parameters . <eos> the manifold structure and the task parameters are learned using an alternating optimization framework . <eos> when the manifold structure is fixed , our method decomposes across tasks which can be learnt independently . <eos> an approximation of the manifold regularization scheme is presented that preserves the convexity of the single task learning problem , and makes the proposed mtl framework efficient and easy to implement . <eos> we show the efficacy of our method on several datasets .
the goal of decentralized optimization over a network is to optimize a global objective formed by a sum of local ( possibly nonsmooth ) convex functions using only local computation and communication . <eos> we develop and analyze distributed algorithms based on dual averaging of subgradients , and we provide sharp bounds on their convergence rates as a function of the network size and topology . <eos> our analysis clearly separates the convergence of the optimization algorithm itself from the effects of communication constraints arising from the network structure . <eos> we show that the number of iterations required by our algorithm scales inversely in the spectral gap of the network . <eos> the sharpness of this prediction is confirmed both by theoretical lower bounds and simulations for various networks .
computing a { \em maximum a posteriori } ( map ) assignment in graphical models is a crucial inference problem for many practical applications . <eos> several provably convergent approaches have been successfully developed using linear programming ( lp ) relaxation of the map problem . <eos> we present an alternative approach , which transforms the map problem into that of inference in a finite mixture of simple bayes nets . <eos> we then derive the expectation maximization ( em ) algorithm for this mixture that also monotonically increases a lower bound on the map assignment until convergence . <eos> the update equations for the em algorithm are remarkably simple , both conceptually and computationally , and can be implemented using a graph-based message passing paradigm similar to max-product computation . <eos> we experiment on the real-world protein design dataset and show that em 's convergence rate is significantly higher than the previous lp relaxation based approach mplp . <eos> em achieves a solution quality within $ 95 $ \ % of optimal for most instances and is often an order-of-magnitude faster than mplp .
we consider structured multi-armed bandit tasks in which the agent is guided by prior structural knowledge that can be exploited to efficiently select the optimal arm ( s ) in situations where the number of arms is large , or even infinite . <eos> we pro- pose a new optimistic , ucb-like , algorithm for non-linearly parameterized bandit problems using the generalized linear model ( glm ) framework . <eos> we analyze the regret of the proposed algorithm , termed glm-ucb , obtaining results similar to those recently proved in the literature for the linear regression case . <eos> the analysis also highlights a key difficulty of the non-linear case which is solved in glm-ucb by focusing on the reward space rather than on the parameter space . <eos> moreover , as the actual efficiency of current parameterized bandit algorithms is often deceiving in practice , we provide an asymptotic argument leading to significantly faster convergence . <eos> simulation studies on real data sets illustrate the performance and the robustness of the proposed glm-ucb approach .
optimal control entails combining probabilities and utilities . <eos> however , for most practical problems probability densities can be represented only approximately . <eos> choosing an approximation requires balancing the benefits of an accurate approximation against the costs of computing it . <eos> we propose a variational framework for achieving this balance and apply it to the problem of how a population code should optimally represent a distribution under resource constraints . <eos> the essence of our analysis is the conjecture that population codes are organized to maximize a lower bound on the log expected utility . <eos> this theory can account for a plethora of experimental data , including the reward-modulation of sensory receptive fields .
during the last years support vector machines ( svms ) have been successfully applied even in situations where the input space $ x $ is not necessarily a subset of $ r^d $ . <eos> examples include svms using probability measures to analyse e.g . histograms or coloured images , svms for text classification and web mining , and svms for applications from computational biology using , e.g. , kernels for trees and graphs . <eos> moreover , svms are known to be consistent to the bayes risk , if either the input space is a complete separable metric space and the reproducing kernel hilbert space ( rkhs ) $ h\subset l_p ( p_x ) $ is dense , or if the svm is based on a universal kernel $ k $ . <eos> so far , however , there are no rkhss of practical interest known that satisfy these assumptions on $ \ch $ or $ k $ if $ x \not\subset r^d $ . <eos> we close this gap by providing a general technique based on taylor-type kernels to explicitly construct universal kernels on compact metric spaces which are not subset of $ r^d $ . <eos> we apply this technique for the following special cases : universal kernels on the set of probability measures , universal kernels based on fourier transforms , and universal kernels for signal processing .
straightforward application of deep belief nets ( dbns ) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent timit phone recognition task . <eos> however , the first-layer gaussian-bernoulli restricted boltzmann machine ( grbm ) has an important limitation , shared with mixtures of diagonal-covariance gaussians : grbms treat different components of the acoustic input vector as conditionally independent given the hidden state . <eos> the mean-covariance restricted boltzmann machine ( mcrbm ) , first introduced for modeling natural images , is a much more representationally efficient and powerful way of modeling the covariance structure of speech data . <eos> every configuration of the precision units of the mcrbm specifies a different precision matrix for the conditional distribution over the acoustic space . <eos> in this work , we use the mcrbm to learn features of speech data that serve as input into a standard dbn . <eos> the mcrbm features combined with dbns allow us to achieve a phone error rate of 20.5\ % , which is superior to all published results on speaker-independent timit to date .
we propose a new lp relaxation for obtaining the map assignment of a binary mrf with pairwise potentials . <eos> our relaxation is derived from reducing the map assignment problem to an instance of a recently proposed bipartite multi-cut problem where the lp relaxation is guaranteed to provide an o ( log k ) approximation where k is the number of vertices adjacent to non-submodular edges in the mrf . <eos> we then propose a combinatorial algorithm to efficiently solve the lp and also provide a lower bound by concurrently solving its dual to within an  approximation . <eos> the algorithm is up to an order of magnitude faster and provides better map scores and bounds than the state of the art message passing algorithm of [ 1 ] that tightens the local marginal polytope with third-order marginal constraints .
games of incomplete information , or bayesian games , are an important game-theoretic model and have many applications in economics . <eos> we propose bayesian action-graph games ( baggs ) , a novel graphical representation for bayesian games . <eos> baggs can represent arbitrary bayesian games , and furthermore can compactly express bayesian games exhibiting commonly encountered types of structure including symmetry , action- and type-specific utility independence , and probabilistic independence of type distributions . <eos> we provide an algorithm for computing expected utility in baggs , and discuss conditions under which the algorithm runs in polynomial time . <eos> bayes-nash equilibria of baggs can be computed by adapting existing algorithms for complete-information normal form games and leveraging our expected utility algorithm . <eos> we show both theoretically and empirically that our approaches improve significantly on the state of the art .
we propose a novel method for inferring whether x causes y or vice versa from joint observations of x and y . <eos> the basic idea is to model the observed data using probabilistic latent variable models , which incorporate the effects of unobserved noise . <eos> to this end , we consider the hypothetical effect variable to be a function of the hypothetical cause variable and an independent noise term ( not necessarily additive ) . <eos> an important novel aspect of our work is that we do not restrict the model class , but instead put general non-parametric priors on this function and on the distribution of the cause . <eos> the causal direction can then be inferred by using standard bayesian model selection . <eos> we evaluate our approach on synthetic data and real-world data and report encouraging results .
we introduce a new bayesian nonparametric approach to identification of sparse dynamic linear systems . <eos> the impulse responses are modeled as gaussian processes whose autocovariances encode the bibo stability constraint , as defined by the recently introduced ? stable spline kernel ? . <eos> sparse solutions are obtained by placing exponential hyperpriors on the scale factors of such kernels . <eos> numerical experiments regarding estimation of armax models show that this technique provides a definite advantage over a group lar algorithm and state-of-the-art parametric identification techniques based on prediction error minimization .
markov networks ( mns ) can incorporate arbitrarily complex features in modeling relational data . <eos> however , this flexibility comes at a sharp price of training an exponentially complex model . <eos> to address this challenge , we propose a novel relational learning approach , which consists of a restricted class of relational mns ( rmns ) called relation tree-based rmn ( treermn ) , and an efficient hidden variable detection algorithm called contrastive variable induction ( cvi ) . <eos> on one hand , the restricted treermn only considers simple ( e.g. , unary and pairwise ) features in relational data and thus achieves computational efficiency ; and on the other hand , the cvi algorithm efficiently detects hidden variables which can capture long range dependencies . <eos> therefore , the resultant approach is highly efficient yet does not sacrifice its expressive power . <eos> empirical results on four real datasets show that the proposed relational learning method can achieve similar prediction quality as the state-of-the-art approaches , but is significantly more efficient in training ; and the induced hidden variables are semantically meaningful and crucial to improve the training speed and prediction qualities of treermns .
most active learning approaches select either informative or representative unlabeled instances to query their labels . <eos> although several active learning algorithms have been proposed to combine the two criterions for query selection , they are usually ad hoc in finding unlabeled instances that are both informative and representative . <eos> we address this challenge by a principled approach , termed quire , based on the min-max view of active learning . <eos> the proposed approach provides a systematic way for measuring and combining the informativeness and representativeness of an instance . <eos> extensive experimental results show that the proposed quire approach outperforms several state-of -the-art active learning approaches .
recent studies have shown that multiple kernel learning is very effective for object recognition , leading to the popularity of kernel learning in computer vision problems . <eos> in this work , we develop an efficient algorithm for multi-label multiple kernel learning ( ml-mkl ) . <eos> we assume that all the classes under consideration share the same combination of kernel functions , and the objective is to find the optimal kernel combination that benefits all the classes . <eos> although several algorithms have been developed for ml-mkl , their computational cost is linear in the number of classes , making them unscalable when the number of classes is large , a challenge frequently encountered in visual object recognition . <eos> we address this computational challenge by developing a framework for ml-mkl that combines the worst-case analysis with stochastic approximation . <eos> our analysis shows that the complexity of our algorithm is $ o ( m^ { 1/3 } \sqrt { ln m } ) $ , where $ m $ is the number of classes . <eos> empirical studies with object recognition show that while achieving similar classification accuracy , the proposed method is significantly more efficient than the state-of-the-art algorithms for ml-mkl .
we propose a new nonparametric learning method based on multivariate dyadic regression trees ( mdrts ) . <eos> unlike traditional dyadic decision trees ( ddts ) or classification and regression trees ( carts ) , mdrts are constructed using penalized empirical risk minimization with a novel sparsity-inducing penalty . <eos> theoretically , we show that mdrts can simultaneously adapt to the unknown sparsity and smoothness of the true regression functions , and achieve the nearly optimal rates of convergence ( in a minimax sense ) for the class of $ ( \alpha , c ) $ -smooth functions . <eos> empirically , mdrts can simultaneously conduct function estimation and variable selection in high dimensions . <eos> to make mdrts applicable for large-scale learning problems , we propose a greedy heuristics . <eos> the superior performance of mdrts are demonstrated on both synthetic and real datasets .
estimating 3d pose from monocular images is a highly ambiguous problem . <eos> physical constraints can be exploited to restrict the space of feasible configurations . <eos> in this paper we propose an approach to constraining the prediction of a discriminative predictor . <eos> we first show that the mean prediction of a gaussian process implicitly satisfies linear constraints if those constraints are satisfied by the training examples . <eos> we then show how , by performing a change of variables , a gp can be forced to satisfy quadratic constraints . <eos> as evidenced by the experiments , our method outperforms state-of-the-art approaches on the tasks of rigid and non-rigid pose estimation .
we provide new theoretical results for apprenticeship learning , a variant of reinforcement learning in which the true reward function is unknown , and the goal is to perform well relative to an observed expert . <eos> we study a common approach to learning from expert demonstrations : using a classification algorithm to learn to imitate the expert 's behavior . <eos> although this straightforward learning strategy is widely-used in practice , it has been subject to very little formal analysis . <eos> we prove that , if the learned classifier has error rate $ \eps $ , the difference between the value of the apprentice 's policy and the expert 's policy is $ o ( \sqrt { \eps } ) $ . <eos> further , we prove that this difference is only $ o ( \eps ) $ when the expert 's policy is close to optimal . <eos> this latter result has an important practical consequence : not only does imitating a near-optimal expert result in a better policy , but far fewer demonstrations are required to successfully imitate such an expert . <eos> this suggests an opportunity for substantial savings whenever the expert is known to be good , but demonstrations are expensive or difficult to obtain .
we address the question of how the approximation error/bellman residual at each iteration of the approximate policy/value iteration algorithms influences the quality of the resulted policy . <eos> we quantify the performance loss as the lp norm of the approximation error/bellman residual at each iteration . <eos> moreover , we show that the performance loss depends on the expectation of the squared radon-nikodym derivative of a certain distribution rather than its supremum -- as opposed to what has been suggested by the previous results . <eos> also our results indicate that the contribution of the approximation/bellman error to the performance loss is more prominent in the later iterations of api/avi , and the effect of an error term in the earlier iterations decays exponentially fast .
in this paper , we consider the problem of policy evaluation for continuous-state systems . <eos> we present a non-parametric approach to policy evaluation , which uses kernel density estimation to represent the system . <eos> the true form of the value function for this model can be determined , and can be computed using galerkin 's method . <eos> furthermore , we also present a unified view of several well-known policy evaluation methods . <eos> in particular , we show that the same galerkin method can be used to derive least-squares temporal difference learning , kernelized temporal difference learning , and a discrete-state dynamic programming solution , as well as our proposed method . <eos> in a numerical evaluation of these algorithms , the proposed approach performed better than the other methods .
many applications in computer vision measure the similarity between images or image patches based on some statistics such as oriented gradients . <eos> these are often modeled implicitly or explicitly with a gaussian noise assumption , leading to the use of the euclidean distance when comparing image descriptors . <eos> in this paper , we show that the statistics of gradient based image descriptors often follow a heavy-tailed distribution , which undermines any principled motivation for the use of euclidean distances . <eos> we advocate for the use of a distance measure based on the likelihood ratio test with appropriate probabilistic models that fit the empirical data distribution . <eos> we instantiate this similarity measure with the gamma-compound-laplace distribution , and show significant improvement over existing distance measures in the application of sift feature matching , at relatively low computational cost .
we study multi-label prediction for structured output spaces , a problem that occurs , for example , in object detection in images , secondary structure prediction in computational biology , and graph matching with symmetries . <eos> conventional multi-label classification techniques are typically not applicable in this situation , because they require explicit enumeration of the label space , which is infeasible in case of structured outputs . <eos> relying on techniques originally designed for single- label structured prediction , in particular structured support vector machines , results in reduced prediction accuracy , or leads to infeasible optimization problems . <eos> in this work we derive a maximum-margin training formulation for multi-label structured prediction that remains computationally tractable while achieving high prediction accuracy . <eos> it also shares most beneficial properties with single-label maximum-margin approaches , in particular a formulation as a convex optimization problem , efficient working set training , and pac-bayesian generalization bounds .
we study the family of p-resistances on graphs for p 1 . <eos> this family generalizes the standard resistance distance . <eos> we prove that for any fixed graph , for p=1 , the p-resistance coincides with the shortest path distance , for p=2 it coincides with the standard resistance distance , and for p it converges to the inverse of the minimal s-t-cut in the graph . <eos> secondly , we consider the special case of random geometric graphs ( such as k-nearest neighbor graphs ) when the number n of vertices in the graph tends to infinity . <eos> we prove that an interesting phase-transition takes place . <eos> there exist two critical thresholds p^* and p^** such that if p < p^* , then the p-resistance depends on meaningful global properties of the graph , whereas if p > p^** , it only depends on trivial local quantities and does not convey any useful information . <eos> we can explicitly compute the critical values : p^* = 1 + 1/ ( d-1 ) and p^** = 1 + 1/ ( d-2 ) where d is the dimension of the underlying space ( we believe that the fact that there is a small gap between p^* and p^** is an artifact of our proofs . <eos> we also relate our findings to laplacian regularization and suggest to use q-laplacians as regularizers , where q satisfies 1/p^* + 1/q = 1 .
we propose maximum covariance unfolding ( mcu ) , a manifold learning algorithm for simultaneous dimensionality reduction of data from different input modalities . <eos> given high dimensional inputs from two different but naturally aligned sources , mcu computes a common low dimensional embedding that maximizes the cross-modal ( inter-source ) correlations while preserving the local ( intra-source ) distances . <eos> in this paper , we explore two applications of mcu . <eos> first we use mcu to analyze eeg-fmri data , where an important goal is to visualize the fmri voxels that are most strongly correlated with changes in eeg traces . <eos> to perform this visualization , we augment mcu with an additional step for metric learning in the high dimensional voxel space . <eos> second , we use mcu to perform cross-modal retrieval of matched image and text samples from wikipedia . <eos> to manage large applications of mcu , we develop a fast implementation based on ideas from spectral graph theory . <eos> these ideas transform the original problem for mcu , one of semidefinite programming , into a simpler problem in semidefinite quadratic linear programming .
is it possible to crowdsource categorization ? <eos> amongst the challenges : ( a ) each annotator has only a partial view of the data , ( b ) different annotators may have different clustering criteria and may produce different numbers of categories , ( c ) the underlying category structure may be hierarchical . <eos> we propose a bayesian model of how annotators may approach clustering and show how one may infer clusters/categories , as well as annotator parameters , using this model . <eos> our experiments , carried out on large collections of images , suggest that bayesian crowdclustering works well and may be superior to single-expert annotations .
we present a new algorithm for exactly solving decision-making problems represented as an influence diagram . <eos> we do not require the usual assumptions of no forgetting and regularity , which allows us to solve problems with limited information . <eos> the algorithm , which implements a sophisticated variable elimination procedure , is empirically shown to outperform a state-of-the-art algorithm in randomly generated problems of up to 150 variables and $ 10^ { 64 } $ strategies .
we propose a novel generative model that is able to reason jointly about the 3d scene layout as well as the 3d location and orientation of objects in the scene . <eos> in particular , we infer the scene topology , geometry as well as traffic activities from a short video sequence acquired with a single camera mounted on a moving car . <eos> our generative model takes advantage of dynamic information in the form of vehicle tracklets as well as static information coming from semantic labels and geometry ( i.e. , vanishing points ) . <eos> experiments show that our approach outperforms a discriminative baseline based on multiple kernel learning ( mkl ) which has access to the same image information . <eos> furthermore , as we reason about objects in 3d , we are able to significantly increase the performance of state-of-the-art object detectors in their ability to estimate object orientation .
we study the problem of active learning in a stream-based setting , allowing the distribution of the examples to change over time . <eos> we prove upper bounds on the number of prediction mistakes and number of label requests for established disagreement-based active learning algorithms , both in the realizable case and under tsybakov noise . <eos> we further prove minimax lower bounds for this problem .
most methods for decision-theoretic online learning are based on the hedge algorithm , which takes a parameter called the learning rate . <eos> in most previous analyses the learning rate was carefully tuned to obtain optimal worst-case performance , leading to suboptimal performance on easy instances , for example when there exists an action that is significantly better than all others . <eos> we propose a new way of setting the learning rate , which adapts to the difficulty of the learning problem : in the worst case our procedure still guarantees optimal performance , but on easy instances it achieves much smaller regret . <eos> in particular , our adaptive method achieves constant regret in a probabilistic setting , when there exists an action that on average obtains strictly smaller loss than all other actions . <eos> we also provide a simulation study comparing our approach to existing methods .
in matrix completion , we are given a matrix where the values of only some of the entries are present , and we want to reconstruct the missing ones . <eos> much work has focused on the assumption that the data matrix has low rank . <eos> we propose a more general assumption based on denoising , so that we expect that the value of a missing entry can be predicted from the values of neighboring points . <eos> we propose a nonparametric version of denoising based on local , iterated averaging with mean-shift , possibly constrained to preserve local low-rank manifold structure . <eos> the few user parameters required ( the denoising scale , number of neighbors and local dimensionality ) and the number of iterations can be estimated by cross-validating the reconstruction error . <eos> using our algorithms as a postprocessing step on an initial reconstruction ( provided by e.g . a low-rank method ) , we show consistent improvements with synthetic , image and motion-capture data .
recently , there has been substantial interest in using large amounts of unlabeled data to learn word representations which can then be used as features in supervised classifiers for nlp tasks . <eos> however , most current approaches are slow to train , do not model context of the word , and lack theoretical grounding . <eos> in this paper , we present a new learning method , low rank multi-view learning ( lr-mvl ) which uses a fast spectral method to estimate low dimensional context-specific word representations from unlabeled data . <eos> these representation features can then be used with any supervised learner . <eos> lr-mvl is extremely fast , gives guaranteed convergence to a global optimum , is theoretically elegant , and achieves state-of-the-art performance on named entity recognition ( ner ) and chunking problems .
we present a novel regularization-based multitask learning ( mtl ) formulation for structured output ( so ) prediction for the case of hierarchical task relations . <eos> structured output learning often results in dif ? cult inference problems and requires large amounts of training data to obtain accurate models . <eos> we propose to use mtl to exploit information available for related structured output learning tasks by means of hierarchical regularization . <eos> due to the combination of example sets , the cost of training models for structured output prediction can easily become infeasible for real world applications . <eos> we thus propose an ef ? cient algorithm based on bundle methods to solve the optimization problems resulting from mtl structured output learning . <eos> we demonstrate the performance of our approach on gene ? nding problems from the application domain of computational biology . <eos> we show that 1 ) our proposed solver achieves much faster convergence than previous methods and 2 ) that the hierarchical so-mtl approach clearly outperforms considered non-mtl methods .
discriminative learning when training and test data belong to different distributions is a challenging and complex task . <eos> often times we have very few or no labeled data from the test or target distribution but may have plenty of labeled data from multiple related sources with different distributions . <eos> the difference in distributions may be in both marginal and conditional probabilities . <eos> most of the existing domain adaptation work focuses on the marginal probability distribution difference between the domains , assuming that the conditional probabilities are similar . <eos> however in many real world applications , conditional probability distribution differences are as commonplace as marginal probability differences . <eos> in this paper we propose a two-stage domain adaptation methodology which combines weighted data from multiple sources based on marginal probability differences ( first stage ) as well as conditional probability differences ( second stage ) , with the target domain data . <eos> the weights for minimizing the marginal probability differences are estimated independently , while the weights for minimizing conditional probability differences are computed simultaneously by exploiting the potential interaction among multiple sources . <eos> we also provide a theoretical analysis on the generalization performance of the proposed multi-source domain adaptation formulation using the weighted rademacher complexity measure . <eos> empirical comparisons with existing state-of-the-art domain adaptation methods using three real-world datasets demonstrate the effectiveness of the proposed approach .
principal components analysis~ ( pca ) is often used as a feature extraction procedure . <eos> given a matrix $ x \in \mathbb { r } ^ { n \times d } $ , whose rows represent $ n $ data points with respect to $ d $ features , the top $ k $ right singular vectors of $ x $ ( the so-called \textit { eigenfeatures } ) , are arbitrary linear combinations of all available features . <eos> the eigenfeatures are very useful in data analysis , including the regularization of linear regression . <eos> enforcing sparsity on the eigenfeatures , i.e. , forcing them to be linear combinations of only a \textit { small } number of actual features ( as opposed to all available features ) , can promote better generalization error and improve the interpretability of the eigenfeatures . <eos> we present deterministic and randomized algorithms that construct such sparse eigenfeatures while \emph { provably } achieving in-sample performance comparable to regularized linear regression . <eos> our algorithms are relatively simple and practically efficient , and we demonstrate their performance on several data sets .
we consider the problem of learning rules from natural language text sources . <eos> these sources , such as news articles and web texts , are created by a writer to communicate information to a reader , where the writer and reader share substantial domain knowledge . <eos> consequently , the texts tend to be concise and mention the minimum information necessary for the reader to draw the correct conclusions . <eos> we study the problem of learning domain knowledge from such concise texts , which is an instance of the general problem of learning in the presence of missing data . <eos> however , unlike standard approaches to missing data , in this setting we know that facts are more likely to be missing from the text in cases where the reader can infer them from the facts that are mentioned combined with the domain knowledge . <eos> hence , we can explicitly model this `` missingness '' process and invert it via probabilistic inference to learn the underlying domain knowledge . <eos> this paper introduces a mention model that models the probability of facts being mentioned in the text based on what other facts have already been mentioned and domain knowledge in the form of horn clause rules . <eos> learning must simultaneously search the space of rules and learn the parameters of the mention model . <eos> we accomplish this via an application of expectation maximization within a markov logic framework . <eos> an experimental evaluation on synthetic and natural text data shows that the method can learn accurate rules and apply them to new texts to make correct inferences . <eos> experiments also show that the method out-performs the standard em approach that assumes mentions are missing at random .
in recent years semidefinite optimization has become a tool of major importance in various optimization and machine learning problems . <eos> in many of these problems the amount of data in practice is so large that there is a constant need for faster algorithms . <eos> in this work we present the first sublinear time approximation algorithm for semidefinite programs which we believe may be useful for such problems in which the size of data may cause even linear time algorithms to have prohibitive running times in practice . <eos> we present the algorithm and its analysis alongside with some theoretical lower bounds and an improved algorithm for the special problem of supervised learning of a distance metric .
knowledge-based support vector machines ( kbsvms ) incorporate advice from domain experts , which can improve generalization significantly . <eos> a major limitation that has not been fully addressed occurs when the expert advice is imperfect , which can lead to poorer models . <eos> we propose a model that extends kbsvms and is able to not only learn from data and advice , but also simultaneously improve the advice . <eos> the proposed approach is particularly effective for knowledge discovery in domains with few labeled examples . <eos> the proposed model contains bilinear constraints , and is solved using two iterative approaches : successive linear programming and a constrained concave-convex approach . <eos> experimental results demonstrate that these algorithms yield useful refinements to expert advice , as well as improve the performance of the learning algorithm overall .
in this paper , we give a new generalization error bound of multiple kernel learning ( mkl ) for a general class of regularizations . <eos> our main target in this paper is dense type regularizations including ? p-mkl that imposes ? p-mixed-norm regularization instead of ? 1-mixed-norm regularization . <eos> according to the recent numerical experiments , the sparse regularization does not necessarily show a good performance compared with dense type regularizations . <eos> motivated by this fact , this paper gives a general theoretical tool to derive fast learning rates that is applicable to arbitrary monotone norm-type regularizations in a unifying manner . <eos> as a by-product of our general result , we show a fast learning rate of ? p-mkl that is tightest among existing bounds . <eos> we also show that our general learning rate achieves the minimax lower bound . <eos> finally , we show that , when the complexities of candidate reproducing kernel hilbert spaces are inhomogeneous , dense type regularization shows better learning rate compared with sparse ? 1 regularization .
graph cut optimization is one of the standard workhorses of image segmentation since for binary random field representations of the image , it gives globally optimal results and there are efficient polynomial time implementations . <eos> often , the random field is applied over a flat partitioning of the image into non-intersecting elements , such as pixels or super-pixels . <eos> in the paper we show that if , instead of a flat partitioning , the image is represented by a hierarchical segmentation tree , then the resulting energy combining unary and boundary terms can still be optimized using graph cut ( with all the corresponding benefits of global optimality and efficiency ) . <eos> as a result of such inference , the image gets partitioned into a set of segments that may come from different layers of the tree . <eos> we apply this formulation , which we call the pylon model , to the task of semantic segmentation where the goal is to separate an image into areas belonging to different semantic classes . <eos> the experiments highlight the advantage of inference on a segmentation tree ( over a flat partitioning ) and demonstrate that the optimization in the pylon model is able to flexibly choose the level of segmentation across the image . <eos> overall , the proposed system has superior segmentation accuracy on several datasets ( graz-02 , stanford background ) compared to previously suggested approaches .
many clustering techniques aim at optimizing empirical criteria that are of the form of a u-statistic of degree two . <eos> given a measure of dissimilarity between pairs of observations , the goal is to minimize the within cluster point scatter over a class of partitions of the feature space . <eos> it is the purpose of this paper to define a general statistical framework , relying on the theory of u-processes , for studying the performance of such clustering methods . <eos> in this setup , under adequate assumptions on the complexity of the subsets forming the partition candidates , the excess of clustering risk is proved to be of the order o ( 1/\sqrt { n } ) . <eos> based on recent results related to the tail behavior of degenerate u-processes , it is also shown how to establish tighter rate bounds . <eos> model selection issues , related to the number of clusters forming the data partition in particular , are also considered .
this paper considers the problem of combining multiple models to achieve a prediction accuracy not much worse than that of the best single model for least squares regression . <eos> it is known that if the models are mis-specified , model averaging is superior to model selection . <eos> specifically , let $ n $ be the sample size , then the worst case regret of the former decays at the rate of $ o ( 1/n ) $ while the worst case regret of the latter decays at the rate of $ o ( 1/\sqrt { n } ) $ . <eos> in the literature , the most important and widely studied model averaging method that achieves the optimal $ o ( 1/n ) $ average regret is the exponential weighted model averaging ( ewma ) algorithm . <eos> however this method suffers from several limitations . <eos> the purpose of this paper is to present a new greedy model averaging procedure that improves ewma . <eos> we prove strong theoretical guarantees for the new procedure and illustrate our theoretical results with empirical examples .
paraphrase detection is the task of examining two sentences and determining whether they have the same meaning . <eos> in order to obtain high accuracy on this task , thorough syntactic and semantic analysis of the two statements is needed . <eos> we introduce a method for paraphrase detection based on recursive autoencoders ( rae ) . <eos> our unsupervised raes are based on a novel unfolding objective and learn feature vectors for phrases in syntactic trees . <eos> these features are used to measure the word- and phrase-wise similarity between two sentences . <eos> since sentences may be of arbitrary length , the resulting matrix of similarity measures is of variable size . <eos> we introduce a novel dynamic pooling layer which computes a fixed-sized representation from the variable-sized matrices . <eos> the pooled representation is then used as input to a classifier . <eos> our method outperforms other state-of-the-art approaches on the challenging msrp paraphrase corpus .
many species show avoidance reactions in response to looming object approaches . <eos> in locusts , the corresponding escape behavior correlates with the activity of the lobula giant movement detector ( lgmd ) neuron . <eos> during an object approach , its firing rate was reported to gradually increase until a peak is reached , and then it declines quickly . <eos> the $ \eta $ -function predicts that the lgmd activity is a product between an exponential function of angular size $ \exp ( -\theta ) $ and angular velocity $ \dot { \theta } $ , and that peak activity is reached before time-to-contact ( ttc ) . <eos> the $ \eta $ -function has become the prevailing lgmd model because it reproduces many experimental observations , and even experimental evidence for the multiplicative operation was reported . <eos> several inconsistencies remain unresolved , though . <eos> here we address these issues with a new model ( $ \psi $ -model ) , which explicitly connects $ \theta $ and $ \dot { \theta } $ to biophysical quantities . <eos> the $ \psi $ -model avoids biophysical problems associated with implementing $ \exp ( \cdot ) $ , implements the multiplicative operation of $ \eta $ via divisive inhibition , and explains why activity peaks could occur after ttc . <eos> it consistently predicts response features of the lgmd , and provides excellent fits to published experimental data , with goodness of fit measures comparable to corresponding fits with the $ \eta $ -function .
this paper presents an approach that predicts the effectiveness of hiv combination therapies by simultaneously addressing several problems affecting the available hiv clinical data sets : the different treatment backgrounds of the samples , the uneven representation of the levels of therapy experience , the missing treatment history information , the uneven therapy representation and the unbalanced therapy outcome representation . <eos> the computational validation on clinical data shows that , compared to the most commonly used approach that does not account for the issues mentioned above , our model has significantly higher predictive power . <eos> this is especially true for samples stemming from patients with longer treatment history and samples associated with rare therapies . <eos> furthermore , our approach is at least as powerful for the remaining samples .
this paper proposes a novel boosting algorithm called vadaboost which is motivated by recent empirical bernstein bounds . <eos> vadaboost iteratively minimizes a cost function that balances the sample mean and the sample variance of the exponential loss . <eos> each step of the proposed algorithm minimizes the cost efficiently by providing weighted data to a weak learner rather than requiring a brute force evaluation of all possible weak learners . <eos> thus , the proposed algorithm solves a key limitation of previous empirical bernstein boosting methods which required brute force enumeration of all possible weak learners . <eos> experimental results confirm that the new algorithm achieves the performance improvements of ebboost yet goes beyond decision stumps to handle any weak learner . <eos> significant performance gains are obtained over adaboost for arbitrary weak learners including decision trees ( cart ) .
this work considers the problem of learning the structure of multivariate linear tree models , which include a variety of directed tree graphical models with continuous , discrete , and mixed latent variables such as linear-gaussian models , hidden markov models , gaussian mixture models , and markov evolutionary trees . <eos> the setting is one where we only have samples from certain observed variables in the tree , and our goal is to estimate the tree structure ( i.e. , the graph of how the underlying hidden variables are connected to each other and to the observed variables ) . <eos> we propose the spectral recursive grouping algorithm , an efficient and simple bottom-up procedure for recovering the tree structure from independent samples of the observed variables . <eos> our finite sample size bounds for exact recovery of the tree structure reveal certain natural dependencies on underlying statistical and structural properties of the underlying joint distribution . <eos> furthermore , our sample complexity guarantees have no explicit dependence on the dimensionality of the observed variables , making the algorithm applicable to many high-dimensional settings . <eos> at the heart of our algorithm is a spectral quartet test for determining the relative topology of a quartet of variables from second-order statistics .
in discrete undirected graphical models , the conditional independence of node labels y is specified by the graph structure . <eos> we study the case where there is another input random vector x ( e.g . observed features ) such that the distribution p ( y | x ) is determined by functions of x that characterize the ( higher-order ) interactions among the y ? s . <eos> the main contribution of this paper is to learn the graph structure and the functions conditioned on x at the same time . <eos> we prove that discrete undirected graphical models with feature x are equivalent to mul- tivariate discrete models . <eos> the reparameterization of the potential functions in graphical models by conditional log odds ratios of the latter offers advantages in representation of the conditional independence structure . <eos> the functional spaces can be flexibly determined by kernels . <eos> additionally , we impose a structure lasso ( slasso ) penalty on groups of functions to learn the graph structure . <eos> these groups with overlaps are designed to enforce hierarchical function selection . <eos> in this way , we are able to shrink higher order interactions to obtain a sparse graph structure .
an accurate model of patient survival time can help in the treatment and care of cancer patients . <eos> the common practice of providing survival time estimates based only on population averages for the site and stage of cancer ignores many important individual differences among patients . <eos> in this paper , we propose a local regression method for learning patient-specific survival time distribution based on patient attributes such as blood tests and clinical assessments . <eos> when tested on a cohort of more than 2000 cancer patients , our method gives survival time predictions that are much more accurate than popular survival analysis models such as the cox and aalen regression models . <eos> our results also show that using patient-specific attributes can reduce the prediction error on survival time by as much as 20 % when compared to using cancer site and stage only .
we propose a novel class of bayesian nonparametric models for sequential data called fragmentation-coagulation processes ( fcps ) . <eos> fcps model a set of sequences using a partition-valued markov process which evolves by splitting and merging clusters . <eos> an fcp is exchangeable , projective , stationary and reversible , and its equilibrium distributions are given by the chinese restaurant process . <eos> as opposed to hidden markov models , fcps allow for flexible modelling of the number of clusters , and they avoid label switching non-identifiability problems . <eos> we develop an efficient gibbs sampler for fcps which uses uniformization and the forward-backward algorithm . <eos> our development of fcps is motivated by applications in population genetics , and we demonstrate the utility of fcps on problems of genotype imputation with phased and unphased snp data .
we present a novel approach to efficiently learn a label tree for large scale classification with many classes . <eos> the key contribution of the approach is a technique to simultaneously determine the structure of the tree and learn the classifiers for each node in the tree . <eos> this approach also allows fine grained control over the efficiency vs accuracy trade-off in designing a label tree , leading to more balanced trees . <eos> experiments are performed on large scale image classification with 10184 classes and 9 million images . <eos> we demonstrate significant improvements in test accuracy and efficiency with less training time and more balanced trees compared to the previous state of the art by bengio et al .
multiclass prediction is the problem of classifying an object into a relevant target class . <eos> we consider the problem of learning a multiclass predictor that uses only few features , and in particular , the number of used features should increase sub-linearly with the number of possible classes . <eos> this implies that features should be shared by several classes . <eos> we describe and analyze the shareboost algorithm for learning a multiclass predictor that uses few shared features . <eos> we prove that shareboost efficiently finds a predictor that uses few shared features ( if such a predictor exists ) and that it has a small generalization error . <eos> we also describe how to use shareboost for learning a non-linear predictor that has a fast evaluation time . <eos> in a series of experiments with natural data sets we demonstrate the benefits of shareboost and evaluate its success relatively to other state-of-the-art approaches .
state-of-the-art statistical methods in neuroscience have enabled us to fit mathematical models to experimental data and subsequently to infer the dynamics of hidden parameters underlying the observable phenomena . <eos> here , we develop a bayesian method for inferring the time-varying mean and variance of the synaptic input , along with the dynamics of each ion channel from a single voltage trace of a neuron . <eos> an estimation problem may be formulated on the basis of the state-space model with prior distributions that penalize large fluctuations in these parameters . <eos> after optimizing the hyperparameters by maximizing the marginal likelihood , the state-space model provides the time-varying parameters of the input signals and the ion channel states . <eos> the proposed method is tested not only on the simulated data from the hodgkin-huxley type models but also on experimental data obtained from a cortical slice in vitro .
in many experiments , the data points collected live in high-dimensional observation spaces , yet can be assigned a set of labels or parameters . <eos> in electrophysiological recordings , for instance , the responses of populations of neurons generally depend on mixtures of experimentally controlled parameters . <eos> the heterogeneity and diversity of these parameter dependencies can make visualization and interpretation of such data extremely difficult . <eos> standard dimensionality reduction techniques such as principal component analysis ( pca ) can provide a succinct and complete description of the data , but the description is constructed independent of the relevant task variables and is often hard to interpret . <eos> here , we start with the assumption that a particularly informative description is one that reveals the dependency of the high-dimensional data on the individual parameters . <eos> we show how to modify the loss function of pca so that the principal components seek to capture both the maximum amount of variance about the data , while also depending on a minimum number of parameters . <eos> we call this method demixed principal component analysis ( dpca ) as the principal components here segregate the parameter dependencies . <eos> we phrase the problem as a probabilistic graphical model , and present a fast expectation-maximization ( em ) algorithm . <eos> we demonstrate the use of this algorithm for electrophysiological data and show that it serves to demix the parameter-dependence of a neural population response .
we prove a new oracle inequality for support vector machines with gaussian rbf kernels solving the regularized least squares regression problem . <eos> to this end , we apply the modulus of smoothness . <eos> with the help of the new oracle inequality we then derive learning rates that can also be achieved by a simple data-dependent parameter selection method . <eos> finally , it turns out that our learning rates are asymptotically optimal for regression functions satisfying certain standard smoothness conditions .
kernel-based reinforcement-learning ( kbrl ) is a method for learning a decision policy from a set of sample transitions which stands out for its strong theoretical guarantees . <eos> however , the size of the approximator grows with the number of transitions , which makes the approach impractical for large problems . <eos> in this paper we introduce a novel algorithm to improve the scalability of kbrl . <eos> we resort to a special decomposition of a transition matrix , called stochastic factorization , to fix the size of the approximator while at the same time incorporating all the information contained in the data . <eos> the resulting algorithm , kernel-based stochastic factorization ( kbsf ) , is much faster but still converges to a unique solution . <eos> we derive a theoretical upper bound for the distance between the value functions computed by kbrl and kbsf . <eos> the effectiveness of our method is illustrated with computational experiments on four reinforcement-learning problems , including a difficult task in which the goal is to learn a neurostimulation policy to suppress the occurrence of seizures in epileptic rat brains . <eos> we empirically demonstrate that the proposed approach is able to compress the information contained in kbrl 's model . <eos> also , on the tasks studied , kbsf outperforms two of the most prominent reinforcement-learning algorithms , namely least-squares policy iteration and fitted q-iteration .
we consider the problem of identifying a sparse set of relevant columns and rows in a large data matrix with highly corrupted entries . <eos> this problem of identifying groups from a collection of bipartite variables such as proteins and drugs , biological species and gene sequences , malware and signatures , etc is commonly referred to as biclustering or co-clustering . <eos> despite its great practical relevance , and although several ad-hoc methods are available for biclustering , theoretical analysis of the problem is largely non-existent . <eos> the problem we consider is also closely related to structured multiple hypothesis testing , an area of statistics that has recently witnessed a flurry of activity . <eos> we make the following contributions : i ) we prove lower bounds on the minimum signal strength needed for successful recovery of a bicluster as a function of the noise variance , size of the matrix and bicluster of interest . <eos> ii ) we show that a combinatorial procedure based on the scan statistic achieves this optimal limit . <eos> iii ) we characterize the snr required by several computationally tractable procedures for biclustering including element-wise thresholding , column/row average thresholding and a convex relaxation approach to sparse singular vector decomposition .
in this paper , we derive a method to refine a bayes network diagnostic model by exploiting constraints implied by expert decisions on test ordering . <eos> at each step , the expert executes an evidence gathering test , which suggests the test 's relative diagnostic value . <eos> we demonstrate that consistency with an expert 's test selection leads to non-convex constraints on the model parameters . <eos> we incorporate these constraints by augmenting the network with nodes that represent the constraint likelihoods . <eos> gibbs sampling , stochastic hill climbing and greedy search algorithms are proposed to find a map estimate that takes into account test ordering constraints and any data available . <eos> we demonstrate our approach on diagnostic sessions from a manufacturing scenario .
there are many settings in which we wish to fit a model of the behavior of individuals but where our data consist only of aggregate information ( counts or low-dimensional contingency tables ) . <eos> this paper introduces collective graphical models -- -a framework for modeling and probabilistic inference that operates directly on the sufficient statistics of the individual model . <eos> we derive a highly-efficient gibbs sampling algorithm for sampling from the posterior distribution of the sufficient statistics conditioned on noisy aggregate observations , prove its correctness , and demonstrate its effectiveness experimentally .
we introduce a gaussian process model of functions which are additive . <eos> an additive function is one which decomposes into a sum of low-dimensional functions , each depending on only a subset of the input variables . <eos> additive gps generalize both generalized additive models , and the standard gp models which use squared-exponential kernels . <eos> hyperparameter learning in this model can be seen as bayesian hierarchical kernel learning ( hkl ) . <eos> we introduce an expressive but tractable parameterization of the kernel function , which allows efficient evaluation of all input interaction terms , whose number is exponential in the input dimension . <eos> the additional structure discoverable by this model results in increased interpretability , as well as state-of-the-art predictive power in regression tasks .
we study the problem of reconstructing an unknown matrix m of rank r and dimension d using o ( rd polylog d ) pauli measurements . <eos> this has applications in quantum state tomography , and is a non-commutative analogue of a well-known problem in compressed sensing : recovering a sparse vector from a few of its fourier coefficients . <eos> we show that almost all sets of o ( rd log^6 d ) pauli measurements satisfy the rank-r restricted isometry property ( rip ) . <eos> this implies that m can be recovered from a fixed ( `` universal '' ) set of pauli measurements , using nuclear-norm minimization ( e.g. , the matrix lasso ) , with nearly-optimal bounds on the error . <eos> a similar result holds for any class of measurements that use an orthonormal operator basis whose elements have small operator norm . <eos> our proof uses dudley 's inequality for gaussian processes , together with bounds on covering numbers obtained via entropy duality .
we consider a multi-armed bandit problem where there are two phases . <eos> the first phase is an experimentation phase where the decision maker is free to explore multiple options . <eos> in the second phase the decision maker has to commit to one of the arms and stick with it . <eos> cost is incurred during both phases with a higher cost during the experimentation phase . <eos> we analyze the regret in this setup , and both propose algorithms and provide upper and lower bounds that depend on the ratio of the duration of the experimentation phase to the duration of the commitment phase . <eos> our analysis reveals that if given the choice , it is optimal to experiment $ \theta ( \ln t ) $ steps and then commit , where $ t $ is the time horizon .
we investigate a discriminatively trained model of person-object interactions for recognizing common human actions in still images . <eos> we build on the locally order-less spatial pyramid bag-of-features model , which was shown to perform extremely well on a range of object , scene and human action recognition tasks . <eos> we introduce three principal contributions . <eos> first , we replace the standard quantized local hog/sift features with stronger discriminatively trained body part and object detectors . <eos> second , we introduce new person-object interaction features based on spatial co-occurrences of individual body parts and objects . <eos> third , we address the combinatorial problem of a large number of possible interaction pairs and propose a discriminative selection procedure using a linear support vector machine ( svm ) with a sparsity inducing regularizer . <eos> learning of action-specific body part and object interactions bypasses the difficult problem of estimating the complete human body pose configuration . <eos> benefits of the proposed model are shown on human action recognition in consumer photographs , outperforming the strong bag-of-features baseline .
we consider the problem of stratified sampling for monte-carlo integration . <eos> we model this problem in a multi-armed bandit setting , where the arms represent the strata , and the goal is to estimate a weighted average of the mean values of the arms . <eos> we propose a strategy that samples the arms according to an upper bound on their standard deviations and compare its estimation quality to an ideal allocation that would know the standard deviations of the arms . <eos> we provide two regret analyses : a distribution-dependent bound o ( n^ { -3/2 } ) that depends on a measure of the disparity of the arms , and a distribution-free bound o ( n^ { -4/3 } ) that does not . <eos> to the best of our knowledge , such a finite-time analysis is new for this problem .
inexpensive rgb-d cameras that give an rgb image together with depth data have become widely available . <eos> in this paper , we use this data to build 3d point clouds of full indoor scenes such as an office and address the task of semantic labeling of these 3d point clouds . <eos> we propose a graphical model that captures various features and contextual relations , including the local visual appearance and shape cues , object co-occurence relationships and geometric relationships . <eos> with a large number of object classes and relations , the model ? s parsimony becomes important and we address that by using multiple types of edge potentials . <eos> the model admits efficient approximate inference , and we train it using a maximum-margin learning approach . <eos> in our experiments over a total of 52 3d scenes of homes and offices ( composed from about 550 views , having 2495 segments labeled with 27 object classes ) , we get a performance of 84.06 % in labeling 17 object classes for offices , and 73.38 % in labeling 17 object classes for home scenes . <eos> finally , we applied these algorithms successfully on a mobile robot for the task of finding objects in large cluttered rooms .
we derive an instantaneous ( per-round ) data-dependent regret bound for stochastic multiarmed bandits with side information ( also known as contextual bandits ) . <eos> the scaling of our regret bound with the number of states ( contexts ) $ n $ goes as $ \sqrt { n i_ { \rho_t } ( s ; a ) } $ , where $ i_ { \rho_t } ( s ; a ) $ is the mutual information between states and actions ( the side information ) used by the algorithm at round $ t $ . <eos> if the algorithm uses all the side information , the regret bound scales as $ \sqrt { n \ln k } $ , where $ k $ is the number of actions ( arms ) . <eos> however , if the side information $ i_ { \rho_t } ( s ; a ) $ is not fully used , the regret bound is significantly tighter . <eos> in the extreme case , when $ i_ { \rho_t } ( s ; a ) = 0 $ , the dependence on the number of states reduces from linear to logarithmic . <eos> our analysis allows to provide the algorithm large amount of side information , let the algorithm to decide which side information is relevant for the task , and penalize the algorithm only for the side information that it is using de facto . <eos> we also present an algorithm for multiarmed bandits with side information with computational complexity that is a linear in the number of actions .
we present a joint image segmentation and labeling model ( jsl ) which , given a bag of figure-ground segment hypotheses extracted at multiple image locations and scales , constructs a joint probability distribution over both the compatible image interpretations ( tilings or image segmentations ) composed from those segments , and over their labeling into categories . <eos> the process of drawing samples from the joint distribution can be interpreted as first sampling tilings , modeled as maximal cliques , from a graph connecting spatially non-overlapping segments in the bag , followed by sampling labels for those segments , conditioned on the choice of a particular tiling . <eos> we learn the segmentation and labeling parameters jointly , based on maximum likelihood with a novel incremental saddle point estimation procedure . <eos> the partition function over tilings and labelings is increasingly more accurately approximated by including incorrect configurations that a not-yet-competent model rates probable during learning . <eos> we show that the proposed methodology matches the current state of the art in the stanford dataset , as well as in voc2010 , where 41.7 % accuracy on the test set is achieved .
in this paper , we consider the 'precis ' problem of sampling k representative yet diverse data points from a large dataset . <eos> this problem arises frequently in applications such as video and document summarization , exploratory data analysis , and pre-filtering . <eos> we formulate a general theory which encompasses not just traditional techniques devised for vector spaces , but also non-euclidean manifolds , thereby enabling these techniques to shapes , human activities , textures and many other image and video based datasets . <eos> we propose intrinsic manifold measures for measuring the quality of a selection of points with respect to their representative power , and their diversity . <eos> we then propose efficient algorithms to optimize the cost function using a novel annealing-based iterative alternation algorithm . <eos> the proposed formulation is applicable to manifolds of known geometry as well as to manifolds whose geometry needs to be estimated from samples . <eos> experimental results show the strength and generality of the proposed approach .
latent variable models are frequently used to identify structure in dichotomous network data , in part because they give rise to a bernoulli product likelihood that is both well understood and consistent with the notion of exchangeable random graphs . <eos> in this article we propose conservative confidence sets that hold with respect to these underlying bernoulli parameters as a function of any given partition of network nodes , enabling us to assess estimates of \emph { residual } network structure , that is , structure that can not be explained by known covariates and thus can not be easily verified by manual inspection . <eos> we demonstrate the proposed methodology by analyzing student friendship networks from the national longitudinal survey of adolescent health that include race , gender , and school year as covariates . <eos> we employ a stochastic expectation-maximization algorithm to fit a logistic regression model that includes these explanatory variables as well as a latent stochastic blockmodel component and additional node-specific effects . <eos> although maximum-likelihood estimates do not appear consistent in this context , we are able to evaluate confidence sets as a function of different blockmodel partitions , which enables us to qualitatively assess the significance of estimated residual network structure relative to a baseline , which models covariates but lacks block structure .
non-negative data are commonly encountered in numerous fields , making non-negative least squares regression ( nnls ) a frequently used tool . <eos> at least relative to its simplicity , it often performs rather well in practice . <eos> serious doubts about its usefulness arise for modern high-dimensional linear models . <eos> even in this setting - unlike first intuition may suggest - we show that for a broad class of designs , nnls is resistant to overfitting and works excellently for sparse recovery when combined with thresholding , experimentally even outperforming l1-regularization . <eos> since nnls also circumvents the delicate choice of a regularization parameter , our findings suggest that nnls may be the method of choice .
we consider the computational complexity of probabilistic inference in latent dirichlet allocation ( lda ) . <eos> first , we study the problem of finding the maximum a posteriori ( map ) assignment of topics to words , where the document 's topic distribution is integrated out . <eos> we show that , when the effective number of topics per document is small , exact inference takes polynomial time . <eos> in contrast , we show that , when a document has a large number of topics , finding the map assignment of topics to words in lda is np-hard . <eos> next , we consider the problem of finding the map topic distribution for a document , where the topic-word assignments are integrated out . <eos> we show that this problem is also np-hard . <eos> finally , we briefly discuss the problem of sampling from the posterior , showing that this is np-hard in one restricted setting , but leaving open the general question .
we introduce a novel active learning framework for video annotation . <eos> by judiciously choosing which frames a user should annotate , we can obtain highly accurate tracks with minimal user effort . <eos> we cast this problem as one of active learning , and show that we can obtain excellent performance by querying frames that , if annotated , would produce a large expected change in the estimated object track . <eos> we implement a constrained tracker and compute the expected change for putative annotations with efficient dynamic programming algorithms . <eos> we demonstrate our framework on four datasets , including two benchmark datasets constructed with key frame annotations obtained by amazon mechanical turk . <eos> our results indicate that we could obtain equivalent labels for a small fraction of the original cost .
we derive here new generalization bounds , based on rademacher complexity theory , for model selection and error estimation of linear ( kernel ) classifiers , which exploit the availability of unlabeled samples . <eos> in particular , two results are obtained : the first one shows that , using the unlabeled samples , the confidence term of the conventional bound can be reduced by a factor of three ; the second one shows that the unlabeled samples can be used to obtain much tighter bounds , by building localized versions of the hypothesis class containing the optimal classifier .
in this paper we consider general rank minimization problems with rank appearing in either objective function or constraint . <eos> we first show that a class of matrix optimization problems can be solved as lower dimensional vector optimization problems . <eos> as a consequence , we establish that a class of rank minimization problems have closed form solutions . <eos> using this result , we then propose penalty decomposition methods for general rank minimization problems . <eos> the convergence results of the pd methods have been shown in the longer version of the paper . <eos> finally , we test the performance of our methods by applying them to matrix completion and nearest low-rank correlation matrix problems . <eos> the computational results demonstrate that our methods generally outperform the existing methods in terms of solution quality and/or speed .
this paper proposes a parsing algorithm for scene understanding which includes four aspects : computing 3d scene layout , detecting 3d objects ( e.g . furniture ) , detecting 2d faces ( windows , doors etc . <eos> ) , and segmenting background . <eos> in contrast to previous scene labeling work that applied discriminative classifiers to pixels ( or super-pixels ) , we use a generative stochastic scene grammar ( ssg ) . <eos> this grammar represents the compositional structures of visual entities from scene categories , 3d foreground/background , 2d faces , to 1d lines . <eos> the grammar includes three types of production rules and two types of contextual relations . <eos> production rules : ( i ) and rules represent the decomposition of an entity into sub-parts ; ( ii ) or rules represent the switching among sub-types of an entity ; ( iii ) set rules rep- resent an ensemble of visual entities . <eos> contextual relations : ( i ) cooperative +relations represent positive links between binding entities , such as hinged faces of a object or aligned boxes ; ( ii ) competitive -relations represents negative links between competing entities , such as mutually exclusive boxes . <eos> we design an efficient mcmc inference algorithm , namely hierarchical cluster sampling , to search in the large solution space of scene configurations . <eos> the algorithm has two stages : ( i ) clustering : it forms all possible higher-level structures ( clusters ) from lower-level entities by production rules and contextual relations . <eos> ( ii ) sampling : it jumps between alternative structures ( clusters ) in each layer of the hierarchy to find the most probable configuration ( represented by a parse tree ) . <eos> in our experiment , we demonstrate the superiority of our algorithm over existing methods on public dataset . <eos> in addition , our approach achieves richer structures in the parse tree .
traditional approaches to probabilistic inference such as loopy belief propagation and gibbs sampling typically compute marginals for it all the unobserved variables in a graphical model . <eos> however , in many real-world applications the user 's interests are focused on a subset of the variables , specified by a query . <eos> in this case it would be wasteful to uniformly sample , say , one million variables when the query concerns only ten . <eos> in this paper we propose a query-specific approach to mcmc that accounts for the query variables and their generalized mutual information with neighboring variables in order to achieve higher computational efficiency . <eos> surprisingly there has been almost no previous work on query-aware mcmc . <eos> we demonstrate the success of our approach with positive experimental results on a wide range of graphical models .
skill discovery algorithms in reinforcement learning typically identify single states or regions in state space that correspond to task-specific subgoals . <eos> however , such methods do not directly address the question of how many distinct skills are appropriate for solving the tasks that the agent faces . <eos> this can be highly inefficient when many identified subgoals correspond to the same underlying skill , but are all used individually as skill goals . <eos> furthermore , skills created in this manner are often only transferable to tasks that share identical state spaces , since corresponding subgoals across tasks are not merged into a single skill goal . <eos> we show that these problems can be overcome by clustering subgoal data defined in an agent-space and using the resulting clusters as templates for skill termination conditions . <eos> clustering via a dirichlet process mixture model is used to discover a minimal , sufficient collection of portable skills .
in this paper we present an algorithm to learn a multi-label classifier which attempts at directly optimising the f-score . <eos> the key novelty of our formulation is that we explicitly allow for assortative ( submodular ) pairwise label interactions , i.e. , we can leverage the co-ocurrence of pairs of labels in order to improve the quality of prediction . <eos> prediction in this model consists of minimising a particular submodular set function , what can be accomplished exactly and efficiently via graph-cuts . <eos> learning however is substantially more involved and requires the solution of an intractable combinatorial optimisation problem . <eos> we present an approximate algorithm for this problem and prove that it is sound in the sense that it never predicts incorrect labels . <eos> we also present a nontrivial test of a sufficient condition for our algorithm to have found an optimal solution . <eos> we present experiments on benchmark multi-label datasets , which attest the value of our proposed technique . <eos> we also make available source code that enables the reproduction of our experiments .
we present theoretical and empirical results for a framework that combines the benefits of apprenticeship and autonomous reinforcement learning . <eos> our approach modifies an existing apprenticeship learning framework that relies on teacher demonstrations and does not necessarily explore the environment . <eos> the first change is replacing previously used mistake bound model learners with a recently proposed framework that melds the kwik and mistake bound supervised learning protocols . <eos> the second change is introducing a communication of expected utility from the student to the teacher . <eos> the resulting system only uses teacher traces when the agent needs to learn concepts it can not efficiently learn on its own .
multi-class gaussian process classifiers ( mgpcs ) are often affected by over-fitting problems when labeling errors occur far from the decision boundaries . <eos> to prevent this , we investigate a robust mgpc ( rmgpc ) which considers labeling errors independently of their distance to the decision boundaries . <eos> expectation propagation is used for approximate inference . <eos> experiments with several datasets in which noise is injected in the class labels illustrate the benefits of rmgpc . <eos> this method performs better than other gaussian process alternatives based on considering latent gaussian noise or heavy-tailed processes . <eos> when no noise is injected in the labels , rmgpc still performs equal or better than the other methods . <eos> finally , we show how rmgpc can be used for successfully identifying data instances which are difficult to classify accurately in practice .
we propose a new sparse bayesian model for multi-task regression and classification . <eos> the model is able to capture correlations between tasks , or more specifically a low-rank approximation of the covariance matrix , while being sparse in the features . <eos> we introduce a general family of group sparsity inducing priors based on matrix-variate gaussian scale mixtures . <eos> we show the amount of sparsity can be learnt from the data by combining an approximate inference approach with type ii maximum likelihood estimation of the hyperparameters . <eos> empirical evaluations on data sets from biology and vision demonstrate the applicability of the model , where on both regression and classification tasks it achieves competitive predictive performance compared to previously proposed methods .
there is much evidence that humans and other animals utilize a combination of model-based and model-free rl methods . <eos> although it has been proposed that these systems may dominate according to their relative statistical efficiency in different circumstances , there is little specific evidence -- especially in humans -- as to the details of this trade-off . <eos> accordingly , we examine the relative performance of different rl approaches under situations in which the statistics of reward are differentially noisy and volatile . <eos> using theory and simulation , we show that model-free td learning is relatively most disadvantaged in cases of high volatility and low noise . <eos> we present data from a decision-making experiment manipulating these parameters , showing that humans shift learning strategies in accord with these predictions . <eos> the statistical circumstances favoring model-based rl are also those that promote a high learning rate , which helps explain why , in psychology , the distinction between these strategies is traditionally conceived in terms of rule-based vs. incremental learning .
off-policy learning , the ability for an agent to learn about a policy other than the one it is following , is a key element of reinforcement learning , and in recent years there has been much work on developing temporal different ( td ) algorithms that are guaranteed to converge under off-policy sampling . <eos> it has remained an open question , however , whether anything can be said a priori about the quality of the td solution when off-policy sampling is employed with function approximation . <eos> in general the answer is no : for arbitrary off-policy sampling the error of the td solution can be unboundedly large , even when the approximator can represent the true value function well . <eos> in this paper we propose a novel approach to address this problem : we show that by considering a certain convex subset of off-policy distributions we can indeed provide guarantees as to the solution quality similar to the on-policy case . <eos> furthermore , we show that we can efficiently project on to this convex set using only samples generated from the system . <eos> the end result is a novel td algorithm that has approximation guarantees even in the case of off-policy sampling and which empirically outperforms existing td methods .
we present an efficient algorithm for the problem of online multiclass prediction with bandit feedback in the fully adversarial setting . <eos> we measure its regret with respect to the log-loss defined in \cite { abernethyr09 } , which is parameterized by a scalar \ ( \alpha\ ) . <eos> we prove that the regret of \newtron is \ ( o ( \log t ) \ ) when \ ( \alpha\ ) is a constant that does not vary with horizon \ ( t\ ) , and at most \ ( o ( t^ { 2/3 } ) \ ) if \ ( \alpha\ ) is allowed to increase to infinity with \ ( t\ ) . <eos> for \ ( \alpha\ ) = \ ( o ( \log t ) \ ) , the regret is bounded by \ ( o ( \sqrt { t } ) \ ) , thus solving the open problem of \cite { kst08 , abernethyr09 } . <eos> our algorithm is based on a novel application of the online newton method \cite { hak07 } . <eos> we test our algorithm and show it to perform well in experiments , even when \ ( \alpha\ ) is a small constant .
we propose an algorithm called sparse manifold clustering and embedding ( smce ) for simultaneous clustering and dimensionality reduction of data lying in multiple nonlinear manifolds . <eos> similar to most dimensionality reduction methods , smce finds a small neighborhood around each data point and connects each point to its neighbors with appropriate weights . <eos> the key difference is that smce finds both the neighbors and the weights automatically . <eos> this is done by solving a sparse optimization problem , which encourages selecting nearby points that lie in the same manifold and approximately span a low-dimensional affine subspace . <eos> the optimal solution encodes information that can be used for clustering and dimensionality reduction using spectral clustering and embedding . <eos> moreover , the size of the optimal neighborhood of a data point , which can be different for different points , provides an estimate of the dimension of the manifold to which the point belongs . <eos> experiments demonstrate that our method can effectively handle multiple manifolds that are very close to each other , manifolds with non-uniform sampling and holes , as well as estimate the intrinsic dimensions of the manifolds .
we analyze the convergence of gradient-based optimization algorithms whose updates depend on delayed stochastic gradient information . <eos> the main application of our results is to the development of distributed minimization algorithms where a master node performs parameter updates while worker nodes compute stochastic gradients based on local information in parallel , which may give rise to delays due to asynchrony . <eos> our main contribution is to show that for smooth stochastic problems , the delays are asymptotically negligible . <eos> in application to distributed optimization , we show $ n $ -node architectures whose optimization error in stochastic problems -- -in spite of asynchronous delays -- -scales asymptotically as $ \order ( 1 / \sqrt { nt } ) $ , which is known to be optimal even in the absence of delays .
we consider loss functions for multiclass prediction problems . <eos> we show when a multiclass loss can be expressed as a `` proper composite loss '' , which is the composition of a proper loss and a link function . <eos> we extend existing results for binary losses to multiclass losses . <eos> we determine the stationarity condition , bregman representation , order-sensitivity , existence and uniqueness of the composite representation for multiclass losses . <eos> we also show that the integral representation for binary proper losses can not be extended to multiclass losses . <eos> we subsume existing results on `` classification calibration '' by relating it to properness . <eos> we draw conclusions concerning the design of multiclass losses .
an agglomerative clustering algorithm merges the most similar pair of clusters at every iteration . <eos> the function that evaluates similarity is traditionally hand- designed , but there has been recent interest in supervised or semisupervised settings in which ground-truth clustered data is available for training . <eos> here we show how to train a similarity function by regarding it as the action-value function of a reinforcement learning problem . <eos> we apply this general method to segment images by clustering superpixels , an application that we call learning to agglomerate superpixel hierarchies ( lash ) . <eos> when applied to a challenging dataset of brain images from serial electron microscopy , lash dramatically improved segmentation accuracy when clustering supervoxels generated by state of the boundary detection algorithms . <eos> the naive strategy of directly training only supervoxel similarities and applying single linkage clustering produced less improvement .
we introduce an approach to learn discriminative visual representations while exploiting external semantic knowledge about object category relationships . <eos> given a hierarchical taxonomy that captures semantic similarity between the objects , we learn a corresponding tree of metrics ( tom ) . <eos> in this tree , we have one metric for each non-leaf node of the object hierarchy , and each metric is responsible for discriminating among its immediate subcategory children . <eos> specifically , a mahalanobis metric learned for a given node must satisfy the appropriate ( dis ) similarity constraints generated only among its subtree members ' training instances . <eos> to further exploit the semantics , we introduce a novel regularizer coupling the metrics that prefers a sparse disjoint set of features to be selected for each metric relative to its ancestor supercategory nodes ' metrics . <eos> intuitively , this reflects that visual cues most useful to distinguish the generic classes ( e.g. , feline vs. canine ) should be different than those cues most useful to distinguish their component fine-grained classes ( e.g. , persian cat vs. siamese cat ) . <eos> we validate our approach with multiple image datasets using the wordnet taxonomy , show its advantages over alternative metric learning approaches , and analyze the meaning of attribute features selected by our algorithm .
we introduce a new convergent variant of q-learning , called speedy q-learning , to address the problem of slow convergence in the standard form of the q-learning algorithm . <eos> we prove a pac bound on the performance of sql , which shows that for an mdp with n state-action pairs and the discount factor \gamma only t=o\big ( \log ( n ) / ( \epsilon^ { 2 } ( 1-\gamma ) ^ { 4 } ) \big ) steps are required for the sql algorithm to converge to an \epsilon-optimal action-value function with high probability . <eos> this bound has a better dependency on 1/\epsilon and 1/ ( 1-\gamma ) , and thus , is tighter than the best available result for q-learning . <eos> our bound is also superior to the existing results for both model-free and model-based instances of batch q-value iteration that are considered to be more efficient than the incremental methods like q-learning .
in this paper , we propose the first exact algorithm for minimizing the difference of two submodular functions ( d.s . <eos> ) , i.e. , the discrete version of the d.c. programming problem . <eos> the developed algorithm is a branch-and-bound-based algorithm which responds to the structure of this problem through the relationship between submodularity and convexity . <eos> the d.s . <eos> programming problem covers a broad range of applications in machine learning because this generalizes the optimization of a wide class of set functions . <eos> we empirically investigate the performance of our algorithm , and illustrate the difference between exact and approximate solutions respectively obtained by the proposed and existing algorithms in feature selection and discriminative structure learning .
while signal estimation under random amplitudes , phase shifts , and additive noise is studied frequently , the problem of estimating a deterministic signal under random time-warpings has been relatively unexplored . <eos> we present a novel framework for estimating the unknown signal that utilizes the action of the warping group to form an equivalence relation between signals . <eos> first , we derive an estimator for the equivalence class of the unknown signal using the notion of karcher mean on the quotient space of equivalence classes . <eos> this step requires the use of fisher-rao riemannian metric and a square-root representation of signals to enable computations of distances and means under this metric . <eos> then , we define a notion of the center of a class and show that the center of the estimated class is a consistent estimator of the underlying unknown signal . <eos> this estimation algorithm has many applications : ( 1 ) registration/alignment of functional data , ( 2 ) separation of phase/amplitude components of functional data , ( 3 ) joint demodulation and carrier estimation , and ( 4 ) sparse modeling of functional data . <eos> here we demonstrate only ( 1 ) and ( 2 ) : given signals are temporally aligned using nonlinear warpings and , thus , separated into their phase and amplitude components . <eos> the proposed method for signal alignment is shown to have state of the art performance using berkeley growth , handwritten signatures , and neuroscience spike train data .
divergence estimators based on direct approximation of density-ratios without going through separate approximation of numerator and denominator densities have been successfully applied to machine learning tasks that involve distribution comparison such as outlier detection , transfer learning , and two-sample homogeneity test . <eos> however , since density-ratio functions often possess high fluctuation , divergence estimation is still a challenging task in practice . <eos> in this paper , we propose to use relative divergences for distribution comparison , which involves approximation of relative density-ratios . <eos> since relative density-ratios are always smoother than corresponding ordinary density-ratios , our proposed method is favorable in terms of the non-parametric convergence speed . <eos> furthermore , we show that the proposed divergence estimator has asymptotic variance independent of the model complexity under a parametric setup , implying that the proposed estimator hardly overfits even with complex models . <eos> through experiments , we demonstrate the usefulness of the proposed approach .
a new le ? vy process prior is proposed for an uncountable collection of covariate- dependent feature-learning measures ; the model is called the kernel beta process ( kbp ) . <eos> available covariates are handled efficiently via the kernel construction , with covariates assumed observed with each data sample ( customer) , and latent covariates learned for each feature ( ? dish ? ) . <eos> each customer selects dishes from an infinite buffet , in a manner analogous to the beta process , with the added constraint that a customer first decides probabilistically whether to considera dish , based on the distance in covariate space between the customer and dish . <eos> if a customer does consider a particular dish , that dish is then selected probabilistically as in the beta process . <eos> the beta process is recovered as a limiting case of the kbp . <eos> an efficient gibbs sampler is developed for computations , and state-of-the-art results are presented for image processing and music analysis tasks .
we address the challenging task of decoupling material properties from lighting properties given a single image . <eos> in the last two decades virtually all works have concentrated on exploiting edge information to address this problem . <eos> we take a different route by introducing a new prior on reflectance , that models reflectance values as being drawn from a sparse set of basis colors . <eos> this results in a random field model with global , latent variables ( basis colors ) and pixel-accurate output reflectance values . <eos> we show that without edge information high-quality results can be achieved , that are on par with methods exploiting this source of information . <eos> finally , we present competitive results by integrating an additional edge model . <eos> we believe that our approach is a solid starting point for future development in this domain .
simultaneous recordings of many neurons embedded within a recurrently-connected cortical network may provide concurrent views into the dynamical processes of that network , and thus its computational function . <eos> in principle , these dynamics might be identified by purely unsupervised , statistical means . <eos> here , we show that a hidden switching linear dynamical systems ( hslds ) model -- -in which multiple linear dynamical laws approximate a nonlinear and potentially non-stationary dynamical process -- -is able to distinguish different dynamical regimes within single-trial motor cortical activity associated with the preparation and initiation of hand movements . <eos> the regimes are identified without reference to behavioural or experimental epochs , but nonetheless transitions between them correlate strongly with external events whose timing may vary from trial to trial . <eos> the hslds model also performs better than recent comparable models in predicting the firing rate of an isolated neuron based on the firing rates of others , suggesting that it captures more of the `` shared variance '' of the data . <eos> thus , the method is able to trace the dynamical processes underlying the coordinated evolution of network activity in a way that appears to reflect its computational role .
topic models are learned via a statistical model of variation within document collections , but designed to extract meaningful semantic structure . <eos> desirable traits include the ability to incorporate annotations or metadata associated with documents ; the discovery of correlated patterns of topic usage ; and the avoidance of parametric assumptions , such as manual specification of the number of topics . <eos> we propose a doubly correlated nonparametric topic ( dcnt ) model , the first model to simultaneously capture all three of these properties . <eos> the dcnt models metadata via a flexible , gaussian regression on arbitrary input features ; correlations via a scalable square-root covariance representation ; and nonparametric selection from an unbounded series of potential topics via a stick-breaking construction . <eos> we validate the semantic structure and predictive performance of the dcnt using a corpus of nips documents annotated by various metadata .
we derive an upper bound on the local rademacher complexity of lp-norm multiple kernel learning , which yields a tighter excess risk bound than global approaches . <eos> previous local approaches analyzed the case p=1 only while our analysis covers all cases $ 1\leq p\leq\infty $ , assuming the different feature mappings corresponding to the different kernels to be uncorrelated . <eos> we also show a lower bound that shows that the bound is tight , and derive consequences regarding excess loss , namely fast convergence rates of the order $ o ( n^ { -\frac { \alpha } { 1+\alpha } } ) $ , where $ \alpha $ is the minimum eigenvalue decay rate of the individual kernels .
we consider the hypothesis testing problem of detecting a shift between the means of two multivariate normal distributions in the high-dimensional setting , allowing for the data dimension p to exceed the sample size n. our contribution is a new test statistic for the two-sample test of means that integrates a random projection with the classical hotelling t squared statistic . <eos> working within a high- dimensional framework that allows ( p , n ) to tend to infinity , we first derive an asymptotic power function for our test , and then provide sufficient conditions for it to achieve greater power than other state-of-the-art tests . <eos> using roc curves generated from simulated data , we demonstrate superior performance against competing tests in the parameter regimes anticipated by our theoretical results . <eos> lastly , we illustrate an advantage of our procedure with comparisons on a high-dimensional gene expression dataset involving the discrimination of different types of cancer .
spectral clustering is based on the spectral relaxation of the normalized/ratio graph cut criterion . <eos> while the spectral relaxation is known to be loose , it has been shown recently that a non-linear eigenproblem yields a tight relaxation of the cheeger cut . <eos> in this paper , we extend this result considerably by providing a characterization of all balanced graph cuts which allow for a tight relaxation . <eos> although the resulting optimization problems are non-convex and non-smooth , we provide an efficient first-order scheme which scales to large graphs . <eos> moreover , our approach comes with the quality guarantee that given any partition as initialization the algorithm either outputs a better partition or it stops immediately .
learning theory has largely focused on two main learning scenarios : the classical statistical setting where instances are drawn i.i.d . <eos> from a fixed distribution , and the adversarial scenario whereby at every time step the worst instance is revealed to the player . <eos> it can be argued that in the real world neither of these assumptions is reasonable . <eos> we define the minimax value of a game where the adversary is restricted in his moves , capturing stochastic and non-stochastic assumptions on data . <eos> building on the sequential symmetrization approach , we define a notion of distribution-dependent rademacher complexity for the spectrum of problems ranging from i.i.d . <eos> to worst-case . <eos> the bounds let us immediately deduce variation-type bounds . <eos> we study a smoothed online learning scenario and show that exponentially small amount of noise can make function classes with infinite littlestone dimension learnable .
determining interactions between entities and the overall organization and clustering of nodes in networks is a major challenge when analyzing biological and social network data . <eos> here we extend the indian buffet process ( ibp ) , a nonparametric bayesian model , to integrate noisy interaction scores with properties of individual entities for inferring interaction networks and clustering nodes within these networks . <eos> we present an application of this method to study how micrornas regulate mrnas in cells . <eos> analysis of synthetic and real data indicates that the method improves upon prior methods , correctly recovers interactions and clusters , and provides accurate biological predictions .
policy gradient is a useful model-free reinforcement learning approach , but it tends to suffer from instability of gradient estimates . <eos> in this paper , we analyze and improve the stability of policy gradient methods . <eos> we first prove that the variance of gradient estimates in the pgpe ( policy gradients with parameter-based exploration ) method is smaller than that of the classical reinforce method under a mild assumption . <eos> we then derive the optimal baseline for pgpe , which contributes to further reducing the variance . <eos> we also theoretically show that pgpe with the optimal baseline is more preferable than reinforce with the optimal baseline in terms of the variance of gradient estimates . <eos> finally , we demonstrate the usefulness of the improved pgpe method through experiments .
we consider regularized risk minimization in a large dictionary of reproducing kernel hilbert spaces ( rkhss ) over which the target function has a sparse representation . <eos> this setting , commonly referred to as sparse multiple kernel learning ( mkl ) , may be viewed as the non-parametric extension of group sparsity in linear models . <eos> while the two dominant algorithmic strands of sparse learning , namely convex relaxations using l1 norm ( e.g. , lasso ) and greedy methods ( e.g. , omp ) , have both been rigorously extended for group sparsity , the sparse mkl literature has so farmainly adopted the former withmild empirical success . <eos> in this paper , we close this gap by proposing a group-omp based framework for sparse multiple kernel learning . <eos> unlike l1-mkl , our approach decouples the sparsity regularizer ( via a direct l0 constraint ) from the smoothness regularizer ( via rkhs norms ) which leads to better empirical performance as well as a simpler optimization procedure that only requires a black-box single-kernel solver . <eos> the algorithmic development and empirical studies are complemented by theoretical analyses in terms of rademacher generalization bounds and sparse recovery conditions analogous to those for omp [ 27 ] and group-omp [ 16 ] .
the l_1 regularized gaussian maximum likelihood estimator has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix , or alternatively the underlying graph structure of a gaussian markov random field , from very limited samples . <eos> we propose a novel algorithm for solving the resulting optimization problem which is a regularized log-determinant program . <eos> in contrast to other state-of-the-art methods that largely use first order gradient information , our algorithm is based on newton 's method and employs a quadratic approximation , but with some modifications that leverage the structure of the sparse gaussian mle problem . <eos> we show that our method is superlinearly convergent , and also present experimental results using synthetic and real application data that demonstrate the considerable improvements in performance of our method when compared to other state-of-the-art methods .
this paper introduces two new frameworks for learning action models for planning . <eos> in the mistake-bounded planning framework , the learner has access to a planner for the given model representation , a simulator , and a planning problem generator , and aims to learn a model with at most a polynomial number of faulty plans . <eos> in the planned exploration framework , the learner does not have access to a problem generator and must instead design its own problems , plan for them , and converge with at most a polynomial number of planning attempts . <eos> the paper reduces learning in these frameworks to concept learning with one-sided error and provides algorithms for successful learning in both frameworks . <eos> a specific family of hypothesis spaces is shown to be efficiently learnable in both the frameworks .
we consider latent structural versions of probit loss and ramp loss . <eos> we show that these surrogate loss functions are consistent in the strong sense that for any feature map ( finite or infinite dimensional ) they yield predictors approaching the infimum task loss achievable by any linear predictor over the given features . <eos> we also give finite sample generalization bounds ( convergence rates ) for these loss functions . <eos> these bounds suggest that probit loss converges more rapidly . <eos> however , ramp loss is more easily optimized and may ultimately be more practical .
how do people determine which elements of a set are most representative of that set ? <eos> we extend an existing bayesian measure of representativeness , which indicates the representativeness of a sample from a distribution , to define a measure of the representativeness of an item to a set . <eos> we show that this measure is formally related to a machine learning method known as bayesian sets . <eos> building on this connection , we derive an analytic expression for the representativeness of objects described by a sparse vector of binary features . <eos> we then apply this measure to a large database of images , using it to determine which images are the most representative members of different sets . <eos> comparing the resulting predictions to human judgments of representativeness provides a test of this measure with naturalistic stimuli , and illustrates how databases that are more commonly used in computer vision and machine learning can be used to evaluate psychological theories .
we discuss new methods for the recovery of signals with block-sparse structure , based on l1-minimization . <eos> our emphasis is on the efficiently computable error bounds for the recovery routines . <eos> we optimize these bounds with respect to the method parameters to construct the estimators with improved statistical properties . <eos> we justify the proposed approach with an oracle inequality which links the properties of the recovery algorithms and the best estimation performance .
this work describes a conceptually simple method for structured sparse coding and dictionary design . <eos> supposing a dictionary with k atoms , we introduce a structure as a set of penalties or interactions between every pair of atoms . <eos> we describe modifications of standard sparse coding algorithms for inference in this setting , and describe experiments showing that these algorithms are efficient . <eos> we show that interesting dictionaries can be learned for interactions that encode tree structures or locally connected structures . <eos> finally , we show that our framework allows us to learn the values of the interactions from the data , rather than having them pre-specified .
recently , mahoney and orecchia demonstrated that popular diffusion-based procedures to compute a quick approximation to the first nontrivial eigenvector of a data graph laplacian exactly solve certain regularized semi-definite programs ( sdps ) . <eos> in this paper , we extend that result by providing a statistical interpretation of their approximation procedure . <eos> our interpretation will be analogous to the manner in which l2-regularized or l1-regularized l2 regression ( often called ridge regression and lasso regression , respectively ) can be interpreted in terms of a gaussian prior or a laplace prior , respectively , on the coefficient vector of the regression problem . <eos> our framework will imply that the solutions to the mahoney-orecchia regularized sdp can be interpreted as regularized estimates of the pseudoinverse of the graph laplacian . <eos> conversely , it will imply that the solution to this regularized estimation problem can be computed very quickly by running , e.g. , the fast diffusion-based pagerank procedure for computing an approximation to the first nontrivial eigenvector of the graph laplacian . <eos> empirical results are also provided to illustrate the manner in which approximate eigenvector computation implicitly performs statistical regularization , relative to running the corresponding exact algorithm .
in this paper we describe a maximum likelihood likelihood approach for dictionary learning in the multiplicative exponential noise model . <eos> this model is prevalent in audio signal processing where it underlies a generative composite model of the power spectrogram . <eos> maximum joint likelihood estimation of the dictionary and expansion coefficients leads to a nonnegative matrix factorization problem where the itakura-saito divergence is used . <eos> the optimality of this approach is in question because the number of parameters ( which include the expansion coefficients ) grows with the number of observations . <eos> in this paper we describe a variational procedure for optimization of the marginal likelihood , i.e. , the likelihood of the dictionary where the activation coefficients have been integrated out ( given a specific prior ) . <eos> we compare the output of both maximum joint likelihood estimation ( i.e. , standard itakura-saito nmf ) and maximum marginal likelihood estimation ( mmle ) on real and synthetical datasets . <eos> the mmle approach is shown to embed automatic model order selection , akin to automatic relevance determination .
a majority of approximate dynamic programming approaches to the reinforcement learning problem can be categorized into greedy value function methods and value-based policy gradient methods . <eos> the former approach , although fast , is well known to be susceptible to the policy oscillation phenomenon . <eos> we take a fresh view to this phenomenon by casting a considerable subset of the former approach as a limiting special case of the latter . <eos> we explain the phenomenon in terms of this view and illustrate the underlying mechanism with artificial examples . <eos> we also use it to derive the constrained natural actor-critic algorithm that can interpolate between the aforementioned approaches . <eos> in addition , it has been suggested in the literature that the oscillation phenomenon might be subtly connected to the grossly suboptimal performance in the tetris benchmark problem of all attempted approximate dynamic programming methods . <eos> we report empirical evidence against such a connection and in favor of an alternative explanation . <eos> finally , we report scores in the tetris problem that improve on existing dynamic programming based results .
the group lasso is an extension of the lasso for feature selection on ( predefined ) non-overlapping groups of features . <eos> the non-overlapping group structure limits its applicability in practice . <eos> there have been several recent attempts to study a more general formulation , where groups of features are given , potentially with overlaps between the groups . <eos> the resulting optimization is , however , much more challenging to solve due to the group overlaps . <eos> in this paper , we consider the efficient optimization of the overlapping group lasso penalized problem . <eos> we reveal several key properties of the proximal operator associated with the overlapping group lasso , and compute the proximal operator by solving the smooth and convex dual problem , which allows the use of the gradient descent type of algorithms for the optimization . <eos> we have performed empirical evaluations using both synthetic and the breast cancer gene expression data set , which consists of 8,141 genes organized into ( overlapping ) gene sets . <eos> experimental results show that the proposed algorithm is more efficient than existing state-of-the-art algorithms .
motor prostheses aim to restore function to disabled patients . <eos> despite compelling proof of concept systems , barriers to clinical translation remain . <eos> one challenge is to develop a low-power , fully-implantable system that dissipates only minimal power so as not to damage tissue . <eos> to this end , we implemented a kalman-filter based decoder via a spiking neural network ( snn ) and tested it in brain-machine interface ( bmi ) experiments with a rhesus monkey . <eos> the kalman filter was trained to predict the arm ? s velocity and mapped on to the snn using the neural engineer- ing framework ( nef ) . <eos> a 2,000-neuron embedded matlab snn implementation runs in real-time and its closed-loop performance is quite comparable to that of the standard kalman filter . <eos> the success of this closed-loop decoder holds promise for hardware snn implementations of statistical signal processing algorithms on neuromorphic chips , which may offer power savings necessary to overcome a major obstacle to the successful clinical translation of neural motor prostheses .
using the $ \ell_1 $ -norm to regularize the estimation of the parameter vector of a linear model leads to an unstable estimator when covariates are highly correlated . <eos> in this paper , we introduce a new penalty function which takes into account the correlation of the design matrix to stabilize the estimation . <eos> this norm , called the trace lasso , uses the trace norm of the selected covariates , which is a convex surrogate of their rank , as the criterion of model complexity . <eos> we analyze the properties of our norm , describe an optimization algorithm based on reweighted least-squares , and illustrate the behavior of this norm on synthetic data , showing that it is more adapted to strong correlations than competing methods such as the elastic net .
the goal of this paper is to investigate the advantages and disadvantages of learning in banach spaces over hilbert spaces . <eos> while many works have been carried out in generalizing hilbert methods to banach spaces , in this paper , we consider the simple problem of learning a parzen window classifier in a reproducing kernel banach space ( rkbs ) -- -which is closely related to the notion of embedding probability measures into an rkbs -- -in order to carefully understand its pros and cons over the hilbert space classifier . <eos> we show that while this generalization yields richer distance measures on probabilities compared to its hilbert space counterpart , it however suffers from serious computational drawback limiting its practical applicability , which therefore demonstrates the need for developing efficient learning algorithms in banach spaces .
brain-computer interfaces ( bcis ) use brain signals to convey a user ? s intent . <eos> some bci approaches begin by decoding kinematic parameters of movements from brain signals , and then proceed to using these signals , in absence of movements , to allow a user to control an output . <eos> recent results have shown that electrocorticographic ( ecog ) recordings from the surface of the brain in humans can give information about kinematic parameters ( e.g. , hand velocity or finger flexion ) . <eos> the decoding approaches in these demonstrations usually employed classical classification/regression algorithms that derive a linear mapping between brain signals and outputs . <eos> however , they typically only incorporate little prior information about the target kinematic parameter . <eos> in this paper , we show that different types of anatomical constraints that govern finger flexion can be exploited in this context . <eos> specifically , we incorporate these constraints in the construction , structure , and the probabilistic functions of a switched non-parametric dynamic system ( snds ) model . <eos> we then apply the resulting snds decoder to infer the flexion of individual fingers from the same ecog dataset used in a recent study . <eos> our results show that the application of the proposed model , which incorporates anatomical constraints , improves decoding performance compared to the results in the previous work . <eos> thus , the results presented in this paper may ultimately lead to neurally controlled hand prostheses with full fine-grained finger articulation .
we show an application of a tree structure for approximate inference in graphical models using the expectation propagation algorithm . <eos> these approximations are typically used over graphs with short-range cycles . <eos> we demonstrate that these approximations also help in sparse graphs with long-range loops , as the ones used in coding theory to approach channel capacity . <eos> for asymptotically large sparse graph , the expectation propagation algorithm together with the tree structure yields a completely disconnected approximation to the graphical model but , for for finite-length practical sparse graphs , the tree structure approximation to the code graph provides accurate estimates for the marginal of each variable .
inference in matrix-variate gaussian models has major applications for multioutput prediction and joint learning of row and column covariances from matrixvariate data . <eos> here , we discuss an approach for efficient inference in such models that explicitly account for iid observation noise . <eos> computational tractability can be retained by exploiting the kronecker product between row and column covariance matrices . <eos> using this framework , we show how to generalize the graphical lasso in order to learn a sparse inverse covariance between features while accounting for a low-rank confounding covariance between samples . <eos> we show practical utility on applications to biology , where we model covariances with more than 100,000 dimensions . <eos> we find greater accuracy in recovering biological network structures and are able to better reconstruct the confounders .
this paper considers the problem of embedding directed graphs in euclidean space while retaining directional information . <eos> we model the observed graph as a sample from a manifold endowed with a vector field , and we design an algo- rithm that separates and recovers the features of this process : the geometry of the manifold , the data density and the vector field . <eos> the algorithm is motivated by our analysis of laplacian-type operators and their continuous limit as generators of diffusions on a manifold . <eos> we illustrate the recovery algorithm on both artificially constructed and real data .
approximate inference is an important technique for dealing with large , intractable graphical models based on the exponential family of distributions . <eos> we extend the idea of approximate inference to the t-exponential family by defining a new t-divergence . <eos> this divergence measure is obtained via convex duality between the log-partition function of the t-exponential family and a new t-entropy . <eos> we illustrate our approach on the bayes point machine with a student 's t-prior .
a number of recent scientific and engineering problems require signals to be decomposed into a product of a slowly varying positive envelope and a quickly varying carrier whose instantaneous frequency also varies slowly over time . <eos> although signal processing provides algorithms for so-called amplitude- and frequency-demodulation ( afd ) , there are well known problems with all of the existing methods . <eos> motivated by the fact that afd is ill-posed , we approach the problem using probabilistic inference . <eos> the new approach , called probabilistic amplitude and frequency demodulation ( pafd ) , models instantaneous frequency using an auto-regressive generalization of the von mises distribution , and the envelopes using gaussian auto-regressive dynamics with a positivity constraint . <eos> a novel form of expectation propagation is used for inference . <eos> we demonstrate that although pafd is computationally demanding , it outperforms previous approaches on synthetic and real signals in clean , noisy and missing data settings .
we consider a general inference setting for discrete probabilistic graphical models where we seek maximum a posteriori ( map ) estimates for a subset of the random variables ( max nodes ) , marginalizing over the rest ( sum nodes ) . <eos> we present a hybrid message-passing algorithm to accomplish this . <eos> the hybrid algorithm passes a mix of sum and max messages depending on the type of source node ( sum or max ) . <eos> we derive our algorithm by showing that it falls out as the solution of a particular relaxation of a variational framework . <eos> we further show that the expectation maximization algorithm can be seen as an approximation to our algorithm . <eos> experimental results on synthetic and real-world datasets , against several baselines , demonstrate the efficacy of our proposed algorithm .
we consider the problem of bayesian inference for continuous time multi-stable stochastic systems which can change both their diffusion and drift parameters at discrete times . <eos> we propose exact inference and sampling methodologies for two specific cases where the discontinuous dynamics is given by a poisson process and a two-state markovian switch . <eos> we test the methodology on simulated data , and apply it to two real data sets in finance and systems biology . <eos> our experimental results show that the approach leads to valid inferences and non-trivial insights .
learning minimum volume sets of an underlying nominal distribution is a very effective approach to anomaly detection . <eos> several approaches to learning minimum volume sets have been proposed in the literature , including the k-point nearest neighbor graph ( k-knng ) algorithm based on the geometric entropy minimization ( gem ) principle [ 4 ] . <eos> the k-knng detector , while possessing several desirable characteristics , suffers from high computation complexity , and in [ 4 ] a simpler heuristic approximation , the leave-one-out knng ( l1o-knng ) was proposed . <eos> in this paper , we propose a novel bipartite k-nearest neighbor graph ( bp-knng ) anomaly detection scheme for estimating minimum volume sets . <eos> our bipartite estimator retains all the desirable theoretical properties of the k-knng , while being computationally simpler than the k-knng and the surrogate l1o-knng detectors . <eos> we show that bp-knng is asymptotically consistent in recovering the p-value of each test point . <eos> experimental results are given that illustrate the superior performance of bp-knng as compared to the l1o-knng and other state of the art anomaly detection schemes .
monte-carlo tree search ( mcts ) has proven to be a powerful , generic planning technique for decision-making in single-agent and adversarial environments . <eos> the stochastic nature of the monte-carlo simulations introduces errors in the value estimates , both in terms of bias and variance . <eos> whilst reducing bias ( typically through the addition of domain knowledge ) has been studied in the mcts literature , comparatively little effort has focused on reducing variance . <eos> this is somewhat surprising , since variance reduction techniques are a well-studied area in classical statistics . <eos> in this paper , we examine the application of some standard techniques for variance reduction in mcts , including common random numbers , antithetic variates and control variates . <eos> we demonstrate how these techniques can be applied to mcts and explore their efficacy on three different stochastic , single-agent settings : pig , ca n't stop and dominion .
neurons in the neocortex code and compute as part of a locally interconnected population . <eos> large-scale multi-electrode recording makes it possible to access these population processes empirically by fitting statistical models to unaveraged data . <eos> what statistical structure best describes the concurrent spiking of cells within a local network ? <eos> we argue that in the cortex , where firing exhibits extensive correlations in both time and space and where a typical sample of neurons still reflects only a very small fraction of the local population , the most appropriate model captures shared variability by a low-dimensional latent process evolving with smooth dynamics , rather than by putative direct coupling . <eos> we test this claim by comparing a latent dynamical model with realistic spiking observations to coupled generalised linear spike-response models ( glms ) using cortical recordings . <eos> we find that the latent dynamical approach outperforms the glm in terms of goodness-of-fit , and reproduces the temporal correlations in the data more accurately . <eos> we also compare models whose observations models are either derived from a gaussian or point-process models , finding that the non-gaussian model provides slightly better goodness-of-fit and more realistic population spike counts .
in this paper , we address the problem of learning the structure of a pairwise graphical model from samples in a high-dimensional setting . <eos> our first main result studies the sparsistency , or consistency in sparsity pattern recovery , properties of a forward-backward greedy algorithm as applied to general statistical models . <eos> as a special case , we then apply this algorithm to learn the structure of a discrete graphical model via neighborhood estimation . <eos> as a corollary of our general result , we derive sufficient conditions on the number of samples n , the maximum node-degree d and the problem size p , as well as other conditions on the model parameters , so that the algorithm recovers all the edges with high probability . <eos> our result guarantees graph selection for samples scaling as n = omega ( d log ( p ) ) , in contrast to existing convex-optimization based algorithms that require a sample complexity of omega ( d^2 log ( p ) ) . <eos> further , the greedy algorithm only requires a restricted strong convexity condition which is typically milder than irrepresentability assumptions . <eos> we corroborate these results using numerical simulations at the end .
topic models have the potential to improve search and browsing by extracting useful semantic themes from web pages and other text documents . <eos> when learned topics are coherent and interpretable , they can be valuable for faceted browsing , results set diversity analysis , and document retrieval . <eos> however , when dealing with small collections or noisy text ( e.g . web search result snippets or blog posts ) , learned topics can be less coherent , less interpretable , and less useful . <eos> to overcome this , we propose two methods to regularize the learning of topic models . <eos> our regularizers work by creating a structured prior over words that reflect broad patterns in the external data . <eos> using thirteen datasets we show that both regularizers improve topic coherence and interpretability while learning a faithful representation of the collection of interest . <eos> overall , this work makes topic models more useful across a broader range of text data .
multi-task learning ( mtl ) learns multiple related tasks simultaneously to improve generalization performance . <eos> alternating structure optimization ( aso ) is a popular mtl method that learns a shared low-dimensional predictive structure on hypothesis spaces from multiple related tasks . <eos> it has been applied successfully in many real world applications . <eos> as an alternative mtl approach , clustered multi-task learning ( cmtl ) assumes that multiple tasks follow a clustered structure , i.e. , tasks are partitioned into a set of groups where tasks in the same group are similar to each other , and that such a clustered structure is unknown a priori . <eos> the objectives in aso and cmtl differ in how multiple tasks are related . <eos> interestingly , we show in this paper the equivalence relationship between aso and cmtl , providing significant new insights into aso and cmtl as well as their inherent relationship . <eos> the cmtl formulation is non-convex , and we adopt a convex relaxation to the cmtl formulation . <eos> we further establish the equivalence relationship between the proposed convex relaxation of cmtl and an existing convex relaxation of aso , and show that the proposed convex cmtl formulation is significantly more efficient especially for high-dimensional data . <eos> in addition , we present three algorithms for solving the convex cmtl formulation . <eos> we report experimental results on benchmark datasets to demonstrate the efficiency of the proposed algorithms .
recent deep learning and unsupervised feature learning systems that learn from unlabeled data have achieved high performance in benchmarks by using extremely large architectures with many features ( hidden units ) at each layer . <eos> unfortunately , for such large architectures the number of parameters usually grows quadratically in the width of the network , thus necessitating hand-coded `` local receptive fields '' that limit the number of connections from lower level features to higher ones ( e.g. , based on spatial locality ) . <eos> in this paper we propose a fast method to choose these connections that may be incorporated into a wide variety of unsupervised training methods . <eos> specifically , we choose local receptive fields that group together those low-level features that are most similar to each other according to a pairwise similarity metric . <eos> this approach allows us to harness the advantages of local receptive fields ( such as improved scalability , and reduced data requirements ) when we do not know how to specify such receptive fields by hand or where our unsupervised training algorithm has no obvious generalization to a topographic setting . <eos> we produce results showing how this method allows us to use even simple unsupervised training algorithms to train successful multi-layered etworks that achieve state-of-the-art results on cifar and stl datasets : 82.0 % and 60.1 % accuracy , respectively .
multi-instance learning ( mil ) considers input as bags of instances , in which labels are assigned to the bags . <eos> mil is useful in many real-world applications . <eos> for example , in image categorization semantic meanings ( labels ) of an image mostly arise from its regions ( instances ) instead of the entire image ( bag ) . <eos> existing mil methods typically build their models using the bag-to-bag ( b2b ) distance , which are often computationally expensive and may not truly reflect the semantic similarities . <eos> to tackle this , in this paper we approach mil problems from a new perspective using the class-to-bag ( c2b ) distance , which directly assesses the relationships between the classes and the bags . <eos> taking into account the two major challenges in mil , high heterogeneity on data and weak label association , we propose a novel maximum margin multi-instance learning ( m3 i ) approach to parameterize the c2b distance by introducing the class specific distance metrics and the locally adaptive significance coefficients . <eos> we apply our new approach to the automatic image categorization tasks on three ( one single-label and two multilabel ) benchmark data sets . <eos> extensive experiments have demonstrated promising results that validate the proposed method .
in standard gaussian process regression input locations are assumed to be noise free . <eos> we present a simple yet effective gp model for training on input points corrupted by i.i.d . <eos> gaussian noise . <eos> to make computations tractable we use a local linear expansion about each input point . <eos> this allows the input noise to be recast as output noise proportional to the squared gradient of the gp posterior mean . <eos> the input noise variances are inferred from the data as extra hyperparameters . <eos> they are trained alongside other hyperparameters by the usual method of maximisation of the marginal likelihood . <eos> training uses an iterative scheme , which alternates between optimising the hyperparameters and calculating the posterior gradient . <eos> analytic predictive moments can then be found for gaussian distributed test points . <eos> we compare our model to others over a range of different regression problems and show that it improves over current methods .
most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions . <eos> while regionlevel models often feature dense pairwise connectivity , pixel-level models are considerably larger and have only permitted sparse graph structures . <eos> in this paper , we consider fully connected crf models defined on the complete set of pixels in an image . <eos> the resulting graphs have billions of edges , making traditional inference algorithms impractical . <eos> our main contribution is a highly efficient approximate inference algorithm for fully connected crf models in which the pairwise edge potentials are defined by a linear combination of gaussian kernels . <eos> our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy .
applications such as robot control and wireless communication require planning under uncertainty . <eos> partially observable markov decision processes ( pomdps ) plan policies for single agents under uncertainty and their decentralized versions ( dec-pomdps ) find a policy for multiple agents . <eos> the policy in infinite-horizon pomdp and dec-pomdp problems has been represented as finite state controllers ( fscs ) . <eos> we introduce a novel class of periodic fscs , composed of layers connected only to the previous and next layer . <eos> our periodic fsc method finds a deterministic finite-horizon policy and converts it to an initial periodic infinite-horizon policy . <eos> this policy is optimized by a new infinite-horizon algorithm to yield deterministic periodic policies , and by a new expectation maximization algorithm to yield stochastic periodic policies . <eos> our method yields better results than earlier planning methods and can compute larger solutions than with regular fscs .
fitted value iteration ( fvi ) with ordinary least squares regression is known to diverge . <eos> we present a new method , `` expansion-constrained ordinary least squares '' ( ecols ) , that produces a linear approximation but also guarantees convergence when used with fvi . <eos> to ensure convergence , we constrain the least squares regression operator to be a non-expansion in the infinity-norm . <eos> we show that the space of function approximators that satisfy this constraint is more rich than the space of `` averagers , '' we prove a minimax property of the ecols residual error , and we give an efficient algorithm for computing the coefficients of ecols based on constraint generation . <eos> we illustrate the algorithmic convergence of fvi with ecols in a suite of experiments , and discuss its properties .
an important task in exploring and analyzing real-world data sets is to detect unusual and interesting phenomena . <eos> in this paper , we study the group anomaly detection problem . <eos> unlike traditional anomaly detection research that focuses on data points , our goal is to discover anomalous aggregated behaviors of groups of points . <eos> for this purpose , we propose the flexible genre model ( fgm ) . <eos> fgm is designed to characterize data groups at both the point level and the group level so as to detect various types of group anomalies . <eos> we evaluate the effectiveness of fgm on both synthetic and real data sets including images and turbulence data , and show that it is superior to existing approaches in detecting group anomalies .
budgeted optimization involves optimizing an unknown function that is costly to evaluate by requesting a limited number of function evaluations at intelligently selected inputs . <eos> typical problem formulations assume that experiments are selected one at a time with a limited total number of experiments , which fail to capture important aspects of many real-world problems . <eos> this paper defines a novel problem formulation with the following important extensions : 1 ) allowing for concurrent experiments ; 2 ) allowing for stochastic experiment durations ; and 3 ) placing constraints on both the total number of experiments and the total experimental time . <eos> we develop both offline and online algorithms for selecting concurrent experiments in this new setting and provide experimental results on a number of optimization benchmarks . <eos> the results show that our algorithms produce highly effective schedules compared to natural baselines .
focusing on short term trend prediction in a financial context , we consider the problem of selective prediction whereby the predictor can abstain from prediction in order to improve performance . <eos> we examine two types of selective mechanisms for hmm predictors . <eos> the first is a rejection in the spirit of chow ? s well-known ambiguity principle . <eos> the second is a specialized mechanism for hmms that identifies low quality hmm states and abstain from prediction in those states . <eos> we call this model selective hmm ( shmm ) . <eos> in both approaches we can trade-off prediction coverage to gain better accuracy in a controlled manner . <eos> we compare performance of the ambiguity-based rejection technique with that of the shmm approach . <eos> our results indicate that both methods are effective , and that the shmm model is superior .
motivated by applications in electronic games as well as teaching systems , we investigate the problem of dynamic difficulty adjustment . <eos> the task here is to repeatedly find a game difficulty setting that is neither ` too easy ' and bores the player , nor ` too difficult ' and overburdens the player . <eos> the contributions of this paper are ( $ i $ ) formulation of difficulty adjustment as an online learning problem on partially ordered sets , ( $ ii $ ) an exponential update algorithm for dynamic difficulty adjustment , ( $ iii $ ) a bound on the number of wrong difficulty settings relative to the best static setting chosen in hindsight , and ( $ iv $ ) an empirical investigation of the algorithm when playing against adversaries .
we provide rigorous guarantees on learning with the weighted trace-norm under arbitrary sampling distributions . <eos> we show that the standard weighted-trace norm might fail when the sampling distribution is not a product distribution ( i.e . when row and column indexes are not selected independently ) , present a corrected variant for which we establish strong learning guarantees , and demonstrate that it works better in practice . <eos> we provide guarantees when weighting by either the true or empirical sampling distribution , and suggest that even if the true distribution is known ( or is uniform ) , weighting by the empirical distribution may be beneficial .
we consider a global optimization problem of a deterministic function f in a semimetric space , given a finite budget of n evaluations . <eos> the function f is assumed to be locally smooth ( around one of its global maxima ) with respect to a semi-metric ? . <eos> we describe two algorithms based on optimistic exploration that use a hierarchical partitioning of the space at all scales . <eos> a first contribution is an algorithm , doo , that requires the knowledge of ? . <eos> we report a finite-sample performance bound in terms of a measure of the quantity of near-optimal states . <eos> we then define a second algorithm , soo , which does not require the knowledge of the semimetric ? <eos> under which f is smooth , and whose performance is almost as good as doo optimally-fitted .
we introduce a variational bayesian inference algorithm which can be widely applied to sparse linear models . <eos> the algorithm is based on the spike and slab prior which , from a bayesian perspective , is the golden standard for sparse inference . <eos> we apply the method to a general multi-task and multiple kernel learning model in which a common set of gaussian process functions is linearly combined with task-specific sparse weights , thus inducing relation between tasks . <eos> this model unifies several sparse linear models , such as generalized linear models , sparse factor analysis and matrix factorization with missing values , so that the variational algorithm can be applied to all these cases . <eos> we demonstrate our approach in multi-output gaussian process regression , multi-class classification , image processing applications and collaborative filtering .
we consider the problem of classification using similarity/distance functions over data . <eos> specifically , we propose a framework for defining the goodness of a ( dis ) similarity function with respect to a given learning task and propose algorithms that have guaranteed generalization properties when working with such good functions . <eos> our framework unifies and generalizes the frameworks proposed by ( balcan-blum 2006 ) and ( wang et al 2007 ) . <eos> an attractive feature of our framework is its adaptability to data - we do not promote a fixed notion of goodness but rather let data dictate it . <eos> we show , by giving theoretical guarantees that the goodness criterion best suited to a problem can itself be learned which makes our approach applicable to a variety of domains and problems . <eos> we propose a landmarking-based approach to obtaining a classifier from such learned goodness criteria . <eos> we then provide a novel diversity based heuristic to perform task-driven selection of landmark points instead of random selection . <eos> we demonstrate the effectiveness of our goodness criteria learning method as well as the landmark selection heuristic on a variety of similarity-based learning datasets and benchmark uci datasets on which our method consistently outperforms existing approaches by a significant margin .
compositional models provide an elegant formalism for representing the visual appearance of highly variable objects . <eos> while such models are appealing from a theoretical point of view , it has been difficult to demonstrate that they lead to performance advantages on challenging datasets . <eos> here we develop a grammar model for person detection and show that it outperforms previous high-performance systems on the pascal benchmark . <eos> our model represents people using a hierarchy of deformable parts , variable structure and an explicit model of occlusion for partially visible objects . <eos> to train the model , we introduce a new discriminative framework for learning structured prediction models from weakly-labeled data .
learning problems such as logistic regression are typically formulated as pure optimization problems defined on some loss function . <eos> we argue that this view ignores the fact that the loss function depends on stochastically generated data which in turn determines an intrinsic scale of precision for statistical estimation . <eos> by considering the statistical properties of the update variables used during the optimization ( e.g . gradients ) , we can construct frequentist hypothesis tests to determine the reliability of these updates . <eos> we utilize subsets of the data for computing updates , and use the hypothesis tests for determining when the batch-size needs to be increased . <eos> this provides computational benefits and avoids overfitting by stopping when the batch-size has become equal to size of the full dataset . <eos> moreover , the proposed algorithms depend on a single interpretable parameter ? <eos> the probability for an update to be in the wrong direction ? <eos> which is set to a single value across all algorithms and datasets . <eos> in this paper , we illustrate these ideas on three l1 regularized coordinate algorithms : l1 -regularized l2 -loss svms , l1 -regularized logistic regression , and the lasso , but we emphasize that the underlying methods are much more generally applicable .
probabilistic programming languages allow modelers to specify a stochastic process using syntax that resembles modern programming languages . <eos> because the program is in machine-readable format , a variety of techniques from compiler design and program analysis can be used to examine the structure of the distribution represented by the probabilistic program . <eos> we show how nonstandard interpretations of probabilistic programs can be used to craft efficient inference algorithms : information about the structure of a distribution ( such as gradients or dependencies ) is generated as a monad-like side computation while executing the program . <eos> these interpretations can be easily coded using special-purpose objects and operator overloading . <eos> we implement two examples of nonstandard interpretations in two different languages , and use them as building blocks to construct inference algorithms : automatic differentiation , which enables gradient based methods , and provenance tracking , which enables efficient construction of global proposals .
classical boosting algorithms , such as adaboost , build a strong classifier without concern about the computational cost . <eos> some applications , in particular in computer vision , may involve up to millions of training examples and features . <eos> in such contexts , the training time may become prohibitive . <eos> several methods exist to accelerate training , typically either by sampling the features , or the examples , used to train the weak learners . <eos> even if those methods can precisely quantify the speed improvement they deliver , they offer no guarantee of being more efficient than any other , given the same amount of time . <eos> this paper aims at shading some light on this problem , i.e . given a fixed amount of time , for a particular problem , which strategy is optimal in order to reduce the training loss the most . <eos> we apply this analysis to the design of new algorithms which estimate on the fly at every iteration the optimal trade-off between the number of samples and the number of features to look at in order to maximize the expected loss reduction . <eos> experiments in object recognition with two standard computer vision data-sets show that the adaptive methods we propose outperform basic sampling and state-of-the-art bandit methods .
biased labelers are a systemic problem in crowdsourcing , and a comprehensive toolbox for handling their responses is still being developed . <eos> a typical crowdsourcing application can be divided into three steps : data collection , data curation , and learning . <eos> at present these steps are often treated separately . <eos> we present bayesian bias mitigation for crowdsourcing ( bbmc ) , a bayesian model to unify all three . <eos> most data curation methods account for the { \it effects } of labeler bias by modeling all labels as coming from a single latent truth . <eos> our model captures the { \it sources } of bias by describing labelers as influenced by shared random effects . <eos> this approach can account for more complex bias patterns that arise in ambiguous or hard labeling tasks and allows us to merge data curation and learning into a single computation . <eos> active learning integrates data collection with learning , but is commonly considered infeasible with gibbs sampling inference . <eos> we propose a general approximation strategy for markov chains to efficiently quantify the effect of a perturbation on the stationary distribution and specialize this approach to active learning . <eos> experiments show bbmc to outperform many common heuristics .
we consider the problem of assigning class labels to an unlabeled test data set , given several labeled training data sets drawn from similar distributions . <eos> this problem arises in several applications where data distributions fluctuate because of biological , technical , or other sources of variation . <eos> we develop a distribution-free , kernel-based approach to the problem . <eos> this approach involves identifying an appropriate reproducing kernel hilbert space and optimizing a regularized empirical risk over the space . <eos> we present generalization error analysis , describe universal kernels , and establish universal consistency of the proposed methodology . <eos> experimental results on flow cytometry data are presented .
we introduce hierarchically supervised latent dirichlet allocation ( hslda ) , a model for hierarchically and multiply labeled bag-of-word data . <eos> examples of such data include web pages and their placement in directories , product descriptions and associated categories from product hierarchies , and free-text clinical records and their assigned diagnosis codes . <eos> out-of-sample label prediction is the primary goal of this work , but improved lower-dimensional representations of the bag-of-word data are also of interest . <eos> we demonstrate hslda on large-scale data from clinical document labeling and retail product categorization tasks . <eos> we show that leveraging the structure from hierarchical labels improves out-of-sample label prediction substantially when compared to models that do not .
speech conveys different yet mixed information ranging from linguistic to speaker-specific components , and each of them should be exclusively used in a specific task . <eos> however , it is extremely difficult to extract a specific information component given the fact that nearly all existing acoustic representations carry all types of speech information . <eos> thus , the use of the same representation in both speech and speaker recognition hinders a system from producing better performance due to interference of irrelevant information . <eos> in this paper , we present a deep neural architecture to extract speaker-specific information from mfccs . <eos> as a result , a multi-objective loss function is proposed for learning speaker-specific characteristics and regularization via normalizing interference of non-speaker related information and avoiding information loss . <eos> with ldc benchmark corpora and a chinese speech corpus , we demonstrate that a resultant speaker-specific representation is insensitive to text/languages spoken and environmental mismatches and hence outperforms mfccs and other state-of-the-art techniques in speaker recognition . <eos> we discuss relevant issues and relate our approach to previous work .
computational analyses of dendritic computations often assume stationary inputs to neurons , ignoring the pulsatile nature of spike-based communication between neurons and the moment-to-moment fluctuations caused by such spiking inputs . <eos> conversely , circuit computations with spiking neurons are usually formalized without regard to the rich nonlinear nature of dendritic processing . <eos> here we address the computational challenge faced by neurons that compute and represent analogue quantities but communicate with digital spikes , and show that reliable computation of even purely linear functions of inputs can require the interplay of strongly nonlinear subunits within the postsynaptic dendritic tree . <eos> our theory predicts a matching of dendritic nonlinearities and synaptic weight distributions to the joint statistics of presynaptic inputs . <eos> this approach suggests normative roles for some puzzling forms of nonlinear dendritic dynamics and plasticity .
we consider the minimization of a convex objective function defined on a hilbert space , which is only available through unbiased estimates of its gradients . <eos> this problem includes standard machine learning algorithms such as kernel logistic regression and least-squares regression , and is commonly referred to as a stochastic approximation problem in the operations research community . <eos> we provide a non-asymptotic analysis of the convergence of two well-known algorithms , stochastic gradient descent ( a.k.a.~robbins-monro algorithm ) as well as a simple modification where iterates are averaged ( a.k.a.~polyak-ruppert averaging ) . <eos> our analysis suggests that a learning rate proportional to the inverse of the number of iterations , while leading to the optimal convergence rate in the strongly convex case , is not robust to the lack of strong convexity or the setting of the proportionality constant . <eos> this situation is remedied when using slower decays together with averaging , robustly leading to the optimal rate of convergence . <eos> we illustrate our theoretical results with simulations on synthetic and standard datasets .
many functional descriptions of spiking neurons assume a cascade structure where inputs are passed through an initial linear filtering stage that produces a low-dimensional signal that drives subsequent nonlinear stages . <eos> this paper presents a novel and systematic parameter estimation procedure for such models and applies the method to two neural estimation problems : ( i ) compressed-sensing based neural mapping from multi-neuron excitation , and ( ii ) estimation of neural receptive yields in sensory neurons . <eos> the proposed estimation algorithm models the neurons via a graphical model and then estimates the parameters in the model using a recently-developed generalized approximate message passing ( gamp ) method . <eos> the gamp method is based on gaussian approximations of loopy belief propagation . <eos> in the neural connectivity problem , the gamp-based method is shown to be computational efficient , provides a more exact modeling of the sparsity , can incorporate nonlinearities in the output and significantly outperforms previous compressed-sensing methods . <eos> for the receptive field estimation , the gamp method can also exploit inherent structured sparsity in the linear weights . <eos> the method is validated on estimation of linear nonlinear poisson ( lnp ) cascade models for receptive fields of salamander retinal ganglion cells .
many studies have uncovered evidence that visual cortex contains specialized regions involved in processing faces but not other object classes . <eos> recent electrophysiology studies of cells in several of these specialized regions revealed that at least some of these regions are organized in a hierarchical manner with viewpoint-specific cells projecting to downstream viewpoint-invariant identity-specific cells ( freiwald and tsao 2010 ) . <eos> a separate computational line of reasoning leads to the claim that some transformations of visual inputs that preserve viewed object identity are class-specific . <eos> in particular , the 2d images evoked by a face undergoing a 3d rotation are not produced by the same image transformation ( 2d ) that would produce the images evoked by an object of another class undergoing the same 3d rotation . <eos> however , within the class of faces , knowledge of the image transformation evoked by 3d rotation can be reliably transferred from previously viewed faces to help identify a novel face at a new viewpoint . <eos> we show , through computational simulations , that an architecture which applies this method of gaining invariance to class-specific transformations is effective when restricted to faces and fails spectacularly when applied across object classes . <eos> we argue here that in order to accomplish viewpoint-invariant face identification from a single example view , visual cortex must separate the circuitry involved in discounting 3d rotations of faces from the generic circuitry involved in processing other objects . <eos> the resulting model of the ventral stream of visual cortex is consistent with the recent physiology results showing the hierarchical organization of the face processing network .
we introduce picodes : a very compact image descriptor which nevertheless allows high performance on object category recognition . <eos> in particular , we address novel-category recognition : the task of defining indexing structures and image representations which enable a large collection of images to be searched for an object category that was not known when the index was built . <eos> instead , the training images defining the category are supplied at query time . <eos> we explicitly learn descriptors of a given length ( from as small as 16 bytes per image ) which have good object-recognition performance . <eos> in contrast to previous work in the domain of object recognition , we do not choose an arbitrary intermediate representation , but explicitly learn short codes . <eos> in contrast to previous approaches to learn compact codes , we optimize explicitly for ( an upper bound on ) classification performance . <eos> optimization directly for binary features is difficult and nonconvex , but we present an alternation scheme and convex upper bound which demonstrate excellent performance in practice . <eos> picodes of 256 bytes match the accuracy of the current best known classifier for the caltech256 benchmark , but they decrease the database storage size by a factor of 100 and speed-up the training and testing of novel classes by orders of magnitude .
extensive evidence suggests that items are not encoded independently in visual short-term memory ( vstm ) . <eos> however , previous research has not quantitatively considered how the encoding of an item influences the encoding of other items . <eos> here , we model the dependencies among vstm representations using a multivariate gaussian distribution with a stimulus-dependent mean and covariance matrix . <eos> we report the results of an experiment designed to determine the specific form of the stimulus-dependence of the mean and the covariance matrix . <eos> we find that the magnitude of the covariance between the representations of two items is a monotonically decreasing function of the difference between the items ' feature values , similar to a gaussian process with a distance-dependent , stationary kernel function . <eos> we further show that this type of covariance function can be explained as a natural consequence of encoding multiple stimuli in a population of neurons with correlated responses .
thompson sampling is one of oldest heuristic to address the exploration / exploitation trade-off , but it is surprisingly not very popular in the literature . <eos> we present here some empirical results using thompson sampling on simulated and real data , and show that it is highly competitive . <eos> and since this heuristic is very easy to implement , we argue that it should be part of the standard baselines to compare against .
a sizable literature has focused on the problem of estimating a low-dimensional feature space capturing a neuron 's stimulus sensitivity . <eos> however , comparatively little work has addressed the problem of estimating the nonlinear function from feature space to a neuron 's output spike rate . <eos> here , we use a gaussian process ( gp ) prior over the infinite-dimensional space of nonlinear functions to obtain bayesian estimates of the `` nonlinearity '' in the linear-nonlinear-poisson ( lnp ) encoding model . <eos> this offers flexibility , robustness , and computational tractability compared to traditional methods ( e.g. , parametric forms , histograms , cubic splines ) . <eos> most importantly , we develop a framework for optimal experimental design based on uncertainty sampling . <eos> this involves adaptively selecting stimuli to characterize the nonlinearity with as little experimental data as possible , and relies on a method for rapidly updating hyperparameters using the laplace approximation . <eos> we apply these methods to data from color-tuned neurons in macaque v1 . <eos> we estimate nonlinearities in the 3d space of cone contrasts , which reveal that v1 combines cone inputs in a highly nonlinear manner . <eos> with simulated experiments , we show that optimal design substantially reduces the amount of data required to estimate this nonlinear combination rule .
we describe a simple algorithm that runs in time poly ( n,1/gamma,1/eps ) and learns an unknown n-dimensional gamma-margin halfspace to accuracy 1-eps in the presence of malicious noise , when the noise rate is allowed to be as high as theta ( eps gamma sqrt ( log ( 1/gamma ) ) ) . <eos> previous efficient algorithms could only learn to accuracy eps in the presence of malicious noise of rate at most theta ( eps gamma ) . <eos> our algorithm does not work by optimizing a convex loss function . <eos> we show that no algorithm for learning gamma-margin halfspaces that minimizes a convex proxy for misclassification error can tolerate malicious noise at a rate greater than theta ( eps gamma ) ; this may partially explain why previous algorithms could not achieve the higher noise tolerance of our new algorithm .
we propose a robust filtering approach based on semi-supervised and multiple instance learning ( mil ) . <eos> we assume that the posterior density would be unimodal if not for the effect of outliers that we do not wish to explicitly model . <eos> therefore , we seek for a point estimate at the outset , rather than a generic approximation of the entire posterior . <eos> our approach can be thought of as a combination of standard finite-dimensional filtering ( extended kalman filter , or unscented filter ) with multiple instance learning , whereby the initial condition comes with a putative set of inlier measurements . <eos> we show how both the state ( regression ) and the inlier set ( classification ) can be estimated iteratively and causally by processing only the current measurement . <eos> we illustrate our approach on visual tracking problems whereby the object of interest ( target ) moves and evolves as a result of occlusions and deformations , and partial knowledge of the target is given in the form of a bounding box ( training set ) .
we develop unified information-theoretic machinery for deriving lower bounds for passive and active learning schemes . <eos> our bounds involve the so-called alexander 's capacity function . <eos> the supremum of this function has been recently rediscovered by hanneke in the context of active learning under the name of `` disagreement coefficient . '' <eos> for passive learning , our lower bounds match the upper bounds of gine and koltchinskii up to constants and generalize analogous results of massart and nedelec . <eos> for active learning , we provide first known lower bounds based on the capacity function rather than the disagreement coefficient .
local coordinate coding ( lcc ) [ 18 ] is a method for modeling functions of data lying on non-linear manifolds . <eos> it provides a set of anchor points which form a local coordinate system , such that each data point on the manifold can be approximated by a linear combination of its anchor points , and the linear weights become the local coordinate coding . <eos> in this paper we propose encoding data using orthogonal anchor planes , rather than anchor points . <eos> our method needs only a few orthogonal anchor planes for coding , and it can linearize any ( \alpha , \beta , p ) -lipschitz smooth nonlinear function with a fixed expected value of the upper-bound approximation error on any high dimensional data . <eos> in practice , the orthogonal coordinate system can be easily learned by minimizing this upper bound using singular value decomposition ( svd ) . <eos> we apply our method to model the coordinates locally in linear svms for classification tasks , and our experiment on mnist shows that using only 50 anchor planes our method achieves 1.72 % error rate , while lcc achieves 1.90 % error rate using 4096 anchor points .
most action potentials in the nervous system take on the form of strong , rapid , and brief voltage deflections known as spikes , in stark contrast to other action potentials , such as in the heart , that are characterized by broad voltage plateaus . <eos> we derive the shape of the neuronal action potential from first principles , by postulating that action potential generation is strongly constrained by the brain 's need to minimize energy expenditure . <eos> for a given height of an action potential , the least energy is consumed when the underlying currents obey the bang-bang principle : the currents giving rise to the spike should be intense , yet short-lived , yielding spikes with sharp onsets and offsets . <eos> energy optimality predicts features in the biophysics that are not per se required for producing the characteristic neuronal action potential : sodium currents should be extraordinarily powerful and inactivate with voltage ; both potassium and sodium currents should have kinetics that have a bell-shaped voltage-dependence ; and the cooperative action of multiple ` gates ' should start the flow of current .
a multilinear subspace regression model based on so called latent variable decomposition is introduced . <eos> unlike standard regression methods which typically employ matrix ( 2d ) data representations followed by vector subspace transformations , the proposed approach uses tensor subspace transformations to model common latent variables across both the independent and dependent data . <eos> the proposed approach aims to maximize the correlation between the so derived latent variables and is shown to be suitable for the prediction of multidimensional dependent data from multidimensional independent data , where for the estimation of the latent variables we introduce an algorithm based on multilinear singular value decomposition ( msvd ) on a specially defined cross-covariance tensor . <eos> it is next shown that in this way we are also able to unify the existing partial least squares ( pls ) and n-way pls regression algorithms within the same framework . <eos> simulations on benchmark synthetic data confirm the advantages of the proposed approach , in terms of its predictive ability and robustness , especially for small sample sizes . <eos> the potential of the proposed technique is further illustrated on a real world task of the decoding of human intracranial electrocorticogram ( ecog ) from a simultaneously recorded scalp electroencephalograph ( eeg ) .
variational methods have been previously explored as a tractable approximation to bayesian inference for neural networks . <eos> however the approaches proposed so far have only been applicable to a few simple network architectures . <eos> this paper introduces an easy-to-implement stochastic variational method ( or equivalently , minimum description length loss function ) that can be applied to most neural networks . <eos> along the way it revisits several common regularisers from a variational perspective . <eos> it also provides a simple pruning heuristic that can both drastically reduce the number of network weights and lead to improved generalisation . <eos> experimental results are provided for a hierarchical multidimensional recurrent neural network applied to the timit speech corpus .
high dimensional time series are endemic in applications of machine learning such as robotics ( sensor data ) , computational biology ( gene expression data ) , vision ( video sequences ) and graphics ( motion capture data ) . <eos> practical nonlinear probabilistic approaches to this data are required . <eos> in this paper we introduce the variational gaussian process dynamical system . <eos> our work builds on recent variational approximations for gaussian process latent variable models to allow for nonlinear dimensionality reduction simultaneously with learning a dynamical prior in the latent space . <eos> the approach also allows for the appropriate dimensionality of the latent space to be automatically determined . <eos> we demonstrate the model on a human motion capture data set and a series of high resolution video sequences .
the efficient coding hypothesis holds that neural receptive fields are adapted to the statistics of the environment , but is agnostic to the timescale of this adaptation , which occurs on both evolutionary and developmental timescales . <eos> in this work we focus on that component of adaptation which occurs during an organism 's lifetime , and show that a number of unsupervised feature learning algorithms can account for features of normal receptive field properties across multiple primary sensory cortices . <eos> furthermore , we show that the same algorithms account for altered receptive field properties in response to experimentally altered environmental statistics . <eos> based on these modeling results we propose these models as phenomenological models of receptive field plasticity during an organism 's lifetime . <eos> finally , due to the success of the same models in multiple sensory areas , we suggest that these algorithms may provide a constructive realization of the theory , first proposed by mountcastle ( 1978 ) , that a qualitatively similar learning algorithm acts throughout primary sensory cortices .
the multi-armed bandit ( mab ) setting is a useful abstraction of many online learning tasks which focuses on the trade-off between exploration and exploitation . <eos> in this setting , an online algorithm has a fixed set of alternatives ( `` arms '' ) , and in each round it selects one arm and then observes the corresponding reward . <eos> while the case of small number of arms is by now well-understood , a lot of recent work has focused on multi-armed bandits with ( infinitely ) many arms , where one needs to assume extra structure in order to make the problem tractable . <eos> in particular , in the lipschitz mab problem there is an underlying similarity metric space , known to the algorithm , such that any two arms that are close in this metric space have similar payoffs . <eos> in this paper we consider the more realistic scenario in which the metric space is *implicit* -- it is defined by the available structure but not revealed to the algorithm directly . <eos> specifically , we assume that an algorithm is given a tree-based classification of arms . <eos> for any given problem instance such a classification implicitly defines a similarity metric space , but the numerical similarity information is not available to the algorithm . <eos> we provide an algorithm for this setting , whose performance guarantees ( almost ) match the best known guarantees for the corresponding instance of the lipschitz mab problem .
we present an asymptotic analysis of viterbi training ( vt ) and contrast it with a more conventional maximum likelihood ( ml ) approach to parameter estimation in hidden markov models . <eos> while ml estimator works by ( locally ) maximizing the likelihood of the observed data , vt seeks to maximize the probability of the most likely hidden state sequence . <eos> we develop an analytical framework based on a generating function formalism and illustrate it on an exactly solvable model of hmm with one unambiguous symbol . <eos> for this particular model the ml objective function is continuously degenerate . <eos> vt objective , in contrast , is shown to have only finite degeneracy . <eos> furthermore , vt converges faster and results in sparser ( simpler ) models , thus realizing an automatic occam 's razor for hmm learning . <eos> for more general scenario vt can be worse compared to ml but still capable of correctly recovering most of the parameters .
unsupervised feature learning has been shown to be effective at learning representations that perform well on image , video and audio classification . <eos> however , many existing feature learning algorithms are hard to use and require extensive hyperparameter tuning . <eos> in this work , we present sparse filtering , a simple new algorithm which is efficient and only has one hyperparameter , the number of features to learn . <eos> in contrast to most other feature learning methods , sparse filtering does not explicitly attempt to construct a model of the data distribution . <eos> instead , it optimizes a simple cost function -- the sparsity of l2-normalized features -- which can easily be implemented in a few lines of matlab code . <eos> sparse filtering scales gracefully to handle high-dimensional inputs , and can also be used to learn meaningful features in additional layers with greedy layer-wise stacking . <eos> we evaluate sparse filtering on natural images , object classification ( stl-10 ) , and phone classification ( timit ) , and show that our method works well on a range of different modalities .
in the vast majority of recent work on sparse estimation algorithms , performance has been evaluated using ideal or quasi-ideal dictionaries ( e.g. , random gaussian or fourier ) characterized by unit $ \ell_2 $ norm , incoherent columns or features . <eos> but in reality , these types of dictionaries represent only a subset of the dictionaries that are actually used in practice ( largely restricted to idealized compressive sensing applications ) . <eos> in contrast , herein sparse estimation is considered in the context of structured dictionaries possibly exhibiting high coherence between arbitrary groups of columns and/or rows . <eos> sparse penalized regression models are analyzed with the purpose of finding , to the extent possible , regimes of dictionary invariant performance . <eos> in particular , a type ii bayesian estimator with a dictionary-dependent sparsity penalty is shown to have a number of desirable invariance properties leading to provable advantages over more conventional penalties such as the $ \ell_1 $ norm , especially in areas where existing theoretical recovery guarantees no longer hold . <eos> this can translate into improved performance in applications such as model selection with correlated features , source localization , and compressive sensing with constrained measurement directions .
we propose an approach for linear unsupervised dimensionality reduction , based on the sparse linear model that has been used to probabilistically interpret sparse coding . <eos> we formulate an optimization problem for learning a linear projection from the original signal domain to a lower-dimensional one in a way that approximately preserves , in expectation , pairwise inner products in the sparse domain . <eos> we derive solutions to the problem , present nonlinear extensions , and discuss relations to compressed sensing . <eos> our experiments using facial images , texture patches , and images of object categories suggest that the approach can improve our ability to recover meaningful structure in many classes of signals .
sparse pca provides a linear combination of small number of features that maximizes variance across data . <eos> although sparse pca has apparent advantages compared to pca , such as better interpretability , it is generally thought to be computationally much more expensive . <eos> in this paper , we demonstrate the surprising fact that sparse pca can be easier than pca in practice , and that it can be reliably applied to very large data sets . <eos> this comes from a rigorous feature elimination pre-processing result , coupled with the favorable fact that features in real-life data typically have exponentially decreasing variances , which allows for many features to be eliminated . <eos> we introduce a fast block coordinate ascent algorithm with much better computational complexity than the existing first-order ones . <eos> we provide experimental results obtained on text corpora involving millions of documents and hundreds of thousands of features . <eos> these results illustrate how sparse pca can help organize a large corpus of text data in a user-interpretable way , providing an attractive alternative approach to topic models .
in this work we use branch-and-bound ( bb ) to efficiently detect objects with deformable part models . <eos> instead of evaluating the classifier score exhaustively over image locations and scales , we use bb to focus on promising image locations . <eos> the core problem is to compute bounds that accommodate part deformations ; for this we adapt the dual trees data structure to our problem . <eos> we evaluate our approach using mixture-of-deformable part models . <eos> we obtain exactly the same results but are 10-20 times faster on average . <eos> we also develop a multiple-object detection variation of the system , where hypotheses for 20 categories are inserted in a common priority queue . <eos> for the problem of finding the strongest category in an image this results in up to a 100-fold speedup .
for a learning problem whose associated excess loss class is $ ( \beta , b ) $ -bernstein , we show that it is theoretically possible to track the same classification performance of the best ( unknown ) hypothesis in our class , provided that we are free to abstain from prediction in some region of our choice . <eos> the ( probabilistic ) volume of this rejected region of the domain is shown to be diminishing at rate $ o ( b\theta ( \sqrt { 1/m } ) ) ^\beta ) $ , where $ \theta $ is hanneke 's disagreement coefficient . <eos> the strategy achieving this performance has computational barriers because it requires empirical error minimization in an agnostic setting . <eos> nevertheless , we heuristically approximate this strategy and develop a novel selective classification algorithm using constrained svms . <eos> we show empirically that the resulting algorithm consistently outperforms the traditional rejection mechanism based on distance from decision boundary .
modern classification tasks usually involve many class labels and can be informed by a broad range of features . <eos> many of these tasks are tackled by constructing a set of classifiers , which are then applied at test time and then pieced together in a fixed procedure determined in advance or at training time . <eos> we present an active classification process at the test time , where each classifier in a large ensemble is viewed as a potential observation that might inform our classification process . <eos> observations are then selected dynamically based on previous observations , using a value-theoretic computation that balances an estimate of the expected classification gain from each observation as well as its computational cost . <eos> the expected classification gain is computed using a probabilistic model that uses the outcome from previous observations . <eos> this active classification process is applied at test time for each individual test instance , resulting in an efficient instance-specific decision path . <eos> we demonstrate the benefit of the active scheme on various real-world datasets , and show that it can achieve comparable or even higher classification accuracy at a fraction of the computational costs of traditional methods .
a bayesian approach to partitioning distance matrices is presented . <eos> it is inspired by the 'translation-invariant wishart-dirichlet ' process ( tiwd ) in ( vogt et al. , 2010 ) and shares a number of advantageous properties like the fully probabilistic nature of the inference model , automatic selection of the number of clusters and applicability in semi-supervised settings . <eos> in addition , our method ( which we call 'fasttiwd ' ) overcomes the main shortcoming of the original tiwd , namely its high computational costs . <eos> the fasttiwd reduces the workload in each iteration of a gibbs sampler from o ( n^3 ) in the tiwd to o ( n^2 ) . <eos> our experiments show that this cost reduction does not compromise the quality of the inferred partitions . <eos> with this new method it is now possible to 'mine ' large relational datasets with a probabilistic model , thereby automatically detecting new and potentially interesting clusters .
although spectral clustering has enjoyed considerable empirical success in machine learning , its theoretical properties are not yet fully developed . <eos> we analyze the performance of a spectral algorithm for hierarchical clustering and show that on a class of hierarchically structured similarity matrices , this algorithm can tolerate noise that grows with the number of data points while still perfectly recovering the hierarchical clusters with high probability . <eos> we additionally improve upon previous results for k-way spectral clustering to derive conditions under which spectral clustering makes no mistakes . <eos> further , using minimax analysis , we derive tight upper and lower bounds for the clustering problem and compare the performance of spectral clustering to these information theoretic limits . <eos> we also present experiments on simulated and real world data illustrating our results .
this manuscript considers the convergence rate of boosting under a large class of losses , including the exponential and logistic losses , where the best previous rate of convergence was o ( exp ( 1/ ? ? ) ) . <eos> first , it is established that the setting of weak learnability aids the entire class , granting a rate o ( ln ( 1/ ? ) ) . <eos> next , the ( disjoint ) conditions under which the infimal empirical risk is attainable are characterized in terms of the sample and weak learning class , and a new proof is given for the known rate o ( ln ( 1/ ? ) ) . <eos> finally , it is established that any instance can be decomposed into two smaller instances resembling the two preceding special cases , yielding a rate o ( 1/ ? <eos> ) , with a matching lower bound for the logistic loss . <eos> the principal technical hurdle throughout this work is the potential unattainability of the infimal empirical risk ; the technique for overcoming this barrier may be of general interest .
variational bayesian matrix factorization ( vbmf ) efficiently approximates the posterior distribution of factorized matrices by assuming matrix-wise independence of the two factors . <eos> a recent study on fully-observed vbmf showed that , under a stronger assumption that the two factorized matrices are column-wise independent , the global optimal solution can be analytically computed . <eos> however , it was not clear how restrictive the column-wise independence assumption is . <eos> in this paper , we prove that the global solution under matrix-wise independence is actually column-wise independent , implying that the column-wise independence assumption is harmless . <eos> a practical consequence of our theoretical finding is that the global solution under matrix-wise independence ( which is a standard setup ) can be obtained analytically in a computationally very efficient way without any iterative algorithms . <eos> we experimentally illustrate advantages of using our analytic solution in probabilistic principal component analysis .
we describe a family of global optimization procedures that automatically decompose optimization problems into smaller loosely coupled problems , then combine the solutions of these with message passing algorithms . <eos> we show empirically that these methods excel in avoiding local minima and produce better solutions with fewer function evaluations than existing global optimization methods . <eos> to develop these methods , we introduce a notion of coupling between variables of optimization that generalizes the notion of coupling that arises from factoring functions into terms that involve small subsets of the variables . <eos> it therefore subsumes the notion of independence between random variables in statistics , sparseness of the hessian in nonlinear optimization , and the generalized distributive law . <eos> despite being more general , this notion of coupling is easier to verify empirically -- making structure estimation easy -- yet it allows us to migrate well-established inference methods on graphical models to the setting of global optimization .
an increasing number of experimental studies indicate that perception encodes a posterior probability distribution over possible causes of sensory stimuli , which is used to act close to optimally in the environment . <eos> one outstanding difficulty with this hypothesis is that the exact posterior will in general be too complex to be represented directly , and thus neurons will have to represent an approximation of this distribution . <eos> two influential proposals of efficient posterior representation by neural populations are : 1 ) neural activity represents samples of the underlying distribution , or 2 ) they represent a parametric representation of a variational approximation of the posterior . <eos> we show that these approaches can be combined for an inference scheme that retains the advantages of both : it is able to represent multiple modes and arbitrary correlations , a feature of sampling methods , and it reduces the represented space to regions of high probability mass , a strength of variational approximations . <eos> neurally , the combined method can be interpreted as a feed-forward preselection of the relevant state space , followed by a neural dynamics implementation of markov chain monte carlo ( mcmc ) to approximate the posterior over the relevant states . <eos> we demonstrate the effectiveness and efficiency of this approach on a sparse coding model . <eos> in numerical experiments on artificial data and image patches , we compare the performance of the algorithms to that of exact em , variational state space selection alone , mcmc alone , and the combined select and sample approach . <eos> the select and sample approach integrates the advantages of the sampling and variational approximations , and forms a robust , neurally plausible , and very efficient model of processing and learning in cortical networks . <eos> for sparse coding we show applications easily exceeding a thousand observed and a thousand hidden dimensions .
most previous research on image categorization has focused on medium-scale data sets , while large-scale image categorization with millions of images from thousands of categories remains a challenge . <eos> with the emergence of structured large-scale dataset such as the imagenet , rich information about the conceptual relationships between images , such as a tree hierarchy among various image categories , become available . <eos> as human cognition of complex visual world benefits from underlying semantic relationships between object classes , we believe a machine learning system can and should leverage such information as well for better performance . <eos> in this paper , we employ such semantic relatedness among image categories for large-scale image categorization . <eos> specifically , a category hierarchy is utilized to properly define loss function and select common set of features for related categories . <eos> an efficient optimization method based on proximal approximation and accelerated parallel gradient method is introduced . <eos> experimental results on a subset of imagenet containing 1.2 million images from 1000 categories demonstrate the effectiveness and promise of our proposed approach .
we are motivated by an application to extract a representative subset of machine learning training data and by the poor empirical performance we observe of the popular minimum norm algorithm . <eos> in fact , for our application , minimum norm can have a running time of about o ( n^7 ) ( o ( n^5 ) oracle calls ) . <eos> we therefore propose a fast approximate method to minimize arbitrary submodular functions . <eos> for a large sub-class of submodular functions , the algorithm is exact . <eos> other submodular functions are iteratively approximated by tight submodular upper bounds , and then repeatedly optimized . <eos> we show theoretical properties , and empirical results suggest significant speedups over minimum norm while retaining higher accuracies .
we consider the problem of recovering the parameter alpha in r^k of a sparse function f , i.e . the number of non-zero entries of alpha is small compared to the number k of features , given noisy evaluations of f at a set of well-chosen sampling points . <eos> we introduce an additional randomisation process , called brownian sensing , based on the computation of stochastic integrals , which produces a gaussian sensing matrix , for which good recovery properties are proven independently on the number of sampling points n , even when the features are arbitrarily non-orthogonal . <eos> under the assumption that f is h ? lder continuous with exponent at least 1/2 , we provide an estimate a of the parameter such that ||\alpha - a||_2 = o ( ||eta||_2\sqrt { n } ) , where eta is the observation noise . <eos> the method uses a set of sampling points uniformly distributed along a one-dimensional curve selected according to the features . <eos> we report numerical experiments illustrating our method .
we investigate the representational power of sum-product networks ( computation networks analogous to neural networks , but whose individual units compute either products or weighted sums ) , through a theoretical analysis that compares deep ( multiple hidden layers ) vs. shallow ( one hidden layer ) architectures . <eos> we prove there exist families of functions that can be represented much more efficiently with a deep network than with a shallow one , i.e . with substantially fewer hidden units . <eos> such results were not available until now , and contribute to motivate recent research involving learning of deep sum-product networks , and more generally motivate research in deep learning .
markov random fields ( mrfs ) have proven very powerful both as density estimators and feature extractors for classification . <eos> however , their use is often limited by an inability to estimate the partition function $ z $ . <eos> in this paper , we exploit the gradient descent training procedure of restricted boltzmann machines ( a type of mrf ) to { \bf track } the log partition function during learning . <eos> our method relies on two distinct sources of information : ( 1 ) estimating the change $ \delta z $ incurred by each gradient update , ( 2 ) estimating the difference in $ z $ over a small set of tempered distributions using bridge sampling . <eos> the two sources of information are then combined using an inference procedure similar to kalman filtering . <eos> learning mrfs through tempered stochastic maximum likelihood , we can estimate $ z $ using no more temperatures than are required for learning . <eos> comparing to both exact values and estimates using annealed importance sampling ( ais ) , we show on several datasets that our method is able to accurately track the log partition function . <eos> in contrast to ais , our method provides this estimate at each time-step , at a computational cost similar to that required for training alone .
a common approach for handling the complexity and inherent ambiguities of 3d human pose estimation is to use pose priors learned from training data . <eos> existing approaches however , are either too simplistic ( linear ) , too complex to learn , or can only learn latent spaces from `` simple data '' , i.e. , single activities such as walking or running . <eos> in this paper , we present an efficient stochastic gradient descent algorithm that is able to learn probabilistic non-linear latent spaces composed of multiple activities . <eos> furthermore , we derive an incremental algorithm for the online setting which can update the latent space without extensive relearning . <eos> we demonstrate the effectiveness of our approach on the task of monocular and multi-view tracking and show that our approach outperforms the state-of-the-art .
given a set v of n vectors in d-dimensional space , we provide an efficient method for computing quality upper and lower bounds of the euclidean distances between a pair of the vectors in v . <eos> for this purpose , we define a distance measure , called the ms-distance , by using the mean and the standard deviation values of vectors in v . <eos> once we compute the mean and the standard deviation values of vectors in v in o ( dn ) time , the ms-distance between them provides upper and lower bounds of euclidean distance between a pair of vectors in v in constant time . <eos> furthermore , these bounds can be refined further such that they converge monotonically to the exact euclidean distance within d refinement steps . <eos> we also provide an analysis on a random sequence of refinement steps which can justify why ms-distance should be refined to provide very tight bounds in a few steps of a typical sequence . <eos> the ms-distance can be used to various problems where the euclidean distance is used to measure the proximity or similarity between objects . <eos> we provide experimental results on the nearest and the farthest neighbor searches .
the object people perceive in an image can depend on its orientation relative to the scene it is in ( its reference frame ) . <eos> for example , the images of the symbols $ \times $ and $ + $ differ by a 45 degree rotation . <eos> although real scenes have multiple images and reference frames , psychologists have focused on scenes with only one reference frame . <eos> we propose an ideal observer model based on nonparametric bayesian statistics for inferring the number of reference frames in a scene and their parameters . <eos> when an ambiguous image could be assigned to two conflicting reference frames , the model predicts two factors should influence the reference frame inferred for the image : the image should be more likely to share the reference frame of the closer object ( { \em proximity } ) and it should be more likely to share the reference frame containing the most objects ( { \em alignment } ) . <eos> we confirm people use both cues using a novel methodology that allows for easy testing of human reference frame inference .
variability in single neuron models is typically implemented either by a stochastic leaky-integrate-and-fire model or by a model of the generalized linear model ( glm ) family . <eos> we use analytical and numerical methods to relate state-of-the-art models from both schools of thought . <eos> first we find the analytical expressions relating the subthreshold voltage from the adaptive exponential integrate-and-fire model ( adex ) to the spike-response model with escape noise ( srm as an example of a glm ) . <eos> then we calculate numerically the link-function that provides the firing probability given a deterministic membrane potential . <eos> we find a mathematical expression for this link-function and test the ability of the glm to predict the firing probability of a neuron receiving complex stimulation . <eos> comparing the prediction performance of various link-functions , we find that a glm with an exponential link-function provides an excellent approximation to the adaptive exponential integrate-and-fire with colored-noise input . <eos> these results help to understand the relationship between the different approaches to stochastic neuron models .
being able to predict the course of arbitrary chemical reactions is essential to the theory and applications of organic chemistry . <eos> previous approaches are not high-throughput , are not generalizable or scalable , or lack sufficient data to be effective . <eos> we describe single mechanistic reactions as concerted electron movements from an electron orbital source to an electron orbital sink . <eos> we use an existing rule-based expert system to derive a dataset consisting of 2,989 productive mechanistic steps and 6.14 million non-productive mechanistic steps . <eos> we then pose identifying productive mechanistic steps as a ranking problem : rank potential orbital interactions such that the top ranked interactions yield the major products . <eos> the machine learning implementation follows a two-stage approach , in which we first train atom level reactivity filters to prune 94.0 % of non-productive reactions with less than a 0.1 % false negative rate . <eos> then , we train an ensemble of ranking models on pairs of interacting orbitals to learn a relative productivity function over single mechanistic reactions in a given system . <eos> without the use of explicit transformation patterns , the ensemble perfectly ranks the productive mechanisms at the top 89.1 % of the time , rising to 99.9 % of the time when top ranked lists with at most four non-productive reactions are considered . <eos> the final system allows multi-step reaction prediction . <eos> furthermore , it is generalizable , making reasonable predictions over reactants and conditions which the rule-based expert system does not handle .
maximum entropy models have become popular statistical models in neuroscience and other areas in biology , and can be useful tools for obtaining estimates of mu- tual information in biological systems . <eos> however , maximum entropy models fit to small data sets can be subject to sampling bias ; i.e . the true entropy of the data can be severely underestimated . <eos> here we study the sampling properties of estimates of the entropy obtained from maximum entropy models . <eos> we show that if the data is generated by a distribution that lies in the model class , the bias is equal to the number of parameters divided by twice the number of observations . <eos> however , in practice , the true distribution is usually outside the model class , and we show here that this misspecification can lead to much larger bias . <eos> we provide a perturba- tive approximation of the maximally expected bias when the true model is out of model class , and we illustrate our results using numerical simulations of an ising model ; i.e . the second-order maximum entropy distribution on binary data .
renewal processes are generalizations of the poisson process on the real line , whose intervals are drawn i.i.d . <eos> from some distribution . <eos> modulated renewal processes allow these distributions to vary with time , allowing the introduction nonstationarity . <eos> in this work , we take a nonparametric bayesian approach , modeling this nonstationarity with a gaussian process . <eos> our approach is based on the idea of uniformization , allowing us to draw exact samples from an otherwise intractable distribution . <eos> we develop a novel and efficient mcmc sampler for posterior inference . <eos> in our experiments , we test these on a number of synthetic and real datasets .
we present an optimization approach for linear svms based on a stochastic primal-dual approach , where the primal step is akin to an importance-weighted sgd , and the dual step is a stochastic update on the importance weights . <eos> this yields an optimization method with a sublinear dependence on the training set size , and the first method for learning linear svms with runtime less then the size of the training set required for learning !
in many clustering problems , we have access to multiple views of the data each of which could be individually used for clustering . <eos> exploiting information from multiple views , one can hope to find a clustering that is more accurate than the ones obtained using the individual views . <eos> since the true clustering would assign a point to the same cluster irrespective of the view , we can approach this problem by looking for clusterings that are consistent across the views , i.e. , corresponding data points in each view should have same cluster membership . <eos> we propose a spectral clustering framework that achieves this goal by co-regularizing the clustering hypotheses , and propose two co-regularization schemes to accomplish this . <eos> experimental comparisons with a number of baselines on two synthetic and three real-world datasets establish the efficacy of our proposed approaches .
the distance dependent chinese restaurant process ( ddcrp ) was recently introduced to accommodate random partitions of non-exchangeable data . <eos> the ddcrp clusters data in a biased way : each data point is more likely to be clustered with other data that are near it in an external sense . <eos> this paper examines the ddcrp in a spatial setting with the goal of natural image segmentation . <eos> we explore the biases of the spatial ddcrp model and propose a novel hierarchical extension better suited for producing `` human-like '' segmentations . <eos> we then study the sensitivity of the models to various distance and appearance hyperparameters , and provide the first rigorous comparison of nonparametric bayesian models in the image segmentation domain . <eos> on unsupervised image segmentation , we demonstrate that similar performance to existing nonparametric bayesian models is possible with substantially simpler models and algorithms .
clustering is a popular problem with many applications . <eos> we consider the k-means problem in the situation where the data is too large to be stored in main memory and must be accessed sequentially , such as from a disk , and where we must use as little memory as possible . <eos> our algorithm is based on recent theoretical results , with significant improvements to make it practical . <eos> our approach greatly simplifies a recently developed algorithm , both in design and in analysis , and eliminates large constant factors in the approximation guarantee , the memory requirements , and the running time . <eos> we then incorporate approximate nearest neighbor search to compute k-means in o ( nk ) ( where n is the number of data points ; note that computing the cost , given a solution , takes 8 ( nk ) time ) . <eos> we show that our algorithm compares favorably to existing algorithms - both theoretically and experimentally , thus providing state-of-the-art performance in both theory and practice .
how can we train a statistical mixture model on a massive data set ? <eos> in this paper , we show how to construct coresets for mixtures of gaussians and natural generalizations . <eos> a coreset is a weighted subset of the data , which guarantees that models fitting the coreset will also provide a good fit for the original data set . <eos> we show that , perhaps surprisingly , gaussian mixtures admit coresets of size independent of the size of the data set . <eos> more precisely , we prove that a weighted set of $ o ( dk^3/\eps^2 ) $ data points suffices for computing a $ ( 1+\eps ) $ -approximation for the optimal model on the original $ n $ data points . <eos> moreover , such coresets can be efficiently constructed in a map-reduce style computation , as well as in a streaming setting . <eos> our results rely on a novel reduction of statistical estimation to problems in computational geometry , as well as new complexity results about mixtures of gaussians . <eos> we empirically evaluate our algorithms on several real data sets , including a density estimation problem in the context of earthquake detection using accelerometers in mobile phones .
storing a new pattern in a palimpsest memory system comes at the cost of interfering with the memory traces of previously stored items . <eos> knowing the age of a pattern thus becomes critical for recalling it faithfully . <eos> this implies that there should be a tight coupling between estimates of age , as a form of familiarity , and the neural dynamics of recollection , something which current theories omit . <eos> using a normative model of autoassociative memory , we show that a dual memory system , consisting of two interacting modules for familiarity and recollection , has best performance for both recollection and recognition . <eos> this finding provides a new window onto actively contentious psychological and neural aspects of recognition memory .
unlike existing nonparametric bayesian models , which rely solely on specially conceived priors to incorporate domain knowledge for discovering improved latent representations , we study nonparametric bayesian inference with regularization on the desired posterior distributions . <eos> while priors can indirectly affect posterior distributions through bayes ' theorem , imposing posterior regularization is arguably more direct and in some cases can be much easier . <eos> we particularly focus on developing infinite latent support vector machines ( ilsvm ) and multi-task infinite latent support vector machines ( mt-ilsvm ) , which explore the large-margin idea in combination with a nonparametric bayesian model for discovering predictive latent features for classification and multi-task learning , respectively . <eos> we present efficient inference methods and report empirical studies on several benchmark datasets . <eos> our results appear to demonstrate the merits inherited from both large-margin learning and bayesian nonparametrics .
we consider an adversarial online learning setting where a decision maker can choose an action in every stage of the game . <eos> in addition to observing the reward of the chosen action , the decision maker gets side observations on the reward he would have obtained had he chosen some of the other actions . <eos> the observation structure is encoded as a graph , where node i is linked to node j if sampling i provides information on the reward of j . <eos> this setting naturally interpolates between the well-known `` experts '' setting , where the decision maker can view all rewards , and the multi-armed bandits setting , where the decision maker can only view the reward of the chosen action . <eos> we develop practical algorithms with provable regret guarantees , which depend on non-trivial graph-theoretic properties of the information feedback structure . <eos> we also provide partially-matching lower bounds .
computing a good strategy in a large extensive form game often demands an extraordinary amount of computer memory , necessitating the use of abstraction to reduce the game size . <eos> typically , strategies from abstract games perform better in the real game as the granularity of abstraction is increased . <eos> this paper investigates two techniques for stitching a base strategy in a coarse abstraction of the full game tree , to expert strategies in fine abstractions of smaller subtrees . <eos> we provide a general framework for creating static experts , an approach that generalizes some previous strategy stitching efforts . <eos> in addition , we show that static experts can create strong agents for both 2-player and 3-player leduc and limit texas hold'em poker , and that a specific class of static experts can be preferred among a number of alternatives . <eos> furthermore , we describe a poker agent that used static experts and won the 3-player events of the 2010 annual computer poker competition .
we present a type of temporal restricted boltzmann machine that defines a probability distribution over an output sequence conditional on an input sequence . <eos> it shares the desirable properties of rbms : efficient exact inference , an exponentially more expressive latent state than hmms , and the ability to model nonlinear structure and dynamics . <eos> we apply our model to a challenging real-world graphics problem : facial expression transfer . <eos> our results demonstrate improved performance over several baselines modeling high-dimensional 2d and 3d data .
psychologists have long been struck by individuals ' limitations in expressing their internal sensations , impressions , and evaluations via rating scales . <eos> instead of using an absolute scale , individuals rely on reference points from recent experience . <eos> this _relativity of judgment_ limits the informativeness of responses on surveys , questionnaires , and evaluation forms . <eos> fortunately , the cognitive processes that map stimuli to responses are not simply noisy , but rather are influenced by recent experience in a lawful manner . <eos> we explore techniques to remove sequential dependencies , and thereby _decontaminate_ a series of ratings to obtain more meaningful human judgments . <eos> in our formulation , the problem is to infer latent ( subjective ) impressions from a sequence of stimulus labels ( e.g. , movie names ) and responses . <eos> we describe an unsupervised approach that simultaneously recovers the impressions and parameters of a contamination model that predicts how recent judgments affect the current response . <eos> we test our _iterated impression inference_ , or i^3 , algorithm in three domains : rating the gap between dots , the desirability of a movie based on an advertisement , and the morality of an action . <eos> we demonstrate significant objective improvements in the quality of the recovered impressions .
we consider the problem of ising and gaussian graphical model selection given n i.i.d . <eos> samples from the model . <eos> we propose an efficient threshold-based algorithm for structure estimation based known as conditional mutual information test . <eos> this simple local algorithm requires only low-order statistics of the data and decides whether two nodes are neighbors in the unknown graph . <eos> under some transparent assumptions , we establish that the proposed algorithm is structurally consistent ( or sparsistent ) when the number of samples scales as n= omega ( j_ { min } ^ { -4 } log p ) , where p is the number of nodes and j_ { min } is the minimum edge potential . <eos> we also prove novel non-asymptotic necessary conditions for graphical model selection .
many fundamental questions in theoretical neuroscience involve optimal decoding and the computation of shannon information rates in populations of spiking neurons . <eos> in this paper , we apply methods from the asymptotic theory of statistical inference to obtain a clearer analytical understanding of these quantities . <eos> we find that for large neural populations carrying a finite total amount of information , the full spiking population response is asymptotically as informative as a single observation from a gaussian process whose mean and covariance can be characterized explicitly in terms of network and single neuron properties . <eos> the gaussian form of this asymptotic sufficient statistic allows us in certain cases to perform optimal bayesian decoding by simple linear transformations , and to obtain closed-form expressions of the shannon information carried by the network . <eos> one technical advantage of the theory is that it may be applied easily even to non-poisson point process network models ; for example , we find that under some conditions , neural populations with strong history-dependent ( non-poisson ) effects carry exactly the same information as do simpler equivalent populations of non-interacting poisson neurons with matched firing rates . <eos> we argue that our findings help to clarify some results from the recent literature on neural decoding and neuroprosthetic design .
we consider feature selection and weighting for nearest neighbor classifiers . <eos> a technical challenge in this scenario is how to cope with the discrete update of nearest neighbors when the feature space metric is changed during the learning process . <eos> this issue , called the target neighbor change , was not properly addressed in the existing feature weighting and metric learning literature . <eos> in this paper , we propose a novel feature weighting algorithm that can exactly and efficiently keep track of the correct target neighbors via sequential quadratic programming . <eos> to the best of our knowledge , this is the first algorithm that guarantees the consistency between target neighbors and the feature space metric . <eos> we further show that the proposed algorithm can be naturally combined with regularization path tracking , allowing computationally efficient selection of the regularization parameter . <eos> we demonstrate the effectiveness of the proposed algorithm through experiments .
probabilistic logics are receiving a lot of attention today because of their expressive power for knowledge representation and learning . <eos> however , this expressivity is detrimental to the tractability of inference , when done at the propositional level . <eos> to solve this problem , various lifted inference algorithms have been proposed that reason at the first-order level , about groups of objects as a whole . <eos> despite the existence of various lifted inference approaches , there are currently no completeness results about these algorithms . <eos> the key contribution of this paper is that we introduce a formal definition of lifted inference that allows us to reason about the completeness of lifted inference algorithms relative to a particular class of probabilistic models . <eos> we then show how to obtain a completeness result using a first-order knowledge compilation approach for theories of formulae containing up to two logical variables .
recovering hidden structure from complex and noisy non-linear data is one of the most fundamental problems in machine learning and statistical inference . <eos> while such data is often high-dimensional , it is of interest to approximate it with a low-dimensional or even one-dimensional space , since many important aspects of data are often intrinsically low-dimensional . <eos> furthermore , there are many scenarios where the underlying structure is graph-like , e.g , river/road networks or various trajectories . <eos> in this paper , we develop a framework to extract , as well as to simplify , a one-dimensional `` skeleton '' from unorganized data using the reeb graph . <eos> our algorithm is very simple , does not require complex optimizations and can be easily applied to unorganized high-dimensional data such as point clouds or proximity graphs . <eos> it can also represent arbitrary graph structures in the data . <eos> we also give theoretical results to justify our method . <eos> we provide a number of experiments to demonstrate the effectiveness and generality of our algorithm , including comparisons to existing methods , such as principal curves . <eos> we believe that the simplicity and practicality of our algorithm will help to promote skeleton graphs as a data analysis tool for a broad range of applications .
this paper studies privacy preserving m-estimators using perturbed histograms . <eos> the proposed approach allows the release of a wide class of m-estimators with both differential privacy and statistical utility without knowing a priori the particular inference procedure . <eos> the performance of the proposed method is demonstrated through a careful study of the convergence rates . <eos> a practical algorithm is given and applied on a real world data set containing both continuous and categorical variables .
we extend the classical problem of predicting a sequence of outcomes from a finite alphabet to the matrix domain . <eos> in this extension , the alphabet of $ n $ outcomes is replaced by the set of all dyads , i.e . outer products $ \u\u^\top $ where $ \u $ is a vector in $ \r^n $ of unit length . <eos> whereas in the classical case the goal is to learn ( i.e . sequentially predict as well as ) the best multinomial distribution , in the matrix case we desire to learn the density matrix that best explains the observed sequence of dyads . <eos> we show how popular online algorithms for learning a multinomial distribution can be extended to learn density matrices . <eos> intuitively , learning the $ n^2 $ parameters of a density matrix is much harder than learning the $ n $ parameters of a multinomial distribution . <eos> completely surprisingly , we prove that the worst-case regrets of certain classical algorithms and their matrix generalizations are identical . <eos> the reason is that the worst-case sequence of dyads share a common eigensystem , i.e . the worst case regret is achieved in the classical case . <eos> so these matrix algorithms learn the eigenvectors without any regret .
for many real-world applications , we often need to select correlated variables -- -such as genetic variations and imaging features associated with alzheimer 's disease -- -in a high dimensional space . <eos> the correlation between variables presents a challenge to classical variable selection methods . <eos> to address this challenge , the elastic net has been developed and successfully applied to many applications . <eos> despite its great success , the elastic net does not exploit the correlation information embedded in the data to select correlated variables . <eos> to overcome this limitation , we present a novel hybrid model , eigennet , that uses the eigenstructures of data to guide variable selection . <eos> specifically , it integrates a sparse conditional classification model with a generative model capturing variable correlations in a principled bayesian framework . <eos> we develop an efficient active-set algorithm to estimate the model via evidence maximization . <eos> experiments on synthetic data and imaging genetics data demonstrated the superior predictive performance of the eigennet over the lasso , the elastic net , and the automatic relevance determination .
motivated by the spread of on-line information in general and on-line petitions in particular , recent research has raised the following combinatorial estimation problem . <eos> there is a tree t that we can not observe directly ( representing the structure along which the information has spread ) , and certain nodes randomly decide to make their copy of the information public . <eos> in the case of a petition , the list of names on each public copy of the petition also reveals a path leading back to the root of the tree . <eos> what can we conclude about the properties of the tree we observe from these revealed paths , and can we use the structure of the observed tree to estimate the size of the full unobserved tree t ? <eos> here we provide the first algorithm for this size estimation task , together with provable guarantees on its performance . <eos> we also establish structural properties of the observed tree , providing the first rigorous explanation for some of the unusual structural phenomena present in the spread of real chain-letter petitions on the internet .
we present explicit classes of probability distributions that can be learned by restricted boltzmann machines ( rbms ) depending on the number of units that they contain , and which are representative for the expressive power of the model . <eos> we use this to show that the maximal kullback-leibler divergence to the rbm model with n visible and m hidden units is bounded from above by ( n-1 ) -log ( m+1 ) . <eos> in this way we can specify the number of hidden units that guarantees a sufficiently rich model containing different classes of distributions and respecting a given error tolerance .
this paper addresses the problem of finding the nearest neighbor ( or one of the $ r $ -nearest neighbors ) of a query object $ q $ in a database of $ n $ objects , when we can only use a comparison oracle . <eos> the comparison oracle , given two reference objects and a query object , returns the reference object most similar to the query object . <eos> the main problem we study is how to search the database for the nearest neighbor ( nn ) of a query , while minimizing the questions . <eos> the difficulty of this problem depends on properties of the underlying database . <eos> we show the importance of a characterization : \emph { combinatorial disorder } $ d $ which defines approximate triangle inequalities on ranks . <eos> we present a lower bound of $ \omega ( d\log \frac { n } { d } +d^2 ) $ average number of questions in the search phase for any randomized algorithm , which demonstrates the fundamental role of $ d $ for worst case behavior . <eos> we develop a randomized scheme for nn retrieval in $ o ( d^3\log^2 n+ d\log^2 n \log\log n^ { d^3 } ) $ questions . <eos> the learning requires asking $ o ( n d^3\log^2 n+ d \log^2 n \log\log n^ { d^3 } ) $ questions and $ o ( n\log^2n/\log ( 2d ) ) $ bits to store .
machine learning competitions such as the netflix prize have proven reasonably successful as a method of ? crowdsourcing prediction tasks . <eos> but these compe- titions have a number of weaknesses , particularly in the incentive structure they create for the participants . <eos> we propose a new approach , called a crowdsourced learning mechanism , in which participants collaboratively ? learn a hypothesis for a given prediction task . <eos> the approach draws heavily from the concept of a prediction market , where traders bet on the likelihood of a future event . <eos> in our framework , the mechanism continues to publish the current hypothesis , and par- ticipants can modify this hypothesis by wagering on an update . <eos> the critical in- centive property is that a participant will profit an amount that scales according to how much her update improves performance on a released test set .
we consider a statistical framework in which recurrent networks of spiking neurons learn to generate spatio-temporal spike patterns . <eos> given biologically realistic stochastic neuronal dynamics we derive a tractable learning rule for the synaptic weights towards hidden and visible neurons that leads to optimal recall of the training sequences . <eos> we show that learning synaptic weights towards hidden neurons significantly improves the storing capacity of the network . <eos> furthermore , we derive an approximate online learning rule and show that our learning rule is consistent with spike-timing dependent plasticity in that if a presynaptic spike shortly precedes a postynaptic spike , potentiation is induced and otherwise depression is elicited .
efficient coding provides a powerful principle for explaining early sensory coding . <eos> most attempts to test this principle have been limited to linear , noiseless models , and when applied to natural images , have yielded oriented filters consistent with responses in primary visual cortex . <eos> here we show that an efficient coding model that incorporates biologically realistic ingredients ? <eos> input and output noise , nonlinear response functions , and a metabolic cost on the firing rate ? <eos> predicts receptive fields and response nonlinearities similar to those observed in the retina . <eos> specifically , we develop numerical methods for simultaneously learning the linear filters and response nonlinearities of a population of model neurons , so as to maximize information transmission subject to metabolic costs . <eos> when applied to an ensemble of natural images , the method yields filters that are center-surround and nonlinearities that are rectifying . <eos> the filters are organized into two populations , with on- and off-centers , which independently tile the visual space . <eos> as observed in the primate retina , the off-center neurons are more numerous and have filters with smaller spatial extent . <eos> in the absence of noise , our method reduces to a generalized version of independent components analysis , with an adapted nonlinear contrastfunction ; in this case , the optimal filters are localized and oriented .
factored decentralized partially observable markov decision processes ( dec-pomdps ) form a powerful framework for multiagent planning under uncertainty , but optimal solutions require a rigid history-based policy representation . <eos> in this paper we allow inter-agent communication which turns the problem in a centralized multiagent pomdp ( mpomdp ) . <eos> we map belief distributions over state factors to an agent 's local actions by exploiting structure in the joint mpomdp policy . <eos> the key point is that when sparse dependencies between the agents ' decisions exist , often the belief over its local state factors is sufficient for an agent to unequivocally identify the optimal action , and communication can be avoided . <eos> we formalize these notions by casting the problem into convex optimization form , and present experimental results illustrating the savings in communication that we can obtain .
this paper studies the problem of accurately recovering a sparse vector $ \beta^ { \star } $ from highly corrupted linear measurements $ y = x \beta^ { \star } + e^ { \star } + w $ where $ e^ { \star } $ is a sparse error vector whose nonzero entries may be unbounded and $ w $ is a bounded noise . <eos> we propose a so-called extended lasso optimization which takes into consideration sparse prior information of both $ \beta^ { \star } $ and $ e^ { \star } $ . <eos> our first result shows that the extended lasso can faithfully recover both the regression and the corruption vectors . <eos> our analysis is relied on a notion of extended restricted eigenvalue for the design matrix $ x $ . <eos> our second set of results applies to a general class of gaussian design matrix $ x $ with i.i.d rows $ \oper n ( 0 , \sigma ) $ , for which we provide a surprising phenomenon : the extended lasso can recover exact signed supports of both $ \beta^ { \star } $ and $ e^ { \star } $ from only $ \omega ( k \log p \log n ) $ observations , even the fraction of corruption is arbitrarily close to one . <eos> our analysis also shows that this amount of observations required to achieve exact signed support is optimal .
we derive a plausible learning rule updating the synaptic efficacies for feedforward , feedback and lateral connections between observed and latent neurons . <eos> operating in the context of a generative model for distributions of spike sequences , the learning mechanism is derived from variational inference principles . <eos> the synaptic plasticity rules found are interesting in that they are strongly reminiscent of experimentally found results on spike time dependent plasticity , and in that they differ for excitatory and inhibitory neurons . <eos> a simulation confirms the method 's applicability to learning both stationary and temporal spike patterns .
consider a sequence of bits where we are trying to predict the next bit from the previous bits . <eos> assume we are allowed to say ` predict 0 ' or ` predict 1 ' , and our payoff is $ +1 $ if the prediction is correct and $ -1 $ otherwise . <eos> we will say that at each point in time the loss of an algorithm is the number of wrong predictions minus the number of right predictions so far . <eos> in this paper we are interested in algorithms that have essentially zero ( expected ) loss over any string at any point in time and yet have small regret with respect to always predicting $ 0 $ or always predicting $ 1 $ . <eos> for a sequence of length $ t $ our algorithm has regret $ 14\epsilon t $ and loss $ 2\sqrt { t } e^ { -\epsilon^2 t } $ in expectation for all strings . <eos> we show that the tradeoff between loss and regret is optimal up to constant factors . <eos> our techniques extend to the general setting of $ n $ experts , where the related problem of trading off regret to the best expert for regret to the 'special ' expert has been studied by even-dar et al . ( colt'07 ) . <eos> we obtain essentially zero loss with respect to the special expert and optimal loss/regret tradeoff , improving upon the results of even-dar et al ( colt'07 ) and settling the main question left open in their paper . <eos> the strong loss bounds of the algorithm have some surprising consequences . <eos> first , we obtain a parameter free algorithm for the experts problem that has optimal regret bounds with respect to $ k $ -shifting optima , i.e . bounds with respect to the optimum that is allowed to change arms multiple times . <eos> moreover , for { \em any window of size $ n $ } the regret of our algorithm to any expert never exceeds $ o ( \sqrt { n ( \log n+\log t ) } ) $ , where $ n $ is the number of experts and $ t $ is the time horizon , while maintaining the essentially zero loss property .
the f-measure , originally introduced in information retrieval , is nowadays routinely used as a performance metric for problems such as binary classification , multi-label classification , and structured output prediction . <eos> optimizing this measure remains a statistically and computationally challenging problem , since no closed-form maximizer exists . <eos> current algorithms are approximate and typically rely on additional assumptions regarding the statistical distribution of the binary response variables . <eos> in this paper , we present an algorithm which is not only computationally efficient but also exact , regardless of the underlying distribution . <eos> the algorithm requires only a quadratic number of parameters of the joint distribution ( with respect to the number of binary responses ) . <eos> we illustrate its practical performance by means of experimental results for multi-label classification .
stochastic gradient descent ( sgd ) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks . <eos> several researchers have recently proposed schemes to parallelize sgd , but all require performance-destroying memory locking and synchronization . <eos> this work aims to show using novel theoretical analysis , algorithms , and implementation that sgd can be implemented *without any locking* . <eos> we present an update scheme called hogwild which allows processors access to shared memory with the possibility of overwriting each other 's work . <eos> we show that when the associated optimization problem is sparse , meaning most gradient updates only modify small parts of the decision variable , then hogwild achieves a nearly optimal rate of convergence . <eos> we demonstrate experimentally that hogwild outperforms alternative schemes that use locking by an order of magnitude .
loopy belief propagation performs approximate inference on graphical models with loops . <eos> one might hope to compensate for the approximation by adjusting model parameters . <eos> learning algorithms for this purpose have been explored previously , and the claim has been made that every set of locally consistent marginals can arise from belief propagation run on a graphical model . <eos> on the contrary , here we show that many probability distributions have marginals that can not be reached by belief propagation using any set of model parameters or any learning algorithm . <eos> we call such marginals ` unbelievable . ' <eos> this problem occurs whenever the hessian of the bethe free energy is not positive-definite at the target marginals . <eos> all learning algorithms for belief propagation necessarily fail in these cases , producing beliefs or sets of beliefs that may even be worse than the pre-learning approximation . <eos> we then show that averaging inaccurate beliefs , each obtained from belief propagation using model parameters perturbed about some learned mean values , can achieve the unbelievable marginals .
many real-world networks are described by both connectivity information and features for every node . <eos> to better model and understand these networks , we present structure preserving metric learning ( spml ) , an algorithm for learning a mahalanobis distance metric from a network such that the learned distances are tied to the inherent connectivity structure of the network . <eos> like the graph embedding algorithm structure preserving embedding , spml learns a metric which is structure preserving , meaning a connectivity algorithm such as k-nearest neighbors will yield the correct connectivity when applied using the distances from the learned metric . <eos> we show a variety of synthetic and real-world experiments where spml predicts link patterns from node features more accurately than standard techniques . <eos> we further demonstrate a method for optimizing spml based on stochastic gradient descent which removes the running-time dependency on the size of the network and allows the method to easily scale to networks of thousands of nodes and millions of edges .
a model of human visual search is proposed . <eos> it predicts both response time ( rt ) and error rates ( rt ) as a function of image parameters such as target contrast and clutter . <eos> the model is an ideal observer , in that it optimizes the bayes ratio of tar- get present vs target absent . <eos> the ratio is computed on the firing pattern of v1/v2 neurons , modeled by poisson distributions . <eos> the optimal mechanism for integrat- ing information over time is shown to be a ? soft max ? <eos> of diffusions , computed over the visual field by hypercolumnsof neurons that share the same receptive field and have different response properties to image features . <eos> an approximation of the optimal bayesian observer , based on integrating local decisions , rather than diffusions , is also derived ; it is shown experimentally to produce very similar pre- dictions . <eos> a psychophyisics experiment is proposed that may discriminate between which mechanism is used in the human brain .
latent tree graphical models are natural tools for expressing long range and hierarchical dependencies among many variables which are common in computer vision , bioinformatics and natural language processing problems . <eos> however , existing models are largely restricted to discrete and gaussian variables due to computational constraints ; furthermore , algorithms for estimating the latent tree structure and learning the model parameters are largely restricted to heuristic local search . <eos> we present a method based on kernel embeddings of distributions for latent tree graphical models with continuous and non-gaussian variables . <eos> our method can recover the latent tree structures with provable guarantees and perform local-minimum free parameter learning and efficient inference . <eos> experiments on simulated and real data show the advantage of our proposed approach .
we introduce the piecewise-constant conditional intensity model , a model for learning temporal dependencies in event streams . <eos> we describe a closed-form bayesian approach to learning these models , and describe an importance sampling algorithm for forecasting future events using these models , using a proposal distribution based on poisson superposition . <eos> we then use synthetic data , supercomputer event logs , and web search query logs to illustrate that our learning algorithm can efficiently learn nonlinear temporal dependencies , and that our importance sampling algorithm can effectively forecast future events .
crowdsourcing systems , in which tasks are electronically distributed to numerous `` information piece-workers '' , have emerged as an effective paradigm for human-powered solving of large scale problems in domains such as image classification , data entry , optical character recognition , recommendation , and proofreading . <eos> because these low-paid workers can be unreliable , nearly all crowdsourcers must devise schemes to increase confidence in their answers , typically by assigning each task multiple times and combining the answers in some way such as majority voting . <eos> in this paper , we consider a general model of such rowdsourcing tasks , and pose the problem of minimizing the total price ( i.e. , number of task assignments ) that must be paid to achieve a target overall reliability . <eos> we give new algorithms for deciding which tasks to assign to which workers and for inferring correct answers from the workers answers . <eos> we show that our algorithm significantly outperforms majority voting and , in fact , are asymptotically optimal through comparison to an oracle that knows the reliability of every worker .
cancer has complex patterns of progression that include converging as well as diverging progressional pathways . <eos> vogelstein 's path model of colon cancer was a pioneering contribution to cancer research . <eos> since then , several attempts have been made at obtaining mathematical models of cancer progression , devising learning algorithms , and applying these to cross-sectional data . <eos> beerenwinkel { \em et al . } <eos> provided , what they coined , em-like algorithms for oncogenetic trees ( ots ) and mixtures of such . <eos> given the small size of current and future data sets , it is important to minimize the number of parameters of a model . <eos> for this reason , we too focus on tree-based models and introduce hidden-variable oncogenetic trees ( hots ) . <eos> in contrast to ots , hots allow for errors in the data and thereby provide more realistic modeling . <eos> we also design global structural em algorithms for learning hots and mixtures of hots ( hot-mixtures ) . <eos> the algorithms are global in the sense that , during the m-step , they find a structure that yields a global maximum of the expected complete log-likelihood rather than merely one that improves it . <eos> the algorithm for single hots performs very well on reasonable-sized data sets , while that for hot-mixtures requires data sets of sizes obtainable only with tomorrow 's more cost-efficient technologies .
this paper studies the problem of semi-supervised learning from the vector field perspective . <eos> many of the existing work use the graph laplacian to ensure the smoothness of the prediction function on the data manifold . <eos> however , beyond smoothness , it is suggested by recent theoretical work that we should ensure second order smoothness for achieving faster rates of convergence for semi-supervised regression problems . <eos> to achieve this goal , we show that the second order smoothness measures the linearity of the function , and the gradient field of a linear function has to be a parallel vector field . <eos> consequently , we propose to find a function which minimizes the empirical error , and simultaneously requires its gradient field to be as parallel as possible . <eos> we give a continuous objective function on the manifold and discuss how to discretize it by using random points . <eos> the discretized optimization problem turns out to be a sparse linear system which can be solved very efficiently . <eos> the experimental results have demonstrated the effectiveness of our proposed approach .
metric learning has become a very active research field . <eos> the most popular representative -- mahalanobis metric learning -- can be seen as learning a linear transformation and then computing the euclidean metric in the transformed space . <eos> since a linear transformation might not always be appropriate for a given learning problem , kernelized versions of various metric learning algorithms exist . <eos> however , the problem then becomes finding the appropriate kernel function . <eos> multiple kernel learning addresses this limitation by learning a linear combination of a number of predefined kernels ; this approach can be also readily used in the context of multiple-source learning to fuse different data sources . <eos> surprisingly , and despite the extensive work on multiple kernel learning for svms , there has been no work in the area of metric learning with multiple kernel learning . <eos> in this paper we fill this gap and present a general approach for metric learning with multiple kernel learning . <eos> our approach can be instantiated with different metric learning algorithms provided that they satisfy some constraints . <eos> experimental evidence suggests that our approach outperforms metric learning with an unweighted kernel combination and metric learning with cross-validation based kernel selection .
learning sparse representations on data adaptive dictionaries is a state-of-the-art method for modeling data . <eos> but when the dictionary is large and the data dimension is high , it is a computationally challenging problem . <eos> we explore three aspects of the problem . <eos> first , we derive new , greatly improved screening tests that quickly identify codewords that are guaranteed to have zero weights . <eos> second , we study the properties of random projections in the context of learning sparse representations . <eos> finally , we develop a hierarchical framework that uses incremental random projections and screening to learn , in small stages , a hierarchically structured dictionary for sparse representations . <eos> empirical results show that our framework can learn informative hierarchical sparse representations more efficiently .
nonparametric bayesian methods are developed for analysis of multi-channel spike-train data , with the feature learning and spike sorting performed jointly . <eos> the feature learning and sorting are performed simultaneously across all channels . <eos> dictionary learning is implemented via the beta-bernoulli process , with spike sorting performed via the dynamic hierarchical dirichlet process ( dhdp ) , with these two models coupled . <eos> the dhdp is augmented to eliminate refractoryperiod violations , it allows the ? appearance and ? disappearance of neurons over time , and it models smooth variation in the spike statistics .
we consider large matrices of low rank . <eos> we address the problem of recovering such matrices when most of the entries are unknown . <eos> matrix completion finds applications in recommender systems . <eos> in this setting , the rows of the matrix may correspond to items and the columns may correspond to users . <eos> the known entries are the ratings given by users to some items . <eos> the aim is to predict the unobserved ratings . <eos> this problem is commonly stated in a constrained optimization framework . <eos> we follow an approach that exploits the geometry of the low-rank constraint to recast the problem as an unconstrained optimization problem on the grassmann manifold . <eos> we then apply first- and second-order riemannian trust-region methods to solve it . <eos> the cost of each iteration is linear in the number of known entries . <eos> our methods , rtrmc 1 and 2 , outperform state-of-the-art algorithms on a wide range of problem instances .
it has been argued that perceptual multistability reflects probabilistic inference performed by the brain when sensory input is ambiguous . <eos> alternatively , more traditional explanations of multistability refer to low-level mechanisms such as neuronal adaptation . <eos> we employ a deep boltzmann machine ( dbm ) model of cortical processing to demonstrate that these two different approaches can be combined in the same framework . <eos> based on recent developments in machine learning , we show how neuronal adaptation can be understood as a mechanism that improves probabilistic , sampling-based inference . <eos> using the ambiguous necker cube image , we analyze the perceptual switching exhibited by the model . <eos> we also examine the influence of spatial attention , and explore how binocular rivalry can be modeled with the same approach . <eos> our work joins earlier studies in demonstrating how the principles underlying dbms relate to cortical processing , and offers novel perspectives on the neural implementation of approximate probabilistic inference in the brain .
inferring key unobservable features of individuals is an important task in the applied sciences . <eos> in particular , an important source of data in fields such as marketing , social sciences and medicine is questionnaires : answers in such questionnaires are noisy measures of target unobserved features . <eos> while comprehensive surveys help to better estimate the latent variables of interest , aiming at a high number of questions comes at a price : refusal to participate in surveys can go up , as well as the rate of missing data ; quality of answers can decline ; costs associated with applying such questionnaires can also increase . <eos> in this paper , we cast the problem of refining existing models for questionnaire data as follows : solve a constrained optimization problem of preserving the maximum amount of information found in a latent variable model using only a subset of existing questions . <eos> the goal is to find an optimal subset of a given size . <eos> for that , we first define an information theoretical measure for quantifying the quality of a reduced questionnaire . <eos> three different approximate inference methods are introduced to solve this problem . <eos> comparisons against a simple but powerful heuristic are presented .
for many of the state-of-the-art computer vision algorithms , image segmentation is an important preprocessing step . <eos> as such , several image segmentation algorithms have been proposed , however , with certain reservation due to high computational load and many hand-tuning parameters . <eos> correlation clustering , a graph-partitioning algorithm often used in natural language processing and document clustering , has the potential to perform better than previously proposed image segmentation algorithms . <eos> we improve the basic correlation clustering formulation by taking into account higher-order cluster relationships . <eos> this improves clustering in the presence of local boundary ambiguities . <eos> we first apply the pairwise correlation clustering to image segmentation over a pairwise superpixel graph and then develop higher-order correlation clustering over a hypergraph that considers higher-order relations among superpixels . <eos> fast inference is possible by linear programming relaxation , and also effective parameter learning framework by structured support vector machine is possible . <eos> experimental results on various datasets show that the proposed higher-order correlation clustering outperforms other state-of-the-art image segmentation algorithms .
variational message passing ( vmp ) is an algorithmic implementation of the variational bayes ( vb ) method which applies only in the special case of conjugate exponential family models . <eos> we propose an extension to vmp , which we refer to as non-conjugate variational message passing ( ncvmp ) which aims to alleviate this restriction while maintaining modularity , allowing choice in how expectations are calculated , and integrating into an existing message-passing framework : infer.net . <eos> we demonstrate ncvmp on logistic binary and multinomial regression . <eos> in the multinomial case we introduce a novel variational bound for the softmax factor which is tighter than other commonly used bounds whilst maintaining computational tractability .
high dimensional similarity search in large scale databases becomes an important challenge due to the advent of internet . <eos> for such applications , specialized data structures are required to achieve computational efficiency . <eos> traditional approaches relied on algorithmic constructions that are often data independent ( such as locality sensitive hashing ) or weakly dependent ( such as kd-trees , k-means trees ) . <eos> while supervised learning algorithms have been applied to related problems , those proposed in the literature mainly focused on learning hash codes optimized for compact embedding of the data rather than search efficiency . <eos> consequently such an embedding has to be used with linear scan or another search algorithm . <eos> hence learning to hash does not directly address the search efficiency issue . <eos> this paper considers a new framework that applies supervised learning to directly optimize a data structure that supports efficient large scale search . <eos> our approach takes both search quality and computational cost into consideration . <eos> specifically , we learn a boosted search forest that is optimized using pair-wise similarity labeled examples . <eos> the output of this search forest can be efficiently converted into an inverted indexing data structure , which can leverage modern text search infrastructure to achieve both scalability and efficiency . <eos> experimental results show that our approach significantly outperforms the start-of-the-art learning to hash methods ( such as spectral hashing ) , as well as state-of-the-art high dimensional search algorithms ( such as lsh and k-means trees ) .
we combine three important ideas present in previous work for building classifiers : the semi-supervised hypothesis ( the input distribution contains information about the classifier ) , the unsupervised manifold hypothesis ( data density concentrates near low-dimensional manifolds ) , and the manifold hypothesis for classification ( different classes correspond to disjoint manifolds separated by low density ) . <eos> we exploit a novel algorithm for capturing manifold structure ( high-order contractive auto-encoders ) and we show how it builds a topological atlas of charts , each chart being characterized by the principal singular vectors of the jacobian of a representation mapping . <eos> this representation learning algorithm can be stacked to yield a deep architecture , and we combine it with a domain knowledge-free version of the tangentprop algorithm to encourage the classifier to be insensitive to local directions changes along the manifold . <eos> record-breaking classification results are obtained .
the exploration-exploitation trade-off is among the central challenges of reinforcement learning . <eos> the optimal bayesian solution is intractable in general . <eos> this paper studies to what extent analytic statements about optimal learning are possible if all beliefs are gaussian processes . <eos> a first order approximation of learning of both loss and dynamics , for nonlinear , time-varying systems in continuous time and space , subject to a relatively weak restriction on the dynamics , is described by an infinite-dimensional partial differential equation . <eos> an approximate finite-dimensional projection gives an impression for how this result may be helpful .
neurons typically respond to a restricted number of stimulus features within the high-dimensional space of natural stimuli . <eos> here we describe an explicit model-based interpretation of traditional estimators for a neuron 's multi-dimensional feature space , which allows for several important generalizations and extensions . <eos> first , we show that traditional estimators based on the spike-triggered average ( sta ) and spike-triggered covariance ( stc ) can be formalized in terms of the `` expected log-likelihood '' of a linear-nonlinear-poisson ( lnp ) model with gaussian stimuli . <eos> this model-based formulation allows us to define maximum-likelihood and bayesian estimators that are statistically consistent and efficient in a wider variety of settings , such as with naturalistic ( non-gaussian ) stimuli . <eos> it also allows us to employ bayesian methods for regularization , smoothing , sparsification , and model comparison , and provides bayesian confidence intervals on model parameters . <eos> we describe an empirical bayes method for selecting the number of features , and extend the model to accommodate an arbitrary elliptical nonlinear response function , which results in a more powerful and more flexible model for feature space inference . <eos> we validate these methods using neural data recorded extracellularly from macaque primary visual cortex .
a hallmark of modern machine learning is its ability to deal with high dimensional problems by exploiting structural assumptions that limit the degrees of freedom in the underlying model . <eos> a deep understanding of the capabilities and limits of high dimensional learning methods under specific assumptions such as sparsity , group sparsity , and low rank has been attained . <eos> efforts ( negahban et al. , 2010 , chandrasekaran et al. , 2010 } are now underway to distill this valuable experience by proposing general unified frameworks that can achieve the twin goals of summarizing previous analyses and enabling their application to notions of structure hitherto unexplored . <eos> inspired by these developments , we propose and analyze a general computational scheme based on a greedy strategy to solve convex optimization problems that arise when dealing with structurally constrained high-dimensional problems . <eos> our framework not only unifies existing greedy algorithms by recovering them as special cases but also yields novel ones . <eos> finally , we extend our results to infinite dimensional problems by using interesting connections between smoothness of norms and behavior of martingales in banach spaces .
a nonparametric kernel-based method for realizing bayes ' rule is proposed , based on kernel representations of probabilities in reproducing kernel hilbert spaces . <eos> the prior and conditional probabilities are expressed as empirical kernel mean and covariance operators , respectively , and the kernel mean of the posterior distribution is computed in the form of a weighted sample . <eos> the kernel bayes ' rule can be applied to a wide variety of bayesian inference problems : we demonstrate bayesian computation without likelihood , and filtering with a nonparametric state-space model . <eos> a consistency rate for the posterior estimate is established .
the problem of selecting the right state-representation in a reinforcement learning problem is considered . <eos> several models ( functions mapping past observations to a finite set ) of the observations are given , and it is known that for at least one of these models the resulting state dynamics are indeed markovian . <eos> without knowing neither which of the models is the correct one , nor what are the probabilistic characteristics of the resulting mdp , it is required to obtain as much reward as the optimal policy for the correct model ( or for the best of the correct models , if there are several ) . <eos> we propose an algorithm that achieves that , with a regret of order t^ { 2/3 } where t is the horizon time .
we consider the problem of estimating neural spikes from extracellular voltage recordings . <eos> most current methods are based on clustering , which requires substantial human supervision and produces systematic errors by failing to properly handle temporally overlapping spikes . <eos> we formulate the problem as one of statistical inference , in which the recorded voltage is a noisy sum of the spike trains of each neuron convolved with its associated spike waveform . <eos> joint maximum-a-posteriori ( map ) estimation of the waveforms and spikes is then a blind deconvolution problem in which the coefficients are sparse . <eos> we develop a block-coordinate descent method for approximating the map solution . <eos> we validate our method on data simulated according to the generative model , as well as on real data for which ground truth is available via simultaneous intracellular recordings . <eos> in both cases , our method substantially reduces the number of missed spikes and false positives when compared to a standard clustering algorithm , primarily by recovering temporally overlapping spikes . <eos> the method offers a fully automated alternative to clustering methods that is less susceptible to systematic errors .
we improve the theoretical analysis and empirical performance of algorithms for the stochastic multi-armed bandit problem and the linear stochastic multi-armed bandit problem . <eos> in particular , we show that a simple modification of auer ? s ucb algorithm ( auer , 2002 ) achieves with high probability constant regret . <eos> more importantly , we modify and , consequently , improve the analysis of the algorithm for the for linear stochastic bandit problem studied by auer ( 2002 ) , dani et al . ( 2008 ) , rusmevichientong and tsitsiklis ( 2010 ) , li et al . ( 2010 ) . <eos> our modification improves the regret bound by a logarithmic factor , though experiments show a vast improvement . <eos> in both cases , the improvement stems from the construction of smaller confidence sets . <eos> for their construction we use a novel tail inequality for vector-valued martingales .
for most scene understanding tasks ( such as object detection or depth estimation ) , the classifiers need to consider contextual information in addition to the local features . <eos> we can capture such contextual information by taking as input the features/attributes from all the regions in the image . <eos> however , this contextual dependence also varies with the spatial location of the region of interest , and we therefore need a different set of parameters for each spatial location . <eos> this results in a very large number of parameters . <eos> in this work , we model the independence properties between the parameters for each location and for each task , by defining a markov random field ( mrf ) over the parameters . <eos> in particular , two sets of parameters are encouraged to have similar values if they are spatially close or semantically close . <eos> our method is , in principle , complementary to other ways of capturing context such as the ones that use a graphical model over the labels instead . <eos> in extensive evaluation over two different settings , of multi-class object detection and of multiple scene understanding tasks ( scene categorization , depth estimation , geometric labeling ) , our method beats the state-of-the-art methods in all the four tasks .
recently , image categorization has been an active research topic due to the urgent need to retrieve and browse digital images via semantic keywords . <eos> this paper formulates image categorization as a multi-label classification problem using recent advances in matrix completion . <eos> under this setting , classification of testing data is posed as a problem of completing unknown label entries on a data matrix that concatenates training and testing features with training labels . <eos> we propose two convex algorithms for matrix completion based on a rank minimization criterion specifically tailored to visual data , and prove its convergence properties . <eos> a major advantage of our approach w.r.t . <eos> standard discriminative classification methods for image categorization is its robustness to outliers , background noise and partial occlusions both in the feature and label space . <eos> experimental validation on several datasets shows how our method outperforms state-of-the-art algorithms , while effectively capturing semantic concepts of classes .
we present a probabilistic algorithm for nonlinear inverse reinforcement learning . <eos> the goal of inverse reinforcement learning is to learn the reward function in a markov decision process from expert demonstrations . <eos> while most prior inverse reinforcement learning algorithms represent the reward as a linear combination of a set of features , we use gaussian processes to learn the reward as a nonlinear function , while also determining the relevance of each feature to the expert 's policy . <eos> our probabilistic algorithm allows complex behaviors to be captured from suboptimal stochastic demonstrations , while automatically balancing the simplicity of the learned reward structure against its consistency with the observed actions .
log-linear models are widely used probability models for statistical pattern recognition . <eos> typically , log-linear models are trained according to a convex criterion . <eos> in recent years , the interest in log-linear models has greatly increased . <eos> the optimization of log-linear model parameters is costly and therefore an important topic , in particular for large-scale applications . <eos> different optimization algorithms have been evaluated empirically in many papers . <eos> in this work , we analyze the optimization problem analytically and show that the training of log-linear models can be highly ill-conditioned . <eos> we verify our findings on two handwriting tasks . <eos> by making use of our convergence analysis , we obtain good results on a large-scale continuous handwriting recognition task with a simple and generic approach .
while loopy belief propagation ( lbp ) has been utilized in a wide variety of applications with empirical success , it comes with few theoretical guarantees . <eos> especially , if the interactions of random variables in a graphical model are strong , the behaviors of the algorithm can be difficult to analyze due to underlying phase transitions . <eos> in this paper , we develop a novel approach to the uniqueness problem of the lbp fixed point ; our new ? necessary and sufficient condition is stated in terms of graphs and signs , where the sign denotes the types ( attractive/repulsive ) of the interaction ( i.e. , compatibility function ) on the edge . <eos> in all previous works , uniqueness is guaranteed only in the situations where the strength of the interactions are ? sufficiently small in certain senses . <eos> in contrast , our condition covers arbitrary strong interactions on the specified class of signed graphs . <eos> the result of this paper is based on the recent theoretical advance in the lbp algorithm ; the connection with the graph zeta function .
given one feature of a novel animal , humans readily make inferences about other features of the animal . <eos> for example , winged creatures often fly , and creatures that eat fish often live in the water . <eos> we explore the knowledge that supports these inferences and compare two approaches . <eos> the first approach proposes that humans rely on abstract representations of dependency relationships between features , and is formalized here as a graphical model . <eos> the second approach proposes that humans rely on specific knowledge of previously encountered animals , and is formalized here as a family of exemplar models . <eos> we evaluate these models using a task where participants reason about chimeras , or animals with pairs of features that have not previously been observed to co-occur . <eos> the results support the hypothesis that humans rely on explicit representations of relationships between features .
we study a particular class of cyclic causal models , where each variable is a ( possibly nonlinear ) function of its parents and additive noise . <eos> we prove that the causal graph of such models is generically identifiable in the bivariate , gaussian-noise case . <eos> we also propose a method to learn such models from observational data . <eos> in the acyclic case , the method reduces to ordinary regression , but in the more challenging cyclic case , an additional term arises in the loss function , which makes it a special case of nonlinear independent component analysis . <eos> we illustrate the proposed method on synthetic data .
increasingly , optimization problems in machine learning , especially those arising from high-dimensional statistical estimation , have a large number of variables . <eos> modern statistical estimators developed over the past decade have statistical or sample complexity that depends only weakly on the number of parameters when there is some structure to the problem , such as sparsity . <eos> a central question is whether similar advances can be made in their computational complexity as well . <eos> in this paper , we propose strategies that indicate that such advances can indeed be made . <eos> in particular , we investigate the greedy coordinate descent algorithm , and note that performing the greedy step efficiently weakens the costly dependence on the problem size provided the solution is sparse . <eos> we then propose a suite of methods that perform these greedy steps efficiently by a reduction to nearest neighbor search . <eos> we also devise a more amenable form of greedy descent for composite non-smooth objectives ; as well as several approximate variants of such greedy descent . <eos> we develop a practical implementation of our algorithm that combines greedy coordinate descent with locality sensitive hashing . <eos> without tuning the latter data structure , we are not only able to significantly speed up the vanilla greedy method , but also outperform cyclic descent when the problem size becomes large . <eos> our results indicate the effectiveness of our nearest neighbor strategies , and also point to many open questions regarding the development of computational geometric techniques tailored towards first-order optimization methods .
components estimated by independent component analysis and related methods are typically not independent in real data . <eos> a very common form of nonlinear dependency between the components is correlations in their variances or ener- gies . <eos> here , we propose a principled probabilistic model to model the energy- correlations between the latent variables . <eos> our two-stage model includes a linear mixing of latent signals into the observed ones like in ica . <eos> the main new fea- ture is a model of the energy-correlations based on the structural equation model ( sem ) , in particular , a linear non-gaussian sem . <eos> the sem is closely related to divisive normalization which effectively reduces energy correlation . <eos> our new two- stage model enables estimation of both the linear mixing and the interactions re- lated to energy-correlations , without resorting to approximations of the likelihood function or other non-principled approaches . <eos> we demonstrate the applicability of our method with synthetic dataset , natural images and brain signals .
this paper examines the problem of ranking a collection of objects using pairwise comparisons ( rankings of two objects ) . <eos> in general , the ranking of $ n $ objects can be identified by standard sorting methods using $ n\log_2 n $ pairwise comparisons . <eos> we are interested in natural situations in which relationships among the objects may allow for ranking using far fewer pairwise comparisons . <eos> { specifically , we assume that the objects can be embedded into a $ d $ -dimensional euclidean space and that the rankings reflect their relative distances from a common reference point in $ \r^d $ . <eos> we show that under this assumption the number of possible rankings grows like $ n^ { 2d } $ and demonstrate an algorithm that can identify a randomly selected ranking using just slightly more than $ d\log n $ adaptively selected pairwise comparisons , on average . } <eos> if instead the comparisons are chosen at random , then almost all pairwise comparisons must be made in order to identify any ranking . <eos> in addition , we propose a robust , error-tolerant algorithm that only requires that the pairwise comparisons are probably correct . <eos> experimental studies with synthetic and real datasets support the conclusions of our theoretical analysis .
given a set $ v $ of $ n $ elements we wish to linearly order them using pairwise preference labels which may be non-transitive ( due to irrationality or arbitrary noise ) . <eos> the goal is to linearly order the elements while disagreeing with as few pairwise preference labels as possible . <eos> our performance is measured by two parameters : the number of disagreements ( loss ) and the query complexity ( number of pairwise preference labels ) . <eos> our algorithm adaptively queries at most $ o ( n\poly ( \log n , \eps^ { -1 } ) ) $ preference labels for a regret of $ \eps $ times the optimal loss . <eos> this is strictly better , and often significantly better than what non-adaptive sampling could achieve . <eos> our main result helps settle an open problem posed by learning-to-rank ( from pairwise information ) theoreticians and practitioners : what is a provably correct way to sample preference labels ?
generalized linear models ( glms ) and single index models ( sims ) provide powerful generalizations of linear regression , where the target variable is assumed to be a ( possibly unknown ) 1-dimensional function of a linear predictor . <eos> in general , these problems entail non-convex estimation procedures , and , in practice , iterative local search heuristics are often used . <eos> kalai and sastry ( 2009 ) provided the first provably efficient method , the \emph { isotron } algorithm , for learning sims and glms , under the assumption that the data is in fact generated under a glm and under certain monotonicity and lipschitz ( bounded slope ) constraints . <eos> the isotron algorithm interleaves steps of perceptron-like updates with isotonic regression ( fitting a one-dimensional non-decreasing function ) . <eos> however , to obtain provable performance , the method requires a fresh sample every iteration . <eos> in this paper , we provide algorithms for learning glms and sims , which are both computationally and statistically efficient . <eos> we modify the isotonic regression step in isotron to fit a lipschitz monotonic function , and also provide an efficient $ o ( n \log ( n ) ) $ algorithm for this step , improving upon the previous $ o ( n^2 ) $ algorithm . <eos> we provide a brief empirical study , demonstrating the feasibility of our algorithms in practice .
latent variable mixture models are a powerful tool for exploring the structure in large datasets . <eos> a common challenge for interpreting such models is a desire to impose sparsity , the natural assumption that each data point only contains few latent features . <eos> since mixture distributions are constrained in their l1 norm , typical sparsity techniques based on l1 regularization become toothless , and concave regularization becomes necessary . <eos> unfortunately concave regularization typically results in em algorithms that must perform problematic non-concave m-step maximizations . <eos> in this work , we introduce a technique for circumventing this difficulty , using the so-called mountain pass theorem to provide easily verifiable conditions under which the m-step is well-behaved despite the lacking concavity . <eos> we also develop a correspondence between logarithmic regularization and what we term the pseudo-dirichlet distribution , a generalization of the ordinary dirichlet distribution well-suited for inducing sparsity . <eos> we demonstrate our approach on a text corpus , inferring a sparse topic mixture model for 2,406 weblogs .
the nested chinese restaurant process is extended to design a nonparametric topic-model tree for representation of human choices . <eos> each tree branch corresponds to a type of person , and each node ( topic ) has a corresponding probability vector over items that may be selected . <eos> the observed data are assumed to have associated temporal covariates ( corresponding to the time at which choices are made ) , and we wish to impose that with increasing time it is more probable that topics deeper in the tree are utilized . <eos> this structure is imposed by developing a new ? change point '' stick-breaking model that is coupled with a poisson and product-of-gammas construction . <eos> to share topics across the tree nodes , topic distributions are drawn from a dirichlet process . <eos> as a demonstration of this concept , we analyze real data on course selections of undergraduate students at duke university , with the goal of uncovering and concisely representing structure in the curriculum and in the characteristics of the student body .
mini-batch algorithms have recently received significant attention as a way to speed-up stochastic convex optimization problems . <eos> in this paper , we study how such algorithms can be improved using accelerated gradient methods . <eos> we provide a novel analysis , which shows how standard gradient methods may sometimes be insufficient to obtain a significant speed-up . <eos> we propose a novel accelerated gradient algorithm , which deals with this deficiency , and enjoys a uniformly superior guarantee . <eos> we conclude our paper with experiments on real-world datasets , which validates our algorithm and substantiates our theoretical insights .
domain adaptation algorithms seek to generalize a model trained in a source domain to a new target domain . <eos> in many practical cases , the source and target distributions can differ substantially , and in some cases crucial target features may not have support in the source domain . <eos> in this paper we introduce an algorithm that bridges the gap between source and target domains by slowly adding both the target features and instances in which the current algorithm is the most confident . <eos> our algorithm is a variant of co-training , and we name it coda ( co-training for domain adaptation ) . <eos> unlike the original co-training work , we do not assume a particular feature split . <eos> instead , for each iteration of co-training , we add target features and formulate a single optimization problem which simultaneously learns a target predictor , a split of the feature space into views , and a shared subset of source and target features to include in the predictor . <eos> coda significantly out-performs the state-of-the-art on the 12-domain benchmark data set of blitzer et al.. <eos> indeed , over a wide range ( 65 of 84 comparisons ) of target supervision , ranging from no labeled target data to a relatively large number of target labels , coda achieves the best performance .
many machine learning and signal processing problems can be formulated as linearly constrained convex programs , which could be efficiently solved by the alternating direction method ( adm ) . <eos> however , usually the subproblems in adm are easily solvable only when the linear mappings in the constraints are identities . <eos> to address this issue , we propose a linearized adm ( ladm ) method by linearizing the quadratic penalty term and adding a proximal term when solving the subproblems . <eos> for fast convergence , we also allow the penalty to change adaptively according a novel update rule . <eos> we prove the global convergence of ladm with adaptive penalty ( ladmap ) . <eos> as an example , we apply ladmap to solve low-rank representation ( lrr ) , which is an important subspace clustering technique yet suffers from high computation cost . <eos> by combining ladmap with a skinny svd representation technique , we are able to reduce the complexity $ o ( n^3 ) $ of the original adm based method to $ o ( rn^2 ) $ , where $ r $ and $ n $ are the rank and size of the representation matrix , respectively , hence making lrr possible for large scale applications . <eos> numerical experiments verify that for lrr our ladmap based methods are much faster than state-of-the-art algorithms .
transfer reinforcement learning ( rl ) methods leverage on the experience collected on a set of source tasks to speed-up rl algorithms . <eos> a simple and effective approach is to transfer samples from source tasks and include them in the training set used to solve a target task . <eos> in this paper , we investigate the theoretical properties of this transfer method and we introduce novel algorithms adapting the transfer process on the basis of the similarity between source and target tasks . <eos> finally , we report illustrative experimental results in a continuous chain problem .
the development of statistical models for continuous-time longitudinal network data is of increasing interest in machine learning and social science . <eos> leveraging ideas from survival and event history analysis , we introduce a continuous-time regression modeling framework for network event data that can incorporate both time-dependent network statistics and time-varying regression coefficients . <eos> we also develop an efficient inference scheme that allows our approach to scale to large networks . <eos> on synthetic and real-world data , empirical results demonstrate that the proposed inference approach can accurately estimate the coefficients of the regression model , which is useful for interpreting the evolution of the network ; furthermore , the learned model has systematically better predictive performance compared to standard baseline methods .
reinforcement learning models address animal 's behavioral adaptation to its changing `` external '' environment , and are based on the assumption that pavlovian , habitual and goal-directed responses seek to maximize reward acquisition . <eos> negative-feedback models of homeostatic regulation , on the other hand , are concerned with behavioral adaptation in response to the `` internal '' state of the animal , and assume that animals ' behavioral objective is to minimize deviations of some key physiological variables from their hypothetical setpoints . <eos> building upon the drive-reduction theory of reward , we propose a new analytical framework that integrates learning and regulatory systems , such that the two seemingly unrelated objectives of reward maximization and physiological-stability prove to be identical . <eos> the proposed theory shows behavioral adaptation to both internal and external states in a disciplined way . <eos> we further show that the proposed framework allows for a unified explanation of some behavioral phenomenon like motivational sensitivity of different associative learning mechanism , anticipatory responses , interaction among competing motivational systems , and risk aversion .
we consider the problem of recovering a matrix $ \mathbf { m } $ that is the sum of a low-rank matrix $ \mathbf { l } $ and a sparse matrix $ \mathbf { s } $ from a small set of linear measurements of the form $ \mathbf { y } = \mathcal { a } ( \mathbf { m } ) = \mathcal { a } ( { \bf l } + { \bf s } ) $ . <eos> this model subsumes three important classes of signal recovery problems : compressive sensing , affine rank minimization , and robust principal component analysis . <eos> we propose a natural optimization problem for signal recovery under this model and develop a new greedy algorithm called sparcs to solve it . <eos> sparcs inherits a number of desirable properties from the state-of-the-art cosamp and admira algorithms , including exponential convergence and efficient implementation . <eos> simulation results with video compressive sensing , hyperspectral imaging , and robust matrix completion data sets demonstrate both the accuracy and efficacy of the algorithm .
in recent years , a rich variety of shrinkage priors have been proposed that have great promise in addressing massive regression problems . <eos> in general , these new priors can be expressed as scale mixtures of normals , but have more complex forms and better properties than traditional cauchy and double exponential priors . <eos> we first propose a new class of normal scale mixtures through a novel generalized beta distribution that encompasses many interesting priors as special cases . <eos> this encompassing framework should prove useful in comparing competing priors , considering properties and revealing close connections . <eos> we then develop a class of variational bayes approximations through the new hierarchy presented that will scale more efficiently to the types of truly massive data sets that are now encountered routinely .
diagnosis of alzheimer 's disease ( ad ) at the early stage of the disease development is of great clinical importance . <eos> current clinical assessment that relies primarily on cognitive measures proves low sensitivity and specificity . <eos> the fast growing neuroimaging techniques hold great promise . <eos> research so far has focused on single neuroimaging modalities . <eos> however , as different modalities provide complementary measures for the same disease pathology , fusion of multi-modality data may increase the statistical power in identification of disease-related brain regions . <eos> this is especially true for early ad , at which stage the disease-related regions are most likely to be weak-effect regions that are difficult to be detected from a single modality alone . <eos> we propose a sparse composite linear discriminant analysis model ( sclda ) for identification of disease-related brain regions of early ad from multi-modality data . <eos> sclda uses a novel formulation that decomposes each lda parameter into a product of a common parameter shared by all the modalities and a parameter specific to each modality , which enables joint analysis of all the modalities and borrowing strength from one another . <eos> we prove that this formulation is equivalent to a penalized likelihood with non-convex regularization , which can be solved by the dc ( ( difference of convex functions ) programming . <eos> we show that in using the dc programming , the property of the non-convex regularization in terms of preserving weak-effect features can be nicely revealed . <eos> we perform extensive simulations to show that sclda outperforms existing competing algorithms on feature selection , especially on the ability for identifying weak-effect features . <eos> we apply sclda to the magnetic resonance imaging ( mri ) and positron emission tomography ( pet ) images of 49 ad patients and 67 normal controls ( nc ) . <eos> our study identifies disease-related brain regions consistent with findings in the ad literature .
sparse coding , a method of explaining sensory data with as few dictionary bases as possible , has attracted much attention in computer vision . <eos> for visual object category recognition , l1 regularized sparse coding is combined with spatial pyramid representation to obtain state-of-the-art performance . <eos> however , because of its iterative optimization , applying sparse coding onto every local feature descriptor extracted from an image database can become a major bottleneck . <eos> to overcome this computational challenge , this paper presents `` generalized lasso based approximation of sparse coding '' ( glas ) . <eos> by representing the distribution of sparse coefficients with slice transform , we fit a piece-wise linear mapping function with generalized lasso . <eos> we also propose an efficient post-refinement procedure to perform mutual inhibition between bases which is essential for an overcomplete setting . <eos> the experiments show that glas obtains comparable performance to l1 regularized sparse coding , yet achieves significant speed up demonstrating its effectiveness for large-scale visual recognition problems .
rational models of causal induction have been successful in accounting for people 's judgments about the existence of causal relationships . <eos> however , these models have focused on explaining inferences from discrete data of the kind that can be summarized in a 2 2 contingency table . <eos> this severely limits the scope of these models , since the world often provides non-binary data . <eos> we develop a new rational model of causal induction using continuous dimensions , which aims to diminish the gap between empirical and theoretical approaches and real-world causal induction . <eos> this model successfully predicts human judgments from previous studies better than models of discrete causal inference , and outperforms several other plausible models of causal induction with continuous causes in accounting for people 's inferences in a new experiment .
several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning . <eos> traditionally , hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible . <eos> presently , computer clusters and gpu processors make it possible to run more trials and we show that algorithmic approaches can find better results . <eos> we present hyper-parameter optimization results on tasks of training neural networks and deep belief networks ( dbns ) . <eos> we optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion . <eos> random search has been shown to be sufficiently efficient for learning neural networks for several datasets , but we show it is unreliable for training dbns . <eos> the sequential algorithms are applied to the most difficult dbn learning problems from [ larochelle et al. , 2007 ] and find significantly better results than the best previously reported . <eos> this work contributes novel techniques for making response surface models p ( y|x ) in which many elements of hyper-parameter assignment ( x ) are known to be irrelevant given particular values of other elements .
we study the fundamental problem of learning an unknown large-margin halfspace in the context of parallel computation . <eos> our main positive result is a parallel algorithm for learning a large-margin halfspace that is based on interior point methods from convex optimization and fast parallel algorithms for matrix computations . <eos> we show that this algorithm learns an unknown gamma-margin halfspace over n dimensions using poly ( n,1/gamma ) processors and runs in time ~o ( 1/gamma ) + o ( log n ) . <eos> in contrast , naive parallel algorithms that learn a gamma-margin halfspace in time that depends polylogarithmically on n have omega ( 1/gamma^2 ) runtime dependence on gamma . <eos> our main negative result deals with boosting , which is a standard approach to learning large-margin halfspaces . <eos> we give an information-theoretic proof that in the original pac framework , in which a weak learning algorithm is provided as an oracle that is called by the booster , boosting can not be parallelized : the ability to call the weak learner multiple times in parallel within a single boosting stage does not reduce the overall number of successive stages of boosting that are required .
diversified retrieval and online learning are two core research areas in the design of modern information retrieval systems.in this paper , we propose the linear submodular bandits problem , which is an online learning setting for optimizing a general class of feature-rich submodular utility models for diversified retrieval . <eos> we present an algorithm , called lsbgreedy , and prove that it efficiently converges to a near-optimal model . <eos> as a case study , we applied our approach to the setting of personalized news recommendation , where the system must recommend small sets of news articles selected from tens of thousands of available articles each day . <eos> in a live user study , we found that lsbgreedy significantly outperforms existing online learning approaches .
most online algorithms used in machine learning today are based on variants of mirror descent or follow-the-leader . <eos> in this paper , we present an online algorithm based on a completely different approach , which combines `` random playout '' and randomized rounding of loss subgradients . <eos> as an application of our approach , we provide the first computationally efficient online algorithm for collaborative filtering with trace-norm constrained matrices . <eos> as a second application , we solve an open question linking batch learning and transductive online learning .
we present a computationally efficient technique to compute the distance of high-dimensional appearance descriptor vectors between image windows . <eos> the method exploits the relation between appearance distance and spatial overlap . <eos> we derive an upper bound on appearance distance given the spatial overlap of two windows in an image , and use it to bound the distances of many pairs between two images . <eos> we propose algorithms that build on these basic operations to efficiently solve tasks relevant to many computer vision applications , such as finding all pairs of windows between two images with distance smaller than a threshold , or finding the single pair with the smallest distance . <eos> in experiments on the pascal voc 07 dataset , our algorithms accurately solve these problems while greatly reducing the number of appearance distances computed , and achieve larger speedups than approximate nearest neighbour algorithms based on trees [ 18 ] and on hashing [ 21 ] . <eos> for example , our algorithm finds the most similar pair of windows between two images while computing only 1 % of all distances on average .
we propose a novel adaptive markov chain monte carlo algorithm to compute the partition function . <eos> in particular , we show how to accelerate a flat histogram sampling technique by significantly reducing the number of `` null moves '' in the chain , while maintaining asymptotic convergence properties . <eos> our experiments show that our method converges quickly to highly accurate solutions on a range of benchmark instances , outperforming other state-of-the-art methods such as ijgp , trw , and gibbs sampling both in run-time and accuracy . <eos> we also show how obtaining a so-called density of states distribution allows for efficient weight learning in markov logic theories .
we present a novel class of actor-critic algorithms for actors consisting of sets of interacting modules . <eos> we present , analyze theoretically , and empirically evaluate an update rule for each module , which requires only local information : the module 's input , output , and the td error broadcast by a critic . <eos> such updates are necessary when computation of compatible features becomes prohibitively difficult and are also desirable to increase the biological plausibility of reinforcement learning methods .
the problem of multiclass boosting is considered . <eos> a new framework , based on multi-dimensional codewords and predictors is introduced . <eos> the optimal set of codewords is derived , and a margin enforcing loss proposed . <eos> the resulting risk is minimized by gradient descent on a multidimensional functional space . <eos> two algorithms are proposed : 1 ) cd-mcboost , based on coordinate descent , updates one predictor component at a time , 2 ) gd-mcboost , based on gradient descent , updates all components jointly . <eos> the algorithms differ in the weak learners that they support but are both shown to be 1 ) bayes consistent , 2 ) margin enforcing , and 3 ) convergent to the global minimum of the risk . <eos> they also reduce to adaboost when there are only two classes . <eos> experiments show that both methods outperform previous multiclass boosting approaches on a number of datasets .
artists , advertisers , and photographers are routinely presented with the task of creating an image that a viewer will remember . <eos> while it may seem like image memorability is purely subjective , recent work shows that it is not an inexplicable phenomenon : variation in memorability of images is consistent across subjects , suggesting that some images are intrinsically more memorable than others , independent of a subjects ' contexts and biases . <eos> in this paper , we used the publicly available memorability dataset of isola et al. , and augmented the object and scene annotations with interpretable spatial , content , and aesthetic image properties . <eos> we used a feature-selection scheme with desirable explaining-away properties to determine a compact set of attributes that characterizes the memorability of any individual image . <eos> we find that images of enclosed spaces containing people with visible faces are memorable , while images of vistas and peaceful scenes are not . <eos> contrary to popular belief , unusual or aesthetically pleasing scenes do not tend to be highly memorable . <eos> this work represents one of the first attempts at understanding intrinsic image memorability , and opens a new domain of investigation at the interface between human cognition and computer vision .
we consider the problem of optimizing the sum of a smooth convex function and a non-smooth convex function using proximal-gradient methods , where an error is present in the calculation of the gradient of the smooth term or in the proximity operator with respect to the second term . <eos> we show that the basic proximal-gradient method , the basic proximal-gradient method with a strong convexity assumption , and the accelerated proximal-gradient method achieve the same convergence rates as in the error-free case , provided the errors decrease at an appropriate rate . <eos> our experimental results on a structured sparsity problem indicate that sequences of errors with these appealing theoretical properties can lead to practical performance improvements .
we analyze the statistical performance of a recently proposed convex tensor decomposition algorithm . <eos> conventionally tensor decomposition has been formulated as non-convex optimization problems , which hindered the analysis of their performance . <eos> we show under some conditions that the mean squared error of the convex method scales linearly with the quantity we call the normalized rank of the true tensor . <eos> the current analysis naturally extends the analysis of convex low-rank matrix estimation to tensors . <eos> furthermore , we show through numerical experiments that our theory can precisely predict the scaling behaviour in practice .
although the standard formulations of prediction problems involve fully-observed and noiseless data drawn in an i.i.d . <eos> manner , many applications involve noisy and/or missing data , possibly involving dependencies . <eos> we study these issues in the context of high-dimensional sparse linear regression , and propose novel estimators for the cases of noisy , missing , and/or dependent data . <eos> many standard approaches to noisy or missing data , such as those using the em algorithm , lead to optimization problems that are inherently non-convex , and it is difficult to establish theoretical guarantees on practical algorithms . <eos> while our approach also involves optimizing non-convex programs , we are able to both analyze the statistical error associated with any global optimum , and prove that a simple projected gradient descent algorithm will converge in polynomial time to a small neighborhood of the set of global minimizers . <eos> on the statistical side , we provide non-asymptotic bounds that hold with high probability for the cases of noisy , missing , and/or dependent data . <eos> on the computational side , we prove that under the same types of conditions required for statistical consistency , the projected gradient descent algorithm will converge at geometric rates to a near-global minimizer . <eos> we illustrate these theoretical predictions with simulations , showing agreement with the predicted scalings .
many nonparametric regressors were recently shown to converge at rates that depend only on the intrinsic dimension of data . <eos> these regressors thus escape the curse of dimension when high-dimensional data has low intrinsic dimension ( e.g . a manifold ) . <eos> we show that $ k $ -nn regression is also adaptive to intrinsic dimension . <eos> in particular our rates are local to a query $ x $ and depend only on the way masses of balls centered at $ x $ vary with radius . <eos> furthermore , we show a simple way to choose $ k = k ( x ) $ locally at any $ x $ so as to nearly achieve the minimax rate at $ x $ in terms of the unknown intrinsic dimension in the vicinity of $ x $ . <eos> we also establish that the minimax rate does not depend on a particular choice of metric space or distribution , but rather that this minimax rate holds for any metric space and doubling measure .
when used to learn high dimensional parametric probabilistic models , the clas- sical maximum likelihood ( ml ) learning often suffers from computational in- tractability , which motivates the active developments of non-ml learning meth- ods . <eos> yet , because of their divergent motivations and forms , the objective func- tions of many non-ml learning methods are seemingly unrelated , and there lacks a unified framework to understand them . <eos> in this work , based on an information geometric view of parametric learning , we introduce a general non-ml learning principle termed as minimum kl contraction , where we seek optimal parameters that minimizes the contraction of the kl divergence between the two distributions after they are transformed with a kl contraction operator . <eos> we then show that the objective functions of several important or recently developed non-ml learn- ing methods , including contrastive divergence [ 12 ] , noise-contrastive estimation [ 11 ] , partial likelihood [ 7 ] , non-local contrastive objectives [ 31 ] , score match- ing [ 14 ] , pseudo-likelihood [ 3 ] , maximum conditional likelihood [ 17 ] , maximum mutual information [ 2 ] , maximum marginal likelihood [ 9 ] , and conditional and marginal composite likelihood [ 24 ] , can be unified under the minimum kl con- traction framework with different choices of the kl contraction operators .
we consider a class of sparsity-inducing regularization terms based on submodular functions . <eos> while previous work has focused on non-decreasing functions , we explore symmetric submodular functions and their \lova extensions . <eos> we show that the lovasz extension may be seen as the convex envelope of a function that depends on level sets ( i.e. , the set of indices whose corresponding components of the underlying predictor are greater than a given constant ) : this leads to a class of convex structured regularization terms that impose prior knowledge on the level sets , and not only on the supports of the underlying predictors . <eos> we provide a unified set of optimization algorithms , such as proximal operators , and theoretical guarantees ( allowed level sets and recovery conditions ) . <eos> by selecting specific submodular functions , we give a new interpretation to known norms , such as the total variation ; we also define new norms , in particular ones that are based on order statistics with application to clustering and outlier detection , and on noisy cuts in graphs with application to change point detection in the presence of outliers .
multi-structure model fitting has traditionally taken a two-stage approach : first , sample a ( large ) number of model hypotheses , then select the subset of hypotheses that optimise a joint fitting and model selection criterion . <eos> this disjoint two-stage approach is arguably suboptimal and inefficient - if the random sampling did not retrieve a good set of hypotheses , the optimised outcome will not represent a good fit . <eos> to overcome this weakness we propose a new multi-structure fitting approach based on reversible jump mcmc . <eos> instrumental in raising the effectiveness of our method is an adaptive hypothesis generator , whose proposal distribution is learned incrementally and online . <eos> we prove that this adaptive proposal satisfies the diminishing adaptation property crucial for ensuring ergodicity in mcmc . <eos> our method effectively conducts hypothesis sampling and optimisation simultaneously , and gives superior computational efficiency over other methods .
we consider the problem of computing the euclidean projection of a vector of length $ p $ onto a non-negative max-heap -- -an ordered tree where the values of the nodes are all nonnegative and the value of any parent node is no less than the value ( s ) of its child node ( s ) . <eos> this euclidean projection plays a building block role in the optimization problem with a non-negative max-heap constraint . <eos> such a constraint is desirable when the features follow an ordered tree structure , that is , a given feature is selected for the given regression/classification task only if its parent node is selected . <eos> in this paper , we show that such euclidean projection problem admits an analytical solution and we develop a top-down algorithm where the key operation is to find the so-called \emph { maximal root-tree } of the subtree rooted at each node . <eos> a naive approach for finding the maximal root-tree is to enumerate all the possible root-trees , which , however , does not scale well . <eos> we reveal several important properties of the maximal root-tree , based on which we design a bottom-up algorithm with merge for efficiently finding the maximal root-tree . <eos> the proposed algorithm has a ( worst-case ) linear time complexity for a sequential list , and $ o ( p^2 ) $ for a general tree . <eos> we report simulation results showing the effectiveness of the max-heap for regression with an ordered tree structure . <eos> empirical results show that the proposed algorithm has an expected linear time complexity for many special cases including a sequential list , a full binary tree , and a tree with depth 1 .
we propose a novel inference framework for finding maximal cliques in a weighted graph that satisfy hard constraints . <eos> the constraints specify the graph nodes that must belong to the solution as well as mutual exclusions of graph nodes , i.e. , sets of nodes that can not belong to the same solution . <eos> the proposed inference is based on a novel particle filter algorithm with state permeations . <eos> we apply the inference framework to a challenging problem of learning part-based , deformable object models . <eos> two core problems in the learning framework , matching of image patches and finding salient parts , are formulated as two instances of the problem of finding maximal cliques with hard constraints . <eos> our learning framework yields discriminative part based object models that achieve very good detection rate , and outperform other methods on object classes with large deformation .
despite the recent trend of increasingly large datasets for object detection , there still exist many classes with few training examples . <eos> to overcome this lack of training data for certain classes , we propose a novel way of augmenting the training data for each class by borrowing and transforming examples from other classes . <eos> our model learns which training instances from other classes to borrow and how to transform the borrowed examples so that they become more similar to instances from the target class . <eos> our experimental results demonstrate that our new object detector , with borrowed and transformed examples , improves upon the current state-of-the-art detector on the challenging sun09 object detection dataset .
in this paper , we consider the problem of compressed sensing where the goal is to recover almost all the sparse vectors using a small number of fixed linear measurements . <eos> for this problem , we propose a novel partial hard-thresholding operator leading to a general family of iterative algorithms . <eos> while one extreme of the family yields well known hard thresholding algorithms like iti and htp , the other end of the spectrum leads to a novel algorithm that we call orthogonal matching pursuit with replacement ( ompr ) . <eos> ompr , like the classic greedy algorithm omp , adds exactly one coordinate to the support at each iteration , based on the correlation with the current residual . <eos> however , unlike omp , ompr also removes one coordinate from the support . <eos> this simple change allows us to prove the best known guarantees for ompr in terms of the restricted isometry property ( a condition on the measurement matrix ) . <eos> in contrast , omp is known to have very weak performance guarantees under rip . <eos> we also extend ompr using locality sensitive hashing to get ompr-hash , the first provably sub-linear ( in dimensionality ) algorithm for sparse recovery . <eos> our proof techniques are novel and flexible enough to also permit the tightest known analysis of popular iterative algorithms such as cosamp and subspace pursuit . <eos> we provide experimental results on large problems providing recovery for vectors of size up to million dimensions . <eos> we demonstrate that for large-scale problems our proposed methods are more robust and faster than the existing methods .
we introduce the gamma-exponential process ( gep ) , a prior over a large family of continuous time stochastic processes . <eos> a hierarchical version of this prior ( hgep ; the hierarchical gep ) yields a useful model for analyzing complex time series . <eos> models based on hgeps display many attractive properties : conjugacy , exchangeability and closed-form predictive distribution for the waiting times , and exact gibbs updates for the time scale parameters . <eos> after establishing these properties , we show how posterior inference can be carried efficiently using particle mcmc methods [ 1 ] . <eos> this yields a mcmc algorithm that can resample entire sequences atomically while avoiding the complications of introducing slice and stick auxiliary variables of the beam sampler [ 2 ] . <eos> we applied our model to the problem of estimating the disease progression in multiple sclerosis [ 3 ] , and to rna evolutionary modeling [ 4 ] . <eos> in both domains , we found that our model outperformed the standard rate matrix estimation approach .
the performance of markov chain monte carlo methods is often sensitive to the scaling and correlations between the random variables of interest . <eos> an important source of information about the local correlation and scale is given by the hessian matrix of the target distribution , but this is often either computationally expensive or infeasible . <eos> in this paper we propose mcmc samplers that make use of quasi-newton approximations from the optimization literature , that approximate the hessian of the target distribution from previous samples and gradients generated by the sampler . <eos> a key issue is that mcmc samplers that depend on the history of previous states are in general not valid . <eos> we address this problem by using limited memory quasi-newton methods , which depend only on a fixed window of previous samples . <eos> on several real world datasets , we show that the quasi-newton sampler is a more effective sampler than standard hamiltonian monte carlo at a fraction of the cost of mcmc methods that require higher-order derivatives .
we propose an online prediction version of submodular set cover with connections to ranking and repeated active learning . <eos> in each round , the learning algorithm chooses a sequence of items . <eos> the algorithm then receives a monotone submodular function and suffers loss equal to the cover time of the function : the number of items needed , when items are selected in order of the chosen sequence , to achieve a coverage constraint . <eos> we develop an online learning algorithm whose loss converges to approximately that of the best sequence in hindsight . <eos> our proposed algorithm is readily extended to a setting where multiple functions are revealed at each round and to bandit and contextual bandit settings .
we study the empirical strategies that humans follow as they teach a target concept with a simple 1d threshold to a robot . <eos> previous studies of computational teaching , particularly the teaching dimension model and the curriculum learning principle , offer contradictory predictions on what optimal strategy the teacher should follow in this teaching task . <eos> we show through behavioral studies that humans employ three distinct teaching strategies , one of which is consistent with the curriculum learning principle , and propose a novel theoretical framework as a potential explanation for this strategy . <eos> this framework , which assumes a teaching goal of minimizing the learner 's expected generalization error at each iteration , extends the standard teaching dimension model and offers a theoretical justification for curriculum learning .
independent components analysis ( ica ) and its variants have been successfully used for unsupervised feature learning . <eos> however , standard ica requires an orthonoramlity constraint to be enforced , which makes it dif ? cult to learn overcomplete features . <eos> in addition , ica is sensitive to whitening . <eos> these properties make it challenging to scale ica to high dimensional data . <eos> in this paper , we propose a robust soft reconstruction cost for ica that allows us to learn highly overcomplete sparse features even on unwhitened data . <eos> our formulation reveals formal connections between ica and sparse autoencoders , which have previously been observed only empirically . <eos> our algorithm can be used in conjunction with off-the-shelf fast unconstrained optimizers . <eos> we show that the soft reconstruction cost can also be used to prevent replicated features in tiled convolutional neural networks . <eos> using our method to learn highly overcomplete sparse features and tiled convolutional neural networks , we obtain competitive performances on a wide variety of object recognition tasks . <eos> we achieve state-of-the-art test accuracies on the stl-10 and hollywood2 datasets .
synaptic plasticity underlies learning and is thus central for development , memory , and recovery from injury . <eos> however , it is often difficult to detect changes in synaptic strength in vivo , since intracellular recordings are experimentally challenging . <eos> here we present two methods aimed at inferring changes in the coupling between pairs of neurons from extracellularly recorded spike trains . <eos> first , using a generalized bilinear model with poisson output we estimate time-varying coupling assuming that all changes are spike-timing-dependent . <eos> this approach allows model-based estimation of stdp modification functions from pairs of spike trains . <eos> then , using recursive point-process adaptive filtering methods we estimate more general variation in coupling strength over time . <eos> using simulations of neurons undergoing spike-timing dependent modification , we show that the true modification function can be recovered . <eos> using multi-electrode data from motor cortex we then illustrate the use of this technique on in vivo data .
with the advent of crowdsourcing services it has become quite cheap and reasonably effective to get a dataset labeled by multiple annotators in a short amount of time . <eos> various methods have been proposed to estimate the consensus labels by correcting for the bias of annotators with different kinds of expertise . <eos> often we have low quality annotators or spammers -- annotators who assign labels randomly ( e.g. , without actually looking at the instance ) . <eos> spammers can make the cost of acquiring labels very expensive and can potentially degrade the quality of the consensus labels . <eos> in this paper we formalize the notion of a spammer and define a score which can be used to rank the annotators -- -with the spammers having a score close to zero and the good annotators having a high score close to one .
we develop and demonstrate automatic image description methods using a large captioned photo collection . <eos> one contribution is our technique for the automatic collection of this new dataset -- performing a huge number of flickr queries and then filtering the noisy results down to 1 million images with associated visually relevant captions . <eos> such a collection allows us to approach the extremely challenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results . <eos> we also develop methods incorporating many state of the art , but fairly noisy , estimates of image content to produce even more pleasing results . <eos> finally we introduce a new objective performance measure for image captioning .
bayesian filtering of stochastic stimuli has received a great deal of attention re- cently . <eos> it has been applied to describe the way in which biological systems dy- namically represent and make decisions about the environment . <eos> there have been no exact results for the error in the biologically plausible setting of inference on point process , however . <eos> we present an exact analysis of the evolution of the mean- squared error in a state estimation task using gaussian-tuned point processes as sensors . <eos> this allows us to study the dynamics of the error of an optimal bayesian decoder , providing insights into the limits obtainable in this task . <eos> this is done for markovian and a class of non-markovian gaussian processes . <eos> we find that there is an optimal tuning width for which the error is minimized . <eos> this leads to a char- acterization of the optimal encoding for the setting as a function of the statistics of the stimulus , providing a mathematically sound primer for an ecological theory of sensory processing .
we show that the lambda-return target used in the td ( lambda ) family of algorithms is the maximum likelihood estimator for a specific model of how the variance of an n-step return estimate increases with n. we introduce the gamma-return estimator , an alternative target based on a more accurate model of variance , which defines the td_gamma family of complex-backup temporal difference learning algorithms . <eos> we derive td_gamma , the gamma-return equivalent of the original td ( lambda ) algorithm , which eliminates the lambda parameter but can only perform updates at the end of an episode and requires time and space proportional to the episode length . <eos> we then derive a second algorithm , td_gamma ( c ) , with a capacity parameter c. td_gamma ( c ) requires c times more time and memory than td ( lambda ) and is incremental and online . <eos> we show that td_gamma outperforms td ( lambda ) for any setting of lambda on 4 out of 5 benchmark domains , and that td_gamma ( c ) performs as well as or better than td_gamma for intermediate settings of c .
extracting good representations from images is essential for many computer vision tasks . <eos> in this paper , we propose hierarchical matching pursuit ( hmp ) , which builds a feature hierarchy layer-by-layer using an efficient matching pursuit encoder . <eos> it includes three modules : batch ( tree ) orthogonal matching pursuit , spatial pyramid max pooling , and contrast normalization . <eos> we investigate the architecture of hmp , and show that all three components are critical for good performance . <eos> to speed up the orthogonal matching pursuit , we propose a batch tree orthogonal matching pursuit that is particularly suitable to encode a large number of observations that share the same large dictionary . <eos> hmp is scalable and can efficiently handle full-size images . <eos> in addition , hmp enables linear support vector machines ( svm ) to match the performance of nonlinear svm while being scalable to large datasets . <eos> we compare hmp with many state-of-the-art algorithms including convolutional deep belief networks , sift based single layer sparse coding , and kernel based feature learning . <eos> hmp consistently yields superior accuracy on three types of image classification problems : object recognition ( caltech-101 ) , scene recognition ( mit-scene ) , and static event recognition ( uiuc-sports ) .
we introduce hd ( or `` hierarchical-deep '' ) models , a new compositional learning architecture that integrates deep learning models with structured hierarchical bayesian models . <eos> specifically we show how we can learn a hierarchical dirichlet process ( hdp ) prior over the activities of the top-level features in a deep boltzmann machine ( dbm ) . <eos> this compound hdp-dbm model learns to learn novel concepts from very few training examples , by learning low-level generic features , high-level features that capture correlations among low-level features , and a category hierarchy for sharing priors over the high-level features that are typical of different kinds of concepts . <eos> we present efficient learning and inference algorithms for the hdp-dbm model and show that it is able to learn new concepts from very few examples on cifar-100 object recognition , handwritten character recognition , and human motion capture datasets .
this paper addresses the problem of minimizing a convex , lipschitz function $ f $ over a convex , compact set $ x $ under a stochastic bandit feedback model . <eos> in this model , the algorithm is allowed to observe noisy realizations of the function value $ f ( x ) $ at any query point $ x \in x $ . <eos> we demonstrate a generalization of the ellipsoid algorithm that incurs $ o ( \poly ( d ) \sqrt { t } ) $ regret . <eos> since any algorithm has regret at least $ \omega ( \sqrt { t } ) $ on this problem , our algorithm is optimal in terms of the scaling with $ t $ .
predicting the nodes of a given graph is a fascinating theoretical problem with applications in several domains . <eos> since graph sparsification via spanning trees retains enough information while making the task much easier , trees are an important special case of this problem . <eos> although it is known how to predict the nodes of an unweighted tree in a nearly optimal way , in the weighted case a fully satisfactory algorithm is not available yet . <eos> we fill this hole and introduce an efficient node predictor , shazoo , which is nearly optimal on any weighted tree . <eos> moreover , we show that shazoo can be viewed as a common nontrivial generalization of both previous approaches for unweighted trees and weighted lines . <eos> experiments on real-world datasets confirm that shazoo performs well in that it fully exploits the structure of the input tree , and gets very close to ( and sometimes better than ) less scalable energy minimization methods .
pomdp planning faces two major computational challenges : large state spaces and long planning horizons . <eos> the recently introduced monte carlo value iteration ( mcvi ) can tackle pomdps with very large discrete state spaces or continuous state spaces , but its performance degrades when faced with long planning horizons . <eos> this paper presents macro-mcvi , which extends mcvi by exploiting macro-actions for temporal abstraction . <eos> we provide sufficient conditions for macro-mcvi to inherit the good theoretical properties of mcvi . <eos> macro-mcvi does not require explicit construction of probabilistic models for macro-actions and is thus easy to apply in practice . <eos> experiments show that macro-mcvi substantially improves the performance of mcvi with suitable macro-actions .
we study the problem of identifying the best arm in each of the bandits in a multi-bandit multi-armed setting . <eos> we first propose an algorithm called gap-based exploration ( gape ) that focuses on the arms whose mean is close to the mean of the best arm in the same bandit ( i.e. , small gap ) . <eos> we then introduce an algorithm , called gape-v , which takes into account the variance of the arms in addition to their gap . <eos> we prove an upper-bound on the probability of error for both algorithms . <eos> since gape and gape-v need to tune an exploration parameter that depends on the complexity of the problem , which is often unknown in advance , we also introduce variations of these algorithms that estimate this complexity online . <eos> finally , we evaluate the performance of these algorithms and compare them to other allocation strategies on a number of synthetic problems .
the difficulty in inverse reinforcement learning ( irl ) arises in choosing the best reward function since there are typically an infinite number of reward functions that yield the given behaviour data as optimal . <eos> using a bayesian framework , we address this challenge by using the maximum a posteriori ( map ) estimation for the reward function , and show that most of the previous irl algorithms can be modeled into our framework . <eos> we also present a gradient method for the map estimation based on the ( sub ) differentiability of the posterior distribution . <eos> we show the effectiveness of our approach by comparing the performance of the proposed method to those of the previous algorithms .
we derive algorithms for generalised tensor factorisation ( gtf ) by building upon the well-established theory of generalised linear models . <eos> our algorithms are general in the sense that we can compute arbitrary factorisations in a message passing framework , derived for a broad class of exponential family distributions including special cases such as tweedie 's distributions corresponding to $ \beta $ -divergences . <eos> by bounding the step size of the fisher scoring iteration of the glm , we obtain general updates for real data and multiplicative updates for non-negative data . <eos> the gtf framework is , then extended easily to address the problems when multiple observed tensors are factorised simultaneously . <eos> we illustrate our coupled factorisation approach on synthetic data as well as on a musical audio restoration problem .
we describe a novel technique for feature combination in the bag-of-words model of image classification . <eos> our approach builds discriminative compound words from primitive cues learned independently from training images . <eos> our main observation is that modeling joint-cue distributions independently is more statistically robust for typical classification problems than attempting to empirically estimate the dependent , joint-cue distribution directly . <eos> we use information theoretic vocabulary compression to find discriminative combinations of cues and the resulting vocabulary of portmanteau words is compact , has the cue binding property , and supports individual weighting of cues in the final image representation . <eos> state-of-the-art results on both the oxford flower-102 and caltech-ucsd bird-200 datasets demonstrate the effectiveness of our technique compared to other , significantly more complex approaches to multi-cue image representation
vector auto-regressive models ( var ) are useful tools for analyzing time series data . <eos> in quite a few modern time series modelling tasks , the collection of reliable time series turns out to be a major challenge , either due to the slow progression of the dynamic process of interest , or inaccessibility of repetitive measurements of the same dynamic process over time . <eos> in those situations , however , we observe that it is often easier to collect a large amount of non-sequence samples , or snapshots of the dynamic process of interest . <eos> in this work , we assume a small amount of time series data are available , and propose methods to incorporate non-sequence data into penalized least-square estimation of var models . <eos> we consider non-sequence data as samples drawn from the stationary distribution of the underlying var model , and devise a novel penalization scheme based on the discrete-time lyapunov equation concerning the covariance of the stationary distribution . <eos> experiments on synthetic and video data demonstrate the effectiveness of the proposed methods .
most existing multiple-instance learning ( mil ) algorithms assume data instances and/or data bags are independently and identically distributed . <eos> but there often exists rich additional dependency/structure information between instances/bags within many applications of mil . <eos> ignoring this structure information limits the performance of existing mil algorithms . <eos> this paper explores the research problem as multiple instance learning on structured data ( milsd ) and formulates a novel framework that considers additional structure information . <eos> in particular , an effective and efficient optimization algorithm has been proposed to solve the original non-convex optimization problem by using a combination of concave-convex constraint programming ( cccp ) method and an adapted cutting plane method , which deals with two sets of constraints caused by learning on instances within individual bags and learning on structured data . <eos> our method has the nice convergence property , with specified precision on each set of constraints . <eos> experimental results on three different applications , i.e. , webpage classification , market targeting , and protein fold identification , clearly demonstrate the advantages of the proposed method over state-of-the-art methods .
we study the problem of learning to track a large quantity of homogeneous objects such as cell tracking in cell culture study and developmental biology . <eos> reliable cell tracking in time-lapse microscopic image sequences is important for modern biomedical research . <eos> existing cell tracking methods are usually kept simple and use only a small number of features to allow for manual parameter tweaking or grid search . <eos> we propose a structured learning approach that allows to learn optimum parameters automatically from a training set . <eos> this allows for the use of a richer set of features which in turn affords improved tracking compared to recently reported methods on two public benchmark sequences .
many practitioners of reinforcement learning problems have observed that oftentimes the performance of the agent reaches very close to the optimal performance even though the estimated ( action- ) value function is still far from the optimal one . <eos> the goal of this paper is to explain and formalize this phenomenon by introducing the concept of the action-gap regularity . <eos> as a typical result , we prove that for an agent following the greedy policy \ ( \hat { \pi } \ ) with respect to an action-value function \ ( \hat { q } \ ) , the performance loss \ ( e [ v^* ( x ) - v^ { \hat { x } } ( x ) ] \ ) is upper bounded by \ ( o ( || \hat { q } - q^*||_\infty^ { 1+\zeta } \ ) ) , in which \ ( \zeta > = 0\ ) is the parameter quantifying the action-gap regularity . <eos> for \ ( \zeta > 0\ ) , our results indicate smaller performance loss compared to what previous analyses had suggested . <eos> finally , we show how this regularity affects the performance of the family of approximate value iteration algorithms .
this work introduces divide-factor-combine ( dfc ) , a parallel divide-and-conquer framework for noisy matrix factorization . <eos> dfc divides a large-scale matrix factorization task into smaller subproblems , solves each subproblem in parallel using an arbitrary base matrix factorization algorithm , and combines the subproblem solutions using techniques from randomized matrix approximation . <eos> our experiments with collaborative filtering , video background modeling , and simulated data demonstrate the near-linear to super-linear speed-ups attainable with this approach . <eos> moreover , our analysis shows that dfc enjoys high-probability recovery guarantees comparable to those of its base algorithm .
how should we design experiments to maximize performance of a complex system , taking into account uncontrollable environmental conditions ? <eos> how should we select relevant documents ( ads ) to display , given information about the user ? <eos> these tasks can be formalized as contextual bandit problems , where at each round , we receive context ( about the experimental conditions , the query ) , and have to choose an action ( parameters , documents ) . <eos> the key challenge is to trade off exploration by gathering data for estimating the mean payoff function over the context-action space , and to exploit by choosing an action deemed optimal based on the gathered data . <eos> we model the payoff function as a sample from a gaussian process defined over the joint context-action space , and develop cgp-ucb , an intuitive upper-confidence style algorithm . <eos> we show that by mixing and matching kernels for contexts and actions , cgp-ucb can handle a variety of practical applications . <eos> we further provide generic tools for deriving regret bounds when using such composite kernel functions . <eos> lastly , we evaluate our algorithm on two case studies , in the context of automated vaccine design and sensor management . <eos> we show that context-sensitive optimization outperforms no or naive use of context .
we propose a novel kernel approach to dimension reduction for supervised learning : feature extraction and variable selection ; the former constructs a small number of features from predictors , and the latter finds a subset of predictors . <eos> first , a method of linear feature extraction is proposed using the gradient of regression function , based on the recent development of the kernel method . <eos> in comparison with other existing methods , the proposed one has wide applicability without strong assumptions on the regressor or type of variables , and uses computationally simple eigendecomposition , thus applicable to large data sets . <eos> second , in combination of a sparse penalty , the method is extended to variable selection , following the approach by chen et al . ( 2010 ) . <eos> experimental results show that the proposed methods successfully find effective features and variables without parametric models .
a common challenge for bayesian models of perception is the fact that the two fundamental bayesian components , the prior distribution and the likelihood function , are formally unconstrained . <eos> here we argue that a neural system that emulates bayesian inference is naturally constrained by the way it represents sensory information in populations of neurons . <eos> more specifically , we show that an efficient coding principle creates a direct link between prior and likelihood based on the underlying stimulus distribution . <eos> the resulting bayesian estimates can show biases away from the peaks of the prior distribution , a behavior seemingly at odds with the traditional view of bayesian estimation , yet one that has been reported in human perception . <eos> we demonstrate that our framework correctly accounts for the repulsive biases previously reported for the perception of visual orientation , and show that the predicted tuning characteristics of the model neurons match the reported orientation tuning properties of neurons in primary visual cortex . <eos> our results suggest that efficient coding is a promising hypothesis in constraining bayesian models of perceptual inference .
an important way to make large training sets is to gather noisy labels from crowds of nonexperts . <eos> we propose a minimax entropy principle to improve the quality of these labels . <eos> our method assumes that labels are generated by a probability distribution over workers , items , and labels . <eos> by maximizing the entropy of this distribution , the method naturally infers item confusability and worker expertise . <eos> we infer the ground truth by minimizing the entropy of this distribution , which we show minimizes the kullback-leibler ( kl ) divergence between the probability distribution and the unknown truth . <eos> we show that a simple coordinate descent scheme can optimize minimax entropy . <eos> empirically , our results are substantially better than previously published methods for the same problem .
bipartite matching problems characterize many situations , ranging from ranking in information retrieval to correspondence in vision . <eos> exact inference in real-world applications of these problems is intractable , making efficient approximation methods essential for learning and inference . <eos> in this paper we propose a novel { \it sequential matching } sampler based on the generalization of the plackett-luce model , which can effectively make large moves in the space of matchings . <eos> this allows the sampler to match the difficult target distributions common in these problems : highly multimodal distributions with well separated modes . <eos> we present experimental results with bipartite matching problems - ranking and image correspondence - which show that the sequential matching sampler efficiently approximates the target distribution , significantly outperforming other sampling approaches .
given $ \alpha , \epsilon $ , we study the time complexity required to improperly learn a halfspace with misclassification error rate of at most $ ( 1+\alpha ) \ , l^*_\gamma + \epsilon $ , where $ l^*_\gamma $ is the optimal $ \gamma $ -margin error rate . <eos> for $ \alpha = 1/\gamma $ , polynomial time and sample complexity is achievable using the hinge-loss . <eos> for $ \alpha = 0 $ , \cite { shalevshsr11 } showed that $ \poly ( 1/\gamma ) $ time is impossible , while learning is possible in time $ \exp ( \tilde { o } ( 1/\gamma ) ) $ . <eos> an immediate question , which this paper tackles , is what is achievable if $ \alpha \in ( 0,1/\gamma ) $ . <eos> we derive positive results interpolating between the polynomial time for $ \alpha = 1/\gamma $ and the exponential time for $ \alpha=0 $ . <eos> in particular , we show that there are cases in which $ \alpha = o ( 1/\gamma ) $ but the problem is still solvable in polynomial time . <eos> our results naturally extend to the adversarial online learning model and to the pac learning with malicious noise model .
clustering is a key component in data analysis toolbox . <eos> despite its importance , scalable algorithms often eschew rich statistical models in favor of simpler descriptions such as $ k $ -means clustering . <eos> in this paper we present a sampler , capable of estimating mixtures of exponential families . <eos> at its heart lies a novel proposal distribution using random projections to achieve high throughput in generating proposals , which is crucial for clustering models with large numbers of clusters .
warped gaussian processes ( wgp ) [ 1 ] model output observations in regression tasks as a parametric nonlinear transformation of a gaussian process ( gp ) . <eos> the use of this nonlinear transformation , which is included as part of the probabilistic model , was shown to enhance performance by providing a better prior model on several data sets . <eos> in order to learn its parameters , maximum likelihood was used . <eos> in this work we show that it is possible to use a non-parametric nonlinear transformation in wgp and variationally integrate it out . <eos> the resulting bayesian wgp is then able to work in scenarios in which the maximum likelihood wgp failed : low data regime , data with censored values , classification , etc . <eos> we demonstrate the superior performance of bayesian warped gps on several real data sets .
we address the problem of comparing the risks of two given predictive models - for instance , a baseline model and a challenger - as confidently as possible on a fixed labeling budget . <eos> this problem occurs whenever models can not be compared on held-out training data , possibly because the training data are unavailable or do not reflect the desired test distribution . <eos> in this case , new test instances have to be drawn and labeled at a cost . <eos> we devise an active comparison method that selects instances according to an instrumental sampling distribution . <eos> we derive the sampling distribution that maximizes the power of a statistical test applied to the observed empirical risks , and thereby minimizes the likelihood of choosing the inferior model . <eos> empirically , we investigate model selection problems on several classification and regression tasks and study the accuracy of the resulting p-values .
in this work we consider a setting where we have a very large number of related tasks with few examples from each individual task . <eos> rather than either learning each task individually ( and having a large generalization error ) or learning all the tasks together using a single hypothesis ( and suffering a potentially large inherent error ) , we consider learning a small pool of { \em shared hypotheses } . <eos> each task is then mapped to a single hypothesis in the pool ( hard association ) . <eos> we derive vc dimension generalization bounds for our model , based on the number of tasks , shared hypothesis and the vc dimension of the hypotheses class . <eos> we conducted experiments with both synthetic problems and sentiment of reviews , which strongly support our approach .
recent work in unsupervised feature learning has focused on the goal of discovering high-level features from unlabeled images . <eos> much progress has been made in this direction , but in most cases it is still standard to use a large amount of labeled data in order to construct detectors sensitive to object classes or other complex patterns in the data . <eos> in this paper , we aim to test the hypothesis that unsupervised feature learning methods , provided with only unlabeled data , can learn high-level , invariant features that are sensitive to commonly-occurring objects . <eos> though a handful of prior results suggest that this is possible when each object class accounts for a large fraction of the data ( as in many labeled datasets ) , it is unclear whether something similar can be accomplished when dealing with completely unlabeled data . <eos> a major obstacle to this test , however , is scale : we can not expect to succeed with small datasets or with small numbers of learned features . <eos> here , we propose a large-scale feature learning system that enables us to carry out this experiment , learning 150,000 features from tens of millions of unlabeled images . <eos> based on two scalable clustering algorithms ( k-means and agglomerative clustering ) , we find that our simple system can discover features sensitive to a commonly occurring object class ( human faces ) and can also combine these into detectors invariant to significant global distortions like large translations and scale .
we consider the estimation of an i.i.d.\ vector $ \xbf \in \r^n $ from measurements $ \ybf \in \r^m $ obtained by a general cascade model consisting of a known linear transform followed by a probabilistic componentwise ( possibly nonlinear ) measurement channel . <eos> we present a method , called adaptive generalized approximate message passing ( adaptive gamp ) , that enables joint learning of the statistics of the prior and measurement channel along with estimation of the unknown vector $ \xbf $ . <eos> the proposed algorithm is a generalization of a recently-developed method by vila and schniter that uses expectation-maximization ( em ) iterations where the posteriors in the e-steps are computed via approximate message passing . <eos> the techniques can be applied to a large class of learning problems including the learning of sparse priors in compressed sensing or identification of linear-nonlinear cascade models in dynamical systems and neural spiking processes . <eos> we prove that for large i.i.d.\ gaussian transform matrices the asymptotic componentwise behavior of the adaptive gamp algorithm is predicted by a simple set of scalar state evolution equations . <eos> this analysis shows that the adaptive gamp method can yield asymptotically consistent parameter estimates , which implies that the algorithm achieves a reconstruction quality equivalent to the oracle algorithm that knows the correct parameter values . <eos> the adaptive gamp methodology thus provides a systematic , general and computationally efficient method applicable to a large range of complex linear-nonlinear models with provable guarantees .
we consider estimation of multiple high-dimensional gaussian graphical models corresponding to a single set of nodes under several distinct conditions . <eos> we assume that most aspects of the networks are shared , but that there are some structured differences between them . <eos> specifically , the network differences are generated from node perturbations : a few nodes are perturbed across networks , and most or all edges stemming from such nodes differ between networks . <eos> this corresponds to a simple model for the mechanism underlying many cancers , in which the gene regulatory network is disrupted due to the aberrant activity of a few specific genes . <eos> we propose to solve this problem using the structured joint graphical lasso , a convex optimization problem that is based upon the use of a novel symmetric overlap norm penalty , which we solve using an alternating directions method of multipliers algorithm . <eos> our proposal is illustrated on synthetic data and on an application to brain cancer gene expression data .
margin is one of the most important concepts in machine learning . <eos> previous margin bounds , both for svm and for boosting , are dimensionality independent . <eos> a major advantage of this dimensionality independency is that it can explain the excellent performance of svm whose feature spaces are often of high or infinite dimension . <eos> in this paper we address the problem whether such dimensionality independency is intrinsic for the margin bounds . <eos> we prove a dimensionality dependent pac-bayes margin bound . <eos> the bound is monotone increasing with respect to the dimension when keeping all other factors fixed . <eos> we show that our bound is strictly sharper than a previously well-known pac-bayes margin bound if the feature space is of finite dimension ; and the two bounds tend to be equivalent as the dimension goes to infinity . <eos> in addition , we show that the vc bound for linear classifiers can be recovered from our bound under mild conditions . <eos> we conduct extensive experiments on benchmark datasets and find that the new bound is useful for model selection and is significantly sharper than the dimensionality independent pac-bayes margin bound as well as the vc bound for linear classifiers .
multiple-output regression models require estimating multiple functions , one for each output . <eos> to improve parameter estimation in such models , methods based on structural regularization of the model parameters are usually needed . <eos> in this paper , we present a multiple-output regression model that leverages the covariance structure of the functions ( i.e. , how the multiple functions are related with each other ) as well as the conditional covariance structure of the outputs . <eos> this is in contrast with existing methods that usually take into account only one of these structures . <eos> more importantly , unlike most of the other existing methods , none of these structures need be known a priori in our model , and are learned from the data . <eos> several previously proposed structural regularization based multiple-output regression models turn out to be special cases of our model . <eos> moreover , in addition to being a rich model for multiple-output regression , our model can also be used in estimating the graphical model structure of a set of variables ( multivariate outputs ) conditioned on another set of variables ( inputs ) . <eos> experimental results on both synthetic and real datasets demonstrate the effectiveness of our method .
alzheimer disease ( ad ) is a neurodegenerative disorder characterized by progressive impairment of memory and other cognitive functions . <eos> regression analysis has been studied to relate neuroimaging measures to cognitive status . <eos> however , whether these measures have further predictive power to infer a trajectory of cognitive performance over time is still an under-explored but important topic in ad research . <eos> we propose a novel high-order multi-task learning model to address this issue . <eos> the proposed model explores the temporal correlations existing in data features and regression tasks by the structured sparsity-inducing norms . <eos> in addition , the sparsity of the model enables the selection of a small number of mri measures while maintaining high prediction accuracy . <eos> the empirical studies , using the baseline mri and serial cognitive data of the adni cohort , have yielded promising results .
the lovasz $ \theta $ function of a graph , is a fundamental tool in combinatorial optimization and approximation algorithms . <eos> computing $ \theta $ involves solving a sdp and is extremely expensive even for moderately sized graphs . <eos> in this paper we establish that the lovasz $ \theta $ function is equivalent to a kernel learning problem related to one class svm . <eos> this interesting connection opens up many opportunities bridging graph theoretic algorithms and machine learning . <eos> we show that there exist graphs , which we call $ svm-\theta $ graphs , on which the lovasz $ \theta $ function can be approximated well by a one-class svm . <eos> this leads to a novel use of svm techniques to solve algorithmic problems in large graphs e.g . identifying a planted clique of size $ \theta ( { \sqrt { n } } ) $ in a random graph $ g ( n , \frac { 1 } { 2 } ) $ . <eos> a classic approach for this problem involves computing the $ \theta $ function , however it is not scalable due to sdp computation . <eos> we show that the random graph with a planted clique is an example of $ svm-\theta $ graph , and as a consequence a svm based approach easily identifies the clique in large graphs and is competitive with the state-of-the-art . <eos> further , we introduce the notion of a `` common orthogonal labeling '' which extends the notion of a `` orthogonal labelling of a single graph ( used in defining the $ \theta $ function ) to multiple graphs . <eos> the problem of finding the optimal common orthogonal labelling is cast as a multiple kernel learning problem and is used to identify a large common dense region in multiple graphs . <eos> the proposed algorithm achieves an order of magnitude scalability compared to the state of the art .
we consider the problem of cardinality penalized optimization of a convex function over the probability simplex with additional convex constraints . <eos> it 's well-known that the classical l1 regularizer fails to promote sparsity on the probability simplex since l1 norm on the probability simplex is trivially constant . <eos> we propose a direct relaxation of the minimum cardinality problem and show that it can be efficiently solved using convex programming . <eos> as a first application we consider recovering a sparse probability measure given moment constraints , in which our formulation becomes linear programming , hence can be solved very efficiently . <eos> a sufficient condition for exact recovery of the minimum cardinality solution is derived for arbitrary affine constraints . <eos> we then develop a penalized version for the noisy setting which can be solved using second order cone programs . <eos> the proposed method outperforms known heuristics based on l1 norm . <eos> as a second application we consider convex clustering using a sparse gaussian mixture and compare our results with the well known soft k-means algorithm .
we study statistical risk minimization problems under a version of privacy in which the data is kept confidential even from the learner . <eos> in this local privacy framework , we show sharp upper and lower bounds on the convergence rates of statistical estimation procedures . <eos> as a consequence , we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility , measured by convergence rate , of any statistical estimator .
learning temporal dependencies between variables over continuous time is an important and challenging task . <eos> continuous-time bayesian networks effectively model such processes but are limited by the number of conditional intensity matrices , which grows exponentially in the number of parents per variable . <eos> we develop a partition-based representation using regression trees and forests whose parameter spaces grow linearly in the number of node splits . <eos> using a multiplicative assumption we show how to update the forest likelihood in closed form , producing efficient model updates . <eos> our results show multiplicative forests can be learned from few temporal trajectories with large gains in performance and scalability .
sparse linear ( or generalized linear ) models combine a standard likelihood function with a sparse prior on the unknown coefficients . <eos> these priors can conveniently be expressed as a maximization over zero-mean gaussians with different variance hyperparameters . <eos> standard map estimation ( type i ) involves maximizing over both the hyperparameters and coefficients , while an empirical bayesian alternative ( type ii ) first marginalizes the coefficients and then maximizes over the hyperparameters , leading to a tractable posterior approximation . <eos> the underlying cost functions can be related via a dual-space framework from wipf et al . ( 2011 ) , which allows both the type i or type ii objectives to be expressed in either coefficient or hyperparmeter space . <eos> this perspective is useful because some analyses or extensions are more conducive to development in one space or the other . <eos> herein we consider the estimation of a trade-off parameter balancing sparsity and data fit . <eos> as this parameter is effectively a variance , natural estimators exist by assessing the problem in hyperparameter ( variance ) space , transitioning natural ideas from type ii to solve what is much less intuitive for type i . <eos> in contrast , for analyses of update rules and sparsity properties of local and global solutions , as well as extensions to more general likelihood models , we can leverage coefficient-space techniques developed for type i and apply them to type ii . <eos> for example , this allows us to prove that type ii-inspired techniques can be successful recovering sparse coefficients when unfavorable restricted isometry properties ( rip ) lead to failure of popular l1 reconstructions . <eos> it also facilitates the analysis of type ii when non-gaussian likelihood models lead to intractable integrations .
we address the problem of general supervised learning when data can only be accessed through an ( indefinite ) similarity function between data points . <eos> existing work on learning with indefinite kernels has concentrated solely on binary/multiclass classification problems . <eos> we propose a model that is generic enough to handle any supervised learning task and also subsumes the model previously proposed for classification . <eos> we give a `` goodness '' criterion for similarity functions w.r.t . <eos> a given supervised learning task and then adapt a well-known landmarking technique to provide efficient algorithms for supervised learning using `` good '' similarity functions . <eos> we demonstrate the effectiveness of our model on three important supervised learning problems : a ) real-valued regression , b ) ordinal regression and c ) ranking where we show that our method guarantees bounded generalization error . <eos> furthermore , for the case of real-valued regression , we give a natural goodness definition that , when used in conjunction with a recent result in sparse vector recovery , guarantees a sparse predictor with bounded generalization error . <eos> finally , we report results of our learning algorithms on regression and ordinal regression tasks using non-psd similarity functions and demonstrate the effectiveness of our algorithms , especially that of the sparse landmark selection algorithm that achieves significantly higher accuracies than the baseline methods while offering reduced computational costs .
derivative free optimization ( dfo ) is attractive when the objective function 's derivatives are not available and evaluations are costly . <eos> moreover , if the function evaluations are noisy , then approximating gradients by finite differences is difficult . <eos> this paper gives quantitative lower bounds on the performance of dfo with noisy function evaluations , exposing a fundamental and unavoidable gap between optimization performance based on noisy evaluations versus noisy gradients . <eos> this challenges the conventional wisdom that the method of finite differences is comparable to a stochastic gradient . <eos> however , there are situations in which dfo is unavoidable , and for such situations we propose a new dfo algorithm that is proved to be near optimal for the class of strongly convex objective functions . <eos> a distinctive feature of the algorithm is that it only uses boolean-valued function comparisons , rather than evaluations . <eos> this makes the algorithm useful in an even wider range of applications , including optimization based on paired comparisons from human subjects , for example . <eos> remarkably , we show that regardless of whether dfo is based on noisy function evaluations or boolean-valued function comparisons , the convergence rate is the same .
we show how binary classification methods developed to work on i.i.d . <eos> data can be used for solving statistical problems that are seemingly unrelated to classification and concern highly-dependent time series . <eos> specifically , the problems of time-series clustering , homogeneity testing and the three-sample problem are addressed . <eos> the algorithms that we construct for solving these problems are based on a new metric between time-series distributions , which can be evaluated using binary classification methods . <eos> universal consistency of the proposed algorithms is proven under most general assumptions . <eos> the theoretical results are illustrated with experiments on synthetic and real-world data .
statistical relational learning models combine the power of first-order logic , the de facto tool for handling relational structure , with that of probabilistic graphical models , the de facto tool for handling uncertainty . <eos> lifted probabilistic inference algorithms for them have been the subject of much recent research . <eos> the main idea in these algorithms is to improve the speed , accuracy and scalability of existing graphical models ' inference algorithms by exploiting symmetry in the first-order representation . <eos> in this paper , we consider blocked gibbs sampling , an advanced variation of the classic gibbs sampling algorithm and lift it to the first-order level . <eos> we propose to achieve this by partitioning the first-order atoms in the relational model into a set of disjoint clusters such that exact lifted inference is polynomial in each cluster given an assignment to all other atoms not in the cluster . <eos> we propose an approach for constructing such clusters and determining their complexity and show how it can be used to trade accuracy with computational complexity in a principled manner . <eos> our experimental evaluation shows that lifted gibbs sampling is superior to the propositional algorithm in terms of accuracy and convergence .
we present a method for approximate inference for a broad class of non-conjugate probabilistic models . <eos> in particular , for the family of generalized linear model target densities we describe a rich class of variational approximating densities which can be best fit to the target by minimizing the kullback-leibler divergence . <eos> our approach is based on using the fourier representation which we show results in efficient and scalable inference .
the ability to predict action content from neural signals in real time before the action occurs has been long sought in the neuroscientific study of decision-making , agency and volition . <eos> on-line real-time ( ort ) prediction is important for understanding the relation between neural correlates of decision-making and conscious , voluntary action as well as for brain-machine interfaces . <eos> here , epilepsy patients , implanted with intracranial depth microelectrodes or subdural grid electrodes for clinical purposes , participated in a ? matching-pennies game against an opponent . <eos> in each trial , subjects were given a 5 s countdown , after which they had to raise their left or right hand immediately as the ? go signal appeared on a computer screen . <eos> they won a fixed amount of money if they raised a different hand than their opponent and lost that amount otherwise . <eos> the question we here studied was the extent to which neural precursors of the subjects decisions can be detected in intracranial local field potentials ( lfp ) prior to the onset of the action . <eos> we found that combined low-frequency ( 0.1 ? 5 hz ) lfp signals from 10 electrodes were predictive of the intended left-/right-hand movements before the onset of the go signal . <eos> our ort system predicted which hand the patient would raise 0.5 s before the go signal with 68 ? 3 % accuracy in two patients . <eos> based on these results , we constructed an ort system that tracked up to 30 electrodes simultaneously , and tested it on retrospective data from 7 patients . <eos> on average , we could predict the correct hand choice in 83 % of the trials , which rose to 92 % if we let the system drop 3/10 of the trials on which it was less confident . <eos> our system demonstrates for the first time ? the feasibility of accurately predicting a binary action on single trials in real time for patients with intracranial recordings , well before the action occurs .
the expected return is a widely used objective in decision making under uncertainty . <eos> many algorithms , such as value iteration , have been proposed to optimize it . <eos> in risk-aware settings , however , the expected return is often not an appropriate objective to optimize . <eos> we propose a new optimization objective for risk-aware planning and show that it has desirable theoretical properties . <eos> we also draw connections to previously proposed objectives for risk-aware planing : minmax , exponential utility , percentile and mean minus variance . <eos> our method applies to an extended class of markov decision processes : we allow costs to be stochastic as long as they are bounded . <eos> additionally , we present an efficient algorithm for optimizing the proposed objective . <eos> synthetic and real-world experiments illustrate the effectiveness of our method , at scale .
two-alternative forced choice ( 2afc ) and go/nogo ( gng ) tasks are behavioral choice paradigms commonly used to study sensory and cognitive processing in choice behavior . <eos> while gng is thought to isolate the sensory/decisional component by removing the need for response selection , a consistent bias towards the go response ( higher hits and false alarm rates ) in the gng task suggests possible fundamental differences in the sensory or cognitive processes engaged in the two tasks . <eos> existing mechanistic models of these choice tasks , mostly variants of the drift-diffusion model ( ddm ; [ 1,2 ] ) and the related leaky competing accumulator models [ 3,4 ] capture various aspects of behavior but do not address the provenance of the go bias . <eos> we postulate that this `` impatience '' to go is a strategic adjustment in response to the implicit asymmetry in the cost structure of gng : the nogo response requires waiting until the response deadline , while a go response immediately terminates the current trial . <eos> we show that a bayes-risk minimizing decision policy that minimizes both error rate and average decision delay naturally exhibits the experimentally observed bias . <eos> the optimal decision policy is formally equivalent to a ddm with a time-varying threshold that initially rises after stimulus onset , and collapses again near the response deadline . <eos> the initial rise is due to the fading temporal advantage of choosing the go response over the fixed-delay nogo response . <eos> we show that fitting a simpler , fixed-threshold ddm to the optimal model reproduces the counterintuitive result of a higher threshold in gng than 2afc decision-making , previously observed in direct ddm fit to behavioral data [ 2 ] , although such approximations can not reproduce the go bias . <eos> thus , observed discrepancies between gng and 2afc decision-making may arise from rational strategic adjustments to the cost structure , and need not imply additional differences in the underlying sensory and cognitive processes .
sum-product networks are a new deep architecture that can perform fast , exact inference on high-treewidth models . <eos> only generative methods for training spns have been proposed to date . <eos> in this paper , we present the first discriminative training algorithms for spns , combining the high accuracy of the former with the representational power and tractability of the latter . <eos> we show that the class of tractable discriminative spns is broader than the class of tractable generative ones , and propose an efficient backpropagation-style algorithm for computing the gradient of the conditional log likelihood . <eos> standard gradient descent suffers from the diffusion problem , but networks with many layers can be learned reliably using hardgradient descent , where marginal inference is replaced by mpe inference ( i.e. , inferring the most probable state of the non-evidence variables ) . <eos> the resulting updates have a simple and intuitive form . <eos> we test discriminative spns on standard image classification tasks . <eos> we obtain the best results to date on the cifar-10 dataset , using fewer features than prior methods with an spn architecture that learns local image structure discriminatively . <eos> we also report the highest published test accuracy on stl-10 even though we only use the labeled portion of the dataset .
we present a reformulation of the information bottleneck ( ib ) problem in terms of copula , using the equivalence between mutual information and negative copula entropy . <eos> focusing on the gaussian copula we extend the analytical ib solution available for the multivariate gaussian case to distributions with a gaussian dependence structure but arbitrary marginal densities , also called meta-gaussian distributions . <eos> this opens new possibles applications of ib to continuous data and provides a solution more robust to outliers .
this paper describes a new approach for computing nonnegative matrix factorizations ( nmfs ) with linear programming . <eos> the key idea is a data-driven model for the factorization , in which the most salient features in the data are used to express the remaining features . <eos> more precisely , given a data matrix x , the algorithm identifies a matrix c that satisfies x = cx and some linear constraints . <eos> the matrix c selects features , which are then used to compute a low-rank nmf of x . <eos> a theoretical analysis demonstrates that this approach has the same type of guarantees as the recent nmf algorithm of arora et al.~ ( 2012 ) . <eos> in contrast with this earlier work , the proposed method has ( 1 ) better noise tolerance , ( 2 ) extends to more general noise models , and ( 3 ) leads to efficient , scalable algorithms . <eos> experiments with synthetic and real datasets provide evidence that the new approach is also superior in practice . <eos> an optimized c++ implementation of the new algorithm can factor a multi-gigabyte matrix in a matter of minutes .
we consider the problem of adaptive stratified sampling for monte carlo integration of a differentiable function given a finite number of evaluations to the function . <eos> we construct a sampling scheme that samples more often in regions where the function oscillates more , while allocating the samples such that they are well spread on the domain ( this notion shares similitude with low discrepancy ) . <eos> we prove that the estimate returned by the algorithm is almost as accurate as the estimate that an optimal oracle strategy ( that would know the variations of the function everywhere ) would return , and we provide a finite-sample analysis .
in hierarchical classification , the prediction paths may be required to always end at leaf nodes . <eos> this is called mandatory leaf node prediction ( mlnp ) and is particularly useful when the leaf nodes have much stronger semantic meaning than the internal nodes . <eos> however , while there have been a lot of mlnp methods in hierarchical multiclass classification , performing mlnp in hierarchical multilabel classification is much more difficult . <eos> in this paper , we propose a novel mlnp algorithm that ( i ) considers the global hierarchy structure ; and ( ii ) can be used on hierarchies of both trees and dags . <eos> we show that one can efficiently maximize the joint posterior probability of all the node labels by a simple greedy algorithm . <eos> moreover , this can be further extended to the minimization of the expected symmetric loss . <eos> experiments are performed on a number of real-world data sets with tree- and dag-structured label hierarchies . <eos> the proposed method consistently outperforms other hierarchical and flat multilabel classification methods .
we consider the problem of estimating shannon 's entropy h in the under-sampled regime , where the number of possible symbols may be unknown or countably infinite . <eos> pitman-yor processes ( a generalization of dirichlet processes ) provide tractable prior distributions over the space of countably infinite discrete distributions , and have found major applications in bayesian non-parametric statistics and machine learning . <eos> here we show that they also provide natural priors for bayesian entropy estimation , due to the remarkable fact that the moments of the induced posterior distribution over h can be computed analytically . <eos> we derive formulas for the posterior mean ( bayes ' least squares estimate ) and variance under such priors . <eos> moreover , we show that a fixed dirichlet or pitman-yor process prior implies a narrow prior on h , meaning the prior strongly determines the entropy estimate in the under-sampled regime . <eos> we derive a family of continuous mixing measures such that the resulting mixture of pitman-yor processes produces an approximately flat ( improper ) prior over h. we explore the theoretical properties of the resulting estimator , and show that it performs well on data sampled from both exponential and power-law tailed distributions .
the use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters . <eos> unfortunately , this tuning is often a ? black art ? <eos> requiring expert experience , rules of thumb , or sometimes brute-force search . <eos> there is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand . <eos> in this work , we consider this problem through the framework of bayesian optimization , in which a learning algorithm ? s generalization performance is modeled as a sample from a gaussian process ( gp ) . <eos> we show that certain choices for the nature of the gp , such as the type of kernel and the treatment of its hyperparameters , can play a crucial role in obtaining a good optimizer that can achieve expert-level performance . <eos> we describe new algorithms that take into account the variable cost ( duration ) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation . <eos> we show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent dirichlet allocation , structured svms and convolutional neural networks .
we describe efficient implementations of the proximity calculation for a useful class of functions ; the implementations exploit the piece-wise linear nature of the dual problem . <eos> the second part of the paper applies the previous result to acceleration of convex minimization problems , and leads to an elegant quasi-newton method . <eos> the optimization method compares favorably against state-of-the-art alternatives . <eos> the algorithm has extensive applications including signal processing , sparse regression and recovery , and machine learning and classification .
we present a simplex algorithm for linear programming in a linear classification formulation . <eos> the paramount complexity parameter in linear classification problems is called the margin . <eos> we prove that for margin values of practical interest our simplex variant performs a polylogarithmic number of pivot steps in the worst case , and its overall running time is near linear . <eos> this is in contrast to general linear programming , for which no sub-polynomial pivot rule is known .
a patient 's risk for adverse events is affected by temporal processes including the nature and timing of diagnostic and therapeutic activities , and the overall evolution of the patient 's pathophysiology over time . <eos> yet many investigators ignore this temporal aspect when modeling patient risk , considering only the patient 's current or aggregate state . <eos> we explore representing patient risk as a time series . <eos> in doing so , patient risk stratification becomes a time-series classification task . <eos> the task differs from most applications of time-series analysis , like speech processing , since the time series itself must first be extracted . <eos> thus , we begin by defining and extracting approximate \textit { risk processes } , the evolving approximate daily risk of a patient . <eos> once obtained , we use these signals to explore different approaches to time-series classification with the goal of identifying high-risk patterns . <eos> we apply the classification to the specific task of identifying patients at risk of testing positive for hospital acquired colonization with \textit { clostridium difficile } . <eos> we achieve an area under the receiver operating characteristic curve of 0.79 on a held-out set of several hundred patients . <eos> our two-stage approach to risk stratification outperforms classifiers that consider only a patient 's current state ( p $ < $ 0.05 ) .
residue-residue contact prediction is a fundamental problem in protein structure prediction . <eos> hower , despite considerable research efforts , contact prediction methods are still largely unreliable . <eos> here we introduce a novel deep machine-learning architecture which consists of a multidimensional stack of learning modules . <eos> for contact prediction , the idea is implemented as a three-dimensional stack of neural networks nn^k_ { ij } , where i and j index the spatial coordinates of the contact map and k indexes `` time '' . <eos> the temporal dimension is introduced to capture the fact that protein folding is not an instantaneous process , but rather a progressive refinement . <eos> networks at level k in the stack can be trained in supervised fashion to refine the predictions produced by the previous level , hence addressing the problem of vanishing gradients , typical of deep architectures . <eos> increased accuracy and generalization capabilities of this approach are established by rigorous comparison with other classical machine learning approaches for contact prediction . <eos> the deep approach leads to an accuracy for difficult long-range contacts of about 30 % , roughly 10 % above the state-of-the-art . <eos> many variations in the architectures and the training algorithms are possible , leaving room for further improvements . <eos> furthermore , the approach is applicable to other problems with strong underlying spatial and temporal components .
to learn reliable rules that can generalize to novel situations , the brain must be capable of imposing some form of regularization . <eos> here we suggest , through theoretical and computational arguments , that the combination of noise with synchronization provides a plausible mechanism for regularization in the nervous system . <eos> the functional role of regularization is considered in a general context in which coupled computational systems receive inputs corrupted by correlated noise . <eos> noise on the inputs is shown to impose regularization , and when synchronization upstream induces time-varying correlations across noise variables , the degree of regularization can be calibrated over time . <eos> the resulting qualitative behavior matches experimental data from visual cortex .
we study consistency properties of surrogate loss functions for general multiclass classification problems , defined by a general loss matrix . <eos> we extend the notion of classification calibration , which has been studied for binary and multiclass 0-1 classification problems ( and for certain other specific learning problems ) , to the general multiclass setting , and derive necessary and sufficient conditions for a surrogate loss to be classification calibrated with respect to a loss matrix in this setting . <eos> we then introduce the notion of \emph { classification calibration dimension } of a multiclass loss matrix , which measures the smallest ` size ' of a prediction space for which it is possible to design a convex surrogate that is classification calibrated with respect to the loss matrix . <eos> we derive both upper and lower bounds on this quantity , and use these results to analyze various loss matrices . <eos> in particular , as one application , we provide a different route from the recent result of duchi et al.\ ( 2010 ) for analyzing the difficulty of designing ` low-dimensional ' convex surrogates that are consistent with respect to pairwise subset ranking losses . <eos> we anticipate the classification calibration dimension may prove to be a useful tool in the study and design of surrogate losses for general multiclass learning problems .
we strengthen recent connections between prediction markets and learning by showing that a natural class of market makers can be understood as performing stochastic mirror descent when trader demands are sequentially drawn from a fixed distribution . <eos> this provides new insights into how market prices ( and price paths ) may be interpreted as a summary of the market 's belief distribution by relating them to the optimization problem being solved . <eos> in particular , we show that the stationary point of the stochastic process of prices generated by the market is equal to the market 's walrasian equilibrium of classic market analysis . <eos> together , these results suggest how traditional market making mechanisms might be replaced with general purpose learning algorithms while still retaining guarantees about their behaviour .
graphical model selection refers to the problem of estimating the unknown graph structure given observations at the nodes in the model . <eos> we consider a challenging instance of this problem when some of the nodes are latent or hidden . <eos> we characterize conditions for tractable graph estimation and develop efficient methods with provable guarantees . <eos> we consider the class of ising models markov on locally tree-like graphs , which are in the regime of correlation decay . <eos> we propose an efficient method for graph estimation , and establish its structural consistency when the number of samples $ n $ scales as $ n = \omega ( \theta_ { \min } ^ { -\delta \eta ( \eta+1 ) -2 } \log p ) $ , where $ \theta_ { \min } $ is the minimum edge potential , $ \delta $ is the depth ( i.e. , distance from a hidden node to the nearest observed nodes ) , and $ \eta $ is a parameter which depends on the minimum and maximum node and edge potentials in the ising model . <eos> the proposed method is practical to implement and provides flexibility to control the number of latent variables and the cycle lengths in the output graph . <eos> we also present necessary conditions for graph estimation by any method and show that our method nearly matches the lower bound on sample requirements .
one of the enduring challenges in markov chain monte carlo methodology is the development of proposal mechanisms to make moves distant from the current point , that are accepted with high probability and at low computational cost . <eos> the recent introduction of locally adaptive mcmc methods based on the natural underlying riemannian geometry of such models goes some way to alleviating these problems for certain classes of models for which the metric tensor is analytically tractable , however computational efficiency is not assured due to the necessity of potentially high-dimensional matrix operations at each iteration . <eos> in this paper we firstly investigate a sampling-based approach for approximating the metric tensor and suggest a valid mcmc algorithm that extends the applicability of riemannian manifold mcmc methods to statistical models that do not admit an analytically computable metric tensor . <eos> secondly , we show how the approximation scheme we consider naturally motivates the use of l1 regularisation to improve estimates and obtain a sparse approximate inverse of the metric , which enables stable and sparse approximations of the local geometry to be made . <eos> we demonstrate the application of this algorithm for inferring the parameters of a realistic system of ordinary differential equations using a biologically motivated robust student-t error model , for which the expected fisher information is analytically intractable .
our personal social networks are big and cluttered , and currently there is no good way to organize them . <eos> social networking sites allow users to manually categorize their friends into social circles ( e.g . ` circles ' on google+ , and ` lists ' on facebook and twitter ) , however they are laborious to construct and must be updated whenever a user 's network grows . <eos> we define a novel machine learning task of identifying users ' social circles . <eos> we pose the problem as a node clustering problem on a user 's ego-network , a network of connections between her friends . <eos> we develop a model for detecting circles that combines network structure as well as user profile information . <eos> for each circle we learn its members and the circle-specific user profile similarity metric . <eos> modeling node membership to multiple circles allows us to detect overlapping as well as hierarchically nested circles . <eos> experiments show that our model accurately identifies circles on a diverse set of data from facebook , google+ , and twitter for all of which we obtain hand-labeled ground-truth data .
boolean satisfiability ( sat ) as a canonical np-complete decision problem is one of the most important problems in computer science . <eos> in practice , real-world sat sentences are drawn from a distribution that may result in efficient algorithms for their solution . <eos> such sat instances are likely to have shared characteristics and substructures . <eos> this work approaches the exploration of a family of sat solvers as a learning problem . <eos> in particular , we relate polynomial time solvability of a sat subset to a notion of margin between sentences mapped by a feature function into a hilbert space . <eos> provided this mapping is based on polynomial time computable statistics of a sentence , we show that the existance of a margin between these data points implies the existance of a polynomial time solver for that sat subset based on the davis-putnam-logemann-loveland algorithm . <eos> furthermore , we show that a simple perceptron-style learning rule will find an optimal sat solver with a bounded number of training updates . <eos> we derive a linear time computable set of features and show analytically that margins exist for important polynomial special cases of sat . <eos> empirical results show an order of magnitude improvement over a state-of-the-art sat solver on a hardware verification task .
we present a truncation-free online variational inference algorithm for bayesian nonparametric models . <eos> unlike traditional ( online ) variational inference algorithms that require truncations for the model or the variational distribution , our method adapts model complexity on the fly . <eos> our experiments for dirichlet process mixture models and hierarchical dirichlet process topic models on two large-scale data sets show better performance than previous online variational inference algorithms .
we present a new variational inference algorithm for gaussian processes with non-conjugate likelihood functions . <eos> this includes binary and multi-class classification , as well as ordinal regression . <eos> our method constructs a convex lower bound , which can be optimized by using an efficient fixed point update method . <eos> we then show empirically that our new approach is much faster than existing methods without any degradation in performance .
we propose a novel bayesian approach to solve stochastic optimization problems that involve ? nding extrema of noisy , nonlinear functions . <eos> previous work has focused on representing possible functions explicitly , which leads to a two-step procedure of ? rst , doing inference over the function space and second , ? nding the extrema of these functions . <eos> here we skip the representation step and directly model the distribution over extrema . <eos> to this end , we devise a non-parametric conjugate prior where the natural parameter corresponds to a given kernel function and the suf ? cient statistic is composed of the observed function values . <eos> the resulting posterior distribution directly captures the uncertainty over the maximum of the unknown function .
we derive a novel norm that corresponds to the tightest convex relaxation of sparsity combined with an $ \ell_2 $ penalty . <eos> we show that this new norm provides a tighter relaxation than the elastic net , and is thus a good replacement for the lasso or the elastic net in sparse prediction problems . <eos> but through studying our new norm , we also bound the looseness of the elastic net , thus shedding new light on it and providing justification for its use .
a key problem in statistics and machine learning is the determination of network structure from data . <eos> we consider the case where the structure of the graph to be reconstructed is known to be scale-free . <eos> we show that in such cases it is natural to formulate structured sparsity inducing priors using submodular functions , and we use their lovasz extension to obtain a convex relaxation . <eos> for tractable classes such as gaussian graphical models , this leads to a convex optimization problem that can be efficiently solved . <eos> we show that our method results in an improvement in the accuracy of reconstructed networks for synthetic data . <eos> we also show how our prior encourages scale-free reconstructions on a bioinfomatics dataset .
multi-metric learning techniques learn local metric tensors in different parts of a feature space . <eos> with such an approach , even simple classifiers can be competitive with the state-of-the-art because the distance measure locally adapts to the structure of the data . <eos> the learned distance measure is , however , non-metric , which has prevented multi-metric learning from generalizing to tasks such as dimensionality reduction and regression in a principled way . <eos> we prove that , with appropriate changes , multi-metric learning corresponds to learning the structure of a riemannian manifold . <eos> we then show that this structure gives us a principled way to perform dimensionality reduction and regression according to the learned metrics . <eos> algorithmically , we provide the first practical algorithm for computing geodesics according to the learned metrics , as well as algorithms for computing exponential and logarithmic maps on the riemannian manifold . <eos> together , these tools let many euclidean algorithms take advantage of multi-metric learning . <eos> we illustrate the approach on regression and dimensionality reduction tasks that involve predicting measurements of the human body from shape data .
hashing is a common method to reduce large , potentially infinite feature vectors to a fixed-size table . <eos> in reinforcement learning , hashing is often used in conjunction with tile coding to represent states in continuous spaces . <eos> hashing is also a promising approach to value function approximation in large discrete domains such as go and hearts , where feature vectors can be constructed by exhaustively combining a set of atomic features . <eos> unfortunately , the typical use of hashing in value function approximation results in biased value estimates due to the possibility of collisions . <eos> recent work in data stream summaries has led to the development of the tug-of-war sketch , an unbiased estimator for approximating inner products . <eos> our work investigates the application of this new data structure to linear value function approximation . <eos> although in the reinforcement learning setting the use of the tug-of-war sketch leads to biased value estimates , we show that this bias can be orders of magnitude less than that of standard hashing . <eos> we provide empirical results on two rl benchmark domains and fifty-five atari 2600 games to highlight the superior learning performance of tug-of-war hashing .
in conventional causal discovery , structural equation models ( sem ) are directly applied to the observed variables , meaning that the causal effect can be represented as a function of the direct causes themselves . <eos> however , in many real world problems , there are significant dependencies in the variances or energies , which indicates that causality may possibly take place at the level of variances or energies . <eos> in this paper , we propose a probabilistic causal scale-mixture model with spatiotemporal variance dependencies to represent a specific type of generating mechanism of the observations . <eos> in particular , the causal mechanism including contemporaneous and temporal causal relations in variances or energies is represented by a structural vector autoregressive model ( svar ) . <eos> we prove the identifiability of this model under the non-gaussian assumption on the innovation processes . <eos> we also propose algorithms to estimate the involved parameters and discover the contemporaneous causal structure . <eos> experiments on synthesis and real world data are conducted to show the applicability of the proposed model and algorithms .
the minimax kl-divergence of any distribution from all distributions in a collection p has several practical implications . <eos> in compression , it is called redundancy and represents the least additional number of bits over the entropy needed to encode the output of any distribution in p. in online estimation and learning , it is the lowest expected log-loss regret when guessing a sequence of random values generated by a distribution in p. in hypothesis testing , it upper bounds the largest number of distinguishable distributions in p. motivated by problems ranging from population estimation to text classification and speech recognition , several machine-learning and information-theory researchers have recently considered label-invariant observations and properties induced by i.i.d . <eos> distributions . <eos> a sufficient statistic for all these properties is the data ? s profile , the multiset of the number of times each data element appears . <eos> improving on a sequence of previous works , we show that the redundancy of the collection of distributions induced over profiles by length-n i.i.d . <eos> sequences is between 0.3 n1/3 and n1/3 log2 n , in particular , establishing its exact growth power .
this paper considers a wide spectrum of regularized stochastic optimization problems where both the loss function and regularizer can be non-smooth . <eos> we develop a novel algorithm based on the regularized dual averaging ( rda ) method , that can simultaneously achieve the optimal convergence rates for both convex and strongly convex loss . <eos> in particular , for strongly convex loss , it achieves the optimal rate of $ o ( \frac { 1 } { n } +\frac { 1 } { n^2 } ) $ for $ n $ iterations , which improves the best known rate $ o ( \frac { \log n } { n } ) $ of previous stochastic dual averaging algorithms . <eos> in addition , our method constructs the final solution directly from the proximal mapping instead of averaging of all previous iterates . <eos> for widely used sparsity-inducing regularizers ( e.g. , $ \ell_1 $ -norm ) , it has the advantage of encouraging sparser solutions . <eos> we further develop a multi-stage extension using the proposed algorithm as a subroutine , which achieves the uniformly-optimal rate $ o ( \frac { 1 } { n } +\exp\ { -n\ } ) $ for strongly convex loss .
the sum-product network ( spn ) is a recently-proposed deep model consisting of a network of sum and product nodes , and has been shown to be competitive with state-of-the-art deep models on certain difficult tasks such as image completion . <eos> designing an spn network architecture that is suitable for the task at hand is an open question . <eos> we propose an algorithm for learning the spn architecture from data . <eos> the idea is to cluster variables ( as opposed to data instances ) in order to identify variable subsets that strongly interact with one another . <eos> nodes in the spn network are then allocated towards explaining these interactions . <eos> experimental evidence shows that learning the spn architecture significantly improves its performance compared to using a previously-proposed static architecture .
imitation learning has been shown to be successful in solving many challenging real-world problems . <eos> some recent approaches give strong performance guarantees by training the policy iteratively . <eos> however , it is important to note that these guarantees depend on how well the policy we found can imitate the oracle on the training data . <eos> when there is a substantial difference between the oracle 's ability and the learner 's policy space , we may fail to find a policy that has low error on the training set . <eos> in such cases , we propose to use a coach that demonstrates easy-to-learn actions for the learner and gradually approaches the oracle . <eos> by a reduction of learning by demonstration to online learning , we prove that coaching can yield a lower regret bound than using the oracle . <eos> we apply our algorithm to a novel cost-sensitive dynamic feature selection problem , a hard decision problem that considers a user-specified accuracy-cost trade-off . <eos> experimental results on uci datasets show that our method outperforms state-of-the-art imitation learning methods in dynamic features selection and two static feature selection methods .
in this paper , we consider the $ \ell_1 $ regularized sparse inverse covariance matrix estimation problem with a very large number of variables . <eos> even in the face of this high dimensionality , and with limited number of samples , recent work has shown this estimator to have strong statistical guarantees in recovering the true structure of the sparse inverse covariance matrix , or alternatively the underlying graph structure of the corresponding gaussian markov random field . <eos> our proposed algorithm divides the problem into smaller sub-problems , and uses the solutions of the sub-problems to build a good approximation for the original problem . <eos> we derive a bound on the distance of the approximate solution to the true solution . <eos> based on this bound , we propose a clustering algorithm that attempts to minimize this bound , and in practice , is able to find effective partitions of the variables . <eos> we further use the approximate solution , i.e. , solution resulting from solving the sub-problems , as an initial point to solve the original problem , and achieve a much faster computational procedure . <eos> as an example , a recent state-of-the-art method , quic requires 10 hours to solve a problem ( with 10,000 nodes ) that arises from a climate application , while our proposed algorithm , divide and conquer quic ( dc-quic ) only requires one hour to solve the problem .
this paper presents a novel non-parametric approximate dynamic programming ( adp ) algorithm that enjoys graceful , dimension-independent approximation and sample complexity guarantees . <eos> in particular , we establish both theoretically and computationally that our proposal can serve as a viable alternative to state-of-the-art parametric adp algorithms , freeing the designer from carefully specifying an approximation architecture . <eos> we accomplish this by developing a kernel-based mathematical program for adp . <eos> via a computational study on a controlled queueing network , we show that our non-parametric procedure is competitive with parametric adp approaches .
we present a new algorithm for differentially private data release , based on a simple combination of the exponential mechanism with the multiplicative weights update rule . <eos> our mwem algorithm achieves what are the best known and nearly optimal theoretical guarantees , while at the same time being simple to implement and experimentally more accurate on actual data sets than existing techniques .
the paper addresses the problem of generating multiple hypotheses for prediction tasks that involve interaction with users or successive components in a cascade . <eos> given a set of multiple hypotheses , such components/users have the ability to automatically rank the results and thus retrieve the best one . <eos> the standard approach for handling this scenario is to learn a single model and then produce m-best maximum a posteriori ( map ) hypotheses from this model . <eos> in contrast , we formulate this multiple { \em choice } learning task as a multiple-output structured-output prediction problem with a loss function that captures the natural setup of the problem . <eos> we present a max-margin formulation that minimizes an upper-bound on this loss-function . <eos> experimental results on the problems of image co-segmentation and protein side-chain prediction show that our method outperforms conventional approaches used for this scenario and leads to substantial improvements in prediction accuracy .
we consider derivative-free algorithms for stochastic optimization problems that use only noisy function values rather than gradients , analyzing their finite-sample convergence rates . <eos> we show that if pairs of function values are available , algorithms that use gradient estimates based on random perturbations suffer a factor of at most $ \sqrt { \dim } $ in convergence rate over traditional stochastic gradient methods , where $ \dim $ is the dimension of the problem . <eos> we complement our algorithmic development with information-theoretic lower bounds on the minimax convergence rate of such problems , which show that our bounds are sharp with respect to all problem-dependent quantities : they can not be improved by more than constant factors .
this paper adresses the inverse reinforcement learning ( irl ) problem , that is inferring a reward for which a demonstrated expert behavior is optimal . <eos> we introduce a new algorithm , scirl , whose principle is to use the so-called feature expectation of the expert as the parameterization of the score function of a multi-class classifier . <eos> this approach produces a reward function for which the expert policy is provably near-optimal . <eos> contrary to most of existing irl algorithms , scirl does not require solving the direct rl problem . <eos> moreover , with an appropriate heuristic , it can succeed with only trajectories sampled according to the expert behavior . <eos> this is illustrated on a car driving simulator .
active learning can substantially improve the yield of neurophysiology experiments by adaptively selecting stimuli to probe a neuron 's receptive field ( rf ) in real time . <eos> bayesian active learning methods maintain a posterior distribution over the rf , and select stimuli to maximally reduce posterior entropy on each time step . <eos> however , existing methods tend to rely on simple gaussian priors , and do not exploit uncertainty at the level of hyperparameters when determining an optimal stimulus . <eos> this uncertainty can play a substantial role in rf characterization , particularly when rfs are smooth , sparse , or local in space and time . <eos> in this paper , we describe a novel framework for active learning under hierarchical , conditionally gaussian priors . <eos> our algorithm uses sequential markov chain monte carlo sampling ( `` particle filtering '' with mcmc ) over hyperparameters to construct a mixture-of-gaussians representation of the rf posterior , and selects optimal stimuli using an approximate infomax criterion . <eos> the core elements of this algorithm are parallelizable , making it computationally efficient for real-time experiments . <eos> we apply our algorithm to simulated and real neural data , and show that it can provide highly accurate receptive field estimates from very limited data , even with a small number of hyperparameter samples .
we propose an efficient , generalized , nonparametric , statistical kolmogorov-smirnov test for detecting distributional change in high-dimensional data . <eos> to implement the test , we introduce a novel , hierarchical , minimum-volume sets estimator to represent the distributions to be tested . <eos> our work is motivated by the need to detect changes in data streams , and the test is especially efficient in this context . <eos> we provide the theoretical foundations of our test and show its superiority over existing methods .
markov logic is a widely used tool in statistical relational learning , which uses a weighted first-order logic knowledge base to specify a markov random field ( mrf ) or a conditional random field ( crf ) . <eos> in many applications , a markov logic network ( mln ) is trained in one domain , but used in a different one . <eos> this paper focuses on dynamic markov logic networks , where the domain of time points typically varies between training and testing . <eos> it has been previously pointed out that the marginal probabilities of truth assignments to ground atoms can change if one extends or reduces the domains of predicates in an mln . <eos> we show that in addition to this problem , the standard way of unrolling a markov logic theory into a mrf may result in time-inhomogeneity of the underlying markov chain . <eos> furthermore , even if these representational problems are not significant for a given domain , we show that the more practical problem of generating samples in a sequential conditional random field for the next slice relying on the samples from the previous slice has high computational cost in the general case , due to the need to estimate a normalization factor for each sample . <eos> we propose a new discriminative model , slice normalized dynamic markov logic networks ( sn-dmln ) , that suffers from none of these issues . <eos> it supports efficient online inference , and can directly model influences between variables within a time slice that do not have a causal direction , in contrast with fully directed models ( e.g. , dbns ) . <eos> experimental results show an improvement in accuracy over previous approaches to online inference in dynamic markov logic networks .
recent experiments have demonstrated that humans and animals typically reason probabilistically about their environment . <eos> this ability requires a neural code that represents probability distributions and neural circuits that are capable of implementing the operations of probabilistic inference . <eos> the proposed probabilistic population coding ( ppc ) framework provides a statistically efficient neural representation of probability distributions that is both broadly consistent with physiological measurements and capable of implementing some of the basic operations of probabilistic inference in a biologically plausible way . <eos> however , these experiments and the corresponding neural models have largely focused on simple ( tractable ) probabilistic computations such as cue combination , coordinate transformations , and decision making . <eos> as a result it remains unclear how to generalize this framework to more complex probabilistic computations . <eos> here we address this short coming by showing that a very general approximate inference algorithm known as variational bayesian expectation maximization can be implemented within the linear ppc framework . <eos> we apply this approach to a generic problem faced by any given layer of cortex , namely the identification of latent causes of complex mixtures of spikes . <eos> we identify a formal equivalent between this spike pattern demixing problem and topic models used for document classification , in particular latent dirichlet allocation ( lda ) . <eos> we then construct a neural network implementation of variational inference and learning for lda that utilizes a linear ppc . <eos> this network relies critically on two non-linear operations : divisive normalization and super-linear facilitation , both of which are ubiquitously observed in neural circuits . <eos> we also demonstrate how online learning can be achieved using a variation of hebb ? s rule and describe an extesion of this work which allows us to deal with time varying and correlated latent causes .
users want natural language processing ( nlp ) systems to be both fast and accurate , but quality often comes at the cost of speed . <eos> the field has been manually exploring various speed-accuracy tradeoffs ( for particular problems and datasets ) . <eos> we aim to explore this space automatically , focusing here on the case of agenda-based syntactic parsing \cite { kay-1986 } . <eos> unfortunately , off-the-shelf reinforcement learning techniques fail to learn good policies : the state space is simply too large to explore naively . <eos> an attempt to counteract this by applying imitation learning algorithms also fails : the `` teacher '' is far too good to successfully imitate with our inexpensive features . <eos> moreover , it is not specifically tuned for the known reward function . <eos> we propose a hybrid reinforcement/apprenticeship learning algorithm that , even with only a few inexpensive features , can automatically learn weights that achieve competitive accuracies at significant improvements in speed over state-of-the-art baselines .
we consider sequential prediction algorithms that are given the predictions from a set of models as inputs . <eos> if the nature of the data is changing over time in that different models predict well on different segments of the data , then adaptivity is typically achieved by mixing into the weights in each round a bit of the initial prior ( kind of like a weak restart ) . <eos> however , what if the favored models in each segment are from a small subset , i.e . the data is likely to be predicted well by models that predicted well before ? <eos> curiously , fitting such `` sparse composite models '' is achieved by mixing in a bit of all the past posteriors . <eos> this self-referential updating method is rather peculiar , but it is efficient and gives superior performance on many natural data sets . <eos> also it is important because it introduces a long-term memory : any model that has done well in the past can be recovered quickly . <eos> while bayesian interpretations can be found for mixing in a bit of the initial prior , no bayesian interpretation is known for mixing in past posteriors . <eos> we build atop the `` specialist '' framework from the online learning literature to give the mixing past posteriors update a proper bayesian foundation . <eos> we apply our method to a well-studied multitask learning problem and obtain a new intriguing efficient update that achieves a significantly better bound .
despite the variety of robust regression methods that have been developed , current regression formulations are either np-hard , or allow unbounded response to even a single leverage point . <eos> we present a general formulation for robust regression -- variational m-estimation -- that unifies a number of robust regression methods while allowing a tractable approximation strategy . <eos> we develop an estimator that requires only polynomial-time , while achieving certain robustness and consistency guarantees . <eos> an experimental evaluation demonstrates the effectiveness of the new estimation approach compared to standard methods .
we present a multi-task learning approach to jointly estimate the means of multiple independent data sets . <eos> the proposed multi-task averaging ( mta ) algorithm results in a convex combination of the single-task averages . <eos> we derive the optimal amount of regularization , and show that it can be effectively estimated . <eos> simulations and real data experiments demonstrate that mta both maximum likelihood and james-stein estimators , and that our approach to estimating the amount of regularization rivals cross-validation in performance but is more computationally efficient .
in many applications , one has information , e.g. , labels that are provided in a semi-supervised manner , about a specific target region of a large data set , and one wants to perform machine learning and data analysis tasks nearby that pre-specified target region . <eos> locally-biased problems of this sort are particularly challenging for popular eigenvector-based machine learning and data analysis tools . <eos> at root , the reason is that eigenvectors are inherently global quantities . <eos> in this paper , we address this issue by providing a methodology to construct semi-supervised eigenvectors of a graph laplacian , and we illustrate how these locally-biased eigenvectors can be used to perform locally-biased machine learning . <eos> these semi-supervised eigenvectors capture successively-orthogonalized directions of maximum variance , conditioned on being well-correlated with an input seed set of nodes that is assumed to be provided in a semi-supervised manner . <eos> we also provide several empirical examples demonstrating how these semi-supervised eigenvectors can be used to perform locally-biased learning .
label space dimension reduction ( lsdr ) is an efficient and effective paradigm for multi-label classification with many classes . <eos> existing approaches to lsdr , such as compressive sensing and principal label space transformation , exploit only the label part of the dataset , but not the feature part . <eos> in this paper , we propose a novel approach to lsdr that considers both the label and the feature parts . <eos> the approach , called conditional principal label space transformation , is based on minimizing an upper bound of the popular hamming loss . <eos> the minimization step of the approach can be carried out efficiently by a simple use of singular value decomposition . <eos> in addition , the approach can be extended to a kernelized version that allows the use of sophisticated feature combinations to assist lsdr . <eos> the experimental results verify that the proposed approach is more effective than existing ones to lsdr across many real-world datasets .
this paper addresses the problem of category-level 3d object detection . <eos> given a monocular image , our aim is to localize the objects in 3d by enclosing them with tight oriented 3d bounding boxes . <eos> we propose a novel approach that extends the well-acclaimed deformable part-based model [ felz . ] <eos> to reason in 3d . <eos> our model represents an object class as a deformable 3d cuboid composed of faces and parts , which are both allowed to deform with respect to their anchors on the 3d box . <eos> we model the appearance of each face in fronto-parallel coordinates , thus effectively factoring out the appearance variation induced by viewpoint . <eos> our model reasons about face visibility patters called aspects . <eos> we train the cuboid model jointly and discriminatively and share weights across all aspects to attain efficiency . <eos> inference then entails sliding and rotating the box in 3d and scoring object hypotheses . <eos> while for inference we discretize the search space , the variables are continuous in our model . <eos> we demonstrate the effectiveness of our approach in indoor and outdoor scenarios , and show that our approach outperforms the state-of-the-art in both 2d [ felz09 ] and 3d object detection [ hedau12 ] .
we consider the online distributed non-stochastic experts problem , where the distributed system consists of one coordinator node that is connected to k sites , and the sites are required to communicate with each other via the coordinator . <eos> at each time-step t , one of the k site nodes has to pick an expert from the set { 1 , . <eos> . <eos> . <eos> , n } , and the same site receives information about payoffs of all experts for that round . <eos> the goal of the distributed system is to minimize regret at time horizon t , while simultaneously keeping communication to a minimum . <eos> the two extreme solutions to this problem are : ( i ) full communication : this essentially simulates the non-distributed setting to obtain the optimal o ( \sqrt { log ( n ) t } ) regret bound at the cost of t communication . <eos> ( ii ) no communication : each site runs an independent copy the regret is o ( \sqrt { log ( n ) kt } ) and the communication is 0 . <eos> this paper shows the difficulty of simultaneously achieving regret asymptotically better than \sqrt { kt } and communication better than t. we give a novel algorithm that for an oblivious adversary achieves a non-trivial trade-off : regret o ( \sqrt { k^ { 5 ( 1+\epsilon ) /6 } t } ) and communication o ( t/k^\epsilon ) , for any value of \epsilon in ( 0 , 1/5 ) . <eos> we also consider a variant of the model , where the coordinator picks the expert . <eos> in this model , we show that the label-efficient forecaster of cesa-bianchi et al . ( 2005 ) already gives us strategy that is near optimal in regret vs communication trade-off .
in this paper , we argue for representing networks as a bag of { \it triangular motifs } , particularly for important network problems that current model-based approaches handle poorly due to computational bottlenecks incurred by using edge representations . <eos> such approaches require both 1-edges and 0-edges ( missing edges ) to be provided as input , and as a consequence , approximate inference algorithms for these models usually require $ \omega ( n^2 ) $ time per iteration , precluding their application to larger real-world networks . <eos> in contrast , triangular modeling requires less computation , while providing equivalent or better inference quality . <eos> a triangular motif is a vertex triple containing 2 or 3 edges , and the number of such motifs is $ \theta ( \sum_ { i } d_ { i } ^ { 2 } ) $ ( where $ d_i $ is the degree of vertex $ i $ ) , which is much smaller than $ n^2 $ for low-maximum-degree networks . <eos> using this representation , we develop a novel mixed-membership network model and approximate inference algorithm suitable for large networks with low max-degree . <eos> for networks with high maximum degree , the triangular motifs can be naturally subsampled in a { \it node-centric } fashion , allowing for much faster inference at a small cost in accuracy . <eos> empirically , we demonstrate that our approach , when compared to that of an edge-based model , has faster runtime and improved accuracy for mixed-membership community detection . <eos> we conclude with a large-scale demonstration on an $ n\approx 280,000 $ -node network , which is infeasible for network models with $ \omega ( n^2 ) $ inference cost .
principal components analysis ( pca ) is a standard tool for identifying good low-dimensional approximations to data sets in high dimension . <eos> many current data sets of interest contain private or sensitive information about individuals . <eos> algorithms which operate on such data should be sensitive to the privacy risks in publishing their outputs . <eos> differential privacy is a framework for developing tradeoffs between privacy and the utility of these outputs . <eos> in this paper we investigate the theory and empirical performance of differentially private approximations to pca and propose a new method which explicitly optimizes the utility of the output . <eos> we demonstrate that on real data , there this a large performance gap between the existing methods and our method . <eos> we show that the sample complexity for the two procedures differs in the scaling with the data dimension , and that our method is nearly optimal in terms of this scaling .
we study the scalability of consensus-based distributed optimization algorithms by considering two questions : how many processors should we use for a given problem , and how often should they communicate when communication is not free ? <eos> central to our analysis is a problem-specific value $ r $ which quantifies the communication/computation tradeoff . <eos> we show that organizing the communication among nodes as a $ k $ -regular expander graph~\cite { kregexpanders } yields speedups , while when all pairs of nodes communicate ( as in a complete graph ) , there is an optimal number of processors that depends on $ r $ . <eos> surprisingly , a speedup can be obtained , in terms of the time to reach a fixed level of accuracy , by communicating less and less frequently as the computation progresses . <eos> experiments on a real cluster solving metric learning and non-smooth convex minimization tasks demonstrate strong agreement between theory and practice .
characterizing the information carried by neural populations in the brain requires accurate statistical models of neural spike responses . <eos> the negative-binomial distribution provides a convenient model for over-dispersed spike counts , that is , responses with greater-than-poisson variability . <eos> here we describe a powerful data-augmentation framework for fully bayesian inference in neural models with negative-binomial spiking . <eos> our approach relies on a recently described latent-variable representation of the negative-binomial distribution , which equates it to a polya-gamma mixture of normals . <eos> this framework provides a tractable , conditionally gaussian representation of the posterior that can be used to design efficient em and gibbs sampling based algorithms for inference in regression and dynamic factor models . <eos> we apply the model to neural data from primate retina and show that it substantially outperforms poisson regression on held-out data , and reveals latent structure underlying spike count correlations in simultaneously recorded spike trains .
we study the problem of maximum marginal prediction ( mmp ) in probabilistic graphical models , a task that occurs , for example , as the bayes optimal decision rule under a hamming loss . <eos> mmp is typically performed as a two-stage procedure : one estimates each variable 's marginal probability and then forms a prediction from the states of maximal probability . <eos> in this work we propose a simple yet effective technique for accelerating mmp when inference is sampling-based : instead of the above two-stage procedure we directly estimate the posterior probability of each decision variable . <eos> this allows us to identify the point of time when we are sufficiently certain about any individual decision . <eos> whenever this is the case , we dynamically prune the variable we are confident about from the underlying factor graph . <eos> consequently , at any time only samples of variable whose decision is still uncertain need to be created . <eos> experiments in two prototypical scenarios , multi-label classification and image inpainting , shows that adaptive sampling can drastically accelerate mmp without sacrificing prediction accuracy .
counterfactual regret minimization ( cfr ) is a popular , iterative algorithm for computing strategies in extensive-form games . <eos> the monte carlo cfr ( mccfr ) variants reduce the per iteration time cost of cfr by traversing a sampled portion of the tree . <eos> the previous most effective instances of mccfr can still be very slow in games with many player actions since they sample every action for a given player . <eos> in this paper , we present a new mccfr algorithm , average strategy sampling ( as ) , that samples a subset of the player 's actions according to the player 's average strategy . <eos> our new algorithm is inspired by a new , tighter bound on the number of iterations required by cfr to converge to a given solution quality . <eos> in addition , we prove a similar , tighter bound for as and other popular mccfr variants . <eos> finally , we validate our work by demonstrating that as converges faster than previous mccfr algorithms in both no-limit poker and bluff .
while long term human visual memory can store a remarkable amount of visual information , it tends to degrade over time . <eos> recent works have shown that image memorability is an intrinsic property of an image that can be reliably estimated using state-of-the-art image features and machine learning algorithms . <eos> however , the class of features and image information that is forgotten has not been explored yet . <eos> in this work , we propose a probabilistic framework that models how and which local regions from an image may be forgotten using a data-driven approach that combines local and global images features . <eos> the model automatically discovers memorability maps of individual images without any human annotation . <eos> we incorporate multiple image region attributes in our algorithm , leading to improved memorability prediction of images as compared to previous works .
given their pervasive use , social media , such as twitter , have become a leading source of breaking news . <eos> a key task in the automated identification of such news is the detection of novel documents from a voluminous stream of text documents in a scalable manner . <eos> motivated by this challenge , we introduce the problem of online l1-dictionary learning where unlike traditional dictionary learning , which uses squared loss , the l1-penalty is used for measuring the reconstruction error . <eos> we present an efficient online algorithm for this problem based on alternating directions method of multipliers , and establish a sublinear regret bound for this algorithm . <eos> empirical results on news-stream and twitter data , shows that this online l1-dictionary learning algorithm for novel document detection gives more than an order of magnitude speedup over the previously known batch algorithm , without any significant loss in quality of results . <eos> our algorithm for online l1-dictionary learning could be of independent interest .
a fundamental problem in the analysis of structured relational data like graphs , networks , databases , and matrices is to extract a summary of the common structure underlying relations between individual entities . <eos> relational data are typically encoded in the form of arrays ; invariance to the ordering of rows and columns corresponds to exchangeable arrays . <eos> results in probability theory due to aldous , hoover and kallenberg show that exchangeable arrays can be represented in terms of a random measurable function which constitutes the natural model parameter in a bayesian model . <eos> we obtain a flexible yet simple bayesian nonparametric model by placing a gaussian process prior on the parameter function . <eos> efficient inference utilises elliptical slice sampling combined with a random sparse approximation to the gaussian process . <eos> we demonstrate applications of the model to network data and clarify its relation to models in the literature , several of which emerge as special cases .
we develop a scalable algorithm for posterior inference of overlapping communities in large networks . <eos> our algorithm is based on stochastic variational inference in the mixed-membership stochastic blockmodel . <eos> it naturally interleaves subsampling the network with estimating its community structure . <eos> we apply our algorithm on ten large , real-world networks with up to 60,000 nodes . <eos> it converges several orders of magnitude faster than the state-of-the-art algorithm for mmsb , finds hundreds of communities in large real-world networks , and detects the true communities in 280 benchmark networks with equal or better accuracy compared to other scalable algorithms .
sparse graphical modelling/inverse covariance selection is an important problem in machine learning and has seen significant advances in recent years . <eos> a major focus has been on methods which perform model selection in high dimensions . <eos> to this end , numerous convex $ \ell_1 $ regularization approaches have been proposed in the literature . <eos> it is not however clear which of these methods are optimal in any well-defined sense . <eos> a major gap in this regard pertains to the rate of convergence of proposed optimization methods . <eos> to address this , an iterative thresholding algorithm for numerically solving the $ \ell_1 $ -penalized maximum likelihood problem for sparse inverse covariance estimation is presented . <eos> the proximal gradient method considered in this paper is shown to converge at a linear rate , a result which is the first of its kind for numerically solving the sparse inverse covariance estimation problem . <eos> the convergence rate is provided in closed form , and is related to the condition number of the optimal point . <eos> numerical results demonstrating the proven rate of convergence are presented .
in many practical machine learning problems , the acquisition of labeled data is often expensive and/or time consuming . <eos> this motivates us to study a problem as follows : given a label budget , how to select data points to label such that the learning performance is optimized . <eos> we propose a selective labeling method by analyzing the generalization error of laplacian regularized least squares ( laprls ) . <eos> in particular , we derive a deterministic generalization error bound for laprls trained on subsampled data , and propose to select a subset of data points to label by minimizing this upper bound . <eos> since the minimization is a combinational problem , we relax it into continuous domain and solve it by projected gradient descent . <eos> experiments on benchmark datasets show that the proposed method outperforms the state-of-the-art methods .
parametric policy search algorithms are one of the methods of choice for the optimisation of markov decision processes , with expectation maximisation and natural gradient ascent being considered the current state of the art in the field . <eos> in this article we provide a unifying perspective of these two algorithms by showing that their step-directions in the parameter space are closely related to the search direction of an approximate newton method . <eos> this analysis leads naturally to the consideration of this approximate newton method as an alternative gradient-based method for markov decision processes . <eos> we are able show that the algorithm has numerous desirable properties , absent in the naive application of newton 's method , that make it a viable alternative to either expectation maximisation or natural gradient ascent . <eos> empirical results suggest that the algorithm has excellent convergence and robustness properties , performing strongly in comparison to both expectation maximisation and natural gradient ascent .
determinantal point processes ( dpps ) have recently been proposed as computationally efficient probabilistic models of diverse sets for a variety of applications , including document summarization , image search , and pose estimation . <eos> many dpp inference operations , including normalization and sampling , are tractable ; however , finding the most likely configuration ( map ) , which is often required in practice for decoding , is np-hard , so we must resort to approximate inference . <eos> because dpp probabilities are log-submodular , greedy algorithms have been used in the past with some empirical success ; however , these methods only give approximation guarantees in the special case of dpps with monotone kernels . <eos> in this paper we propose a new algorithm for approximating the map problem based on continuous techniques for submodular function maximization . <eos> our method involves a novel continuous relaxation of the log-probability function , which , in contrast to the multilinear extension used for general submodular functions , can be evaluated and differentiated exactly and efficiently . <eos> we obtain a practical algorithm with a 1/4-approximation guarantee for a general class of non-monotone dpps . <eos> our algorithm also extends to map inference under complex polytope constraints , making it possible to combine dpps with markov random fields , weighted matchings , and other models . <eos> we demonstrate that our approach outperforms greedy methods on both synthetic and real-world data .
we consider an abstract class of optimization problems that are parameterized concavely in a single parameter , and show that the solution path along the parameter can always be approximated with accuracy $ \varepsilon > 0 $ by a set of size $ o ( 1/\sqrt { \varepsilon } ) $ . <eos> a lower bound of size $ \omega ( 1/\sqrt { \varepsilon } ) $ shows that the upper bound is tight up to a constant factor . <eos> we also devise an algorithm that calls a step-size oracle and computes an approximate path of size $ o ( 1/\sqrt { \varepsilon } ) $ . <eos> finally , we provide an implementation of the oracle for soft-margin support vector machines , and a parameterized semi-definite program for matrix completion .
factor analysis models effectively summarise the covariance structure of high dimensional data , but the solutions are typically hard to interpret . <eos> this motivates attempting to find a disjoint partition , i.e . a clustering , of observed variables so that variables in a cluster are highly correlated . <eos> we introduce a bayesian non-parametric approach to this problem , and demonstrate advantages over heuristic methods proposed to date .
this paper aims to take a step forwards making the term `` intrinsic motivation '' from reinforcement learning theoretically well founded , focusing on curiosity-driven learning . <eos> to that end , we consider the setting where , a fixed partition p of a continuous space x being given , and a process \nu defined on x being unknown , we are asked to sequentially decide which cell of the partition to select as well as where to sample \nu in that cell , in order to minimize a loss function that is inspired from previous work on curiosity-driven learning . <eos> the loss on each cell consists of one term measuring a simple worst case quadratic sampling error , and a penalty term proportional to the range of the variance in that cell . <eos> the corresponding problem formulation extends the setting known as active learning for multi-armed bandits to the case when each arm is a continuous region , and we show how an adaptation of recent algorithms for that problem and of hierarchical optimistic sampling algorithms for optimization can be used in order to solve this problem . <eos> the resulting procedure , called hierarchical optimistic region selection driven by curiosity ( horse.c ) is provided together with a finite-time regret analysis .
we present a probabilistic formulation of max-margin matrix factorization and build accordingly a nonparametric bayesian model which automatically resolves the unknown number of latent factors . <eos> our work demonstrates a successful example that integrates bayesian nonparametrics and max-margin learning , which are conventionally two separate paradigms and enjoy complementary advantages . <eos> we develop an efficient variational algorithm for posterior inference , and our extensive empirical studies on large-scale movielens and eachmovie data sets appear to justify the aforementioned dual advantages .
information , disease , and influence diffuse over networks of entities in both natural systems and human society . <eos> analyzing these transmission networks plays an important role in understanding the diffusion processes and predicting events in the future . <eos> however , the underlying transmission networks are often hidden and incomplete , and we observe only the time stamps when cascades of events happen . <eos> in this paper , we attempt to address the challenging problem of uncovering the hidden network only from the cascades . <eos> the structure discovery problem is complicated by the fact that the influence among different entities in a network are heterogeneous , which can not be described by a simple parametric model . <eos> therefore , we propose a kernel-based method which can capture a diverse range of different types of influence without any prior assumption . <eos> in both synthetic and real cascade data , we show that our model can better recover the underlying diffusion network and drastically improve the estimation of the influence functions between networked entities .
topic modeling is a widely used approach to analyzing large text collections . <eos> a small number of multilingual topic models have recently been explored to discover latent topics among parallel or comparable documents , such as in wikipedia . <eos> other topic models that were originally proposed for structured data are also applicable to multilingual documents . <eos> correspondence latent dirichlet allocation ( corrlda ) is one such model ; however , it requires a pivot language to be specified in advance . <eos> we propose a new topic model , symmetric correspondence lda ( symcorrlda ) , that incorporates a hidden variable to control a pivot language , in an extension of corrlda . <eos> we experimented with two multilingual comparable datasets extracted from wikipedia and demonstrate that symcorrlda is more effective than some other existing multilingual topic models .
we investigate a curious relationship between the structure of a discrete graphical model and the support of the inverse of a generalized covariance matrix . <eos> we show that for certain graph structures , the support of the inverse covariance matrix of indicator variables on the vertices of a graph reflects the conditional independence structure of the graph . <eos> our work extends results that have previously been established only in the context of multivariate gaussian graphical models , thereby addressing an open question about the significance of the inverse covariance matrix of a non-gaussian distribution . <eos> based on our population-level results , we show how the graphical lasso may be used to recover the edge structure of certain classes of discrete graphical models , and present simulations to verify our theoretical results .
our central goal is to quantify the long-term progression of pediatric neurological diseases , such as a typical 10-15 years progression of child dystonia . <eos> to this purpose , quantitative models are convincing only if they can provide multi-scale details ranging from neuron spikes to limb biomechanics . <eos> the models also need to be evaluated in hyper-time , i.e . significantly faster than real-time , for producing useful predictions . <eos> we designed a platform with digital vlsi hardware for multi-scale hyper-time emulations of human motor nervous systems . <eos> the platform is constructed on a scalable , distributed array of field programmable gate array ( fpga ) devices . <eos> all devices operate asynchronously with 1 millisecond time granularity , and the overall system is accelerated to 365x real-time . <eos> each physiological component is implemented using models from well documented studies and can be flexibly modified . <eos> thus the validity of emulation can be easily advised by neurophysiologists and clinicians . <eos> for maximizing the speed of emulation , all calculations are implemented in combinational logic instead of clocked iterative circuits . <eos> this paper presents the methodology of building fpga modules in correspondence to components of a monosynaptic spinal loop . <eos> results of emulated activities are shown . <eos> the paper also discusses the rationale of approximating neural circuitry by organizing neurons with sparse interconnections . <eos> in conclusion , our platform allows introducing various abnormalities into the neural emulation such that the emerging motor symptoms can be analyzed . <eos> it compels us to test the origins of childhood motor disorders and predict their long-term progressions .
we consider the problem of performing efficient sum-product computations in an online setting over a tree . <eos> a natural application of our methods is to compute the marginal distribution at a vertex in a tree-structured markov random field . <eos> belief propagation can be used to solve this problem , but requires time linear in the size of the tree , and is therefore too slow in an online setting where we are continuously receiving new data and computing individual marginals . <eos> with our method we aim to update the data and compute marginals in time that is no more than logarithmic in the size of the tree , and is often significantly less . <eos> we accomplish this via a hierarchical covering structure that caches previous local sum-product computations . <eos> our contribution is three-fold : we i ) give a linear time algorithm to find an optimal hierarchical cover of a tree ; ii ) give a sum-productlike algorithm to efficiently compute marginals with respect to this cover ; and iii ) apply ? i and ? ii to find an efficient algorithm with a regret bound for the online allocation problem in a multi-task setting .
this paper describes a new acoustic model based on variational gaussian process dynamical system ( vgpds ) for phoneme classification . <eos> the proposed model overcomes the limitations of the classical hmm in modeling the real speech data , by adopting a nonlinear and nonparametric model . <eos> in our model , the gp prior on the dynamics function enables representing the complex dynamic structure of speech , while the gp prior on the emission function successfully models the global dependency over the observations . <eos> additionally , we introduce variance constraint to the original vgpds for mitigating sparse approximation error of the kernel matrix . <eos> the effectiveness of the proposed model is demonstrated with extensive experimental results including parameter estimation , classification performance on the synthetic and benchmark datasets .
both random fourier features and the nystr ? m method have been successfully applied to efficient kernel learning . <eos> in this work , we investigate the fundamental difference between these two approaches , and how the difference could affect their generalization performances . <eos> unlike approaches based on random fourier features where the basis functions ( i.e. , cosine and sine functions ) are sampled from a distribution { \it independent } from the training data , basis functions used by the nystr ? m method are randomly sampled from the training examples and are therefore { \it data dependent } . <eos> by exploring this difference , we show that when there is a large gap in the eigen-spectrum of the kernel matrix , approaches based the nystr ? m method can yield impressively better generalization error bound than random fourier features based approach . <eos> we empirically verify our theoretical findings on a wide range of large data sets .
discrete mixtures are used routinely in broad sweeping applications ranging from unsupervised settings to fully supervised multi-task learning . <eos> indeed , finite mixtures and infinite mixtures , relying on dirichlet processes and modifications , have become a standard tool . <eos> one important issue that arises in using discrete mixtures is low separation in the components ; in particular , different components can be introduced that are very similar and hence redundant . <eos> such redundancy leads to too many clusters that are too similar , degrading performance in unsupervised learning and leading to computational problems and an unnecessarily complex model in supervised settings . <eos> redundancy can arise in the absence of a penalty on components placed close together even when a bayesian approach is used to learn the number of components . <eos> to solve this problem , we propose a novel prior that generates components from a repulsive process , automatically penalizing redundant components . <eos> we characterize this repulsive prior theoretically and propose a markov chain monte carlo sampling algorithm for posterior computation . <eos> the methods are illustrated using synthetic examples and an iris data set .
latent svms ( lsvms ) are a class of powerful tools that have been successfully applied to many applications in computer vision . <eos> however , a limitation of lsvms is that they rely on linear models . <eos> for many computer vision tasks , linear models are suboptimal and nonlinear models learned with kernels typically perform much better . <eos> therefore it is desirable to develop the kernel version of lsvm . <eos> in this paper , we propose kernel latent svm ( klsvm ) -- a new learning framework that combines latent svms and kernel methods . <eos> we develop an iterative training algorithm to learn the model parameters . <eos> we demonstrate the effectiveness of klsvm using three different applications in visual recognition . <eos> our klsvm formulation is very general and can be applied to solve a wide range of applications in computer vision and machine learning .
in this paper , we present a bayesian framework for multilabel classification using compressed sensing . <eos> the key idea in compressed sensing for multilabel classification is to first project the label vector to a lower dimensional space using a random transformation and then learn regression functions over these projections . <eos> our approach considers both of these components in a single probabilistic model , thereby jointly optimizing over compression as well as learning tasks . <eos> we then derive an efficient variational inference scheme that provides joint posterior distribution over all the unobserved labels . <eos> the two key benefits of the model are that a ) it can naturally handle datasets that have missing labels and b ) it can also measure uncertainty in prediction . <eos> the uncertainty estimate provided by the model naturally allows for active learning paradigms where an oracle provides information about labels that promise to be maximally informative for the prediction task . <eos> our experiments show significant boost over prior methods in terms of prediction performance over benchmark datasets , both in the fully labeled and the missing labels case . <eos> finally , we also highlight various useful active learning scenarios that are enabled by the probabilistic model .
we offer a regularized , kernel extension of the multi-set , orthogonal procrustes problem , or hyperalignment . <eos> our new method , called kernel hyperalignment , expands the scope of hyperalignment to include nonlinear measures of similarity and enables the alignment of multiple datasets with a large number of base features . <eos> with direct application to fmri data analysis , kernel hyperalignment is well-suited for multi-subject alignment of large rois , including the entire cortex . <eos> we conducted experiments using real-world , multi-subject fmri data .
recent spiking network models of bayesian inference and unsupervised learning frequently assume either inputs to arrive in a special format or employ complex computations in neuronal activation functions and synaptic plasticity rules . <eos> here we show in a rigorous mathematical treatment how homeostatic processes , which have previously received little attention in this context , can overcome common theoretical limitations and facilitate the neural implementation and performance of existing models . <eos> in particular , we show that homeostatic plasticity can be understood as the enforcement of a 'balancing ' posterior constraint during probabilistic inference and learning with expectation maximization . <eos> we link homeostatic dynamics to the theory of variational inference , and show that nontrivial terms , which typically appear during probabilistic inference in a large class of models , drop out . <eos> we demonstrate the feasibility of our approach in a spiking winner-take-all architecture of bayesian inference and learning . <eos> finally , we sketch how the mathematical framework can be extended to richer recurrent network architectures . <eos> altogether , our theory provides a novel perspective on the interplay of homeostatic processes and synaptic plasticity in cortical microcircuits , and points to an essential role of homeostasis during inference and learning in spiking networks .
we describe the tradeoff between the performance in a visual recognition problem and the control authority that the agent can exercise on the sensing process . <eos> we focus on the problem of ? visual search ? <eos> of an object in an otherwise known and static scene , propose a measure of control authority , and relate it to the expected risk and its proxy ( conditional entropy of the posterior density ) . <eos> we show this analytically , as well as empirically by simulation using the simplest known model that captures the phenomenology of image formation , including scaling and occlusions . <eos> we show that a passiveagent given a training set can provide no guarantees on performance beyond what is afforded by the priors , and that an omnipotentagent , capable of infinite control authority , can achieve arbitrarily good performance ( asymptotically ) .
memristive devices have recently been proposed as efficient implementations of plastic synapses in neuromorphic systems . <eos> the plasticity in these memristive devices , i.e . their resistance change , is defined by the applied waveforms . <eos> this behavior resembles biological synapses , whose plasticity is also triggered by mechanisms that are determined by local waveforms . <eos> however , learning in memristive devices has so far been approached mostly on a pragmatic technological level . <eos> the focus seems to be on finding any waveform that achieves spike-timing-dependent plasticity ( stdp ) , without regard to the biological veracity of said waveforms or to further important forms of plasticity . <eos> bridging this gap , we make use of a plasticity model driven by neuron waveforms that explains a large number of experimental observations and adapt it to the characteristics of the recently introduced bifeo $ _3 $ memristive material . <eos> based on this approach , we show stdp for the first time for this material , with learning window replication superior to previous memristor-based stdp implementations . <eos> we also demonstrate in measurements that it is possible to overlay short and long term plasticity at a memristive device in the form of the well-known triplet plasticity . <eos> to the best of our knowledge , this is the first implementations of triplet plasticity on any physical memristive device .
in this paper , we develop a novel approach to the problem of learning sparse representations in the context of fused sparsity and unknown noise level . <eos> we propose an algorithm , termed scaled fused dantzig selector ( sfds ) , that accomplishes the aforementioned learning task by means of a second-order cone program . <eos> a special emphasize is put on the particular instance of fused sparsity corresponding to the learning in presence of outliers . <eos> we establish finite sample risk bounds and carry out an experimental evaluation on both synthetic and real data .
in the superset label learning problem ( sll ) , each training instance provides a set of candidate labels of which one is the true label of the instance . <eos> as in ordinary regression , the candidate label set is a noisy version of the true label . <eos> in this work , we solve the problem by maximizing the likelihood of the candidate label sets of training instances . <eos> we propose a probabilistic model , the logistic stickbreaking conditional multinomial model ( lsb-cmm ) , to do the job . <eos> the lsbcmm is derived from the logistic stick-breaking process . <eos> it first maps data points to mixture components and then assigns to each mixture component a label drawn from a component-specific multinomial distribution . <eos> the mixture components can capture underlying structure in the data , which is very useful when the model is weakly supervised . <eos> this advantage comes at little cost , since the model introduces few additional parameters . <eos> experimental tests on several real-world problems with superset labels show results that are competitive or superior to the state of the art . <eos> the discovered underlying structures also provide improved explanations of the classification predictions .
we present very efficient active learning algorithms for link classification in signed networks . <eos> our algorithms are motivated by a stochastic model in which edge labels are obtained through perturbations of a initial sign assignment consistent with a two-clustering of the nodes . <eos> we provide a theoretical analysis within this model , showing that we can achieve an optimal ( to whithin a constant factor ) number of mistakes on any graph $ g = ( v , e ) $ such that $ |e| $ is at least order of $ |v|^ { 3/2 } $ by querying at most order of $ |v|^ { 3/2 } $ edge labels . <eos> more generally , we show an algorithm that achieves optimality to within a factor of order $ k $ by querying at most order of $ |v| + ( |v|/k ) ^ { 3/2 } $ edge labels . <eos> the running time of this algorithm is at most of order $ |e| + |v|\log|v| $ .
the ability to learn a policy for a sequential decision problem with continuous state space using on-line data is a long-standing challenge . <eos> this paper presents a new reinforcement-learning algorithm , called ikbsf , which extends the benefits of kernel-based learning to the on-line scenario . <eos> as a kernel-based method , the proposed algorithm is stable and has good convergence properties . <eos> however , unlike other similar algorithms , ikbsf 's space complexity is independent of the number of sample transitions , and as a result it can process an arbitrary amount of data . <eos> we present theoretical results showing that ikbsf can approximate ( to any level of accuracy ) the value function that would be learned by an equivalent batch non-parametric kernel-based reinforcement learning approximator . <eos> in order to show the effectiveness of the proposed algorithm in practice , we apply ikbsf to the challenging three-pole balancing task , where the ability to process a large number of transitions is crucial for achieving a high success rate .
this paper shows how sparse , high-dimensional probability distributions could be represented by neurons with exponential compression . <eos> the representation is a novel application of compressive sensing to sparse probability distributions rather than to the usual sparse signals . <eos> the compressive measurements correspond to expected values of nonlinear functions of the probabilistically distributed variables . <eos> when these expected values are estimated by sampling , the quality of the compressed representation is limited only by the quality of sampling . <eos> since the compression preserves the geometric structure of the space of sparse probability distributions , probabilistic computation can be performed in the compressed domain . <eos> interestingly , functions satisfying the requirements of compressive sensing can be implemented as simple perceptrons . <eos> if we use perceptrons as a simple model of feedforward computation by neurons , these results show that the mean activity of a relatively small number of neurons can accurately represent a high-dimensional joint distribution implicitly , even without accounting for any noise correlations . <eos> this comprises a novel hypothesis for how neurons could encode probabilities in the brain .
we propose two classes of second-order optimization methods for solving the sparse inverse covariance estimation problem . <eos> the first approach , which we call the newton-lasso method , minimizes a piecewise quadratic model of the objective function at every iteration to generate a step . <eos> we employ the fast iterative shrinkage thresholding method ( fista ) to solve this subproblem . <eos> the second approach , which we call the orthant-based newton method , is a two-phase algorithm that first identifies an orthant face and then minimizes a smooth quadratic approximation of the objective function using the conjugate gradient method . <eos> these methods exploit the structure of the hessian to efficiently compute the search direction and to avoid explicitly storing the hessian . <eos> we show that quasi-newton methods are also effective in this context , and describe a limited memory bfgs variant of the orthant-based newton method . <eos> we present numerical results that suggest that all the techniques described in this paper have attractive properties and constitute useful tools for solving the sparse inverse covariance estimation problem . <eos> comparisons with the method implemented in the quic software package are presented .
pedigrees , or family trees , are directed graphs used to identify sites of the genome that are correlated with the presence or absence of a disease . <eos> with the advent of genotyping and sequencing technologies , there has been an explosion in the amount of data available , both in the number of individuals and in the number of sites . <eos> some pedigrees number in the thousands of individuals . <eos> meanwhile , analysis methods have remained limited to pedigrees of < 100 individuals which limits analyses to many small independent pedigrees . <eos> disease models , such those used for the linkage analysis log-odds ( lod ) estimator , have similarly been limited . <eos> this is because linkage anlysis was originally designed with a different task in mind , that of ordering the sites in the genome , before there were technologies that could reveal the order . <eos> lods are difficult to interpret and nontrivial to extend to consider interactions among sites . <eos> these developments and difficulties call for the creation of modern methods of pedigree analysis . <eos> drawing from recent advances in graphical model inference and transducer theory , we introduce a simple yet powerful formalism for expressing genetic disease models . <eos> we show that these disease models can be turned into accurate and efficient estimators . <eos> the technique we use for constructing the variational approximation has potential applications to inference in other large-scale graphical models . <eos> this method allows inference on larger pedigrees than previously analyzed in the literature , which improves disease site prediction .
we present a new algorithm for independent component analysis ( ica ) which has provable performance guarantees . <eos> in particular , suppose we are given samples of the form $ y = ax + \eta $ where $ a $ is an unknown $ n \times n $ matrix and $ x $ is chosen uniformly at random from $ \ { +1 , -1\ } ^n $ , $ \eta $ is an $ n $ -dimensional gaussian random variable with unknown covariance $ \sigma $ : we give an algorithm that provable recovers $ a $ and $ \sigma $ up to an additive $ \epsilon $ whose running time and sample complexity are polynomial in $ n $ and $ 1 / \epsilon $ . <eos> to accomplish this , we introduce a novel `` quasi-whitening '' step that may be useful in other contexts in which the covariance of gaussian noise is not known in advance . <eos> we also give a general framework for finding all local optima of a function ( given an oracle for approximately finding just one ) and this is a crucial step in our algorithm , one that has been overlooked in previous attempts , and allows us to control the accumulation of error when we find the columns of $ a $ one by one via local search .
in this paper , we consider the problem of debugging large pipelines by human labeling . <eos> we represent the execution of a pipeline using a directed acyclic graph of and and or nodes , where each node represents a data item produced by some operator in the pipeline . <eos> we assume that each operator assigns a confidence to each of its output data . <eos> we want to reduce the uncertainty in the output by issuing queries to a human expert , where a query consists of checking if a given data item is correct . <eos> in this paper , we consider the problem of asking the optimal set of queries to minimize the resulting output uncertainty . <eos> we perform a detailed evaluation of the complexity of the problem for various classes of graphs . <eos> we give efficient algorithms for the problem for trees , and show that , for a general dag , the problem is intractable .
we present a new formulation for attacking binary classification problems . <eos> instead of relying on convex losses and regularisers such as in svms , logistic regression and boosting , or instead non-convex but continuous formulations such as those encountered in neural networks and deep belief networks , our framework entails a non-convex but \emph { discrete } formulation , where estimation amounts to finding a map configuration in a graphical model whose potential functions are low-dimensional discrete surrogates for the misclassification loss . <eos> we argue that such a discrete formulation can naturally account for a number of issues that are typically encountered in either the convex or the continuous non-convex paradigms , or both . <eos> by reducing the learning problem to a map inference problem , we can immediately translate the guarantees available for many inference settings to the learning problem itself . <eos> we empirically demonstrate in a number of experiments that this approach is promising in dealing with issues such as severe label noise , while still having global optimality guarantees . <eos> due to the discrete nature of the formulation , it also allows for \emph { direct } regularisation through cardinality-based penalties , such as the $ \ell_0 $ pseudo-norm , thus providing the ability to perform feature selection and trade-off interpretability and predictability in a principled manner . <eos> we also outline a number of open problems arising from the formulation .
variational methods provide a computationally scalable alternative to monte carlo methods for large-scale , bayesian nonparametric learning . <eos> in practice , however , conventional batch and online variational methods quickly become trapped in local optima . <eos> in this paper , we consider a nonparametric topic model based on the hierarchical dirichlet process ( hdp ) , and develop a novel online variational inference algorithm based on split-merge topic updates . <eos> we derive a simpler and faster variational approximation of the hdp , and show that by intelligently splitting and merging components of the variational posterior , we can achieve substantially better predictions of test data than conventional online and batch variational algorithms . <eos> for streaming analysis of large datasets where batch analysis is infeasible , we show that our split-merge updates better capture the nonparametric properties of the underlying model , allowing continual learning of new topics .
statistical models for networks have been typically committed to strong prior assumptions concerning the form of the modeled distributions . <eos> moreover , the vast majority of currently available models are explicitly designed for capturing some specific graph properties ( such as power-law degree distributions ) , which makes them unsuitable for application to domains where the behavior of the target quantities is not known a priori . <eos> the key contribution of this paper is twofold . <eos> first , we introduce the fiedler delta statistic , based on the laplacian spectrum of graphs , which allows to dispense with any parametric assumption concerning the modeled network properties . <eos> second , we use the defined statistic to develop the fiedler random field model , which allows for efficient estimation of edge distributions over large-scale random networks . <eos> after analyzing the dependence structure involved in fiedler random fields , we estimate them over several real-world networks , showing that they achieve a much higher modeling accuracy than other well-known statistical approaches .
this paper introduces a novel classification method for functional magnetic resonance imaging datasets with tens of classes . <eos> the method is designed to make predictions using information from as many brain locations as possible , instead of resorting to feature selection , and does this by decomposing the pattern of brain activation into differently informative sub-regions . <eos> we provide results over a complex semantic processing dataset that show that the method is competitive with state-of-the-art feature selection and also suggest how the method may be used to perform group or exploratory analyses of complex class structure .
a challenging problem in hierarchical classification is to leverage the hierarchical relations among classes for improving classification performance . <eos> an even greater challenge is to do so in a manner that is computationally feasible for the large scale problems usually encountered in practice . <eos> this paper proposes a set of bayesian methods to model hierarchical dependencies among class labels using multivari- ate logistic regression . <eos> specifically , the parent-child relationships are modeled by placing a hierarchical prior over the children nodes centered around the parame- ters of their parents ; thereby encouraging classes nearby in the hierarchy to share similar model parameters . <eos> we present new , efficient variational algorithms for tractable posterior inference in these models , and provide a parallel implementa- tion that can comfortably handle large-scale problems with hundreds of thousands of dimensions and tens of thousands of classes . <eos> we run a comparative evaluation on multiple large-scale benchmark datasets that highlights the scalability of our approach , and shows a significant performance advantage over the other state-of- the-art hierarchical methods .
we describe how the pre-training algorithm for deep boltzmann machines ( dbms ) is related to the pre-training algorithm for deep belief networks and we show that under certain conditions , the pre-training procedure improves the variational lower bound of a two-hidden-layer dbm . <eos> based on this analysis , we develop a different method of pre-training dbms that distributes the modelling work more evenly over the hidden layers . <eos> our results on the mnist and norb datasets demonstrate that the new pre-training algorithm allows us to learn better generative models .
in regression problems over $ \real^d $ , the unknown function $ f $ often varies more in some coordinates than in others . <eos> we show that weighting each coordinate $ i $ with the estimated norm of the $ i $ th derivative of $ f $ is an efficient way to significantly improve the performance of distance-based regressors , e.g . kernel and $ k $ -nn regressors . <eos> we propose a simple estimator of these derivative norms and prove its consistency . <eos> moreover , the proposed estimator is efficiently learned online .
we consider the problem of identifying patterns in a data set that exhibit anomalous behavior , often referred to as anomaly detection . <eos> in most anomaly detection algorithms , the dissimilarity between data samples is calculated by a single criterion , such as euclidean distance . <eos> however , in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns . <eos> in such a case , multiple criteria can be defined , and one can test for anomalies by scalarizing the multiple criteria by taking some linear combination of them . <eos> if the importance of the different criteria are not known in advance , the algorithm may need to be executed multiple times with different choices of weights in the linear combination . <eos> in this paper , we introduce a novel non-parametric multi-criteria anomaly detection method using pareto depth analysis ( pda ) . <eos> pda uses the concept of pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights . <eos> the proposed pda approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria .
we describe a new model for learning meaningful representations of text documents from an unlabeled collection of documents . <eos> this model is inspired by the recently proposed replicated softmax , an undirected graphical model of word counts that was shown to learn a better generative model and more meaningful document representations . <eos> specifically , we take inspiration from the conditional mean-field recursive equations of the replicated softmax in order to define a neural network architecture that estimates the probability of observing a new word in a given document given the previously observed words . <eos> this paradigm also allows us to replace the expensive softmax distribution over words with a hierarchical distribution over paths in a binary tree of words . <eos> the end result is a model whose training complexity scales logarithmically with the vocabulary size instead of linearly as in the replicated softmax . <eos> our experiments show that our model is competitive both as a generative model of documents and as a document representation learning algorithm .
an effective strategy to exploit the supervising side information for discovering predictive topic representations is to impose discriminative constraints induced by such information on the posterior distributions under a topic model . <eos> this strategy has been adopted by a number of supervised topic models , such as medlda , which employs max-margin posterior constraints . <eos> however , unlike the likelihood-based supervised topic models , of which posterior inference can be carried out using the bayes ' rule , the max-margin posterior constraints have made monte carlo methods infeasible or at least not directly applicable , thereby limited the choice of inference algorithms to be based on variational approximation with strict mean field assumptions . <eos> in this paper , we develop two efficient monte carlo methods under much weaker assumptions for max-margin supervised topic models based on an importance sampler and a collapsed gibbs sampler , respectively , in a convex dual formulation . <eos> we report thorough experimental results that compare our approach favorably against existing alternatives in both accuracy and efficiency .
we introduce a new family of matrix norms , the `` local max '' norms , generalizing existing methods such as the max norm , the trace norm ( nuclear norm ) , and the weighted or smoothed weighted trace norms , which have been extensively used in the literature as regularizers for matrix reconstruction problems . <eos> we show that this new family can be used to interpolate between the ( weighted or unweighted ) trace norm and the more conservative max norm . <eos> we test this interpolation on simulated data and on the large-scale netflix and movielens ratings data , and find improved accuracy relative to the existing matrix norms . <eos> we also provide theoretical results showing learning guarantees for some of the new norms .
a brain-computer interface ( bci ) allows users to communicatewith a computer without using their muscles . <eos> bci based on sensori-motor rhythms use imaginary motor tasks , such as moving the right or left hand to send control signals . <eos> the performances of a bci can vary greatly across users but also depend on the tasks used , making the problem of appropriate task selection an important issue . <eos> this study presents a new procedure to automatically select as fast as possible a discriminant motor task for a brain-controlled button . <eos> we develop for this purpose an adaptive algorithm ucb-classif based on the stochastic bandit theory . <eos> this shortens the training stage , thereby allowing the exploration of a greater variety of tasks . <eos> by not wasting time on inefficient tasks , and focusing on the most promising ones , this algorithm results in a faster task selection and a more efficient use of the bci training session . <eos> comparing the proposed method to the standard practice in task selection , for a fixed time budget , ucb-classif leads to an improve classification rate , and for a fix classification rate , to a reduction of the time spent in training by 50 % .
undirected graphical models , or markov networks , such as gaussian graphical models and ising models enjoy popularity in a variety of applications . <eos> in many settings , however , data may not follow a gaussian or binomial distribution assumed by these models . <eos> we introduce a new class of graphical models based on generalized linear models ( glm ) by assuming that node-wise conditional distributions arise from exponential families . <eos> our models allow one to estimate networks for a wide class of exponential distributions , such as the poisson , negative binomial , and exponential , by fitting penalized glms to select the neighborhood for each node . <eos> a major contribution of this paper is the rigorous statistical analysis showing that with high probability , the neighborhood of our graphical models can be recovered exactly . <eos> we provide examples of high-throughput genomic networks learned via our glm graphical models for multinomial and poisson distributed data .
while compressive sensing ( cs ) has been one of the most vibrant and active research fields in the past few years , most development only applies to linear models . <eos> this limits its application and excludes many areas where cs ideas could make a difference . <eos> this paper presents a novel extension of cs to the phase retrieval problem , where intensity measurements of a linear system are used to recover a complex sparse signal . <eos> we propose a novel solution using a lifting technique -- cprl , which relaxes the np-hard problem to a nonsmooth semidefinite program . <eos> our analysis shows that cprl inherits many desirable properties from cs , such as guarantees for exact recovery . <eos> we further provide scalable numerical solvers to accelerate its implementation . <eos> the source code of our algorithms will be provided to the public .
a gaze concurrence is a point in 3d where the gaze directions of two or more people intersect . <eos> it is a strong indicator of social saliency because the attention of the participating group is focused on that point . <eos> in scenes occupied by large groups of people , multiple concurrences may occur and transition over time . <eos> in this paper , we present a method to construct a 3d social saliency field and locate multiple gaze concurrences that occur in a social scene from videos taken by head-mounted cameras . <eos> we model the gaze as a cone-shaped distribution emanating from the center of the eyes , capturing the variation of eye-in-head motion . <eos> we calibrate the parameters of this distribution by exploiting the fixed relationship between the primary gaze ray and the head-mounted camera pose . <eos> the resulting gaze model enables us to build a social saliency field in 3d . <eos> we estimate the number and 3d locations of the gaze concurrences via provably convergent modeseeking in the social saliency field . <eos> our algorithm is applied to reconstruct multiple gaze concurrences in several real world scenes and evaluated quantitatively against motion-captured ground truth .
user preferences for items can be inferred from either explicit feedback , such as item ratings , or implicit feedback , such as rental histories . <eos> research in collaborative filtering has concentrated on explicit feedback , resulting in the development of accurate and scalable models . <eos> however , since explicit feedback is often difficult to collect it is important to develop effective models that take advantage of the more widely available implicit feedback . <eos> we introduce a probabilistic approach to collaborative filtering with implicit feedback based on modelling the user 's item selection process . <eos> in the interests of scalability , we restrict our attention to tree-structured distributions over items and develop a principled and efficient algorithm for learning item trees from data . <eos> we also identify a problem with a widely used protocol for evaluating implicit feedback models and propose a way of addressing it using a small quantity of explicit feedback data .
applications of bayesian nonparametric methods require learning and inference algorithms which efficiently explore models of unbounded complexity . <eos> we develop new markov chain monte carlo methods for the beta process hidden markov model ( bp-hmm ) , enabling discovery of shared activity patterns in large video and motion capture databases . <eos> by introducing split-merge moves based on sequential allocation , we allow large global changes in the shared feature structure . <eos> we also develop data-driven reversible jump moves which more reliably discover rare or unique behaviors . <eos> our proposals apply to any choice of conjugate likelihood for observed data , and we show success with multinomial , gaussian , and autoregressive emission models . <eos> together , these innovations allow tractable analysis of hundreds of time series , where previous inference required clever initialization and at least ten thousand burn-in iterations for just six sequences .
we describe an approach to speed-up inference with latent variable pcfgs , which have been shown to be highly effective for natural language parsing . <eos> our approach is based on a tensor formulation recently introduced for spectral estimation of latent-variable pcfgs coupled with a tensor decomposition algorithm well-known in the multilinear algebra literature . <eos> we also describe an error bound for this approximation , which bounds the difference between the probabilities calculated by the algorithm and the true probabilities that the approximated model gives . <eos> empirical evaluation on real-world natural language parsing data demonstrates a significant speed-up at minimal cost for parsing performance .
the problem of multiple change point estimation is considered for sequences with unknown number of change points . <eos> a consistency framework is suggested that is suitable for highly dependent time-series , and an asymptotically consistent algorithm is proposed . <eos> in order for the consistency to be established the only assumption required is that the data is generated by stationary ergodic time-series distributions . <eos> no modeling , independence or parametric assumptions are made ; the data are allowed to be dependent and the dependence can be of arbitrary form . <eos> the theoretical results are complemented with experimental evaluations .
we develop a bayesian nonparametric extension of the popular plackett-luce choice model that can handle an infinite number of choice items . <eos> our framework is based on the theory of random atomic measures , with the prior specified by a gamma process . <eos> we derive a posterior characterization and a simple and effective gibbs sampler for posterior simulation . <eos> we then develop a time-varying extension of our model , and apply our model to the new york times lists of weekly bestselling books .
value pursuit iteration ( vpi ) is an approximate value iteration algorithm that finds a close to optimal policy for reinforcement learning and planning problems with large state spaces . <eos> vpi has two main features : first , it is a nonparametric algorithm that finds a good sparse approximation of the optimal value function given a dictionary of features . <eos> the algorithm is almost insensitive to the number of irrelevant features . <eos> second , after each iteration of vpi , the algorithm adds a set of functions based on the currently learned value function to the dictionary . <eos> this increases the representation power of the dictionary in a way that is directly relevant to the goal of having a good approximation of the optimal value function . <eos> we theoretically study vpi and provide a finite-sample error upper bound for it .
we consider the problem of recovering a sequence of vectors , $ ( x_k ) _ { k=0 } ^k $ , for which the increments $ x_k-x_ { k-1 } $ are $ s_k $ -sparse ( with $ s_k $ typically smaller than $ s_1 $ ) , based on linear measurements $ ( y_k = a_k x_k + e_k ) _ { k=1 } ^k $ , where $ a_k $ and $ e_k $ denote the measurement matrix and noise , respectively . <eos> assuming each $ a_k $ obeys the restricted isometry property ( rip ) of a certain order -- -depending only on $ s_k $ -- -we show that in the absence of noise a convex program , which minimizes the weighted sum of the $ \ell_1 $ -norm of successive differences subject to the linear measurement constraints , recovers the sequence $ ( x_k ) _ { k=1 } ^k $ \emph { exactly } . <eos> this is an interesting result because this convex program is equivalent to a standard compressive sensing problem with a highly-structured aggregate measurement matrix which does not satisfy the rip requirements in the standard sense , and yet we can achieve exact recovery . <eos> in the presence of bounded noise , we propose a quadratically-constrained convex program for recovery and derive bounds on the reconstruction error of the sequence . <eos> we supplement our theoretical analysis with simulations and an application to real video data . <eos> these further support the validity of the proposed approach for acquisition and recovery of signals with time-varying sparsity .
crowdsourcing has become a popular paradigm for labeling large datasets . <eos> however , it has given rise to the computational task of aggregating the crowdsourced labels provided by a collection of unreliable annotators . <eos> we approach this problem by transforming it into a standard inference problem in graphical models , and applying approximate variational methods , including belief propagation ( bp ) and mean field ( mf ) . <eos> we show that our bp algorithm generalizes both majority voting and a recent algorithm by karger et al , while our mf method is closely related to a commonly used em algorithm . <eos> in both cases , we find that the performance of the algorithms critically depends on the choice of a prior distribution on the workers ' reliability ; by choosing the prior properly , both bp and mf ( and em ) perform surprisingly well on both simulated and real-world datasets , competitive with state-of-the-art algorithms based on more complicated modeling assumptions .
hierarchical hidden markov models ( hhmms ) are sophisticated stochastic models that enable us to capture a hierarchical context characterization of sequence data . <eos> however , existing hhmm parameter estimation methods require large computations of time complexity o ( tn^ { 2d } ) at least for model inference , where d is the depth of the hierarchy , n is the number of states in each level , and t is the sequence length . <eos> in this paper , we propose a new inference method of hhmms for which the time complexity is o ( tn^ { d+1 } ) . <eos> a key idea of our algorithm is application of the forward-backward algorithm to `` state activation probabilities '' . <eos> the notion of a state activation , which offers a simple formalization of the hierarchical transition behavior of hhmms , enables us to conduct model inference efficiently . <eos> we present some experiments to demonstrate that our proposed method works more efficiently to estimate hhmm parameters than do some existing methods such as the flattening method and gibbs sampling method .
probabilistic approaches to computer vision typically assume a centralized setting , with the algorithm granted access to all observed data points . <eos> however , many problems in wide-area surveillance can benefit from distributed modeling , either because of physical or computational constraints . <eos> most distributed models to date use algebraic approaches ( such as distributed svd ) and as a result can not explicitly deal with missing data . <eos> in this work we present an approach to estimation and learning of generative probabilistic models in a distributed context where certain sensor data can be missing . <eos> in particular , we show how traditional centralized models , such as probabilistic pca and missing-data ppca , can be learned when the data is distributed across a network of sensors . <eos> we demonstrate the utility of this approach on the problem of distributed affine structure from motion . <eos> our experiments suggest that the accuracy of the learned probabilistic structure and motion models rivals that of traditional centralized factorization methods while being able to handle challenging situations such as missing or noisy observations .
in compressive sensing magnetic resonance imaging ( cs-mri ) , one can reconstruct a mr image with good quality from only a small number of measurements . <eos> this can significantly reduce mr scanning time . <eos> according to structured sparsity theory , the measurements can be further reduced to $ \mathcal { o } ( k+\log n ) $ for tree-sparse data instead of $ \mathcal { o } ( k+k\log n ) $ for standard $ k $ -sparse data with length $ n $ . <eos> however , few of existing algorithms has utilized this for cs-mri , while most of them use total variation and wavelet sparse regularization . <eos> on the other side , some algorithms have been proposed for tree sparsity regularization , but few of them has validated the benefit of tree structure in cs-mri . <eos> in this paper , we propose a fast convex optimization algorithm to improve cs-mri . <eos> wavelet sparsity , gradient sparsity and tree sparsity are all considered in our model for real mr images . <eos> the original complex problem is decomposed to three simpler subproblems then each of the subproblems can be efficiently solved with an iterative scheme . <eos> numerous experiments have been conducted and show that the proposed algorithm outperforms the state-of-the-art cs-mri algorithms , and gain better reconstructions results on real mr images than general tree based solvers or algorithms .
a model connecting visual tracking and saliency has recently been proposed . <eos> this model is based on the saliency hypothesis for tracking which postulates that tracking is achieved by the top-down tuning , based on target features , of discriminant center-surround saliency mechanisms over time . <eos> in this work , we identify three main predictions that must hold if the hypothesis were true : 1 ) tracking reliability should be larger for salient than for non-salient targets , 2 ) tracking reliability should have a dependence on the defining variables of saliency , namely feature contrast and distractor heterogeneity , and must replicate the dependence of saliency on these variables , and 3 ) saliency and tracking can be implemented with common low level neural mechanisms . <eos> we confirm that the first two predictions hold by reporting results from a set of human behavior studies on the connection between saliency and tracking . <eos> we also show that the third prediction holds by constructing a common neurophysiologically plausible architecture that can computationally solve both saliency and tracking . <eos> this architecture is fully compliant with the standard physiological models of v1 and mt , and with what is known about attentional control in area lip , while explaining the results of the human behavior experiments .
subspace learning seeks a low dimensional representation of data that enables accurate reconstruction . <eos> however , in many applications , data is obtained from multiple sources rather than a single source ( e.g . an object might be viewed by cameras at different angles , or a document might consist of text and images ) . <eos> the conditional independence of separate sources imposes constraints on their shared latent representation , which , if respected , can improve the quality of the learned low dimensional representation . <eos> in this paper , we present a convex formulation of multi-view subspace learning that enforces conditional independence while reducing dimensionality . <eos> for this formulation , we develop an efficient algorithm that recovers an optimal data reconstruction by exploiting an implicit convex regularizer , then recovers the corresponding latent representation and reconstruction model , jointly and optimally . <eos> experiments illustrate that the proposed method produces high quality results .
we propose a new stochastic gradient method for optimizing the sum of ? <eos> a finite set of smooth functions , where the sum is strongly convex . ? <eos> while standard stochastic gradient methods ? <eos> converge at sublinear rates for this problem , the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence ? rate . <eos> in a machine learning context , numerical experiments indicate that the new algorithm can dramatically outperform standard ? <eos> algorithms , both in terms of optimizing the training error and reducing the test error quickly .
we describe a latent variable model for supervised dimensionality reduction and distance metric learning . <eos> the model discovers linear projections of high dimensional data that shrink the distance between similarly labeled inputs and expand the distance between differently labeled ones . <eos> the model ? s continuous latent variables locate pairs of examples in a latent space of lower dimensionality . <eos> the model differs significantly from classical factor analysis in that the posterior distribution over these latent variables is not always multivariate gaussian . <eos> nevertheless we show that inference is completely tractable and derive an expectation-maximization ( em ) algorithm for parameter estimation . <eos> we also compare the model to other approaches in distance metric learning . <eos> the model ? s main advantage is its simplicity : at each iteration of the em algorithm , the distance metric is re-estimated by solving an unconstrained least-squares problem . <eos> experiments show that these simple updates are highly effective .
we introduce a new notion of classification accuracy based on the top $ \tau $ -quantile values of a scoring function , a relevant criterion in a number of problems arising for search engines . <eos> we define an algorithm optimizing a convex surrogate of the corresponding loss , and show how its solution can be obtained by solving several convex optimization problems . <eos> we also present margin-based guarantees for this algorithm based on the $ \tau $ -quantile of the functions in the hypothesis set . <eos> finally , we report the results of several experiments evaluating the performance of our algorithm . <eos> in a comparison in a bipartite setting with several algorithms seeking high precision at the top , our algorithm achieves a better performance in precision at the top .
in this paper , we study latent factor models with the dependency structure in the latent space . <eos> we propose a general learning framework which induces sparsity on the undirected graphical model imposed on the vector of latent factors . <eos> a novel latent factor model slfa is then proposed as a matrix factorization problem with a special regularization term that encourages collaborative reconstruction . <eos> the main benefit ( novelty ) of the model is that we can simultaneously learn the lower-dimensional representation for data and model the pairwise relationships between latent factors explicitly . <eos> an on-line learning algorithm is devised to make the model feasible for large-scale learning problems . <eos> experimental results on two synthetic data and two real-world data sets demonstrate that pairwise relationships and latent factors learned by our model provide a more structured way of exploring high-dimensional data , and the learned representations achieve the state-of-the-art classification performance .
topic modeling is a generalization of clustering that posits that observations ( words in a document ) are generated by \emph { multiple } latent factors ( topics ) , as opposed to just one . <eos> this increased representational power comes at the cost of a more challenging unsupervised learning problem of estimating the topic-word distributions when only words are observed , and the topics are hidden . <eos> this work provides a simple and efficient learning procedure that is guaranteed to recover the parameters for a wide class of topic models , including latent dirichlet allocation ( lda ) . <eos> for lda , the procedure correctly recovers both the topic-word distributions and the parameters of the dirichlet prior over the topic mixtures , using only trigram statistics ( \emph { i.e . } , third order moments , which may be estimated with documents containing just three words ) . <eos> the method , called excess correlation analysis , is based on a spectral decomposition of low-order moments via two singular value decompositions ( svds ) . <eos> moreover , the algorithm is scalable , since the svds are carried out only on $ k \times k $ matrices , where $ k $ is the number of latent factors ( topics ) and is typically much smaller than the dimension of the observation ( word ) space .
we show a principled way of deriving online learning algorithms from a minimax analysis . <eos> various upper bounds on the minimax value , previously thought to be non-constructive , are shown to yield algorithms . <eos> this allows us to seamlessly recover known methods and to derive new ones , also capturing such `` unorthodox '' methods as follow the perturbed leader and the r^2 forecaster . <eos> understanding the inherent complexity of the learning problem thus leads to the development of algorithms . <eos> to illustrate our approach , we present several new algorithms , including a family of randomized methods that use the idea of a `` random play out '' . <eos> new versions of the follow-the-perturbed-leader algorithms are presented , as well as methods based on the littlestone 's dimension , efficient methods for matrix completion with trace norm , and algorithms for the problems of transductive learning and prediction with static experts .
we study the average case performance of multi-task gaussian process ( gp ) regression as captured in the learning curve , i.e.\ the average bayes error for a chosen task versus the total number of examples $ n $ for all tasks . <eos> for gp covariances that are the product of an input-dependent covariance function and a free-form inter-task covariance matrix , we show that accurate approximations for the learning curve can be obtained for an arbitrary number of tasks $ t $ . <eos> we use these to study the asymptotic learning behaviour for large $ n $ . <eos> surprisingly , multi-task learning can be asymptotically essentially useless : examples from other tasks only help when the degree of inter-task correlation , $ \rho $ , is near its maximal value $ \rho=1 $ . <eos> this effect is most extreme for learning of smooth target functions as described by e.g.\ squared exponential kernels . <eos> we also demonstrate that when learning { \em many } tasks , the learning curves separate into an initial phase , where the bayes error on each task is reduced down to a plateau value by `` collective learning '' even though most tasks have not seen examples , and a final decay that occurs only once the number of examples is proportional to the number of tasks .
we study the problem of identifying the best arm ( s ) in the stochastic multi-armed bandit setting . <eos> this problem has been studied in the literature from two different perspectives : fixed budget and fixed confidence . <eos> we propose a unifying approach that leads to a meta-algorithm called unified gap-based exploration ( ugape ) , with a common structure and similar theoretical analysis for these two settings . <eos> we prove a performance bound for the two versions of the algorithm showing that the two problems are characterized by the same notion of complexity . <eos> we also show how the ugape algorithm as well as its theoretical analysis can be extended to take into account the variance of the arms and to multiple bandits . <eos> finally , we evaluate the performance of ugape and compare it with a number of existing fixed budget and fixed confidence algorithms .
we address the problem of estimating the difference between two probability densities . <eos> a naive approach is a two-step procedure of first estimating two densities separately and then computing their difference . <eos> however , such a two-step procedure does not necessarily work well because the first step is performed without regard to the second step and thus a small estimation error incurred in the first stage can cause a big error in the second stage . <eos> in this paper , we propose a single-shot procedure for directly estimating the density difference without separately estimating two densities . <eos> we derive a non-parametric finite-sample error bound for the proposed single-shot density-difference estimator and show that it achieves the optimal convergence rate . <eos> we then show how the proposed density-difference estimator can be utilized in l2-distance approximation . <eos> finally , we experimentally demonstrate the usefulness of the proposed method in robust distribution comparison such as class-prior estimation and change-point detection .
formal exploration approaches in model-based reinforcement learning estimate the accuracy of the currently learned model without consideration of the empirical prediction error . <eos> for example , pac-mdp approaches such as rmax base their model certainty on the amount of collected data , while bayesian approaches assume a prior over the transition dynamics . <eos> we propose extensions to such approaches which drive exploration solely based on empirical estimates of the learner 's accuracy and learning progress . <eos> we provide a `` sanity check '' theoretical analysis , discussing the behavior of our extensions in the standard stationary finite state-action case . <eos> we then provide experimental studies demonstrating the robustness of these exploration measures in cases of non-stationary environments or where original approaches are misled by wrong domain assumptions .
since its inception , the modus operandi of multi-task learning ( mtl ) has been to minimize the task-wise mean of the empirical risks . <eos> we introduce a generalized loss-compositional paradigm for mtl that includes a spectrum of formulations as a subfamily . <eos> one endpoint of this spectrum is minimax mtl : a new mtl formulation that minimizes the maximum of the tasks ' empirical risks . <eos> via a certain relaxation of minimax mtl , we obtain a continuum of mtl formulations spanning minimax mtl and classical mtl . <eos> the full paradigm itself is loss-compositional , operating on the vector of empirical risks . <eos> it incorporates minimax mtl , its relaxations , and many new mtl formulations as special cases . <eos> we show theoretically that minimax mtl tends to avoid worst case outcomes on newly drawn test tasks in the learning to learn ( ltl ) test setting . <eos> the results of several mtl formulations on synthetic and real problems in the mtl and ltl test settings are encouraging .
we develop convergent minimization algorithms for bethe variational approximations which explicitly constrain marginal estimates to families of valid distributions . <eos> while existing message passing algorithms define fixed point iterations corresponding to stationary points of the bethe free energy , their greedy dynamics do not distinguish between local minima and maxima , and can fail to converge . <eos> for continuous estimation problems , this instability is linked to the creation of invalid marginal estimates , such as gaussians with negative variance . <eos> conversely , our approach leverages multiplier methods with well-understood convergence properties , and uses bound projection methods to ensure that marginal approximations are valid at all iterations . <eos> we derive general algorithms for discrete and gaussian pairwise markov random fields , showing improvements over standard loopy belief propagation . <eos> we also apply our method to a hybrid model with both discrete and continuous variables , showing improvements over expectation propagation .
we develop and analyze stochastic optimization algorithms for problems in which the expected loss is strongly convex , and the optimum is ( approximately ) sparse . <eos> previous approaches are able to exploit only one of these two structures , yielding a $ \order ( \pdim/t ) $ convergence rate for strongly convex objectives in $ \pdim $ dimensions and $ \order ( \sqrt { \spindex ( \log\pdim ) /t } ) $ convergence rate when the optimum is $ \spindex $ -sparse . <eos> our algorithm is based on successively solving a series of $ \ell_1 $ -regularized optimization problems using nesterov 's dual averaging algorithm . <eos> we establish that the error of our solution after $ t $ iterations is at most $ \order ( \spindex ( \log\pdim ) /t ) $ , with natural extensions to approximate sparsity . <eos> our results apply to locally lipschitz losses including the logistic , exponential , hinge and least-squares losses . <eos> by recourse to statistical minimax results , we show that our convergence rates are optimal up to constants . <eos> the effectiveness of our approach is also confirmed in numerical simulations where we compare to several baselines on a least-squares regression problem .
we study surrogate losses for learning to rank , in a framework where the rankings are induced by scores and the task is to learn the scoring function . <eos> we focus on the calibration of surrogate losses with respect to a ranking evaluation metric , where the calibration is equivalent to the guarantee that near-optimal values of the surrogate risk imply near-optimal values of the risk defined by the evaluation metric . <eos> we prove that if a surrogate loss is a convex function of the scores , then it is not calibrated with respect to two evaluation metrics widely used for search engine evaluation , namely the average precision and the expected reciprocal rank . <eos> we also show that such convex surrogate losses can not be calibrated with respect to the pairwise disagreement , an evaluation metric used when learning from pairwise preferences . <eos> our results cast lights on the intrinsic difficulty of some ranking problems , as well as on the limitations of learning-to-rank algorithms based on the minimization of a convex surrogate risk .
diversified ranking is a fundamental task in machine learning . <eos> it is broadly applicable in many real world problems , e.g. , information retrieval , team assembling , product search , etc . <eos> in this paper , we consider a generic setting where we aim to diversify the top-k ranking list based on an arbitrary relevance function and an arbitrary similarity function among all the examples . <eos> we formulate it as an optimization problem and show that in general it is np-hard . <eos> then , we show that for a large volume of the parameter space , the proposed objective function enjoys the diminishing returns property , which enables us to design a scalable , greedy algorithm to find the near-optimal solution . <eos> experimental results on real data sets demonstrate the effectiveness of the proposed algorithm .
we propose a novel method for scalable parallelization of smc algorithms , entangled monte carlo simulation ( emc ) . <eos> emc avoids the transmission of particles between nodes , and instead reconstructs them from the particle genealogy . <eos> in particular , we show that we can reduce the communication to the particle weights for each machine while efficiently maintaining implicit global coherence of the parallel simulation . <eos> we explain methods to efficiently maintain a genealogy of particles from which any particle can be reconstructed . <eos> we demonstrate using examples from bayesian phylogenetic that the computational gain from parallelization using emc significantly outweighs the cost of particle reconstruction . <eos> the timing experiments show that reconstruction of particles is indeed much more efficient as compared to transmission of particles .
sudderth , wainwright , and willsky conjectured that the bethe approximation corresponding to any fixed point of the belief propagation algorithm over an attractive , pairwise binary graphical model provides a lower bound on the true partition function . <eos> in this work , we resolve this conjecture in the affirmative by demonstrating that , for any graphical model with binary variables whose potential functions ( not necessarily pairwise ) are all log-supermodular , the bethe partition function always lower bounds the true partition function . <eos> the proof of this result follows from a new variant of the ? four functions ? <eos> theorem that may be of independent interest .
the empirical success of the belief propagation approximate inference algorithm has inspired numerous theoretical and algorithmic advances . <eos> yet , for continuous non-gaussian domains performing belief propagation remains a challenging task : recent innovations such as nonparametric or kernel belief propagation , while useful , come with a substantial computational cost and offer little theoretical guarantees , even for tree structured models . <eos> in this work we present nonparanormal bp for performing efficient inference on distributions parameterized by a gaussian copulas network and any univariate marginals . <eos> for tree structured networks , our approach is guaranteed to be exact for this powerful class of non-gaussian models . <eos> importantly , the method is as efficient as standard gaussian bp , and its convergence properties do not depend on the complexity of the univariate marginals , even when a nonparametric representation is used .
we study the problem of estimating , in the sense of optimal transport metrics , a measure which is assumed supported on a manifold embedded in a hilbert space . <eos> by establishing a precise connection between optimal transport metrics , optimal quantization , and learning theory , we derive new probabilistic bounds for the performance of a classic algorithm in unsupervised learning ( k-means ) , when used to produce a probability measure derived from the data . <eos> in the course of the analysis , we arrive at new lower bounds , as well as probabilistic bounds on the convergence rate of the empirical law of large numbers , which , unlike existing bounds , are applicable to a wide class of measures .
continuous relaxations play an important role in discrete optimization , but have not seen much use in approximate probabilistic inference . <eos> here we show that a general form of the gaussian integral trick makes it possible to transform a wide class of discrete variable undirected models into fully continuous systems . <eos> the continuous representation allows the use of gradient-based hamiltonian monte carlo for inference , results in new ways of estimating normalization constants ( partition functions ) , and in general opens up a number of new avenues for inference in difficult discrete systems . <eos> we demonstrate some of these continuous relaxation inference algorithms on a number of illustrative problems .
positive definite operator-valued kernels generalize the well-known notion of reproducing kernels , and are naturally adapted to multi-output learning situations . <eos> this paper addresses the problem of learning a finite linear combination of infinite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts . <eos> we study this problem in the case of kernel ridge regression for functional responses with an lr-norm constraint on the combination coefficients . <eos> the resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues . <eos> we propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinate-descent procedure . <eos> we experimentally validate our approach on a functional regression task in the context of finger movement prediction in brain-computer interfaces .
non-linear dynamical systems ( ds ) have been used extensively for building generative models of human behavior . <eos> its applications range from modeling brain dynamics to encoding motor commands . <eos> many schemes have been proposed for encoding robot motions using dynamical systems with a single attractor placed at a predefined target in state space . <eos> although these enable the robots to react against sudden perturbations without any re-planning , the motions are always directed towards a single target . <eos> in this work , we focus on combining several such ds with distinct attractors , resulting in a multi-stable ds . <eos> we show its applicability in reach-to-grasp tasks where the attractors represent several grasping points on the target object . <eos> while exploiting multiple attractors provides more flexibility in recovering from unseen perturbations , it also increases the complexity of the underlying learning problem . <eos> here we present the augmented-svm ( a-svm ) model which inherits region partitioning ability of the well known svm classifier and is augmented with novel constraints derived from the individual ds . <eos> the new constraints modify the original svm dual whose optimal solution then results in a new class of support vectors ( sv ) . <eos> these new sv ensure that the resulting multi-stable ds incurs minimum deviation from the original dynamics and is stable at each of the attractors within a finite region of attraction . <eos> we show , via implementations on a simulated 10 degrees of freedom mobile robotic platform , that the model is capable of real-time motion generation and is able to adapt on-the-fly to perturbations .
this paper explores unsupervised learning of parsing models along two directions . <eos> first , which models are identifiable from infinite data ? <eos> we use a general technique for numerically checking identifiability based on the rank of a jacobian matrix , and apply it to several standard constituency and dependency parsing models . <eos> second , for identifiable models , how do we estimate the parameters efficiently ? <eos> em suffers from local optima , while recent work using spectral methods can not be directly applied since the topology of the parse tree varies across sentences . <eos> we develop a strategy , unmixing , which deals with this additional complexity for restricted classes of parsing models .
in this work , we consider the problem of modeling the dynamic structure of human activities in the attributes space . <eos> a video sequence is first represented in a semantic feature space , where each feature encodes the probability of occurrence of an activity attribute at a given time . <eos> a generative model , denoted the binary dynamic system ( bds ) , is proposed to learn both the distribution and dynamics of different activities in this space . <eos> the bds is a non-linear dynamic system , which extends both the binary principal component analysis ( pca ) and classical linear dynamic systems ( lds ) , by combining binary observation variables with a hidden gauss-markov state process . <eos> in this way , it integrates the representation power of semantic modeling with the ability of dynamic systems to capture the temporal structure of time-varying processes . <eos> an algorithm for learning bds parameters , inspired by a popular lds learning method from dynamic textures , is proposed . <eos> a similarity measure between bdss , which generalizes the binet-cauchy kernel for lds , is then introduced and used to design activity classifiers . <eos> the proposed method is shown to outperform similar classifiers derived from the kernel dynamic system ( kds ) and state-of-the-art approaches for dynamics-based or attribute-based action recognition .
numerical integration is an key component of many problems in scientific computing , statistical modelling , and machine learning . <eos> bayesian quadrature is a model-based method for numerical integration which , relative to standard monte carlo methods , offers increased sample efficiency and a more robust estimate of the uncertainty in the estimated integral . <eos> we propose a novel bayesian quadrature approach for numerical integration when the integrand is non-negative , such as the case of computing the marginal likelihood , predictive distribution , or normalising constant of a probabilistic model . <eos> our approach approximately marginalises the quadrature model 's hyperparameters in closed form , and introduces an active learning scheme to optimally select function evaluations , as opposed to using monte carlo samples . <eos> we demonstrate our method on both a number of synthetic benchmarks and a real scientific problem from astronomy .
modelling natural images with sparse coding ( sc ) has faced two main challenges : ? exibly representing varying pixel intensities and realistically representing lowlevel image components . <eos> this paper proposes a novel multiple-cause generative model of low-level image statistics that generalizes the standard sc model in two crucial points : ( 1 ) it uses a spike-and-slab prior distribution for a more realistic representation of component absence/intensity , and ( 2 ) the model uses the highly nonlinear combination rule of maximal causes analysis ( mca ) instead of a linear combination . <eos> the major challenge is parameter optimization because a model with either ( 1 ) or ( 2 ) results in strongly multimodal posteriors . <eos> we show for the ? rst time that a model combining both improvements can be trained ef ? ciently while retaining the rich structure of the posteriors . <eos> we design an exact piecewise gibbs sampling method and combine this with a variational method based on preselection of latent dimensions . <eos> this combined training scheme tackles both analytical and computational intractability and enables application of the model to a large number of observed and hidden dimensions . <eos> applying the model to image patches we study the optimal encoding of images by simple cells in v1 and compare the model ? s predictions with in vivo neural recordings . <eos> in contrast to standard sc , we ? nd that the optimal prior favors asymmetric and bimodal activity of simple cells . <eos> testing our model for consistency we ? nd that the average posterior is approximately equal to the prior . <eos> furthermore , we ? nd that the model predicts a high percentage of globular receptive ? elds alongside gabor-like ? elds . <eos> similarly high percentages are observed in vivo . <eos> our results thus argue in favor of improvements of the standard sparse coding model for simple cells by using ? exible priors and nonlinear combinations .
we introduce a joint model of network content and context designed for exploratory analysis of email networks via visualization of topic-specific communication patterns . <eos> our model is an admixture model for text and network attributes which uses multinomial distributions over words as mixture components for explaining text and latent euclidean positions of actors as mixture components for explaining network attributes . <eos> we validate the appropriateness of our model by achieving state-of-the-art performance on a link prediction task and by achieving semantic coherence equivalent to that of latent dirichlet allocation . <eos> we demonstrate the capability of our model for descriptive , explanatory , and exploratory analysis by investigating the inferred topic-specific communication patterns of a new government email dataset , the new hanover county email corpus .
probabilistic latent variable models are one of the cornerstones of machine learning . <eos> they offer a convenient and coherent way to specify prior distributions over unobserved structure in data , so that these unknown properties can be inferred via posterior inference . <eos> such models are useful for exploratory analysis and visualization , for building density models of data , and for providing features that can be used for later discriminative tasks . <eos> a significant limitation of these models , however , is that draws from the prior are often highly redundant due to i.i.d . <eos> assumptions on internal parameters . <eos> for example , there is no preference in the prior of a mixture model to make components non-overlapping , or in topic model to ensure that co-ocurring words only appear in a small number of topics . <eos> in this work , we revisit these independence assumptions for probabilistic latent variable models , replacing the underlying i.i.d.\ prior with a determinantal point process ( dpp ) . <eos> the dpp allows us to specify a preference for diversity in our latent variables using a positive definite kernel function . <eos> using a kernel between probability distributions , we are able to define a dpp on probability measures . <eos> we show how to perform map inference with dpp priors in latent dirichlet allocation and in mixture models , leading to better intuition for the latent variable representation and quantitatively improved unsupervised feature extraction , without compromising the generative aspects of the model .
approaches to audio classification and retrieval tasks largely rely on detection-based discriminative models . <eos> we submit that such models make a simplistic assumption in mapping acoustics directly to semantics , whereas the actual process is likely more complex . <eos> we present a generative model that maps acoustics in a hierarchical manner to increasingly higher-level semantics . <eos> our model has 2 layers with the first being generic sound units with no clear semantic associations , while the second layer attempts to find patterns over the generic sound units . <eos> we evaluate our model on a large-scale retrieval task from trecvid 2011 , and report significant improvements over standard baselines .
this paper introduces timeline trees , which are partial models of partially observable environments . <eos> timeline trees are given some specific predictions to make and learn a decision tree over history . <eos> the main idea of timeline trees is to use temporally abstract features to identify and split on features of key events , spread arbitrarily far apart in the past ( whereas previous decision-tree-based methods have been limited to a finite suffix of history ) . <eos> experiments demonstrate that timeline trees can learn to make high quality predictions in complex , partially observable environments with high-dimensional observations ( e.g . an arcade game ) .
sparse learning models typically combine a smooth loss with a nonsmooth penalty , such as trace norm . <eos> although recent developments in sparse approximation have offered promising solution methods , current approaches either apply only to matrix-norm constrained problems or provide suboptimal convergence rates . <eos> in this paper , we propose a boosting method for regularized learning that guarantees $ \epsilon $ accuracy within $ o ( 1/\epsilon ) $ iterations . <eos> performance is further accelerated by interlacing boosting with fixed-rank local optimization -- -exploiting a simpler local objective than previous work . <eos> the proposed method yields state-of-the-art performance on large-scale problems . <eos> we also demonstrate an application to latent multiview learning for which we provide the first efficient weak-oracle .
mirror descent with an entropic regularizer is known to achieve shifting regret bounds that are logarithmic in the dimension . <eos> this is done using either a carefully designed projection or by a weight sharing technique . <eos> via a novel unified analysis , we show that these two approaches deliver essentially equivalent bounds on a notion of regret generalizing shifting , adaptive , discounted , and other related regrets . <eos> our analysis also captures and extends the generalized weight sharing technique of bousquet and warmuth , and can be refined in several ways , including improvements for small losses and adaptive tuning of parameters .
this paper is concerned with the statistical consistency of ranking methods . <eos> recently , it was proven that many commonly used pairwise ranking methods are inconsistent with the weighted pairwise disagreement loss ( wpdl ) , which can be viewed as the true loss of ranking , even in a low-noise setting . <eos> this result is interesting but also surprising , given that the pairwise ranking methods have been shown very effective in practice . <eos> in this paper , we argue that the aforementioned result might not be conclusive , depending on what kind of assumptions are used . <eos> we give a new assumption that the labels of objects to rank lie in a rank-differentiable probability space ( rdps ) , and prove that the pairwise ranking methods become consistent with wpdl under this assumption . <eos> what is especially inspiring is that rdps is actually not stronger than but similar to the low-noise setting . <eos> our studies provide theoretical justifications of some empirical findings on pairwise ranking methods that are unexplained before , which bridge the gap between theory and applications .
we derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space . <eos> the proposed algorithm combines state aggregation with the use of upper confidence bounds for implementing optimism in the face of uncertainty . <eos> beside the existence of an optimal policy which satisfies the poisson equation , the only assumptions made are hoelder continuity of rewards and transition probabilities .
methods for efficiently estimating the shannon entropy of data streams have important applications in learning , data mining , and network anomaly detections ( e.g. , the ddos attacks ) . <eos> for nonnegative data streams , the method of compressed counting ( cc ) based on maximally-skewed stable random projections can provide accurate estimates of the shannon entropy using small storage . <eos> however , cc is no longer applicable when entries of data streams can be below zero , which is a common scenario when comparing two streams . <eos> in this paper , we propose an algorithm for entropy estimation in general data streams which allow negative entries . <eos> in our method , the shannon entropy is approximated by the finite difference of two correlated frequency moments estimated from correlated samples of symmetric stable random variables . <eos> our experiments confirm that this method is able to substantially better approximate the shannon entropy compared to the prior state-of-the-art .
the restricted boltzmann machine ( rbm ) is a popular density model that is also good for extracting features . <eos> a main source of tractability in rbm models is the model 's assumption that given an input , hidden units activate independently from one another . <eos> sparsity and competition in the hidden representation is believed to be beneficial , and while an rbm with competition among its hidden units would acquire some of the attractive properties of sparse coding , such constraints are not added due to the widespread belief that the resulting model would become intractable . <eos> in this work , we show how a dynamic programming algorithm developed in 1981 can be used to implement exact sparsity in the rbm 's hidden units . <eos> we then expand on this and show how to pass derivatives through a layer of exact sparsity , which makes it possible to fine-tune a deep belief network ( dbn ) consisting of rbms with sparse hidden layers . <eos> we show that sparsity in the rbm 's hidden layer improves the performance of both the pre-trained representations and of the fine-tuned model .
in this paper , we consider the problem of clustering data points into low-dimensional subspaces in the presence of outliers . <eos> we pose the problem using a density estimation formulation with an associated generative model . <eos> based on this probability model , we first develop an iterative expectation-maximization ( em ) algorithm and then derive its global solution . <eos> in addition , we develop two bayesian methods based on variational bayesian ( vb ) approximation , which are capable of automatic dimensionality selection . <eos> while the first method is based on an alternating optimization scheme for all unknowns , the second method makes use of recent results in vb matrix factorization leading to fast and effective estimation . <eos> both methods are extended to handle sparse outliers for robustness and can handle missing values . <eos> experimental results suggest that proposed methods are very effective in clustering and identifying outliers .
statistical decision theory axiomatically assumes that the relative desirability of different options that humans perceive is well described by assigning them option-specific scalar utility functions . <eos> however , this assumption is refuted by observed human behavior , including studies wherein preferences have been shown to change systematically simply through variation in the set of choice options presented . <eos> in this paper , we show that interpreting desirability as a relative comparison between available options at any particular decision instance results in a rational theory of value-inference that explains heretofore intractable violations of rational choice behavior in human subjects . <eos> complementarily , we also characterize the conditions under which a rational agent selecting optimal options indicated by dynamic value inference in our framework will behave identically to one whose preferences are encoded using a static ordinal utility function .
for modeling data matrices , this paper introduces probabilistic co-subspace addition ( pcsa ) model by simultaneously capturing the dependent structures among both rows and columns . <eos> briefly , pcsa assumes that each entry of a matrix is generated by the additive combination of the linear mappings of two features , which distribute in the row-wise and column-wise latent subspaces . <eos> consequently , it captures the dependencies among entries intricately , and is able to model the non-gaussian and heteroscedastic density . <eos> variational inference is proposed on pcsa for approximate bayesian learning , where the updating for posteriors is formulated into the problem of solving sylvester equations . <eos> furthermore , pcsa is extended to tackling and filling missing values , to adapting its sparseness , and to modelling tensor data . <eos> in comparison with several state-of-art approaches , experiments demonstrate the effectiveness and efficiency of bayesian ( sparse ) pcsa on modeling matrix ( tensor ) data and filling missing values .
in this paper we introduce context-sensitive decision forests - a new perspective to exploit contextual information in the popular decision forest framework for the object detection problem . <eos> they are tree-structured classifiers with the ability to access intermediate prediction ( here : classification and regression ) information during training and inference time . <eos> this intermediate prediction is available to each sample , which allows us to develop context-based decision criteria , used for refining the prediction process . <eos> in addition , we introduce a novel split criterion which in combination with a priority based way of constructing the trees , allows more accurate regression mode selection and hence improves the current context information . <eos> in our experiments , we demonstrate improved results for the task of pedestrian detection on the challenging tud data set when compared to state-of-the-art methods .
we present a novel method in the family of particle mcmc methods that we refer to as particle gibbs with ancestor sampling ( pg-as ) . <eos> similarly to the existing pg with backward simulation ( pg-bs ) procedure , we use backward sampling to ( considerably ) improve the mixing of the pg kernel . <eos> instead of using separate forward and backward sweeps as in pg-bs , however , we achieve the same effect in a single forward sweep . <eos> we apply the pg-as framework to the challenging class of non-markovian state-space models . <eos> we develop a truncation strategy of these models that is applicable in principle to any backward-simulation-based method , but which is particularly well suited to the pg-as framework . <eos> in particular , as we show in a simulation study , pg-as can yield an order-of-magnitude improved accuracy relative to pg-bs due to its robustness to the truncation error . <eos> several application examples are discussed , including rao-blackwellized particle smoothing and inference in degenerate state-space models .
large scale $ \ell_1 $ -regularized loss minimization problems arise in numerous applications such as compressed sensing and high dimensional supervised learning , including classification and regression problems . <eos> high performance algorithms and implementations are critical to efficiently solving these problems . <eos> building upon previous work on coordinate descent algorithms for $ \ell_1 $ regularized problems , we introduce a novel family of algorithms called block-greedy coordinate descent that includes , as special cases , several existing algorithms such as scd , greedy cd , shotgun , and thread-greedy . <eos> we give a unified convergence analysis for the family of block-greedy algorithms . <eos> the analysis suggests that block-greedy coordinate descent can better exploit parallelism if features are clustered so that the maximum inner product between features in different blocks is small . <eos> our theoretical convergence analysis is supported with experimental results using data from diverse real-world applications . <eos> we hope that algorithmic approaches and convergence analysis we provide will not only advance the field , but will also encourage researchers to systematically explore the design space of algorithms for solving large-scale $ \ell_1 $ -regularization problems .
the variational bayesian ( vb ) approach is one of the best tractable approximations to the bayesian estimation , and it was demonstrated to perform well in many applications . <eos> however , its good performance was not fully understood theoretically . <eos> for example , vb sometimes produces a sparse solution , which is regarded as a practical advantage of vb , but such sparsity is hardly observed in the rigorous bayesian estimation . <eos> in this paper , we focus on probabilistic pca and give more theoretical insight into the empirical success of vb . <eos> more specifically , for the situation where the noise variance is unknown , we derive a sufficient condition for perfect recovery of the true pca dimensionality in the large-scale limit when the size of an observed matrix goes to infinity . <eos> in our analysis , we obtain bounds for a noise variance estimator and simple closed-form solutions for other parameters , which themselves are actually very useful for better implementation of vb-pca .
this paper provides the first -- -to the best of our knowledge -- - analysis of online learning algorithms for multiclass problems when the { \em confusion } matrix is taken as a performance measure . <eos> the work builds upon recent and elegant results on noncommutative concentration inequalities , i.e . concentration inequalities that apply to matrices , and more precisely to matrix martingales . <eos> we do establish generalization bounds for online learning algorithm and show how the theoretical study motivate the proposition of a new confusion-friendly learning procedure . <eos> this learning algorithm , called \copa ( for confusion passive-aggressive ) is a passive-aggressive learning algorithm ; it is shown that the update equations for \copa can be computed analytically , thus allowing the user from having to recours to any optimization package to implement it .
by developing data augmentation methods unique to the negative binomial ( nb ) distribution , we unite seemingly disjoint count and mixture models under the nb process framework . <eos> we develop fundamental properties of the models and derive efficient gibbs sampling inference . <eos> we show that the gamma-nb process can be reduced to the hierarchical dirichlet process with normalization , highlighting its unique theoretical , structural and computational advantages . <eos> a variety of nb processes with distinct sharing mechanisms are constructed and applied to topic modeling , with connections to existing algorithms , showing the importance of inferring both the nb dispersion and probability parameters .
we theoretically analyze and compare the following five popular multiclass classification methods : one vs. all , all pairs , tree-based classifiers , error correcting output codes ( ecoc ) with randomly generated code matrices , and multiclass svm . <eos> in the first four methods , the classification is based on a reduction to binary classification . <eos> we consider the case where the binary classifier comes from a class of vc dimension $ d $ , and in particular from the class of halfspaces over $ \reals^d $ . <eos> we analyze both the estimation error and the approximation error of these methods . <eos> our analysis reveals interesting conclusions of practical relevance , regarding the success of the different approaches under various conditions . <eos> our proof technique employs tools from vc theory to analyze the \emph { approximation error } of hypothesis classes . <eos> this is in sharp contrast to most , if not all , previous uses of vc theory , which only deal with estimation error .
we study the problem of adaptive control of a high dimensional linear quadratic ( lq ) system . <eos> previous work established the asymptotic convergence to an optimal controller for various adaptive control schemes . <eos> more recently , an asymptotic regret bound of $ \tilde { o } ( \sqrt { t } ) $ was shown for $ t \gg p $ where $ p $ is the dimension of the state space . <eos> in this work we consider the case where the matrices describing the dynamic of the lq system are sparse and their dimensions are large . <eos> we present an adaptive control scheme that for $ p \gg 1 $ and $ t \gg \polylog ( p ) $ achieves a regret bound of $ \tilde { o } ( p \sqrt { t } ) $ . <eos> in particular , our algorithm has an average cost of $ ( 1+\eps ) $ times the optimum cost after $ t = \polylog ( p ) o ( 1/\eps^2 ) $ . <eos> this is in comparison to previous work on the dense dynamics where the algorithm needs $ \omega ( p ) $ samples before it can estimate the unknown dynamic with any significant accuracy . <eos> we believe our result has prominent applications in the emerging area of computational advertising , in particular targeted online advertising and advertising in social networks .
we present an approach to detecting and analyzing the 3d configuration of objects in real-world images with heavy occlusion and clutter . <eos> we focus on the application of finding and analyzing cars . <eos> we do so with a two-stage model ; the first stage reasons about 2d shape and appearance variation due to within-class variation ( station wagons look different than sedans ) and changes in viewpoint . <eos> rather than using a view-based model , we describe a compositional representation that models a large number of effective views and shapes using a small number of local view-based templates . <eos> we use this model to propose candidate detections and 2d estimates of shape . <eos> these estimates are then refined by our second stage , using an explicit 3d model of shape and viewpoint . <eos> we use a morphable model to capture 3d within-class variation , and use a weak-perspective camera model to capture viewpoint . <eos> we learn all model parameters from 2d annotations . <eos> we demonstrate state-of-the-art accuracy for detection , viewpoint estimation , and 3d shape reconstruction on challenging images from the pascal voc 2011 dataset .
early stages of sensory systems face the challenge of compressing information from numerous receptors onto a much smaller number of projection neurons , a so called communication bottleneck . <eos> to make more efficient use of limited bandwidth , compression may be achieved using predictive coding , whereby predictable , or redundant , components of the stimulus are removed . <eos> in the case of the retina , srinivasan et al . ( 1982 ) suggested that feedforward inhibitory connections subtracting a linear prediction generated from nearby receptors implement such compression , resulting in biphasic center-surround receptive fields . <eos> however , feedback inhibitory circuits are common in early sensory circuits and furthermore their dynamics may be nonlinear . <eos> can such circuits implement predictive coding as well ? <eos> here , solving the transient dynamics of nonlinear reciprocal feedback circuits through analogy to a signal-processing algorithm called linearized bregman iteration we show that nonlinear predictive coding can be implemented in an inhibitory feedback circuit . <eos> in response to a step stimulus , interneuron activity in time constructs progressively less sparse but more accurate representations of the stimulus , a temporally evolving prediction . <eos> this analysis provides a powerful theoretical framework to interpret and understand the dynamics of early sensory processing in a variety of physiological experiments and yields novel predictions regarding the relation between activity and stimulus statistics .
we propose a multiresolution gaussian process to capture long-range , non-markovian dependencies while allowing for abrupt changes . <eos> the multiresolution gp hierarchically couples a collection of smooth gps , each defined over an element of a random nested partition . <eos> long-range dependencies are captured by the top-level gp while the partition points define the abrupt changes . <eos> due to the inherent conjugacy of the gps , one can analytically marginalize the gps and compute the conditional likelihood of the observations given the partition tree . <eos> this allows for efficient inference of the partition itself , for which we employ graph-theoretic techniques . <eos> we apply the multiresolution gp to the analysis of magnetoencephalography ( meg ) recordings of brain activity .
we propose a deep boltzmann machine for learning a generative model of multimodal data . <eos> we show how to use the model to extract a meaningful representation of multimodal data . <eos> we find that the learned representation is useful for classification and information retreival tasks , and hence conforms to some notion of semantic similarity . <eos> the model defines a probability density over the space of multimodal inputs . <eos> by sampling from the conditional distributions over each data modality , it possible to create the representation even when some data modalities are missing . <eos> our experimental results on bi-modal data consisting of images and text show that the multimodal dbm can learn a good generative model of the joint space of image and text inputs that is useful for information retrieval from both unimodal and multimodal queries . <eos> we further demonstrate that our model can significantly outperform svms and lda on discriminative tasks . <eos> finally , we compare our model to other deep learning methods , including autoencoders and deep belief networks , and show that it achieves significant gains .
in this paper , we provide a new framework to study the generalization bound of the learning process for domain adaptation . <eos> without loss of generality , we consider two kinds of representative domain adaptation settings : one is domain adaptation with multiple sources and the other is domain adaptation combining source and target data . <eos> in particular , we introduce two quantities that capture the inherent characteristics of domains . <eos> for either kind of domain adaptation , based on the two quantities , we then develop the specific hoeffding-type deviation inequality and symmetrization inequality to achieve the corresponding generalization bound based on the uniform entropy number . <eos> by using the resultant generalization bound , we analyze the asymptotic convergence and the rate of convergence of the learning process for such kind of domain adaptation . <eos> meanwhile , we discuss the factors that affect the asymptotic behavior of the learning process . <eos> the numerical experiments support our results .
we present a novel $ l_1 $ regularized off-policy convergent td-learning method ( termed ro-td ) , which is able to learn sparse representations of value functions with low computational complexity . <eos> the algorithmic framework underlying ro-td integrates two key ideas : off-policy convergent gradient td methods , such as tdc , and a convex-concave saddle-point formulation of non-smooth convex optimization , which enables first-order solvers and feature selection using online convex regularization . <eos> a detailed theoretical and experimental analysis of ro-td is presented . <eos> a variety of experiments are presented to illustrate the off-policy convergence , sparse feature selection capability and low computational cost of the ro-td algorithm .
we present a novel approach to low-level vision problems that combines sparse coding and deep networks pre-trained with denoising auto-encoder ( da ) . <eos> we propose an alternative training scheme that successfully adapts da , originally designed for unsupervised feature learning , to the tasks of image denoising and blind inpainting . <eos> our method achieves state-of-the-art performance in the image denoising task . <eos> more importantly , in blind image inpainting task , the proposed method provides solutions to some complex problems that have not been tackled before . <eos> specifically , we can automatically remove complex patterns like superimposed text from an image , rather than simple patterns like pixels missing at random . <eos> moreover , the proposed method does not need the information regarding the region that requires inpainting to be given a priori . <eos> experimental results demonstrate the effectiveness of the proposed method in the tasks of image denoising and blind inpainting . <eos> we also show that our new training scheme for da is more effective and can improve the performance of unsupervised feature learning .
recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance . <eos> in this paper , we consider the problem of training a deep network with billions of parameters using tens of thousands of cpu cores . <eos> we have developed a software framework called distbelief that can utilize computing clusters with thousands of machines to train large models . <eos> within this framework , we have developed two algorithms for large-scale distributed training : ( i ) downpour sgd , an asynchronous stochastic gradient descent procedure supporting a large number of model replicas , and ( ii ) sandblaster , a framework that supports for a variety of distributed batch optimization procedures , including a distributed implementation of l-bfgs . <eos> downpour sgd and sandblaster l-bfgs both increase the scale and speed of deep network training . <eos> we have successfully used our system to train a deep network 100x larger than previously reported in the literature , and achieves state-of-the-art performance on imagenet , a visual object recognition task with 16 million images and 21k categories . <eos> we show that these same techniques dramatically accelerate the training of a more modestly sized deep network for a commercial speech recognition service . <eos> although we focus on and report performance of these methods as applied to training large neural networks , the underlying algorithms are applicable to any gradient-based machine learning algorithm .
one of the main challenges in data clustering is to define an appropriate similarity measure between two objects . <eos> crowdclustering addresses this challenge by defining the pairwise similarity based on the manual annotations obtained through crowdsourcing . <eos> despite its encouraging results , a key limitation of crowdclustering is that it can only cluster objects when their manual annotations are available . <eos> to address this limitation , we propose a new approach for clustering , called \textit { semi-crowdsourced clustering } that effectively combines the low-level features of objects with the manual annotations of a subset of the objects obtained via crowdsourcing . <eos> the key idea is to learn an appropriate similarity measure , based on the low-level features of objects , from the manual annotations of only a small portion of the data to be clustered . <eos> one difficulty in learning the pairwise similarity measure is that there is a significant amount of noise and inter-worker variations in the manual annotations obtained via crowdsourcing . <eos> we address this difficulty by developing a metric learning algorithm based on the matrix completion method . <eos> our empirical study with two real-world image data sets shows that the proposed algorithm outperforms state-of-the-art distance metric learning algorithms in both clustering accuracy and computational efficiency .
we study the problem of diverse feature selection in linear regression : selecting a small subset of diverse features that can predict a given objective . <eos> diversity is useful for several reasons such as interpretability , robustness to noise , etc . <eos> we propose several spectral regularizers that capture a notion of diversity of features and show that these are all submodular set functions . <eos> these regularizers , when added to the objective function for linear regression , result in approximately submodular functions , which can then be maximized approximately by efficient greedy and local search algorithms , with provable guarantees . <eos> we compare our algorithms to traditional greedy and $ \ell_1 $ -regularization schemes and show that we obtain a more diverse set of features that result in the regression problem being stable under perturbations .
this paper suggests a learning-theoretic perspective on how synaptic plasticity benefits global brain functioning . <eos> we introduce a model , the selectron , that ( i ) arises as the fast time constant limit of leaky integrate-and-fire neurons equipped with spiking timing dependent plasticity ( stdp ) and ( ii ) is amenable to theoretical analysis . <eos> we show that the selectron encodes reward estimates into spikes and that an error bound on spikes is controlled by a spiking margin and the sum of synaptic weights . <eos> moreover , the efficacy of spikes ( their usefulness to other reward maximizing selectrons ) also depends on total synaptic strength . <eos> finally , based on our analysis , we propose a regularized version of stdp , and show the regularization improves the robustness of neuronal learning when faced with multiple stimuli .
typical object detectors trained on images perform poorly on video , as there is a clear distinction in domain between the two types of data . <eos> in this paper , we tackle the problem of adapting object detectors learned from images to work well on videos . <eos> we treat the problem as one of unsupervised domain adaptation , in which we are given labeled data from the source domain ( image ) , but only unlabeled data from the target domain ( video ) . <eos> our approach , self-paced domain adaptation , seeks to iteratively adapt the detector by re-training the detector with automatically discovered target domain examples , starting with the easiest first . <eos> at each iteration , the algorithm adapts by considering an increased number of target domain examples , and a decreased number of source domain examples . <eos> to discover target domain examples from the vast amount of video data , we introduce a simple , robust approach that scores trajectory tracks instead of bounding boxes . <eos> we also show how rich and expressive features specific to the target domain can be incorporated under the same framework . <eos> we show promising results on the 2011 trecvid multimedia event detection and labelme video datasets that illustrate the benefit of our approach to adapt object detectors to video .
we introduce a class of discrete divergences on sets ( equivalently binary vectors ) that we call the submodular-bregman divergences . <eos> we consider two kinds , defined either from tight modular upper or tight modular lower bounds of a submodular function . <eos> we show that the properties of these divergences are analogous to the ( standard continuous ) bregman divergence . <eos> we demonstrate how they generalize many useful divergences , including the weighted hamming distance , squared weighted hamming , weighted precision , recall , conditional mutual information , and a generalized kl-divergence on sets . <eos> we also show that the generalized bregman divergence on the lov ? asz extension of a submodular function , which we call the lov ? asz-bregman divergence , is a continuous extension of a submodular bregman divergence . <eos> we point out a number of applications , and in particular show that a proximal algorithm defined through the submodular bregman divergence provides a framework for many mirror-descent style algorithms related to submodular function optimization . <eos> we also show that a generalization of the k-means algorithm using the lov ? asz bregman divergence is natural in clustering scenarios where ordering is important . <eos> a unique property of this algorithm is that computing the mean ordering is extremely efficient unlike other order based distance measures .
we uncover relations between robust mdps and risk-sensitive mdps . <eos> the objective of a robust mdp is to minimize a function , such as the expectation of cumulative cost , for the worst case when the parameters have uncertainties . <eos> the objective of a risk-sensitive mdp is to minimize a risk measure of the cumulative cost when the parameters are known . <eos> we show that a risk-sensitive mdp of minimizing the expected exponential utility is equivalent to a robust mdp of minimizing the worst-case expectation with a penalty for the deviation of the uncertain parameters from their nominal values , which is measured with the kullback-leibler divergence . <eos> we also show that a risk-sensitive mdp of minimizing an iterated risk measure that is composed of certain coherent risk measures is equivalent to a robust mdp of minimizing the worst-case expectation when the possible deviations of uncertain parameters from their nominal values are characterized with a concave function .
we explore the hypothesis that the neuronal spike generation mechanism is an analog-to-digital converter , which rectifies low-pass filtered summed synaptic currents and encodes them into spike trains linearly decodable in post-synaptic neurons . <eos> to digitally encode an analog current waveform , the sampling rate of the spike generation mechanism must exceed its nyquist rate . <eos> such oversampling is consistent with the experimental observation that the precision of the spike-generation mechanism is an order of magnitude greater than the cut-off frequency of dendritic low-pass filtering . <eos> to achieve additional reduction in the error of analog-to-digital conversion , electrical engineers rely on noise-shaping . <eos> if noise-shaping were used in neurons , it would introduce correlations in spike timing to reduce low-frequency ( up to nyquist ) transmission error at the cost of high-frequency one ( from nyquist to sampling rate ) . <eos> using experimental data from three different classes of neurons , we demonstrate that biological neurons utilize noise-shaping . <eos> we also argue that rectification by the spike-generation mechanism may improve energy efficiency and carry out de-noising . <eos> finally , the zoo of ion channels in neurons may be viewed as a set of predictors , various subsets of which are activated depending on the statistics of the input current .
statistical features of neuronal spike trains are known to be non-poisson . <eos> here , we investigate the extent to which the non-poissonian feature affects the efficiency of transmitting information on fluctuating firing rates . <eos> for this purpose , we introduce the kullbuck-leibler ( kl ) divergence as a measure of the efficiency of information encoding , and assume that spike trains are generated by time-rescaled renewal processes . <eos> we show that the kl divergence determines the lower bound of the degree of rate fluctuations below which the temporal variation of the firing rates is undetectable from sparse data . <eos> we also show that the kl divergence , as well as the lower bound , depends not only on the variability of spikes in terms of the coefficient of variation , but also significantly on the higher-order moments of interspike interval ( isi ) distributions . <eos> we examine three specific models that are commonly used for describing the stochastic nature of spikes ( the gamma , inverse gaussian ( ig ) and lognormal isi distributions ) , and find that the time-rescaled renewal process with the ig distribution achieves the largest kl divergence , followed by the lognormal and gamma distributions .
conditional markov chains ( also known as linear-chain conditional random fields in the literature ) are a versatile class of discriminative models for the distribution of a sequence of hidden states conditional on a sequence of observable variables . <eos> large-sample properties of conditional markov chains have been first studied by sinn and poupart [ 1 ] . <eos> the paper extends this work in two directions : first , mixing properties of models with unbounded feature functions are being established ; second , necessary conditions for model identifiability and the uniqueness of maximum likelihood estimates are being given .
many tasks in text and speech processing and computational biology require estimating functions mapping strings to real numbers . <eos> a broad class of such functions can be defined by weighted automata . <eos> spectral methods based on the singular value decomposition of a hankel matrix have been recently proposed for learning a probability distribution represented by a weighted automaton from a training sample drawn according to this same target distribution . <eos> in this paper , we show how spectral methods can be extended to the problem of learning a general weighted automaton from a sample generated by an arbitrary distribution . <eos> the main obstruction to this approach is that , in general , some entries of the hankel matrix may be missing . <eos> we present a solution to this problem based on solving a constrained matrix completion problem . <eos> combining these two ingredients , matrix completion and spectral method , a whole new family of algorithms for learning general weighted automata is obtained . <eos> we present generalization bounds for a particular algorithm in this family . <eos> the proofs rely on a joint stability analysis of matrix completion and spectral learning .
neural adaptation underlies the ability of neurons to maximize encoded information over a wide dynamic range of input stimuli . <eos> while adaptation is an intrinsic feature of neuronal models like the hodgkin-huxley model , the challenge is to integrate adaptation in models of neural computation . <eos> recent computational models like the adaptive spike response model implement adaptation as spike-based addition of fixed-size fast spike-triggered threshold dynamics and slow spike-triggered currents . <eos> such adaptation has been shown to accurately model neural spiking behavior over a limited dynamic range . <eos> taking a cue from kinetic models of adaptation , we propose a multiplicative adaptive spike response model where the spike-triggered adaptation dynamics are scaled multiplicatively by the adaptation state at the time of spiking . <eos> we show that unlike the additive adaptation model , the firing rate in the multiplicative adaptation model saturates to a maximum spike-rate . <eos> when simulating variance switching experiments , the model also quantitatively fits the experimental data over a wide dynamic range . <eos> furthermore , dynamic threshold models of adaptation suggest a straightforward interpretation of neural activity in terms of dynamic signal encoding with shifted and weighted exponential kernels . <eos> we show that when thus encoding rectified filtered stimulus signals , the multiplicative adaptive spike response model achieves a high coding efficiency and maintains this efficiency over changes in the dynamic signal range of several orders of magnitude , without changing model parameters .
we estimate the sample complexity of a recent robust estimator for a generalized version of the inverse covariance matrix . <eos> this estimator is used in a convex algorithm for robust subspace recovery ( i.e. , robust pca ) . <eos> our model assumes a sub-gaussian underlying distribution and an i.i.d.~sample from it . <eos> our main result shows with high probability that the norm of the difference between the generalized inverse covariance of the underlying distribution and its estimator from an i.i.d.~sample of size $ n $ is of order $ o ( n^ { -0.5+\eps } ) $ for arbitrarily small $ \eps > 0 $ ( affecting the probabilistic estimate ) ; this rate of convergence is close to one of direct covariance and inverse covariance estimation , i.e. , $ o ( n^ { -0.5 } ) $ . <eos> our precise probabilistic estimate implies for some natural settings that the sample complexity of the generalized inverse covariance estimation when using the frobenius norm is $ o ( d^ { 2+\delta } ) $ for arbitrarily small $ \delta > 0 $ ( whereas the sample complexity of direct covariance estimation with frobenius norm is $ o ( d^ { 2 } ) $ ) . <eos> these results provide similar rates of convergence and sample complexity for the corresponding robust subspace recovery algorithm , which are close to those of pca . <eos> to the best of our knowledge , this is the only work analyzing the sample complexity of any robust pca algorithm .
we present a new model based on gaussian processes ( gps ) for learning pairwise preferences expressed by multiple users . <eos> inference is simplified by using a \emph { preference kernel } for gps which allows us to combine supervised gp learning of user preferences with unsupervised dimensionality reduction for multi-user systems . <eos> the model not only exploits collaborative information from the shared structure in user behavior , but may also incorporate user features if they are available . <eos> approximate inference is implemented using a combination of expectation propagation and variational bayes . <eos> finally , we present an efficient active learning strategy for querying preferences . <eos> the proposed technique performs favorably on real-world data against state-of-the-art multi-user preference learning algorithms .
the question of aggregating pairwise comparisons to obtain a global ranking over a collection of objects has been of interest for a very long time : be it ranking of online gamers ( e.g . msr ? s trueskill system ) and chess players , aggregating social opinions , or deciding which product to sell based on transactions . <eos> in most settings , in addition to obtaining ranking , finding scoresfor each object ( e.g . player ? s rating ) is of interest to understanding the intensity of the preferences . <eos> in this paper , we propose a novel iterative rank aggregation algorithm for discovering scores for objects from pairwise comparisons . <eos> the algorithm has a natural random walk interpretation over the graph of objects with edges present between two objects if they are compared ; the scores turn out to be the stationary probability of this random walk . <eos> the algorithm is model independent . <eos> to establish the efficacy of our method , however , we consider the popular bradley-terry-luce ( btl ) model in which each object has an associated score which determines the probabilistic outcomes of pairwise comparisons between objects . <eos> we bound the finite sample error rates between the scores assumed by the btl model and those estimated by our algorithm . <eos> this , in essence , leads to order-optimal dependence on the number of samples required to learn the scores well by our algorithm . <eos> indeed , the experimental evaluation shows that our ( model independent ) algorithm performs as well as the maximum likelihood estimator of the btl model and outperforms a recently proposed algorithm by ammar and shah [ 1 ] .
in categorical data there is often structure in the number of variables that take on each label . <eos> for example , the total number of objects in an image and the number of highly relevant documents per query in web search both tend to follow a structured distribution . <eos> in this paper , we study a probabilistic model that explicitly includes a prior distribution over such counts , along with a count-conditional likelihood that defines probabilities over all subsets of a given size . <eos> when labels are binary and the prior over counts is a poisson-binomial distribution , a standard logistic regression model is recovered , but for other count distributions , such priors induce global dependencies and combinatorics that appear to complicate learning and inference . <eos> however , we demonstrate that simple , efficient learning procedures can be derived for more general forms of this model . <eos> we illustrate the utility of the formulation by exploring applications to multi-object classification , learning to rank , and top-k classification .
the computational modelling of the primary auditory cortex ( a1 ) has been less fruitful than that of the primary visual cortex ( v1 ) due to the less organized properties of a1 . <eos> greater disorder has recently been demonstrated for the tonotopy of a1 that has traditionally been considered to be as ordered as the retinotopy of v1 . <eos> this disorder appears to be incongruous , given the uniformity of the neocortex ; however , we hypothesized that both a1 and v1 would adopt an efficient coding strategy and that the disorder in a1 reflects natural sound statistics . <eos> to provide a computational model of the tonotopic disorder in a1 , we used a model that was originally proposed for the smooth v1 map . <eos> in contrast to natural images , natural sounds exhibit distant correlations , which were learned and reflected in the disordered map . <eos> the auditory model predicted harmonic relationships among neighbouring a1 cells ; furthermore , the same mechanism used to model v1 complex cells reproduced nonlinear responses similar to the pitch selectivity . <eos> these results contribute to the understanding of the sensory cortices of different modalities in a novel and integrated manner .
the task of assigning a set of relevant tags to an image is challenging due to the size and variability of tag vocabularies . <eos> consequently , most existing algorithms focus on tag assignment and fix an often large number of hand-crafted features to describe image characteristics . <eos> in this paper we introduce a hierarchical model for learning representations of full sized color images from the pixel level , removing the need for engineered feature representations and subsequent feature selection . <eos> we benchmark our model on the stl-10 recognition dataset , achieving state-of-the-art performance . <eos> when our features are combined with tagprop ( guillaumin et al . ) , we outperform or compete with existing annotation approaches that use over a dozen distinct image descriptors . <eos> furthermore , using 256-bit codes and hamming distance for training tagprop , we exchange only a small reduction in performance for efficient storage and fast comparisons . <eos> in our experiments , using deeper architectures always outperform shallow ones .
given pairwise dissimilarities between data points , we consider the problem of finding a subset of data points called representatives or exemplars that can efficiently describe the data collection . <eos> we formulate the problem as a row-sparsity regularized trace minimization problem which can be solved efficiently using convex programming . <eos> the solution of the proposed optimization program finds the representatives and the probability that each data point is associated to each one of the representatives . <eos> we obtain the range of the regularization parameter for which the solution of the proposed optimization program changes from selecting one representative to selecting all data points as the representatives . <eos> when data points are distributed around multiple clusters according to the dissimilarities , we show that the data in each cluster select only representatives from that cluster . <eos> unlike metric-based methods , our algorithm does not require that the pairwise dissimilarities be metrics and can be applied to dissimilarities that are asymmetric or violate the triangle inequality . <eos> we demonstrate the effectiveness of the proposed algorithm on synthetic data as well as real-world datasets of images and text .
keypoint matching between pairs of images using popular descriptors like sift or a faster variant called surf is at the heart of many computer vision algorithms including recognition , mosaicing , and structure from motion . <eos> for real-time mobile applications , very fast but less accurate descriptors like brief and related methods use a random sampling of pairwise comparisons of pixel intensities in an image patch . <eos> here , we introduce locally uniform comparison image descriptor ( lucid ) , a simple description method based on permutation distances between the ordering of intensities of rgb values between two patches . <eos> lucid is computable in linear time with respect to patch size and does not require floating point computation . <eos> an analysis reveals an underlying issue that limits the potential of brief and related approaches compared to lucid . <eos> experiments demonstrate that lucid is faster than brief , and its accuracy is directly comparable to surf while being more than an order of magnitude faster .
rich and complex time-series data , such as those generated from engineering sys- tems , financial markets , videos or neural recordings are now a common feature of modern data analysis . <eos> explaining the phenomena underlying these diverse data sets requires flexible and accurate models . <eos> in this paper , we promote gaussian process dynamical systems as a rich model class appropriate for such analysis . <eos> in particular , we present a message passing algorithm for approximate inference in gpdss based on expectation propagation . <eos> by phrasing inference as a general mes- sage passing problem , we iterate forward-backward smoothing . <eos> we obtain more accurate posterior distributions over latent structures , resulting in improved pre- dictive performance compared to state-of-the-art gpds smoothers , which are spe- cial cases of our general iterative message passing algorithm . <eos> hence , we provide a unifying approach within which to contextualize message passing in gpdss .
this paper proposes a novel image representation called a graphical gaussian vector , which is a counterpart of the codebook and local feature matching approaches . <eos> in our method , we model the distribution of local features as a gaussian markov random field ( gmrf ) which can efficiently represent the spatial relationship among local features . <eos> we consider the parameter of gmrf as a feature vector of the image . <eos> using concepts of information geometry , proper parameters and a metric from the gmrf can be obtained . <eos> finally we define a new image feature by embedding the metric into the parameters , which can be directly applied to scalable linear classifiers . <eos> our method obtains superior performance over the state-of-the-art methods in the standard object recognition datasets and comparable performance in the scene dataset . <eos> as the proposed method simply calculates the local auto-correlations of local features , it is able to achieve both high classification accuracy and high efficiency .
some of the most compelling applications of online convex optimization , including online prediction and classification , are unconstrained : the natural feasible set is r^n . <eos> existing algorithms fail to achieve sub-linear regret in this setting unless constraints on the comparator point x* are known in advance . <eos> we present an algorithm that , without such prior knowledge , offers near-optimal regret bounds with respect to _any_ choice of x* . <eos> in particular , regret with respect to x* = 0 is _constant_ . <eos> we then prove lower bounds showing that our algorithm 's guarantees are optimal in this setting up to constant factors .
multiple kernel learning ( mkl ) generalizes svms to the setting where one simultaneously trains a linear classifier and chooses an optimal combination of given base kernels . <eos> model complexity is typically controlled using various norm regularizations on the vector of base kernel mixing coefficients . <eos> existing methods , however , neither regularize nor exploit potentially useful information pertaining to how kernels in the input set 'interact ' ; that is , higher order kernel-pair relationships that can be easily obtained via unsupervised ( similarity , geodesics ) , supervised ( correlation in errors ) , or domain knowledge driven mechanisms ( which features were used to construct the kernel ? ) . <eos> we show that by substituting the norm penalty with an arbitrary quadratic function q \succeq 0 , one can impose a desired covariance structure on mixing coefficient selection , and use this as an inductive bias when learning the concept . <eos> this formulation significantly generalizes the widely used 1- and 2-norm mkl objectives . <eos> we explore the model ? s utility via experiments on a challenging neuroimaging problem , where the goal is to predict a subject ? s conversion to alzheimer ? s disease ( ad ) by exploiting aggregate information from several distinct imaging modalities . <eos> here , our new model outperforms the state of the art ( p-values < < 10 ? 3 ) . <eos> we briefly discuss ramifications in terms of learning bounds ( rademacher complexity ) .
we present a novel method for learning densities with bounded support which enables us to incorporate ` hard ' topological constraints . <eos> in particular , we show how emerging techniques from computational algebraic topology and the notion of persistent homology can be combined with kernel based methods from machine learning for the purpose of density estimation . <eos> the proposed formalism facilitates learning of models with bounded support in a principled way , and -- by incorporating persistent homology techniques in our approach -- we are able to encode algebraic-topological constraints which are not addressed in current state-of the art probabilistic models . <eos> we study the behaviour of our method on two synthetic examples for various sample sizes and exemplify the benefits of the proposed approach on a real-world data-set by learning a motion model for a racecar . <eos> we show how to learn a model which respects the underlying topological structure of the racetrack , constraining the trajectories of the car .
in a large visual multi-class detection framework , the timeliness of results can be crucial . <eos> our method for timely multi-class detection aims to give the best possible performance at any single point after a start time ; it is terminated at a deadline time . <eos> toward this goal , we formulate a dynamic , closed-loop policy that infers the contents of the image in order to decide which detector to deploy next . <eos> in contrast to previous work , our method significantly diverges from the predominant greedy strategies , and is able to learn to take actions with deferred values . <eos> we evaluate our method with a novel timeliness measure , computed as the area under an average precision vs. time curve . <eos> experiments are conducted on the eminent pascal voc object detection dataset . <eos> if execution is stopped when only half the detectors have been run , our method obtains $ 66\ % $ better ap than a random ordering , and $ 14\ % $ better performance than an intelligent baseline . <eos> on the timeliness measure , our method obtains at least $ 11\ % $ better performance . <eos> our code , to be made available upon publication , is easily extensible as it treats detectors and classifiers as black boxes and learns from execution traces using reinforcement learning .
this paper describes gradient methods based on a scaled metric on the grassmann manifold for low-rank matrix completion . <eos> the proposed methods significantly improve canonical gradient methods especially on ill-conditioned matrices , while maintaining established global convegence and exact recovery guarantees . <eos> a connection between a form of subspace iteration for matrix completion and the scaled gradient descent procedure is also established . <eos> the proposed conjugate gradient method based on the scaled gradient outperforms several existing algorithms for matrix completion and is competitive with recently proposed methods .
fine-grained recognition refers to a subordinate level of recognition , such are recognizing different species of birds , animals or plants . <eos> it differs from recognition of basic categories , such as humans , tables , and computers , in that there are global similarities in shape or structure shared within a category , and the differences are in the details of the object parts . <eos> we suggest that the key to identifying the fine-grained differences lies in finding the right alignment of image regions that contain the same object parts . <eos> we propose a template model for the purpose , which captures common shape patterns of object parts , as well as the co-occurence relation of the shape patterns . <eos> once the image regions are aligned , extracted features are used for classification . <eos> learning of the template model is efficient , and the recognition results we achieve significantly outperform the state-of-the-art algorithms .
we develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers ' positions on specific political issues . <eos> our model can be used to explore how a lawmaker 's voting patterns deviate from what is expected and how that deviation depends on what is being voted on . <eos> we derive approximate posterior inference algorithms based on variational methods . <eos> across 12 years of legislative data , we demonstrate both improvement in heldout predictive performance and the model 's utility in interpreting an inherently multi-dimensional space .
graphical models are a very useful tool to describe and understand natural phenomena , from gene expression to climate change and social interactions . <eos> the topological structure of these graphs/networks is a fundamental part of the analysis , and in many cases the main goal of the study . <eos> however , little work has been done on incorporating prior topological knowledge onto the estimation of the underlying graphical models from sample data . <eos> in this work we propose extensions to the basic joint regression model for network estimation , which explicitly incorporate graph-topological constraints into the corresponding optimization approach . <eos> the first proposed extension includes an eigenvector centrality constraint , thereby promoting this important prior topological property . <eos> the second developed extension promotes the formation of certain motifs , triangle-shaped ones in particular , which are known to exist for example in genetic regulatory networks . <eos> the presentation of the underlying formulations , which serve as examples of the introduction of topological constraints in network estimation , is complemented with examples in diverse datasets demonstrating the importance of incorporating such critical prior knowledge .
the dominant visual search paradigm for object class detection is sliding windows . <eos> although simple and effective , it is also wasteful , unnatural and rigidly hardwired . <eos> we propose strategies to search for objects which intelligently explore the space of windows by making sequential observations at locations decided based on previous observations . <eos> our strategies adapt to the class being searched and to the content of a particular test image . <eos> their driving force is exploiting context as the statistical relation between the appearance of a window and its location relative to the object , as observed in the training set . <eos> in addition to being more elegant than sliding windows , we demonstrate experimentally on the pascal voc 2010 dataset that our strategies evaluate two orders of magnitude fewer windows while at the same time achieving higher detection accuracy .
reservoir computing is a new , powerful and flexible machine learning technique that is easily implemented in hardware . <eos> recently , by using a time-multiplexed architecture , hardware reservoir computers have reached performance comparable to digital implementations . <eos> operating speeds allowing for real time information operation have been reached using optoelectronic systems . <eos> at present the main performance bottleneck is the readout layer which uses slow , digital postprocessing . <eos> we have designed an analog readout suitable for time-multiplexed optoelectronic reservoir computers , capable of working in real time . <eos> the readout has been built and tested experimentally on a standard benchmark task . <eos> its performance is better than non-reservoir methods , with ample room for further improvement . <eos> the present work thereby overcomes one of the major limitations for the future development of hardware reservoir computers .
bayesian inference provides a unifying framework for addressing problems in machine learning , artificial intelligence , and robotics , as well as the problems facing the human mind . <eos> unfortunately , exact bayesian inference is intractable in all but the simplest models . <eos> therefore minds and machines have to approximate bayesian inference . <eos> approximate inference algorithms can achieve a wide range of time-accuracy tradeoffs , but what is the optimal tradeoff ? <eos> we investigate time-accuracy tradeoffs using the metropolis-hastings algorithm as a metaphor for the mind 's inference algorithm ( s ) . <eos> we find that reasonably accurate decisions are possible long before the markov chain has converged to the posterior distribution , i.e . during the period known as burn-in . <eos> therefore the strategy that is optimal subject to the mind 's bounded processing speed and opportunity costs may perform so few iterations that the resulting samples are biased towards the initial value . <eos> the resulting cognitive process model provides a rational basis for the anchoring-and-adjustment heuristic . <eos> the model 's quantitative predictions are tested against published data on anchoring in numerical estimation tasks . <eos> our theoretical and empirical results suggest that the anchoring bias is consistent with approximate bayesian inference .
there is no generally accepted way to define wavelets on permutations . <eos> we address this issue by introducing the notion of coset based multiresolution analysis ( cmra ) on the symmetric group ; find the corresponding wavelet functions ; and describe a fast wavelet transform of o ( n^p ) complexity with small p for sparse signals ( in contrast to the o ( n^q n ! ) <eos> complexity typical of ffts ) . <eos> we discuss potential applications in ranking , sparse approximation , and multi-object tracking .
we present a new graph-based approach for incorporating domain knowledge in reinforcement learning applications . <eos> the domain knowledge is given as a weighted graph , or a kernel matrix , that loosely indicates which states should have similar optimal actions . <eos> we first introduce a bias into the policy search process by deriving a distribution on policies such that policies that disagree with the provided graph have low probabilities . <eos> this distribution corresponds to a markov random field . <eos> we then present a reinforcement and an apprenticeship learning algorithms for finding such policy distributions . <eos> we also illustrate the advantage of the proposed approach on three problems : swing-up cart-balancing with nonuniform and smooth frictions , gridworlds , and teaching a robot to grasp new objects .
the interaction between the patient 's expected outcome of an intervention and the inherent effects of that intervention can have extraordinary effects . <eos> thus in clinical trials an effort is made to conceal the nature of the administered intervention from the participants in the trial i.e . to blind it . <eos> yet , in practice perfect blinding is impossible to ensure or even verify . <eos> the current standard is follow up the trial with an auxiliary questionnaire , which allows trial participants to express their belief concerning the assigned intervention and which is used to compute a measure of the extent of blinding in the trial . <eos> if the estimated extent of blinding exceeds a threshold the trial is deemed sufficiently blinded ; otherwise , the trial is deemed to have failed . <eos> in this paper we make several important contributions . <eos> firstly , we identify a series of fundamental problems of the aforesaid practice and discuss them in context of the most commonly used blinding measures . <eos> secondly , motivated by the highlighted problems , we formulate a novel method for handling imperfectly blinded trials . <eos> we too adopt a post-trial feedback questionnaire but interpret the collected data using an original approach , fundamentally different from those previously proposed . <eos> unlike previous approaches , ours is void of any ad hoc free parameters , is robust to small changes in auxiliary data and is not predicated on any strong assumptions used to interpret participants ' feedback .
given a probabilistic graphical model , its density of states is a function that , for any likelihood value , gives the number of configurations with that probability . <eos> we introduce a novel message-passing algorithm called density propagation ( dp ) for estimating this function . <eos> we show that dp is exact for tree-structured graphical models and is , in general , a strict generalization of both sum-product and max-product algorithms . <eos> further , we use density of states and tree decomposition to introduce a new family of upper and lower bounds on the partition function . <eos> for any tree decompostion , the new upper bound based on finer-grained density of state information is provably at least as tight as previously known bounds based on convexity of the log-partition function , and strictly stronger if a general condition holds . <eos> we conclude with empirical evidence of improvement over convex relaxations and mean-field based bounds .
this paper proposes an efficient online learning algorithm to track the smoothing functions of additive models . <eos> the key idea is to combine the linear representation of additive models with a recursive least squares ( rls ) filter . <eos> in order to quickly track changes in the model and put more weight on recent data , the rls filter uses a forgetting factor which exponentially weights down observations by the order of their arrival . <eos> the tracking behaviour is further enhanced by using an adaptive forgetting factor which is updated based on the gradient of the a priori errors . <eos> using results from lyapunov stability theory , upper bounds for the learning rate are analyzed . <eos> the proposed algorithm is applied to 5 years of electricity load data provided by the french utility company electricite de france ( edf ) . <eos> compared to state-of-the-art methods , it achieves a superior performance in terms of model tracking and prediction accuracy .
we develop a novel approach for supervised learning based on adaptively partitioning the feature space into different regions and learning local region-specific classifiers . <eos> we formulate an empirical risk minimization problem that incorporates both partitioning and classification in to a single global objective . <eos> we show that space partitioning can be equivalently reformulated as a supervised learning problem and consequently any discriminative learning method can be utilized in conjunction with our approach . <eos> nevertheless , we consider locally linear schemes by learning linear partitions and linear region classifiers . <eos> locally linear schemes can not only approximate complex decision boundaries and ensure low training error but also provide tight control on over-fitting and generalization error . <eos> we train locally linear classifiers by using lda , logistic regression and perceptrons , and so our scheme is scalable to large data sizes and high-dimensions . <eos> we present experimental results demonstrating improved performance over state of the art classification techniques on benchmark datasets . <eos> we also show improved robustness to label noise .
unsupervised clustering of scattered , noisy and high-dimensional data points is an important and difficult problem . <eos> continuous relaxations of balanced cut problems yield excellent clustering results . <eos> this paper provides rigorous convergence results for two algorithms that solve the relaxed cheeger cut minimization . <eos> the first algorithm is a new steepest descent algorithm and the second one is a slight modification of the inverse power method algorithm \cite { pro : heinbuhler10onespec } . <eos> while the steepest descent algorithm has better theoretical convergence properties , in practice both algorithm perform equally . <eos> we also completely characterize the local minima of the relaxed problem in terms of the original balanced cut problem , and relate this characterization to the convergence of the algorithms .
abstract given samples from distributions $ p $ and $ q $ , a two-sample test determines whether to reject the null hypothesis that $ p=q $ , based on the value of a test statistic measuring the distance between the samples . <eos> one choice of test statistic is the maximum mean discrepancy ( mmd ) , which is a distance between embeddings of the probability distributions in a reproducing kernel hilbert space . <eos> the kernel used in obtaining these embeddings is thus critical in ensuring the test has high power , and correctly distinguishes unlike distributions with high probability . <eos> a means of parameter selection for the two-sample test based on the mmd is proposed . <eos> for a given test level ( an upper bound on the probability of making a type i error ) , the kernel is chosen so as to maximize the test power , and minimize the probability of making a type ii error . <eos> the test statistic , test threshold , and optimization over the kernel parameters are obtained with cost linear in the sample size . <eos> these properties make the kernel selection and test procedures suited to data streams , where the observations can not all be stored in memory . <eos> in experiments , the new kernel selection approach yields a more powerful test than earlier kernel selection heuristics .
we study two communication-efficient algorithms for distributed statistical optimization on large-scale data . <eos> the first algorithm is an averaging method that distributes the $ n $ data samples evenly to $ m $ machines , performs separate minimization on each subset , and then averages the estimates . <eos> we provide a sharp analysis of this average mixture algorithm , showing that under a reasonable set of conditions , the combined parameter achieves mean-squared error that decays as $ \order ( n^ { -1 } + ( n/m ) ^ { -2 } ) $ . <eos> whenever $ m \le \sqrt { n } $ , this guarantee matches the best possible rate achievable by a centralized algorithm having access to all $ n $ samples . <eos> the second algorithm is a novel method , based on an appropriate form of the bootstrap . <eos> requiring only a single round of communication , it has mean-squared error that decays as $ \order ( n^ { -1 } + ( n/m ) ^ { -3 } ) $ , and so is more robust to the amount of parallelization . <eos> we complement our theoretical results with experiments on large-scale problems from the microsoft learning to rank dataset .
multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks . <eos> it has been successfully applied to many applications including computer vision and biomedical informatics . <eos> most of the existing multi-task sparse feature learning algorithms are formulated as a convex sparse regularization problem , which is usually suboptimal , due to its looseness for approximating an $ \ell_0 $ -type regularizer . <eos> in this paper , we propose a non-convex formulation for multi-task sparse feature learning based on a novel regularizer . <eos> to solve the non-convex optimization problem , we propose a multi-stage multi-task feature learning ( msmtfl ) algorithm . <eos> moreover , we present a detailed theoretical analysis showing that msmtfl achieves a better parameter estimation error bound than the convex formulation . <eos> empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of msmtfl in comparison with the state of the art multi-task sparse feature learning algorithms .
we apply salient feature detection and tracking in videos to simulate fixations and smooth pursuit in human vision . <eos> with tracked sequences as input , a hierarchical network of modules learns invariant features using a temporal slowness constraint . <eos> the network encodes invariance which are increasingly complex with hierarchy . <eos> although learned from videos , our features are spatial instead of spatial-temporal , and well suited for extracting features from still images . <eos> we applied our features to four datasets ( coil-100 , caltech 101 , stl-10 , pubfig ) , and observe a consistent improvement of 4 % to 5 % in classification accuracy . <eos> with this approach , we achieve state-of-the-art recognition accuracy 61 % on stl-10 dataset .
we formulate clustering aggregation as a special instance of maximum-weight independent set ( mwis ) problem . <eos> for a given dataset , an attributed graph is constructed from the union of the input clusterings generated by different underlying clustering algorithms with different parameters . <eos> the vertices , which represent the distinct clusters , are weighted by an internal index measuring both cohesion and separation . <eos> the edges connect the vertices whose corresponding clusters overlap . <eos> intuitively , an optimal aggregated clustering can be obtained by selecting an optimal subset of non-overlapping clusters partitioning the dataset together . <eos> we formalize this intuition as the mwis problem on the attributed graph , i.e. , finding the heaviest subset of mutually non-adjacent vertices . <eos> this mwis problem exhibits a special structure . <eos> since the clusters of each input clustering form a partition of the dataset , the vertices corresponding to each clustering form a maximal independent set ( mis ) in the attributed graph . <eos> we propose a variant of simulated annealing method that takes advantage of this special structure . <eos> our algorithm starts from each mis , which is close to a distinct local optimum of the mwis problem , and utilizes a local search heuristic to explore its neighborhood in order to find the mwis . <eos> extensive experiments on many challenging datasets show that : 1. our approach to clustering aggregation automatically decides the optimal number of clusters ; 2. it does not require any parameter tuning for the underlying clustering algorithms ; 3. it can combine the advantages of different underlying clustering algorithms to achieve superior performance ; 4. it is robust against moderate or even bad input clusterings .
this paper examines the possibility of a ` reject option ' in the context of least squares regression . <eos> it is shown that using rejection it is theoretically possible to learn ` selective ' regressors that can $ \epsilon $ -pointwise track the best regressor in hindsight from the same hypothesis class , while rejecting only a bounded portion of the domain . <eos> moreover , the rejected volume vanishes with the training set size , under certain conditions . <eos> we then develop efficient and exact implementation of these selective regressors for the case of linear regression . <eos> empirical evaluation over a suite of real-world datasets corroborates the theoretical analysis and indicates that our selective regressors can provide substantial advantage by reducing estimation error .
in this paper , a novel , computationally fast , and alternative algorithm for com- puting weighted v-statistics in resampling both univariate and multivariate data is proposed . <eos> to avoid any real resampling , we have linked this problem with finite group action and converted it into a problem of orbit enumeration . <eos> for further computational cost reduction , an efficient method is developed to list all orbits by their symmetry order and calculate all index function orbit sums and data function orbit sums recursively . <eos> the computational complexity analysis shows reduction in the computational cost from n ! <eos> or nn level to low-order polynomial level .
this paper sheds light on some fundamental connections of the diffusion decision making model of neuroscience and cognitive psychology with k-nearest neighbor classification . <eos> we show that conventional k-nearest neighbor classification can be viewed as a special problem of the diffusion decision model in the asymptotic situation . <eos> applying the optimal strategy associated with the diffusion decision model , an adaptive rule is developed for determining appropriate values of k in k-nearest neighbor classification . <eos> making use of the sequential probability ratio test ( sprt ) and bayesian analysis , we propose five different criteria for adaptively acquiring nearest neighbors . <eos> experiments with both synthetic and real datasets demonstrate the effectivness of our classification criteria .
random utility theory models an agent ? s preferences on alternatives by drawing a real-valued score on each alternative ( typically independently ) from a parameterized distribution , and then ranking the alternatives according to scores . <eos> a special case that has received significant attention is the plackett-luce model , for which fast inference methods for maximum likelihood estimators are available . <eos> this paper develops conditions on general random utility models that enable fast inference within a bayesian framework through mc-em , providing concave loglikelihood functions and bounded sets of global maxima solutions . <eos> results on both real-world and simulated data provide support for the scalability of the approach and capability for model selection among general random utility models including plackett-luce .
inference on high-order graphical models has become increasingly important in recent years . <eos> we consider energies with simple 'sparse ' high-order potentials . <eos> previous work in this area uses either specialized message-passing or transforms each high-order potential to the pairwise case . <eos> we take a fundamentally different approach , transforming the entire original problem into a comparatively small instance of a submodular vertex-cover problem . <eos> these vertex-cover instances can then be attacked by standard pairwise methods , where they run much faster ( 4 -- 15 times ) and are often more effective than on the original problem . <eos> we evaluate our approach on synthetic data , and we show that our algorithm can be useful in a fast hierarchical clustering and model estimation framework .
we present a nonparametric bayesian approach to inverse reinforcement learning ( irl ) for multiple reward functions . <eos> most previous irl algorithms assume that the behaviour data is obtained from an agent who is optimizing a single reward function , but this assumption is hard to be met in practice . <eos> our approach is based on integrating the dirichlet process mixture model into bayesian irl . <eos> we provide an efficient metropolis-hastings sampling algorithm utilizing the gradient of the posterior to estimate the underlying reward functions , and demonstrate that our approach outperforms the previous ones via experiments on a number of problem domains .
the integration of excitatory inputs in dendrites is non-linear : multiple excitatory inputs can produce a local depolarization departing from the arithmetic sum of each input 's response taken separately . <eos> if this depolarization is bigger than the arithmetic sum , the dendrite is spiking ; if the depolarization is smaller , the dendrite is saturating . <eos> decomposing a dendritic tree into independent dendritic spiking units greatly extends its computational capacity , as the neuron then maps onto a two layer neural network , enabling it to compute linearly non-separable boolean functions ( lnbfs ) . <eos> how can these lnbfs be implemented by dendritic architectures in practise ? <eos> and can saturating dendrites equally expand computational capacity ? <eos> to adress these questions we use a binary neuron model and boolean algebra . <eos> first , we confirm that spiking dendrites enable a neuron to compute lnbfs using an architecture based on the disjunctive normal form ( dnf ) . <eos> second , we prove that saturating dendrites as well as spiking dendrites also enable a neuron to compute lnbfs using an architecture based on the conjunctive normal form ( cnf ) . <eos> contrary to the dnf-based architecture , a cnf-based architecture leads to a dendritic unit tuning that does not imply the neuron tuning , as has been observed experimentally . <eos> third , we show that one can not use a dnf-based architecture with saturating dendrites . <eos> consequently , we show that an important family of lnbfs implemented with a cnf-architecture can require an exponential number of saturating dendritic units , whereas the same family implemented with either a dnf-architecture or a cnf-architecture always require a linear number of spiking dendritic unit . <eos> this minimization could explain why a neuron spends energetic resources to make its dendrites spike .
we develop a new algorithm to cluster sparse unweighted graphs -- i.e . partition the nodes into disjoint clusters so that there is higher density within clusters , and low across clusters . <eos> by sparsity we mean the setting where both the in-cluster and across cluster edge densities are very small , possibly vanishing in the size of the graph . <eos> sparsity makes the problem noisier , and hence more difficult to solve . <eos> any clustering involves a tradeoff between minimizing two kinds of errors : missing edges within clusters and present edges across clusters . <eos> our insight is that in the sparse case , these must be { \em penalized differently } . <eos> we analyze our algorithm 's performance on the natural , classical and widely studied `` planted partition '' model ( also called the stochastic block model ) ; we show that our algorithm can cluster sparser graphs , and with smaller clusters , than all previous methods . <eos> this is seen empirically as well .
we address a central problem of neuroanatomy , namely , the automatic segmentation of neuronal structures depicted in stacks of electron microscopy ( em ) images . <eos> this is necessary to efficiently map 3d brain structure and connectivity . <eos> to segment { \em biological } neuron membranes , we use a special type of deep { \em artificial } neural network as a pixel classifier . <eos> the label of each pixel ( membrane or non-membrane ) is predicted from raw pixel values in a square window centered on it . <eos> the input layer maps each window pixel to a neuron . <eos> it is followed by a succession of convolutional and max-pooling layers which preserve 2d information and extract features with increasing levels of abstraction . <eos> the output layer produces a calibrated probability for each class . <eos> the classifier is trained by plain gradient descent on a $ 512 \times 512 \times 30 $ stack with known ground truth , and tested on a stack of the same size ( ground truth unknown to the authors ) by the organizers of the isbi 2012 em segmentation challenge . <eos> even without problem-specific post-processing , our approach outperforms competing techniques by a large margin in all three considered metrics , i.e . \emph { rand error } , \emph { warping error } and \emph { pixel error } . <eos> for pixel error , our approach is the only one outperforming a second human observer .
many visual and auditory neurons have response properties that are well explained by pooling the rectified responses of a set of self-similar linear filters . <eos> these filters can not be found using spike-triggered averaging ( sta ) , which estimates only a single filter . <eos> other methods , like spike-triggered covariance ( stc ) , define a multi-dimensional response subspace , but require substantial amounts of data and do not produce unique estimates of the linear filters . <eos> rather , they provide a linear basis for the subspace in which the filters reside . <eos> here , we define a 'subunit ' model as an ln-ln cascade , in which the first linear stage is restricted to a set of shifted ( `` convolutional '' ) copies of a common filter , and the first nonlinear stage consists of rectifying nonlinearities that are identical for all filter outputs ; we refer to these initial ln elements as the 'subunits ' of the receptive field . <eos> the second linear stage then computes a weighted sum of the responses of the rectified subunits . <eos> we present a method for directly fitting this model to spike data . <eos> the method performs well for both simulated and real data ( from primate v1 ) , and the resulting model outperforms sta and stc in terms of both cross-validated accuracy and efficiency .
stochastic differential equations ( sde ) are a natural tool for modelling systems that are inherently noisy or contain uncertainties that can be modelled as stochastic processes . <eos> crucial to the process of using sde to build mathematical models is the ability to estimate parameters of those models from observed data . <eos> over the past few decades , significant progress has been made on this problem , but we are still far from having a definitive solution . <eos> we describe a novel method of approximating a diffusion process that we show to be useful in markov chain monte-carlo ( mcmc ) inference algorithms . <eos> we take the ? white noise that drives a diffusion process and decompose it into two terms . <eos> the first is a ? coloured noise term that can be deterministically controlled by a set of auxilliary variables . <eos> the second term is small and enables us to form a linear gaussian ? small noise approximation . <eos> the decomposition allows us to take a diffusion process of interest and cast it in a form that is amenable to sampling by mcmc methods . <eos> we explain why many state-of-the-art inference methods fail on highly nonlinear inference problems . <eos> we demonstrate experimentally that our method performs well in such situations . <eos> our results show that this method is a promising new tool for use in inference and parameter estimation problems .
many data such as social networks , movie preferences or knowledge bases are multi-relational , in that they describe multiple relationships between entities . <eos> while there is a large body of work focused on modeling these data , few considered modeling these multiple types of relationships jointly . <eos> further , existing approaches tend to breakdown when the number of these types grows . <eos> in this paper , we propose a method for modeling large multi-relational datasets , with possibly thousands of relations . <eos> our model is based on a bilinear structure , which captures the various orders of interaction of the data , but also shares sparse latent factors across different relations . <eos> we illustrate the performance of our approach on standard tensor-factorization datasets where we attain , or outperform , state-of-the-art results . <eos> finally , a nlp application demonstrates our scalability and the ability of our model to learn efficient , and semantically meaningful verb representations .
the cur matrix decomposition is an important extension of nystr ? m approximation to a general matrix . <eos> it approximates any data matrix in terms of a small number of its columns and rows . <eos> in this paper we propose a novel randomized cur algorithm with an expected relative-error bound . <eos> the proposed algorithm has the advantages over the existing relative-error cur algorithms that it possesses tighter theoretical bound and lower time complexity , and that it can avoid maintaining the whole data matrix in main memory . <eos> finally , experiments on several real-world datasets demonstrate significant improvement over the existing relative-error algorithms .
we propose a simple and novel framework for mcmc inference in continuous-time discrete-state systems with pure jump trajectories . <eos> we construct an exact mcmc sampler for such systems by alternately sampling a random discretization of time given a trajectory of the system , and then a new trajectory given the discretization . <eos> the first step can be performed efficiently using properties of the poisson process , while the second step can avail of discrete-time mcmc techniques based on the forward-backward algorithm . <eos> we compare our approach to particle mcmc and a uniformization-based sampler , and show its advantages .
linear support vector machines ( svms ) have become very popular in vision as part of state-of-the-art object recognition and other classification tasks but require high dimensional feature spaces for good performance . <eos> deep learning methods can find more compact representations but current methods employ multilayer perceptrons that require solving a difficult , non-convex optimization problem . <eos> we propose a deep non-linear classifier whose layers are svms and which incorporates random projection as its core stacking element . <eos> our method learns layers of linear svms recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer . <eos> our method scales as linear svms , does not rely on any kernel computations or nonconvex optimization , and exhibits better generalization ability than kernel-based svms . <eos> this is especially true when the number of training samples is smaller than the dimensionality of data , a common scenario in many real-world applications . <eos> the use of random projections is key to our method , as we show in the experiments section , in which we observe a consistent improvement over previous -- often more complicated -- methods on several vision and speech benchmarks .
recent approaches to collaborative filtering have concentrated on estimating an algebraic or statistical model , and using the model for predicting missing ratings . <eos> in this paper we observe that different models have relative advantages in different regions of the input space . <eos> this motivates our approach of using stagewise linear combinations of collaborative filtering algorithms , with non-constant combination coefficients based on kernel smoothing . <eos> the resulting stagewise model is computationally scalable and outperforms a wide selection of state-of-the-art collaborative filtering algorithms .
we develop a method for discovering the parts of an articulated object from aligned meshes capturing various three-dimensional ( 3d ) poses . <eos> we adapt the distance dependent chinese restaurant process ( ddcrp ) to allow nonparametric discovery of a potentially unbounded number of parts , while simultaneously guaranteeing a spatially connected segmentation . <eos> to allow analysis of datasets in which object instances have varying shapes , we model part variability across poses via affine transformations . <eos> by placing a matrix normal-inverse-wishart prior on these affine transformations , we develop a ddcrp gibbs sampler which tractably marginalizes over transformation uncertainty . <eos> analyzing a dataset of humans captured in dozens of poses , we infer parts which provide quantitatively better motion predictions than conventional clustering methods .
how do neural networks learn to represent information ? <eos> here , we address this question by assuming that neural networks seek to generate an optimal population representation for a fixed linear decoder . <eos> we define a loss function for the quality of the population read-out and derive the dynamical equations for both neurons and synapses from the requirement to minimize this loss . <eos> the dynamical equations yield a network of integrate-and-fire neurons undergoing hebbian plasticity . <eos> we show that , through learning , initially regular and highly correlated spike trains evolve towards poisson-distributed and independent spike trains with much lower firing rates . <eos> the learning rule drives the network into an asynchronous , balanced regime where all inputs to the network are represented optimally for the given decoder . <eos> we show that the network dynamics and synaptic plasticity jointly balance the excitation and inhibition received by each unit as tightly as possible and , in doing so , minimize the prediction error between the inputs and the decoded outputs . <eos> in turn , spikes are only signalled whenever this prediction error exceeds a certain value , thereby implementing a predictive coding scheme . <eos> our work suggests that several of the features reported in cortical networks , such as the high trial-to-trial variability , the balance between excitation and inhibition , and spike-timing dependent plasticity , are simply signatures of an efficient , spike-based code .
we describe an approach to incorporating bayesian priors in the maxq framework for hierarchical reinforcement learning ( hrl ) . <eos> we define priors on the primitive environment model and on task pseudo-rewards . <eos> since models for composite tasks can be complex , we use a mixed model-based/model-free learning approach to find an optimal hierarchical policy . <eos> we show empirically that ( i ) our approach results in improved convergence over non-bayesian baselines , given sensible priors , ( ii ) task hierarchies and bayesian priors can be complementary sources of information , and using both sources is better than either alone , ( iii ) taking advantage of the structural decomposition induced by the task hierarchy significantly reduces the computational cost of bayesian reinforcement learning and ( iv ) in this framework , task pseudo-rewards can be learned instead of being manually specified , leading to automatic learning of hierarchically optimal rather than recursively optimal policies .
in stochastic multi -- armed bandits the objective is to solve the exploration -- exploitation dilemma and ultimately maximize the expected reward . <eos> nonetheless , in many practical problems , maximizing the expected reward is not the most desirable objective . <eos> in this paper , we introduce a novel setting based on the principle of risk -- aversion where the objective is to compete against the arm with the best risk -- return trade -- off . <eos> this setting proves to be intrinsically more difficult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm . <eos> using variance as a measure of risk , we introduce two new algorithms , we investigate their theoretical guarantees , and we report preliminary empirical results .
finding maximum aposteriori ( map ) assignments in graphical models is an important task in many applications . <eos> since the problem is generally hard , linear programming ( lp ) relaxations are often used . <eos> solving these relaxations efficiently is thus an important practical problem . <eos> in recent years , several authors have proposed message passing updates corresponding to coordinate descent in the dual lp . <eos> however , these are generally not guaranteed to converge to a global optimum . <eos> one approach to remedy this is to smooth the lp , and perform coordinate descent on the smoothed dual . <eos> however , little is known about the convergence rate of this procedure . <eos> here we perform a thorough rate analysis of such schemes and derive primal and dual convergence rates . <eos> we also provide a simple dual to primal mapping that yields feasible primal solutions with a guaranteed rate of convergence . <eos> empirical evaluation supports our theoretical claims and shows that the method is highly competitive with state of the art approaches that yield global optima .
in many large economic markets , goods are sold through sequential auctions . <eos> such domains include ebay , online ad auctions , wireless spectrum auctions , and the dutch flower auctions . <eos> bidders in these domains face highly complex decision-making problems , as their preferences for outcomes in one auction often depend on the outcomes of other auctions , and bidders have limited information about factors that drive outcomes , such as other bidders ' preferences and past actions . <eos> in this work , we formulate the bidder 's problem as one of price prediction ( i.e. , learning ) and optimization . <eos> we define the concept of stable price predictions and show that ( approximate ) equilibrium in sequential auctions can be characterized as a profile of strategies that ( approximately ) optimize with respect to such ( approximately ) stable price predictions . <eos> we show how equilibria found with our formulation compare to known theoretical equilibria for simpler auction domains , and we find new approximate equilibria for a more complex auction domain where analytical solutions were heretofore unknown .
partially-observable markov decision processes ( pomdps ) provide a powerful model for real-world sequential decision-making problems . <eos> in recent years , point- based value iteration methods have proven to be extremely effective techniques for ? nding ( approximately ) optimal dynamic programming solutions to pomdps when an initial set of belief states is known . <eos> however , no point-based work has provided exact point-based backups for both continuous state and observation spaces , which we tackle in this paper . <eos> our key insight is that while there may be an in ? nite number of possible observations , there are only a ? nite number of observation partitionings that are relevant for optimal decision-making when a ? nite , ? xed set of reachable belief states is known . <eos> to this end , we make two important contributions : ( 1 ) we show how previous exact symbolic dynamic pro- gramming solutions for continuous state mdps can be generalized to continu- ous state pomdps with discrete observations , and ( 2 ) we show how this solution can be further extended via recently developed symbolic methods to continuous state and observations to derive the minimal relevant observation partitioning for potentially correlated , multivariate observation spaces . <eos> we demonstrate proof-of- concept results on uni- and multi-variate state and observation steam plant control .
in the setting of active learning for the multi-armed bandit , where the goal of a learner is to estimate with equal precision the mean of a finite number of arms , recent results show that it is possible to derive strategies based on finite-time confidence bounds that are competitive with the best possible strategy . <eos> we here consider an extension of this problem to the case when the arms are the cells of a finite partition p of a continuous sampling space x \subset \real^d . <eos> our goal is now to build a piecewise constant approximation of a noisy function ( where each piece is one region of p and p is fixed beforehand ) in order to maintain the local quadratic error of approximation on each cell equally low . <eos> although this extension is not trivial , we show that a simple algorithm based on upper confidence bounds can be proved to be adaptive to the function itself in a near-optimal way , when |p| is chosen to be of minimax-optimal order on the class of \alpha-h ? lder functions .
simple gaussian mixture models ( gmms ) learned from pixels of natural image patches have been recently shown to be surprisingly strong performers in modeling the statistics of natural images . <eos> here we provide an in depth analysis of this simple yet rich model . <eos> we show that such a gmm model is able to compete with even the most successful models of natural images in log likelihood scores , denoising performance and sample quality . <eos> we provide an analysis of what such a model learns from natural images as a function of number of mixture components -- - including covariance structure , contrast variation and intricate structures such as textures , boundaries and more . <eos> finally , we show that the salient properties of the gmm learned from natural images can be derived from a simplified dead leaves model which explicitly models occlusion , explaining its surprising success relative to other models .
early stages of visual processing are thought to decorrelate , or whiten , the incoming temporally varying signals . <eos> because the typical correlation time of natural stimuli , as well as the extent of temporal receptive fields of lateral geniculate nucleus ( lgn ) neurons , is much greater than neuronal time constants , such decorrelation must be done in stages combining contributions of multiple neurons . <eos> we propose to model temporal decorrelation in the visual pathway with the lattice filter , a signal processing device for stage-wise decorrelation of temporal signals . <eos> the stage-wise architecture of the lattice filter maps naturally onto the visual pathway ( photoreceptors - > bipolar cells - > retinal ganglion cells - > lgn ) and its filter weights can be learned using hebbian rules in a stage-wise sequential manner . <eos> moreover , predictions of neural activity from the lattice filter model are consistent with physiological measurements in lgn neurons and fruit fly second-order visual neurons . <eos> therefore , the lattice filter model is a useful abstraction that may help unravel visual system function .
when learning features for complex visual recognition problems , labeled image exemplars alone can be insufficient . <eos> while an \emph { object taxonomy } specifying the categories ' semantic relationships could bolster the learning process , not all relationships are relevant to a given visual classification task , nor does a single taxonomy capture all ties that \emph { are } relevant . <eos> in light of these issues , we propose a discriminative feature learning approach that leverages \emph { multiple } hierarchical taxonomies representing different semantic views of the object categories ( e.g. , for animal classes , one taxonomy could reflect their phylogenic ties , while another could reflect their habitats ) . <eos> for each taxonomy , we first learn a tree of semantic kernels , where each node has a mahalanobis kernel optimized to distinguish between the classes in its children nodes . <eos> then , using the resulting \emph { semantic kernel forest } , we learn class-specific kernel combinations to select only those relationships relevant to recognize each object class . <eos> to learn the weights , we introduce a novel hierarchical regularization term that further exploits the taxonomies ' structure . <eos> we demonstrate our method on challenging object recognition datasets , and show that interleaving multiple taxonomic views yields significant accuracy improvements .
the human mind has a remarkable ability to store a vast amount of information in memory , and an even more remarkable ability to retrieve these experiences when needed . <eos> understanding the representations and algorithms that underlie human memory search could potentially be useful in other information retrieval settings , including internet search . <eos> psychological studies have revealed clear regularities in how people search their memory , with clusters of semantically related items tending to be retrieved together . <eos> these findings have recently been taken as evidence that human memory search is similar to animals foraging for food in patchy environments , with people making a rational decision to switch away from a cluster of related information as it becomes depleted . <eos> we demonstrate that the results that were taken as evidence for this account also emerge from a random walk on a semantic network , much like the random web surfer model used in internet search engines . <eos> this offers a simpler and more unified account of how people search their memory , postulating a single process rather than one process for exploring a cluster and one process for switching between clusters .
robust policy optimization acknowledges that risk-aversion plays a vital role in real-world decision-making . <eos> when faced with uncertainty about the effects of actions , the policy that maximizes expected utility over the unknown parameters of the system may also carry with it a risk of intolerably poor performance . <eos> one might prefer to accept lower utility in expectation in order to avoid , or reduce the likelihood of , unacceptable levels of utility under harmful parameter realizations . <eos> in this paper , we take a bayesian approach to parameter uncertainty , but unlike other methods avoid making any distributional assumptions about the form of this uncertainty . <eos> instead we focus on identifying optimization objectives for which solutions can be efficiently approximated . <eos> we introduce percentile measures : a very general class of objectives for robust policy optimization , which encompasses most existing approaches , including ones known to be intractable . <eos> we then introduce a broad subclass of this family for which robust policies can be approximated efficiently . <eos> finally , we frame these objectives in the context of a two-player , zero-sum , extensive-form game and employ a no-regret algorithm to approximate an optimal policy , with computation only polynomial in the number of states and actions of the mdp .
in this paper we study sparsity-inducing nonconvex penalty functions using l ? evy processes . <eos> we define such a penalty as the laplace exponent of a subordinator . <eos> accordingly , we propose a novel approach for the construction of sparsityinducing nonconvex penalties . <eos> particularly , we show that the nonconvex logarithmic ( log ) and exponential ( exp ) penalty functions are the laplace exponents of gamma and compound poisson subordinators , respectively . <eos> additionally , we explore the concave conjugate of nonconvex penalties . <eos> we find that the log and exp penalties are the concave conjugates of negative kullback-leiber ( kl ) distance functions . <eos> furthermore , the relationship between these two penalties is due to asymmetricity of the kl distance .
in this paper we dicuss a novel framework for multiclass learning , defined by a suitable coding/decoding strategy , namely the simplex coding , that allows to generalize to multiple classes a relaxation approach commonly used in binary classification . <eos> in this framework a relaxation error analysis can be developed avoiding constraints on the considered hypotheses class . <eos> moreover , we show that in this setting it is possible to derive the first provably consistent regularized methods with training/tuning complexity which is { \em independent } to the number of classes . <eos> tools from convex analysis are introduced that can be used beyond the scope of this paper .
while finding the exact solution for the map inference problem is intractable for many real-world tasks , map lp relaxations have been shown to be very effective in practice . <eos> however , the most efficient methods that perform block coordinate descent can get stuck in sub-optimal points as they are not globally convergent . <eos> in this work we propose to augment these algorithms with an $ \epsilon $ -descent approach and present a method to efficiently optimize for a descent direction in the subdifferential using a margin-based extension of the fenchel-young duality theorem . <eos> furthermore , the presented approach provides a methodology to construct a primal optimal solution from its dual optimal counterpart . <eos> we demonstrate the efficiency of the presented approach on spin glass models and protein interactions problems and show that our approach outperforms state-of-the-art solvers .
we present a general method for deriving collapsed variational inference algorithms for probabilistic models in the conjugate exponential family . <eos> our method unifies many existing approaches to collapsed variational inference . <eos> our collapsed variational inference leads to a new lower bound on the marginal likelihood . <eos> we exploit the information geometry of the bound to derive much faster optimization methods based on conjugate gradients for these models . <eos> our approach is very general and is easily applied to any model where the mean field update equations have been derived . <eos> empirically we show significant speed-ups for probabilistic models optimized using our bound .
bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty , trading off exploration and exploitation in an ideal way . <eos> unfortunately , finding the resulting bayes-optimal policies is notoriously taxing , since the search space becomes enormous . <eos> in this paper we introduce a tractable , sample-based method for approximate bayes-optimal planning which exploits monte-carlo tree search . <eos> our approach outperformed prior bayesian model-based rl algorithms by a significant margin on several well-known benchmark problems -- because it avoids expensive applications of bayes rule within the search tree by lazily sampling models from the current beliefs . <eos> we illustrate the advantages of our approach by showing it working in an infinite state space domain which is qualitatively out of reach of almost all previous work in bayesian exploration .
we prove a new exponential concentration inequality for a plug-in estimator of the shannon mutual information . <eos> previous results on mutual information estimation only bounded expected error . <eos> the advantage of having the exponential inequality is that , combined with the union bound , we can guarantee accurate estimators of the mutual information for many pairs of random variables simultaneously . <eos> as an application , we show how to use such a result to optimally estimate the density function and graph of a distribution which is markov to a forest graph .
unsupervised joint alignment of images has been demonstrated to improve performance on recognition tasks such as face verification . <eos> such alignment reduces undesired variability due to factors such as pose , while only requiring weak supervision in the form of poorly aligned examples . <eos> however , prior work on unsupervised alignment of complex , real world images has required the careful selection of feature representation based on hand-crafted image descriptors , in order to achieve an appropriate , smooth optimization landscape . <eos> in this paper , we instead propose a novel combination of unsupervised joint alignment with unsupervised feature learning . <eos> specifically , we incorporate deep learning into the { \em congealing } alignment framework . <eos> through deep learning , we obtain features that can represent the image at differing resolutions based on network depth , and that are tuned to the statistics of the specific data being aligned . <eos> in addition , we modify the learning algorithm for the restricted boltzmann machine by incorporating a group sparsity penalty , leading to a topographic organization on the learned filters and improving subsequent alignment results . <eos> we apply our method to the labeled faces in the wild database ( lfw ) . <eos> using the aligned images produced by our proposed unsupervised algorithm , we achieve a significantly higher accuracy in face verification than obtained using the original face images , prior work in unsupervised alignment , and prior work in supervised alignment . <eos> we also match the accuracy for the best available , but unpublished method .
this paper studies a novel discriminative part-based model to represent and recognize object shapes with an ? and-or graph ? . <eos> we define this model consisting of three layers : the leaf-nodes with collaborative edges for localizing local parts , the or-nodes specifying the switch of leaf-nodes , and the root-node encoding the global verification . <eos> a discriminative learning algorithm , extended from the cccp [ 23 ] , is proposed to train the model in a dynamical manner : the model structure ( e.g. , the configuration of the leaf-nodes associated with the or-nodes ) is automatically determined with optimizing the multi-layer parameters during the iteration . <eos> the advantages of our method are two-fold . <eos> ( i ) the and-or graph model enables us to handle well large intra-class variance and background clutters for object shape detection from images . <eos> ( ii ) the proposed learning algorithm is able to obtain the and-or graph representation without requiring elaborate supervision and initialization . <eos> we validate the proposed method on several challenging databases ( e.g. , inria-horse , ethz-shape , and uiuc-people ) , and it outperforms the state-of-the-arts approaches .
identifying patterns from the neuroimaging recordings of brain activity related to the unobservable psychological or mental state of an individual can be treated as a unsupervised pattern recognition problem . <eos> the main challenges , however , for such an analysis of fmri data are : a ) defining a physiologically meaningful feature-space for representing the spatial patterns across time ; b ) dealing with the high-dimensionality of the data ; and c ) robustness to the various artifacts and confounds in the fmri time-series . <eos> in this paper , we present a network-aware feature-space to represent the states of a general network , that enables comparing and clustering such states in a manner that is a ) meaningful in terms of the network connectivity structure ; b ) computationally efficient ; c ) low-dimensional ; and d ) relatively robust to structured and random noise artifacts . <eos> this feature-space is obtained from a spherical relaxation of the transportation distance metric which measures the cost of transporting `` mass '' over the network to transform one function into another . <eos> through theoretical and empirical assessments , we demonstrate the accuracy and efficiency of the approximation , especially for large problems . <eos> while the application presented here is for identifying distinct brain activity patterns from fmri , this feature-space can be applied to the problem of identifying recurring patterns and detecting outliers in measurements on many different types of networks , including sensor , control and social networks .
probabilistic graphical models are powerful tools for analyzing constrained , continuous domains . <eos> however , finding most-probable explanations ( mpes ) in these models can be computationally expensive . <eos> in this paper , we improve the scalability of mpe inference in a class of graphical models with piecewise-linear and piecewise-quadratic dependencies and linear constraints over continuous domains . <eos> we derive algorithms based on a consensus-optimization framework and demonstrate their superior performance over state of the art . <eos> we show empirically that in a large-scale voter-preference modeling problem our algorithms scale linearly in the number of dependencies and constraints .
recent advances in 3d sensing technologies make it possible to easily record color and depth images which together can improve object recognition . <eos> most current methods rely on very well-designed features for this new 3d modality . <eos> we introduce a model based on a combination of convolutional and recursive neural networks ( cnn and rnn ) for learning features and classifying rgb-d images . <eos> the cnn layer learns low-level translationally invariant features which are then given as inputs to multiple , fixed-tree rnns in order to compose higher order features . <eos> rnns can be seen as combining convolution and pooling into one efficient , hierarchical operation . <eos> our main result is that even rnns with random weights compose powerful features . <eos> our model obtains state of the art performance on a standard rgb-d object dataset while being more accurate and faster during training and testing than comparable architectures such as two-layer cnns .
the shape boltzmann machine ( sbm ) has recently been introduced as a state-of-the-art model of foreground/background object shape . <eos> we extend the sbm to account for the foreground object 's parts . <eos> our model , the multinomial sbm ( msbm ) , can capture both local and global statistics of part shapes accurately . <eos> we combine the msbm with an appearance model to form a fully generative model of images of objects . <eos> parts-based image segmentations are obtained simply by performing probabilistic inference in the model . <eos> we apply the model to two challenging datasets which exhibit significant shape and appearance variability , and find that it obtains results that are comparable to the state-of-the-art .
the usability of brain computer interfaces ( bci ) based on the p300 speller is severely hindered by the need for long training times and many repetitions of the same stimulus . <eos> in this contribution we introduce a set of unsupervised hierarchical probabilistic models that tackle both problems simultaneously by incorporating prior knowledge from two sources : information from other training subjects ( through transfer learning ) and information about the words being spelled ( through language models ) . <eos> we show , that due to this prior knowledge , the performance of the unsupervised models parallels and in some cases even surpasses that of supervised models , while eliminating the tedious training session .
time delay is pervasive in neural information processing . <eos> to achieve real-time tracking , it is critical to compensate the transmission and processing delays in a neural system . <eos> in the present study we show that dynamical synapses with short-term depression can enhance the mobility of a continuous attractor network to the extent that the system tracks time-varying stimuli in a timely manner . <eos> the state of the network can either track the instantaneous position of a moving stimulus perfectly ( with zero-lag ) or lead it with an effectively constant time , in agreement with experiments on the head-direction systems in rodents . <eos> the parameter regions for delayed , perfect and anticipative tracking correspond to network states that are static , ready-to-move and spontaneously moving , respectively , demonstrating the strong correlation between tracking performance and the intrinsic dynamics of the network . <eos> we also find that when the speed of the stimulus coincides with the natural speed of the network state , the delay becomes effectively independent of the stimulus amplitude .
we study the problem of estimating a manifold from random samples . <eos> in particular , we consider piecewise constant and piecewise linear estimators induced by k-means and k- ? ats , and analyze their performance . <eos> we extend previous results for k-means in two separate directions . <eos> first , we provide new results for k-means reconstruction on manifolds and , secondly , we prove reconstruction bounds for higher-order approximation ( k- ? ats ) , for which no known results were previously available . <eos> while the results for k-means are novel , some of the technical tools are well-established in the literature . <eos> in the case of k- ? ats , both the results and the mathematical tools are new .
in this paper , we derive a novel algorithm to cluster hidden markov models ( hmms ) according to their probability distributions . <eos> we propose a variational hierarchical em algorithm that i ) clusters a given collection of hmms into groups of hmms that are similar , in terms of the distributions they represent , and ii ) characterizes each group by a `` cluster center '' , i.e. , a novel hmm that is representative for the group . <eos> we illustrate the benefits of the proposed algorithm on hierarchical clustering of motion capture sequences as well as on automatic music tagging .
we develop a probabilistic generative model for representing acoustic event structure at multiple scales via a two-stage hierarchy . <eos> the first stage consists of a spiking representation which encodes a sound with a sparse set of kernels at different frequencies positioned precisely in time . <eos> the coarse time and frequency statistical structure of the first-stage spikes is encoded by a second stage spiking representation , while fine-scale statistical regularities are encoded by recurrent interactions within the first-stage . <eos> when fitted to speech data , the model encodes acoustic features such as harmonic stacks , sweeps , and frequency modulations , that can be composed to represent complex acoustic events . <eos> the model is also able to synthesize sounds from the higher-level representation and provides significant improvement over wavelet thresholding techniques on a denoising task .
we introduce a large-volume box classification for binary prediction , which maintains a subset of weight vectors , and specifically axis-aligned boxes . <eos> our learning algorithm seeks for a box of large volume that contains `` simple '' weight vectors which most of are accurate on the training set . <eos> two versions of the learning process are cast as convex optimization problems , and it is shown how to solve them efficiently . <eos> the formulation yields a natural pac-bayesian performance bound and it is shown to minimize a quantity directly aligned with it . <eos> the algorithm outperforms svm and the recently proposed arow algorithm on a majority of $ 30 $ nlp datasets and binarized usps optical character recognition datasets .
we present a bayesian nonparametric model for genetic sequence data in which a set of genetic sequences is modelled using a markov model of partitions . <eos> the partitions at consecutive locations in the genome are related by their clusters first splitting and then merging . <eos> our model can be thought of as a discrete time analogue of continuous time fragmentation-coagulation processes [ teh et al 2011 ] , preserving the important properties of projectivity , exchangeability and reversibility , while being more scalable . <eos> we apply this model to the problem of genotype imputation , showing improved computational efficiency while maintaining the same accuracies as in [ teh et al 2011 ] .
in this work we study how the stimulus distribution influences the optimal coding of an individual neuron . <eos> closed-form solutions to the optimal sigmoidal tuning curve are provided for a neuron obeying poisson statistics under a given stimulus distribution . <eos> we consider a variety of optimality criteria , including maximizing discriminability , maximizing mutual information and minimizing estimation error under a general $ l_p $ norm . <eos> we generalize the cramer-rao lower bound and show how the $ l_p $ loss can be written as a functional of the fisher information in the asymptotic limit , by proving the moment convergence of certain functions of poisson random variables . <eos> in this manner , we show how the optimal tuning curve depends upon the loss function , and the equivalence of maximizing mutual information with minimizing $ l_p $ loss in the limit as $ p $ goes to zero .
multi-dimensional latent variable models can capture the many latent factors in a text corpus , such as topic , author perspective and sentiment . <eos> we introduce factorial lda , a multi-dimensional latent variable model in which a document is influenced by k different factors , and each word token depends on a k-dimensional vector of latent variables . <eos> our model incorporates structured word priors and learns a sparse product of factors . <eos> experiments on research abstracts show that our model can learn latent factors such as research topic , scientific discipline , and focus ( e.g . methods vs . <eos> applications . ) <eos> our modeling improvements reduce test perplexity and improve human interpretability of the discovered factors .
we study large-scale , nonsmooth , nonconconvex optimization problems . <eos> in particular , we focus on nonconvex problems with \emph { composite } objectives . <eos> this class of problems includes the extensively studied convex , composite objective problems as a special case . <eos> to tackle composite nonconvex problems , we introduce a powerful new framework based on asymptotically \emph { nonvanishing } errors , avoiding the common convenient assumption of eventually vanishing errors . <eos> within our framework we derive both batch and incremental nonconvex proximal splitting algorithms . <eos> to our knowledge , our framework is first to develop and analyze incremental \emph { nonconvex } proximal-splitting algorithms , even if we disregard the ability to handle nonvanishing errors . <eos> we illustrate our theoretical framework by showing how it applies to difficult large-scale , nonsmooth , and nonconvex problems .
we introduce a new prior for use in nonparametric bayesian hierarchical clustering . <eos> the prior is constructed by marginalizing out the time information of kingman ? s coalescent , providing a prior over tree structures which we call the time-marginalized coalescent ( tmc ) . <eos> this allows for models which factorize the tree structure and times , providing two benefits : more flexible priors may be constructed and more efficient gibbs type inference can be used . <eos> we demonstrate this on an example model for density estimation and show the tmc achieves competitive experimental results .
finding contours in natural images is a fundamental problem that serves as the basis of many tasks such as image segmentation and object recognition . <eos> at the core of contour detection technologies are a set of hand-designed gradient features , used by most existing approaches including the state-of-the-art global pb ( gpb ) operator . <eos> in this work , we show that contour detection accuracy can be significantly improved by computing sparse code gradients ( scg ) , which measure contrast using patch representations automatically learned through sparse coding . <eos> we use k-svd and orthogonal matching pursuit for efficient dictionary learning and encoding , and use multi-scale pooling and power transforms to code oriented local neighborhoods before computing gradients and applying linear svm . <eos> by extracting rich representations from pixels and avoiding collapsing them prematurely , sparse code gradients effectively learn how to measure local contrasts and find contours . <eos> we improve the f-measure metric on the bsds500 benchmark to 0.74 ( up from 0.71 of gpb contours ) . <eos> moreover , our learning approach can easily adapt to novel sensor data such as kinect-style rgb-d cameras : sparse code gradients on depth images and surface normals lead to promising contour detection using depth and depth+color , as verified on the nyu depth dataset . <eos> our work combines the concept of oriented gradients with sparse representation and opens up future possibilities for learning contour detection and segmentation .
a new methodology is developed for joint analysis of a matrix and accompanying documents , with the documents associated with the matrix rows/columns . <eos> the documents are modeled with a focused topic model , inferring latent binary features ( topics ) for each document . <eos> a new matrix decomposition is developed , with latent binary features associated with the rows/columns , and with imposition of a low-rank constraint . <eos> the matrix decomposition and topic model are coupled by sharing the latent binary feature vectors associated with each . <eos> the model is applied to roll-call data , with the associated documents defined by the legislation . <eos> state-of-the-art results are manifested for prediction of votes on a new piece of legislation , based only on the observed text legislation . <eos> the coupling of the text and legislation is also demonstrated to yield insight into the properties of the matrix decomposition for roll-call data .
this paper discusses the problem of calibrating posterior class probabilities from partially labelled data . <eos> each instance is assumed to be labelled as belonging to one of several candidate categories , at most one of them being true . <eos> we generalize the concept of proper loss to this scenario , establish a necessary and sufficient condition for a loss function to be proper , and we show a direct procedure to construct a proper loss for partial labels from a conventional proper loss . <eos> the problem can be characterized by the mixing probability matrix relating the true class of the data and the observed labels . <eos> an interesting result is that the full knowledge of this matrix is not required , and losses can be constructed that are proper in a subset of the probability simplex .
mixture distributions are often used to model complex data . <eos> in this paper , we develop a new method that jointly estimates mixture models over multiple data sets by exploiting the statistical dependencies between them . <eos> specifically , we introduce a set of latent dirichlet processes as sources of component models ( atoms ) , and for each data set , we construct a nonparametric mixture model by combining sub-sampled versions of the latent dps . <eos> each mixture model may acquire atoms from different latent dps , while each atom may be shared by multiple mixtures . <eos> this multi-to-multi association distinguishes the proposed method from prior constructions that rely on tree or chain structures , allowing mixture models to be coupled more flexibly . <eos> in addition , we derive a sampling algorithm that jointly infers the model parameters and present experiments on both document analysis and image modeling .
in many applications classification systems often require in the loop human intervention . <eos> in such cases the decision process must be transparent and comprehensible simultaneously requiring minimal assumptions on the underlying data distribution . <eos> to tackle this problem , we formulate it as an axis-alligned subspacefinding task under the assumption that query specific information dictates the complementary use of the subspaces . <eos> we develop a regression-based approach called recip that efficiently solves this problem by finding projections that minimize a nonparametric conditional entropy estimator . <eos> experiments show that the method is accurate in identifying the informative projections of the dataset , picking the correct ones to classify query points and facilitates visual evaluation by users .
a weighted graph is used as an underlying structure of many algorithms like semi-supervised learning and spectral clustering . <eos> the edge weights are usually deter-mined by a single similarity measure , but it often hard if not impossible to capture all relevant aspects of similarity when using a single similarity measure . <eos> in par-ticular , in the case of visual object matching it is beneficial to integrate different similarity measures that focus on different visual representations . <eos> in this paper , a novel approach to integrate multiple similarity measures is pro-posed . <eos> first pairs of similarity measures are combined with a diffusion process on their tensor product graph ( tpg ) . <eos> hence the diffused similarity of each pair of ob-jects becomes a function of joint diffusion of the two original similarities , which in turn depends on the neighborhood structure of the tpg . <eos> we call this process fusion with diffusion ( fd ) . <eos> however , a higher order graph like the tpg usually means significant increase in time complexity . <eos> this is not the case in the proposed approach . <eos> a key feature of our approach is that the time complexity of the dif-fusion on the tpg is the same as the diffusion process on each of the original graphs , moreover , it is not necessary to explicitly construct the tpg in our frame-work . <eos> finally all diffused pairs of similarity measures are combined as a weighted sum . <eos> we demonstrate the advantages of the proposed approach on the task of visual tracking , where different aspects of the appearance similarity between the target object in frame t and target object candidates in frame t+1 are integrated . <eos> the obtained method is tested on several challenge video sequences and the experimental results show that it outperforms state-of-the-art tracking methods .
hashing-based methods provide a very promising approach to large-scale similarity search . <eos> to obtain compact hash codes , a recent trend seeks to learn the hash functions from data automatically . <eos> in this paper , we study hash function learning in the context of multimodal data . <eos> we propose a novel multimodal hash function learning method , called co-regularized hashing ( crh ) , based on a boosted co-regularization framework . <eos> the hash functions for each bit of the hash codes are learned by solving dc ( difference of convex functions ) programs , while the learning for multiple bits proceeds via a boosting procedure so that the bias introduced by the hash functions can be sequentially minimized . <eos> we empirically compare crh with two state-of-the-art multimodal hash function learning methods on two publicly available data sets .
structured output learning has been successfully applied to object localization , where the mapping between an image and an object bounding box can be well captured . <eos> its extension to action localization in videos , however , is much more challenging , because one needs to predict the locations of the action patterns both spatially and temporally , i.e. , identifying a sequence of bounding boxes that track the action in video . <eos> the problem becomes intractable due to the exponentially large size of the structured video space where actions could occur . <eos> we propose a novel structured learning approach for spatio-temporal action localization . <eos> the mapping between a video and a spatio-temporal action trajectory is learned . <eos> the intractable inference and learning problems are addressed by leveraging an efficient max-path search method , thus makes it feasible to optimize the model over the whole structured space . <eos> experiments on two challenging benchmark datasets show that our proposed method outperforms the state-of-the-art methods .
learning the number of clusters is a key problem in data clustering . <eos> we present dip-means , a novel robust incremental method to learn the number of data clusters that may be used as a wrapper around any iterative clustering algorithm of the k-means family . <eos> in contrast to many popular methods which make assumptions about the underlying cluster distributions , dip-means only assumes a fundamental cluster property : each cluster to admit a unimodal distribution . <eos> the proposed algorithm considers each cluster member as a `` viewer '' and applies a univariate statistic hypothesis test for unimodality ( dip-test ) on the distribution of the distances between the viewer and the cluster members . <eos> two important advantages are : i ) the unimodality test is applied on univariate distance vectors , ii ) it can be directly applied with kernel-based methods , since only the pairwise distances are involved in the computations . <eos> experimental results on artificial and real datasets indicate the effectiveness of our method and its superiority over analogous approaches .
hypothesis testing on signals de ? ned on surfaces ( such as the cortical surface ) is a fundamental component of a variety of studies in neuroscience . <eos> the goal here is to identify regions that exhibit changes as a function of the clinical condition under study . <eos> as the clinical questions of interest move towards identifying very early signs of diseases , the corresponding statistical differences at the group level invariably become weaker and increasingly hard to identify . <eos> indeed , after a multiple comparisons correction is adopted ( to account for correlated statistical tests over all surface points ) , very few regions may survive . <eos> in contrast to hypothesis tests on point-wise measurements , in this paper , we make the case for performing statistical analysis on multi-scale shape descriptors that characterize the local topological context of the signal around each surface vertex . <eos> our descriptors are based on recent results from harmonic analysis , that show how wavelet theory extends to non-euclidean settings ( i.e. , irregular weighted graphs ) . <eos> we provide strong evidence that these descriptors successfully pick up group-wise differences , where traditional methods either fail or yield unsatisfactory results . <eos> other than this primary application , we show how the framework allows performing cortical surface smoothing in the native space without mappint to a unit sphere .
although many variants of stochastic gradient descent have been proposed for large-scale convex optimization , most of them require projecting the solution at { \it each } iteration to ensure that the obtained solution stays within the feasible domain . <eos> for complex domains ( e.g. , positive semidefinite cone ) , the projection step can be computationally expensive , making stochastic gradient descent unattractive for large-scale optimization problems . <eos> we address this limitation by developing a novel stochastic gradient descent algorithm that does not need intermediate projections . <eos> instead , only one projection at the last iteration is needed to obtain a feasible solution in the given domain . <eos> our theoretical analysis shows that with a high probability , the proposed algorithms achieve an $ o ( 1/\sqrt { t } ) $ convergence rate for general convex optimization , and an $ o ( \ln t/t ) $ rate for strongly convex optimization under mild conditions about the domain and the objective function .
the problem of estimation of entropy functionals of probability densities has received much attention in the information theory , machine learning and statistics communities . <eos> kernel density plug-in estimators are simple , easy to implement and widely used for estimation of entropy . <eos> however , kernel plug-in estimators suffer from the curse of dimensionality , wherein the mse rate of convergence is glacially slow - of order $ o ( t^ { - { \gamma } / { d } } ) $ , where $ t $ is the number of samples , and $ \gamma > 0 $ is a rate parameter . <eos> in this paper , it is shown that for sufficiently smooth densities , an ensemble of kernel plug-in estimators can be combined via a weighted convex combination , such that the resulting weighted estimator has a superior parametric mse rate of convergence of order $ o ( t^ { -1 } ) $ . <eos> furthermore , it is shown that these optimal weights can be determined by solving a convex optimization problem which does not require training data or knowledge of the underlying density , and therefore can be performed offline . <eos> this novel result is remarkable in that , while each of the individual kernel plug-in estimators belonging to the ensemble suffer from the curse of dimensionality , by appropriate ensemble averaging we can achieve parametric convergence rates .
category-level object detection has a crucial need for informative object representations . <eos> this demand has led to feature descriptors of ever increasing dimensionality like co-occurrence statistics and self-similarity . <eos> in this paper we propose a new object representation based on curvature self-similarity that goes beyond the currently popular approximation of objects using straight lines . <eos> however , like all descriptors using second order statistics , ours also exhibits a high dimensionality . <eos> although improving discriminability , the high dimensionality becomes a critical issue due to lack of generalization ability and curse of dimensionality . <eos> given only a limited amount of training data , even sophisticated learning algorithms such as the popular kernel methods are not able to suppress noisy or superfluous dimensions of such high-dimensional data . <eos> consequently , there is a natural need for feature selection when using present-day informative features and , particularly , curvature self-similarity . <eos> we therefore suggest an embedded feature selection method for svms that reduces complexity and improves generalization capability of object models . <eos> by successfully integrating the proposed curvature self-similarity representation together with the embedded feature selection in a widely used state-of-the-art object detection framework we show the general pertinence of the approach .
we consider infinite-horizon stationary $ \gamma $ -discounted markov decision processes , for which it is known that there exists a stationary optimal policy . <eos> using value and policy iteration with some error $ \epsilon $ at each iteration , it is well-known that one can compute stationary policies that are $ \frac { 2\gamma { ( 1-\gamma ) ^2 } \epsilon $ -optimal . <eos> after arguing that this guarantee is tight , we develop variations of value and policy iteration for computing non-stationary policies that can be up to $ \frac { 2\gamma } { 1-\gamma } \epsilon $ -optimal , which constitutes a significant improvement in the usual situation when $ \gamma $ is close to $ 1 $ . <eos> surprisingly , this shows that the problem of `` computing near-optimal non-stationary policies '' is much simpler than that of `` computing near-optimal stationary policies '' .
in many graph-based machine learning and data mining approaches , the quality of the graph is critical . <eos> however , in real-world applications , especially in semi-supervised learning and unsupervised learning , the evaluation of the quality of a graph is often expensive and sometimes even impossible , due the cost or the unavailability of ground truth . <eos> in this paper , we proposed a robust approach with convex optimization to `` forge '' a graph : with an input of a graph , to learn a graph with higher quality . <eos> our major concern is that an ideal graph shall satisfy all the following constraints : non-negative , symmetric , low rank , and positive semidefinite . <eos> we develop a graph learning algorithm by solving a convex optimization problem and further develop an efficient optimization to obtain global optimal solutions with theoretical guarantees . <eos> with only one non-sensitive parameter , our method is shown by experimental results to be robust and achieve higher accuracy in semi-supervised learning and clustering under various settings . <eos> as a preprocessing of graphs , our method has a wide range of potential applications machine learning and data mining .
a new framework based on the theory of copulas is proposed to address semi-supervised domain adaptation problems . <eos> the presented method factorizes any multivariate density into a product of marginal distributions and bivariate copula functions . <eos> therefore , changes in each of these factors can be detected and corrected to adapt a density model across different learning domains . <eos> importantly , we introduce a novel vine copula model , which allows for this factorization in a non-parametric manner . <eos> experimental results on regression problems with real-world data illustrate the efficacy of the proposed approach when compared to state-of-the-art techniques .
in this paper , we consider bayesian reinforcement learning ( brl ) where actions incur costs in addition to rewards , and thus exploration has to be constrained in terms of the expected total cost while learning to maximize the expected long-term total reward . <eos> in order to formalize cost-sensitive exploration , we use the constrained markov decision process ( cmdp ) as the model of the environment , in which we can naturally encode exploration requirements using the cost function . <eos> we extend beetle , a model-based brl method , for learning in the environment with cost constraints . <eos> we demonstrate the cost-sensitive exploration behaviour in a number of simulated problems .
how does the brain combine prior knowledge with sensory evidence when making decisions under uncertainty ? <eos> two competing descriptive models have been proposed based on experimental data . <eos> the first posits an additive offset to a decision variable , implying a static effect of the prior . <eos> however , this model is inconsistent with recent data from a motion discrimination task involving temporal integration of uncertain sensory evidence . <eos> to explain this data , a second model has been proposed which assumes a time-varying influence of the prior . <eos> here we present a normative model of decision making that incorporates prior knowledge in a principled way . <eos> we show that the additive offset model and the time-varying prior model emerge naturally when decision making is viewed within the framework of partially observable markov decision processes ( pomdps ) . <eos> decision making in the model reduces to ( 1 ) computing beliefs given observations and prior information in a bayesian manner , and ( 2 ) selecting actions based on these beliefs to maximize the expected sum of future rewards . <eos> we show that the model can explain both data previously explained using the additive offset model as well as more recent data on the time-varying influence of prior knowledge on decision making .
we consider the problem of learning control policies via trajectory preference queries to an expert . <eos> in particular , the learning agent can present an expert with short runs of a pair of policies originating from the same state and the expert then indicates the preferred trajectory . <eos> the agent 's goal is to elicit a latent target policy from the expert with as few queries as possible . <eos> to tackle this problem we propose a novel bayesian model of the querying process and introduce two methods that exploit this model to actively select expert queries . <eos> experimental results on four benchmark problems indicate that our model can effectively learn policies from trajectory preference queries and that active query selection can be substantially more efficient than random selection .
we introduce a new discrepancy score between two distributions that gives an indication on their \emph { similarity } . <eos> while much research has been done to determine if two samples come from exactly the same distribution , much less research considered the problem of determining if two finite samples come from similar distributions . <eos> the new score gives an intuitive interpretation of similarity ; it optimally perturbs the distributions so that they best fit each other . <eos> the score is defined between distributions , and can be efficiently estimated from samples . <eos> we provide convergence bounds of the estimated score , and develop hypothesis testing procedures that test if two data sets come from similar distributions . <eos> the statistical power of this procedures is presented in simulations . <eos> we also compare the score 's capacity to detect similarity with that of other known measures on real data .
multi-task learning ( mtl ) aims to improve generalization performance by learning multiple related tasks simultaneously and identifying the shared information among tasks . <eos> most of existing mtl methods focus on learning linear models under the supervised setting . <eos> we propose a novel semi-supervised and nonlinear approach for mtl using vector fields . <eos> a vector field is a smooth mapping from the manifold to the tangent spaces which can be viewed as a directional derivative of functions on the manifold . <eos> we argue that vector fields provide a natural way to exploit the geometric structure of data as well as the shared differential structure of tasks , both are crucial for semi-supervised multi-task learning . <eos> in this paper , we develop multi-task vector field learning ( mtvfl ) which learns the prediction functions and the vector fields simultaneously . <eos> mtvfl has the following key properties : ( 1 ) the vector fields we learned are close to the gradient fields of the prediction functions ; ( 2 ) within each task , the vector field is required to be as parallel as possible which is expected to span a low dimensional subspace ; ( 3 ) the vector fields from all tasks share a low dimensional subspace . <eos> we formalize our idea in a regularization framework and also provide a convex relaxation method to solve the original non-convex problem . <eos> the experimental results on synthetic and real data demonstrate the effectiveness of our proposed approach .
motivated by large-scale multimedia applications we propose to learn mappings from high-dimensional data to binary codes that preserve semantic similarity . <eos> binary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear knn search . <eos> the framework is applicable to broad families of mappings , and uses a flexible form of triplet ranking loss . <eos> we overcome discontinuous optimization of the discrete mappings by minimizing a piecewise-smooth upper bound on empirical loss , inspired by latent structural svms . <eos> we develop a new loss-augmented inference algorithm that is quadratic in the code length . <eos> we show strong retrieval performance on cifar-10 and mnist , with promising classification results using no more than knn on the binary codes .
we propose two new principal component analysis methods in this paper utilizing a semiparametric model . <eos> the according methods are named copula component analysis ( coca ) and copula pca . <eos> the semiparametric model assumes that , after unspecified marginally monotone transformations , the distributions are multivariate gaussian . <eos> the coca and copula pca accordingly estimate the leading eigenvectors of the correlation and covariance matrices of the latent gaussian distribution . <eos> the robust nonparametric rank-based correlation coefficient estimator , spearman ? s rho , is exploited in estimation . <eos> we prove that , under suitable conditions , although the marginal distributions can be arbitrarily continuous , the coca and copula pca estimators obtain fast estimation rates and are feature selection consistent in the setting where the dimension is nearly exponentially large relative to the sample size . <eos> careful numerical experiments on the synthetic and real data are conducted to back up the theoretical results . <eos> we also discuss the relationship with the transelliptical component analysis proposed by han and liu ( 2012 ) .
we introduce a new learning algorithm , named smooth-projected neighborhood pursuit , for estimating high dimensional undirected graphs . <eos> in particularly , we focus on the nonparanormal graphical model and provide theoretical guarantees for graph estimation consistency . <eos> in addition to new computational and theoretical analysis , we also provide an alternative view to analyze the tradeoff between computational efficiency and statistical error under a smoothing optimization framework . <eos> numerical results on both synthetic and real datasets are provided to support our theory .
several machine learning methods allow for abstaining from uncertain predictions . <eos> while being common for settings like conventional classification , abstention has been studied much less in learning to rank . <eos> we address abstention for the label ranking setting , allowing the learner to declare certain pairs of labels as being incomparable and , thus , to predict partial instead of total orders . <eos> in our method , such predictions are produced via thresholding the probabilities of pairwise preferences between labels , as induced by a predicted probability distribution on the set of all rankings . <eos> we formally analyze this approach for the mallows and the plackett-luce model , showing that it produces proper partial orders as predictions and characterizing the expressiveness of the induced class of partial orders . <eos> these theoretical results are complemented by experiments demonstrating the practical usefulness of the approach .
multi-agent plan recognition ( mapr ) aims to recognize dynamic team structures and team behaviors from the observed team traces ( activity sequences ) of a set of intelligent agents . <eos> previous mapr approaches required a library of team activity sequences ( team plans ) be given as input . <eos> however , collecting a library of team plans to ensure adequate coverage is often difficult and costly . <eos> in this paper , we relax this constraint , so that team plans are not required to be provided beforehand . <eos> we assume instead that a set of action models are available . <eos> such models are often already created to describe domain physics ; i.e. , the preconditions and effects of effects actions . <eos> we propose a novel approach for recognizing multi-agent team plans based on such action models rather than libraries of team plans . <eos> we encode the resulting mapr problem as a \emph { satisfiability problem } and solve the problem using a state-of-the-art weighted max-sat solver . <eos> our approach also allows for incompleteness in the observed plan traces . <eos> our empirical studies demonstrate that our algorithm is both effective and efficient in comparison to state-of-the-art mapr methods based on plan libraries .
a key function of brains is undoubtedly the abstraction and maintenance of information from the environment for later use . <eos> neurons in association cortex play an important role in this process : during learning these neurons become tuned to relevant features and represent the information that is required later as a persistent elevation of their activity . <eos> it is however not well known how these neurons acquire their task-relevant tuning . <eos> here we introduce a biologically plausible learning scheme that explains how neurons become selective for relevant information when animals learn by trial and error . <eos> we propose that the action selection stage feeds back attentional signals to earlier processing levels . <eos> these feedback signals interact with feedforward signals to form synaptic tags at those connections that are responsible for the stimulus-response mapping . <eos> a globally released neuromodulatory signal interacts with these tagged synapses to determine the sign and strength of plasticity . <eos> the learning scheme is generic because it can train networks in different tasks , simply by varying inputs and rewards . <eos> it explains how neurons in association cortex learn to ( 1 ) temporarily store task-relevant information in non-linear stimulus-response mapping tasks and ( 2 ) learn to optimally integrate probabilistic evidence for perceptual decision making .
we present a dynamic nonlinear generative model for visual motion based on a latent representation of binary-gated gaussian variables . <eos> trained on sequences of images , the model learns to represent different movement directions in different variables . <eos> we use an online approximate-inference scheme that can be mapped to the dynamics of networks of neurons . <eos> probed with drifting grating stimuli and moving bars of light , neurons in the model show patterns of responses analogous to those of direction-selective simple cells in primary visual cortex . <eos> most model neurons also show speed tuning and respond equally well to a range of motion directions and speeds aligned to the constraint line of their respective preferred speed . <eos> we show how these computations are enabled by a specific pattern of recurrent connections learned by the model .
reinforcement learning ( rl ) methods based on direct policy search ( dps ) have been actively discussed to achieve an efficient approach to complicated markov decision processes ( mdps ) . <eos> although they have brought much progress in practical applications of rl , there still remains an unsolved problem in dps related to model selection for the policy . <eos> in this paper , we propose a novel dps method , { \it weighted likelihood policy search ( wlps ) } , where a policy is efficiently learned through the weighted likelihood estimation . <eos> wlps naturally connects dps to the statistical inference problem and thus various sophisticated techniques in statistics can be applied to dps problems directly . <eos> hence , by following the idea of the { \it information criterion } , we develop a new measurement for model comparison in dps based on the weighted log-likelihood .
probabilistic planning captures the uncertainty of plan execution by probabilistically modeling the effects of actions in the environment , and therefore the probability of reaching different states from a given state and action . <eos> in order to compute a solution for a probabilistic planning problem , planners need to manage the uncertainty associated with the different paths from the initial state to a goal state . <eos> several approaches to manage uncertainty were proposed , e.g. , consider all paths at once , perform determinization of actions , and sampling . <eos> in this paper , we introduce trajectory-based short-sighted stochastic shortest path problems ( ssps ) , a novel approach to manage uncertainty for probabilistic planning problems in which states reachable with low probability are substituted by artificial goals that heuristically estimate their cost to reach a goal state . <eos> we also extend the theoretical results of short-sighted probabilistic planner ( ssipp ) [ ref ] by proving that ssipp always finishes and is asymptotically optimal under sufficient conditions on the structure of short-sighted ssps . <eos> we empirically compare ssipp using trajectory-based short-sighted ssps with the winners of the previous probabilistic planning competitions and other state-of-the-art planners in the triangle tireworld problems . <eos> trajectory-based ssipp outperforms all the competitors and is the only planner able to scale up to problem number 60 , a problem in which the optimal solution contains approximately $ 10^ { 70 } $ states .
the application of the maximum entropy principle to sequence modeling has been popularized by methods such as conditional random fields ( crfs ) . <eos> however , these approaches are generally limited to modeling paths in discrete spaces of low dimensionality . <eos> we consider the problem of modeling distributions over paths in continuous spaces of high dimensionality -- -a problem for which inference is generally intractable . <eos> our main contribution is to show that maximum entropy modeling of high-dimensional , continuous paths is tractable as long as the constrained features possess a certain kind of low dimensional structure . <eos> in this case , we show that the associated { \em partition function } is symmetric and that this symmetry can be exploited to compute the partition function efficiently in a compressed form . <eos> empirical results are given showing an application of our method to maximum entropy modeling of high dimensional human motion capture data .
we study the problem of learning local metrics for nearest neighbor classification . <eos> most previous works on local metric learning learn a number of local unrelated metrics . <eos> while this `` independence '' approach delivers an increased flexibility its downside is the considerable risk of overfitting . <eos> we present a new parametric local metric learning method in which we learn a smooth metric matrix function over the data manifold . <eos> using an approximation error bound of the metric matrix function we learn local metrics as linear combinations of basis metrics defined on anchor points over different regions of the instance space . <eos> we constrain the metric matrix function by imposing on the linear combinations manifold regularization which makes the learned metric matrix function vary smoothly along the geodesics of the data manifold . <eos> our metric learning method has excellent performance both in terms of predictive power and scalability . <eos> we experimented with several large-scale classification problems , tens of thousands of instances , and compared it with several state of the art metric learning methods , both global and local , as well as to svm with automatic kernel selection , all of which it outperforms in a significant manner .
linear chains and trees are basic building blocks in many applications of graphical models . <eos> although exact inference in these models can be performed by dynamic programming , this computation can still be prohibitively expensive with non-trivial target variable domain sizes due to the quadratic dependence on this size . <eos> standard message-passing algorithms for these problems are inefficient because they compute scores on hypotheses for which there is strong negative local evidence . <eos> for this reason there has been significant previous interest in beam search and its variants ; however , these methods provide only approximate inference . <eos> this paper presents new efficient exact inference algorithms based on the combination of it column generation and pre-computed bounds on the model 's cost structure . <eos> improving worst-case performance is impossible . <eos> however , our method substantially speeds real-world , typical-case inference in chains and trees . <eos> experiments show our method to be twice as fast as exact viterbi for wall street journal part-of-speech tagging and over thirteen times faster for a joint part-of-speed and named-entity-recognition task . <eos> our algorithm is also extendable to new techniques for approximate inference , to faster two-best inference , and new opportunities for connections between inference and learning .
we consider the problem of actively learning \textit { multi-index } functions of the form $ f ( \vecx ) = g ( \mata\vecx ) = \sum_ { i=1 } ^k g_i ( \veca_i^t\vecx ) $ from point evaluations of $ f $ . <eos> we assume that the function $ f $ is defined on an $ \ell_2 $ -ball in $ \real^d $ , $ g $ is twice continuously differentiable almost everywhere , and $ \mata \in \mathbb { r } ^ { k \times d } $ is a rank $ k $ matrix , where $ k \ll d $ . <eos> we propose a randomized , active sampling scheme for estimating such functions with uniform approximation guarantees . <eos> our theoretical developments leverage recent techniques from low rank matrix recovery , which enables us to derive an estimator of the function $ f $ along with sample complexity bounds . <eos> we also characterize the noise robustness of the scheme , and provide empirical evidence that the high-dimensional scaling of our sample complexity bounds are quite accurate .
we present a novel multilabel/ranking algorithm working in partial information settings . <eos> the algorithm is based on 2nd-order descent methods , and relies on upper-confidence bounds to trade-off exploration and exploitation . <eos> we analyze this algorithm in a partial adversarial setting , where covariates can be adversarial , but multilabel probabilities are ruled by ( generalized ) linear models . <eos> we show $ o ( t^ { 1/2 } \log t ) $ regret bounds , which improve in several ways on the existing results . <eos> we test the effectiveness of our upper-confidence scheme by contrasting against full-information baselines on real-world multilabel datasets , often obtaining comparable performance .
we advocate the use of a new distribution family ? the transelliptical ? for robust inference of high dimensional graphical models . <eos> the transelliptical family is an extension of the nonparanormal family proposed by liu et al . ( 2009 ) . <eos> just as the nonparanormal extends the normal by transforming the variables using univariate functions , the transelliptical extends the elliptical family in the same way . <eos> we propose a nonparametric rank-based regularization estimator which achieves the parametric rates of convergence for both graph recovery and parameter estimation . <eos> such a result suggests that the extra robustness and flexibility obtained by the semiparametric transelliptical modeling incurs almost no efficiency loss . <eos> we also discuss the relationship between this work with the transelliptical component analysis proposed by han and liu ( 2012 ) .
this paper concerns the problem of matrix completion , which is to estimate a matrix from observations in a small subset of indices . <eos> we propose a calibrated spectrum elastic net method with a sum of the nuclear and frobenius penalties and develop an iterative algorithm to solve the convex minimization problem . <eos> the iterative algorithm alternates between imputing the missing entries in the incomplete matrix by the current guess and estimating the matrix by a scaled soft-thresholding singular value decomposition of the imputed matrix until the resulting matrix converges . <eos> a calibration step follows to correct the bias caused by the frobenius penalty . <eos> under proper coherence conditions and for suitable penalties levels , we prove that the proposed estimator achieves an error bound of nearly optimal order and in proportion to the noise level . <eos> this provides a unified analysis of the noisy and noiseless matrix completion problems . <eos> simulation results are presented to compare our proposal with previous ones .
we trained a large , deep convolutional neural network to classify the 1.3 million high-resolution images in the lsvrc-2010 imagenet training set into the 1000 different classes . <eos> on the test data , we achieved top-1 and top-5 error rates of 39.7\ % and 18.9\ % which is considerably better than the previous state-of-the-art results . <eos> the neural network , which has 60 million parameters and 500,000 neurons , consists of five convolutional layers , some of which are followed by max-pooling layers , and two globally connected layers with a final 1000-way softmax . <eos> to make training faster , we used non-saturating neurons and a very efficient gpu implementation of convolutional nets . <eos> to reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective .
this paper presents a kernel-based discriminative learning framework on probability measures . <eos> rather than relying on large collections of vectorial training examples , our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data . <eos> by representing these probability distributions as mean embeddings in the reproducing kernel hilbert space ( rkhs ) , we are able to apply many standard kernel-based learning techniques in straightforward fashion . <eos> to accomplish this , we construct a generalization of the support vector machine ( svm ) called a support measure machine ( smm ) . <eos> our analyses of smms provides several insights into their relationship to traditional svms . <eos> based on such insights , we propose a flexible svm ( flex-svm ) that places different kernel functions on each training example . <eos> experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework .
the national epidemiologic survey on alcohol and related conditions ( nesarc ) database contains a large amount of information , regarding the way of life , medical conditions , depression , etc. , of a representative sample of the u.s. population . <eos> in the present paper , we are interested in seeking the hidden causes behind the suicide attempts , for which we propose to model the subjects using a nonparametric latent model based on the indian buffet process ( ibp ) . <eos> due to the nature of the data , we need to adapt the observation model for discrete random variables . <eos> we propose a generative model in which the observations are drawn from a multinomial-logit distribution given the ibp matrix . <eos> the implementation of an efficient gibbs sampler is accomplished using the laplace approximation , which allows us to integrate out the weighting factors of the multinomial-logit likelihood model . <eos> finally , the experiments over the nesarc database show that our model properly captures some of the hidden causes that model suicide attempts .
although human object recognition is supposedly robust to viewpoint , much research on human perception indicates that there is a preferred or ? canonical view of objects . <eos> this phenomenon was discovered more than 30 years ago but the canonical view of only a small number of categories has been validated experimentally . <eos> moreover , the explanation for why humans prefer the canonical view over other views remains elusive . <eos> in this paper we ask : can we use internet image collections to learn more about canonical views we start by manually finding the most common view in the results returned by internet search engines when queried with the objects used in psychophysical experiments . <eos> our results clearly show that the most likely view in the search engine corresponds to the same view preferred by human subjects in experiments . <eos> we also present a simple method to find the most likely view in an image collection and apply it to hundreds of categories . <eos> using the new data we have collected we present strong evidence against the two most prominent formal theories of canonical views and provide novel constraints for new theories .
we propose a high dimensional semiparametric scale-invariant principle component analysis , named tca , by utilize the natural connection between the elliptical distribution family and the principal component analysis . <eos> elliptical distribution family includes many well-known multivariate distributions like multivariate gaussian , t and logistic and it is extended to the meta-elliptical by fang et.al ( 2002 ) using the copula techniques . <eos> in this paper we extend the meta-elliptical distribution family to a even larger p family , called transelliptical . <eos> we prove that tca can obtain a near-optimal s log d/n estimation consistency rate in recovering the leading eigenvector of the latent generalized correlation matrix under the transelliptical distribution family , even if the distributions are very heavy-tailed , have infinite second moments , do not have densities and possess arbitrarily continuous marginal distributions . <eos> a feature selection result with explicit rate is also provided . <eos> tca is further implemented in both numerical simulations and largescale stock data to illustrate its empirical usefulness . <eos> both theories and experiments confirm that tca can achieve model flexibility , estimation accuracy and robustness at almost no cost .
the primary application of collaborate filtering ( cf ) is to recommend a small set of items to a user , which entails ranking . <eos> most approaches , however , formulate the cf problem as rating prediction , overlooking the ranking perspective . <eos> in this work we present a method for collaborative ranking that leverages the strengths of the two main cf approaches , neighborhood- and model-based . <eos> our novel method is highly efficient , with only seventeen parameters to optimize and a single hyperparameter to tune , and beats the state-of-the-art collaborative ranking methods . <eos> we also show that parameters learned on one dataset yield excellent results on a very different dataset , without any retraining .
the accurate prediction of molecular energetics in chemical compound space is a crucial ingredient for rational compound design . <eos> the inherently graph-like , non-vectorial nature of molecular data gives rise to a unique and difficult machine learning problem . <eos> in this paper , we adopt a learning-from-scratch approach where quantum-mechanical molecular energies are predicted directly from the raw molecular geometry . <eos> the study suggests a benefit from setting flexible priors and enforcing invariance stochastically rather than structurally . <eos> our results improve the state-of-the-art by a factor of almost three , bringing statistical methods one step closer to the holy grail of `` chemical accuracy '' .
this paper focuses on the problem of learning binary embeddings for efficient retrieval of high-dimensional non-negative data . <eos> such data typically arises in a large number of vision and text applications where counts or frequencies are used as features . <eos> also , cosine distance is commonly used as a measure of dissimilarity between such vectors . <eos> in this work , we introduce a novel spherical quantization scheme to generate binary embedding of such data and analyze its properties . <eos> the number of quantization landmarks in this scheme grows exponentially with data dimensionality resulting in low-distortion quantization . <eos> we propose a very efficient method for computing the binary embedding using such large number of landmarks . <eos> further , a linear transformation is learned to minimize the quantization error by adapting the method to the input data resulting in improved embedding . <eos> experiments on image and text retrieval applications show superior performance of the proposed method over other existing state-of-the-art methods .
we present a new learning strategy based on an efficient blocked gibbs sampler for sparse overcomplete linear models . <eos> particular emphasis is placed on statistical image modeling , where overcomplete models have played an important role in discovering sparse representations . <eos> our gibbs sampler is faster than general purpose sampling schemes while also requiring no tuning as it is free of parameters . <eos> using the gibbs sampler and a persistent variant of expectation maximization , we are able to extract highly sparse distributions over latent sources from data . <eos> when applied to natural images , our algorithm learns source distributions which resemble spike-and-slab distributions . <eos> we evaluate the likelihood and quantitatively compare the performance of the overcomplete linear model to its complete counterpart as well as a product of experts model , which represents another overcomplete generalization of the complete linear model . <eos> in contrast to previous claims , we find that overcomplete representations lead to significant improvements , but that the overcomplete linear model still underperforms other models .
we propose a novel stochastic process that is with probability $ \alpha_i $ being absorbed at current state $ i $ , and with probability $ 1-\alpha_i $ follows a random edge out of it . <eos> we analyze its properties and show its potential for exploring graph structures . <eos> we prove that under proper absorption rates , a random walk starting from a set $ \mathcal { s } $ of low conductance will be mostly absorbed in $ \mathcal { s } $ . <eos> moreover , the absorption probabilities vary slowly inside $ \mathcal { s } $ , while dropping sharply outside $ \mathcal { s } $ , thus implementing the desirable cluster assumption for graph-based learning . <eos> remarkably , the partially absorbing process unifies many popular models arising in a variety of contexts , provides new insights into them , and makes it possible for transferring findings from one paradigm to another . <eos> simulation results demonstrate its promising applications in graph-based learning .
we present a bayesian nonparametric model that discovers implicit social structure from interaction time-series data . <eos> social groups are often formed implicitly , through actions among members of groups . <eos> yet many models of social networks use explicitly declared relationships to infer social structure . <eos> we consider a particular class of hawkes processes , a doubly stochastic point process , that is able to model reciprocity between groups of individuals . <eos> we then extend the infinite relational model by using these reciprocating hawkes processes to parameterise its edges , making events associated with edges co-dependent through time . <eos> our model outperforms general , unstructured hawkes processes as well as structured poisson process-based models at predicting verbal and email turn-taking , and military conflicts among nations .
statistical learning and sequential prediction are two different but related formalisms to study the quality of predictions . <eos> mapping out their relations and transferring ideas is an active area of investigation . <eos> we provide another piece of the puzzle by showing that an important concept in sequential prediction , the mixability of a loss , has a natural counterpart in the statistical setting , which we call stochastic mixability . <eos> just as ordinary mixability characterizes fast rates for the worst-case regret in sequential prediction , stochastic mixability characterizes fast rates in statistical learning . <eos> we show that , in the special case of log-loss , stochastic mixability reduces to a well-known ( but usually unnamed ) martingale condition , which is used in existing convergence theorems for minimum description length and bayesian inference . <eos> in the case of 0/1-loss , it reduces to the margin condition of mammen and tsybakov , and in the case that the model under consideration contains all possible predictors , it is equivalent to ordinary mixability .
latent linear dynamical systems with generalised-linear observation models arise in a variety of applications , for example when modelling the spiking activity of populations of neurons . <eos> here , we show how spectral learning methods for linear systems with gaussian observations ( usually called subspace identification in this context ) can be extended to estimate the parameters of dynamical system models observed through non-gaussian noise models . <eos> we use this approach to obtain estimates of parameters for a dynamical model of neural population data , where the observed spike-counts are poisson-distributed with log-rates determined by the latent dynamical process , possibly driven by external inputs . <eos> we show that the extended system identification algorithm is consistent and accurately recovers the correct parameters on large simulated data sets with much smaller computational cost than approximate expectation-maximisation ( em ) due to the non-iterative nature of subspace identification . <eos> even on smaller data sets , it provides an effective initialization for em , leading to more robust performance and faster convergence . <eos> these benefits are shown to extend to real neural data .
we develop a novel bayesian nonparametric model for random bipartite graphs . <eos> the model is based on the theory of completely random measures and is able to handle a potentially infinite number of nodes . <eos> we show that the model has appealing properties and in particular it may exhibit a power-law behavior . <eos> we derive a posterior characterization , an indian buffet-like generative process for network growth , and a simple and efficient gibbs sampler for posterior simulation . <eos> our model is shown to be well fitted to several real-world social networks .
while human listeners excel at selectively attending to a conversation in a cocktail party , machine performance is still far inferior by comparison . <eos> we show that the cocktail party problem , or the speech separation problem , can be effectively approached via structured prediction . <eos> to account for temporal dynamics in speech , we employ conditional random fields ( crfs ) to classify speech dominance within each time-frequency unit for a sound mixture . <eos> to capture complex , nonlinear relationship between input and output , both state and transition feature functions in crfs are learned by deep neural networks . <eos> the formulation of the problem as classification allows us to directly optimize a measure that is well correlated with human speech intelligibility . <eos> the proposed system substantially outperforms existing ones in a variety of noises .
a number of dependent nonparametric processes have been proposed to model non-stationary data with unknown latent dimensionality . <eos> however , the inference algorithms are often slow and unwieldy , and are in general highly specific to a given model formulation . <eos> in this paper , we describe a wide class of nonparametric processes , including several existing models , and present a slice sampler that allows efficient inference across this class of models .
in this paper , we introduce two novel metric learning algorithms , ? 2-lmnn and gb-lmnn , which are explicitly designed to be non-linear and easy-to-use . <eos> the two approaches achieve this goal in fundamentally different ways : ? 2-lmnn inherits the computational benefits of a linear mapping from linear metric learning , but uses a non-linear ? 2-distance to explicitly capture similarities within histogram data sets ; gb-lmnn applies gradient-boosting to learn non-linear mappings directly in function space and takes advantage of this approach 's robustness , speed , parallelizability and insensitivity towards the single additional hyper-parameter . <eos> on various benchmark data sets , we demonstrate these methods not only match the current state-of-the-art in terms of knn classification error , but in the case of ? 2-lmnn , obtain best results in 19 out of 20 learning settings .
the representer theorem is a property that lies at the foundation of regularization theory and kernel methods . <eos> a class of regularization functionals is said to admit a linear representer theorem if every member of the class admits minimizers that lie in the finite dimensional subspace spanned by the representers of the data . <eos> a recent characterization states that certain classes of regularization functionals with differentiable regularization term admit a linear representer theorem for any choice of the data if and only if the regularization term is a radial nondecreasing function . <eos> in this paper , we extend such result by weakening the assumptions on the regularization term . <eos> in particular , the main result of this paper implies that , for a sufficiently large family of regularization functionals , radial nondecreasing functions are the only lower semicontinuous regularization terms that guarantee existence of a representer theorem for any choice of the data .
in this paper we seek to detect rectangular cuboids and localize their corners in uncalibrated single-view images depicting everyday scenes . <eos> in contrast to recent approaches that rely on detecting vanishing points of the scene and grouping line segments to form cuboids , we build a discriminative parts-based detector that models the appearance of the cuboid corners and internal edges while enforcing consistency to a 3d cuboid model . <eos> our model is invariant to the different 3d viewpoints and aspect ratios and is able to detect cuboids across many different object categories . <eos> we introduce a database of images with cuboid annotations that spans a variety of indoor and outdoor scenes and show qualitative and quantitative results on our collected database . <eos> our model out-performs baseline detectors that use 2d constraints alone on the task of localizing cuboid corners .
we propose an approach to multivariate nonparametric regression that generalizes reduced rank regression for linear models . <eos> an additive model is estimated for each dimension of a $ q $ -dimensional response , with a shared $ p $ -dimensional predictor variable . <eos> to control the complexity of the model , we employ a functional form of the ky-fan or nuclear norm , resulting in a set of function estimates that have low rank . <eos> backfitting algorithms are derived and justified using a nonparametric form of the nuclear norm subdifferential . <eos> oracle inequalities on excess risk are derived that exhibit the scaling behavior of the procedure in the high dimensional setting . <eos> the methods are illustrated on gene expression data .
symmetric positive definite ( spd ) matrices are remarkably pervasive in a multitude of scientific disciplines , including machine learning and optimization . <eos> we consider the fundamental task of measuring distances between two spd matrices ; a task that is often nontrivial whenever an application demands the distance function to respect the non-euclidean geometry of spd matrices . <eos> unfortunately , typical non-euclidean distance measures such as the riemannian metric $ \riem ( x , y ) =\frob { \log ( x\inv { y } ) } $ , are computationally demanding and also complicated to use . <eos> to allay some of these difficulties , we introduce a new metric on spd matrices : this metric not only respects non-euclidean geometry , it also offers faster computation than $ \riem $ while being less complicated to use . <eos> we support our claims theoretically via a series of theorems that relate our metric to $ \riem ( x , y ) $ , and experimentally by studying the nonconvex problem of computing matrix geometric means based on squared distances .
nonnegative matrix factorization ( nmf ) is a promising relaxation technique for clustering analysis . <eos> however , conventional nmf methods that directly approximate the pairwise similarities using the least square error often yield mediocre performance for data in curved manifolds because they can capture only the immediate similarities between data samples . <eos> here we propose a new nmf clustering method which replaces the approximated matrix with its smoothed version using random walk . <eos> our method can thus accommodate farther relationships between data samples . <eos> furthermore , we introduce a novel regularization in the proposed objective function in order to improve over spectral clustering . <eos> the new learning objective is optimized by a multiplicative majorization-minimization algorithm with a scalable implementation for learning the factorizing matrix . <eos> extensive experimental results on real-world datasets show that our method has strong performance in terms of cluster purity .
most existing hashing methods adopt some projection functions to project the original data into several dimensions of real values , and then each of these projected dimensions is quantized into one bit ( zero or one ) by thresholding . <eos> typically , the variances of different projected dimensions are different for existing projection functions such as principal component analysis ( pca ) . <eos> using the same number of bits for different projected dimensions is unreasonable because larger-variance dimensions will carry more information . <eos> although this viewpoint has been widely accepted by many researchers , it is still not verified by either theory or experiment because no methods have been proposed to find a projection with equal variances for different dimensions . <eos> in this paper , we propose a novel method , called isotropic hashing ( isohash ) , to learn projection functions which can produce projected dimensions with isotropic variances ( equal variances ) . <eos> experimental results on real data sets show that isohash can outperform its counterpart with different variances for different dimensions , which verifies the viewpoint that projections with isotropic variances will be better than those with anisotropic variances .
sign-random-projection locality-sensitive hashing ( srp-lsh ) is a probabilistic dimension reduction method which provides an unbiased estimate of angular similarity , yet suffers from the large variance of its estimation . <eos> in this work , we propose the super-bit locality-sensitive hashing ( sblsh ) . <eos> it is easy to implement , which orthogonalizes the random projection vectors in batches , and it is theoretically guaranteed that sblsh also provides an unbiased estimate of angular similarity , yet with a smaller variance when the angle to estimate is within $ ( 0 , \pi/2 ] $ . <eos> the extensive experiments on real data well validate that given the same length of binary code , sblsh may achieve significant mean squared error reduction in estimating pairwise angular similarity . <eos> moreover , sblsh shows the superiority over srp-lsh in approximate nearest neighbor ( ann ) retrieval experiments .
in this paper we apply boosting to learn complex non-linear local visual feature representations , drawing inspiration from its successful application to visual object detection . <eos> the main goal of local feature descriptors is to distinctively represent a salient image region while remaining invariant to viewpoint and illumination changes . <eos> this representation can be improved using machine learning , however , past approaches have been mostly limited to learning linear feature mappings in either the original input or a kernelized input feature space . <eos> while kernelized methods have proven somewhat effective for learning non-linear local feature descriptors , they rely heavily on the choice of an appropriate kernel function whose selection is often difficult and non-intuitive . <eos> we propose to use the boosting-trick to obtain a non-linear mapping of the input to a high-dimensional feature space . <eos> the non-linear feature mapping obtained with the boosting-trick is highly intuitive . <eos> we employ gradient-based weak learners resulting in a learned descriptor that closely resembles the well-known sift . <eos> as demonstrated in our experiments , the resulting descriptor can be learned directly from intensity patches achieving state-of-the-art performance .
in the conventional approaches for supervised parametric learning , relations between data and target variables are provided through training sets consisting of pairs of corresponded data and target variables . <eos> in this work , we describe a new learning scheme for parametric learning , in which the target variables $ \y $ can be modeled with a prior model $ p ( \y ) $ and the relations between data and target variables are estimated through $ p ( \y ) $ and a set of uncorresponded data $ \x $ in training . <eos> we term this method as learning with target priors ( ltp ) . <eos> specifically , ltp learning seeks parameter $ \t $ that maximizes the log likelihood of $ f_\t ( \x ) $ on a uncorresponded training set with regards to $ p ( \y ) $ . <eos> compared to the conventional ( semi ) supervised learning approach , ltp can make efficient use of prior knowledge of the target variables in the form of probabilistic distributions , and thus removes/reduces the reliance on training data in learning . <eos> compared to the bayesian approach , the learned parametric regressor in ltp can be more efficiently implemented and deployed in tasks where running efficiency is critical , such as on-line bci signal decoding . <eos> we demonstrate the effectiveness of the proposed approach on parametric regression tasks for bci signal decoding and pose estimation from video .
we present a novel marginalized particle gaussian process ( mpgp ) regression , which provides a fast , accurate online bayesian filtering framework to model the latent function . <eos> using a state space model established by the data construction procedure , our mpgp recursively filters out the estimation of hidden function values by a gaussian mixture . <eos> meanwhile , it provides a new online method for training hyperparameters with a number of weighted particles . <eos> we demonstrate the estimated performance of our mpgp on both simulated and real large data sets . <eos> the results show that our mpgp is a robust estimation algorithm with high computational efficiency , which outperforms other state-of-art sparse gp methods .
we consider unsupervised estimation of mixtures of discrete graphical models , where the class variable is hidden and each mixture component can have a potentially different markov graph structure and parameters over the observed variables . <eos> we propose a novel method for estimating the mixture components with provable guarantees . <eos> our output is a tree-mixture model which serves as a good approximation to the underlying graphical model mixture . <eos> the sample and computational requirements for our method scale as $ \poly ( p , r ) $ , for an $ r $ -component mixture of $ p $ -variate graphical models , for a wide class of models which includes tree mixtures and mixtures over bounded degree graphs .
in the paper , we consider the problem of link prediction in time-evolving graphs . <eos> we assume that certain graph features , such as the node degree , follow a vector autoregressive ( var ) model and we propose to use this information to improve the accuracy of prediction . <eos> our strategy involves a joint optimization procedure over the space of adjacency matrices and var matrices which takes into account both sparsity and low rank properties of the matrices . <eos> oracle inequalities are derived and illustrate the trade-offs in the choice of smoothing parameters when modeling the joint effect of sparsity and low rank property . <eos> the estimate is computed efficiently using proximal methods through a generalized forward-backward agorithm .
links between probabilistic and non-probabilistic learning algorithms can arise by performing small-variance asymptotics , i.e. , letting the variance of particular distributions in a graphical model go to zero . <eos> for instance , in the context of clustering , such an approach yields precise connections between the k-means and em algorithms . <eos> in this paper , we explore small-variance asymptotics for exponential family dirichlet process ( dp ) and hierarchical dirichlet process ( hdp ) mixture models . <eos> utilizing connections between exponential family distributions and bregman divergences , we derive novel clustering algorithms from the asymptotic limit of the dp and hdp mixtures that feature the scalability of existing hard clustering methods as well as the flexibility of bayesian nonparametric models . <eos> we focus on special cases of our analysis for discrete-data problems , including topic modeling , and we demonstrate the utility of our results by applying variants of our algorithms to problems arising in vision and document analysis .
we study how to automatically select and adapt multiple abstractions or representations of the world to support model-based reinforcement learning . <eos> we address the challenges of transfer learning in heterogeneous environments with varying tasks . <eos> we present an efficient , online framework that , through a sequence of tasks , learns a set of relevant representations to be used in future tasks . <eos> without pre-defined mapping strategies , we introduce a general approach to support transfer learning across different state spaces . <eos> we demonstrate the potential impact of our system through improved jumpstart and faster convergence to near optimum policy in two benchmark domains .
the partition function plays a key role in probabilistic modeling including conditional random fields , graphical models , and maximum likelihood estimation . <eos> to optimize partition functions , this article introduces a quadratic variational upper bound . <eos> this inequality facilitates majorization methods : optimization of complicated functions through the iterative solution of simpler sub-problems . <eos> such bounds remain efficient to compute even when the partition function involves a graphical model ( with small tree-width ) or in latent likelihood settings . <eos> for large-scale problems , low-rank versions of the bound are provided and outperform lbfgs as well as first-order methods . <eos> several learning applications are shown and reduce to fast and convergent update rules . <eos> experimental results show advantages over state-of-the-art optimization methods .
accurate and detailed models of the progression of neurodegenerative diseases such as alzheimer 's ( ad ) are crucially important for reliable early diagnosis and the determination and deployment of effective treatments . <eos> in this paper , we introduce the alpaca ( alzheimer 's disease probabilistic cascades ) model , a generative model linking latent alzheimer 's progression dynamics to observable biomarker data . <eos> in contrast with previous works which model disease progression as a fixed ordering of events , we explicitly model the variability over such orderings among patients which is more realistic , particularly for highly detailed disease progression models . <eos> we describe efficient learning algorithms for alpaca and discuss promising experimental results on a real cohort of alzheimer 's patients from the alzheimer 's disease neuroimaging initiative .
if a piece of information is released from a media site , can it spread , in 1 month , to a million web pages ? <eos> this influence estimation problem is very challenging since both the time-sensitive nature of the problem and the issue of scalability need to be addressed simultaneously . <eos> in this paper , we propose a randomized algorithm for influence estimation in continuous-time diffusion networks . <eos> our algorithm can estimate the influence of every node in a network with $ |\vcal| $ nodes and $ |\ecal| $ edges to an accuracy of $ \epsilon $ using $ n=o ( 1/\epsilon^2 ) $ randomizations and up to logarithmic factors $ o ( n|\ecal|+n|\vcal| ) $ computations . <eos> when used as a subroutine in a greedy influence maximization algorithm , our proposed method is guaranteed to find a set of nodes with an influence of at least $ ( 1 - 1/e ) \operatorname { opt } - 2\epsilon $ , where $ \operatorname { opt } $ is the optimal value . <eos> experiments on both synthetic and real-world data show that the proposed method can easily scale up to networks of millions of nodes while significantly improves over previous state-of-the-arts in terms of the accuracy of the estimated influence and the quality of the selected nodes in maximizing the influence .
the adaptive anonymity problem is formalized where each individual shares their data along with an integer value to indicate their personal level of desired privacy . <eos> this problem leads to a generalization of $ k $ -anonymity to the $ b $ -matching setting . <eos> novel algorithms and theory are provided to implement this type of anonymity . <eos> the relaxation achieves better utility , admits theoretical privacy guarantees that are as strong , and , most importantly , accommodates a variable level of anonymity for each individual . <eos> empirical results confirm improved utility on benchmark and social data-sets .
tensor completion from incomplete observations is a problem of significant practical interest . <eos> however , it is unlikely that there exists an efficient algorithm with provable guarantee to recover a general tensor from a limited number of observations . <eos> in this paper , we study the recovery algorithm for pairwise interaction tensors , which has recently gained considerable attention for modeling multiple attribute data due to its simplicity and effectiveness . <eos> specifically , in the absence of noise , we show that one can exactly recover a pairwise interaction tensor by solving a constrained convex program which minimizes the weighted sum of nuclear norms of matrices from $ o ( nr\log^2 ( n ) ) $ observations . <eos> for the noisy cases , we also prove error bounds for a constrained convex program for recovering the tensors . <eos> our experiments on the synthetic dataset demonstrate that the recovery performance of our algorithm agrees well with the theory . <eos> in addition , we apply our algorithm on a temporal collaborative filtering task and obtain state-of-the-art results .
motivated by an application in computational biology , we consider constrained low-rank matrix factorization problems with $ \ { 0,1\ } $ -constraints on one of the factors . <eos> in addition to the the non-convexity shared with more general matrix factorization schemes , our problem is further complicated by a combinatorial constraint set of size $ 2^ { m \cdot r } $ , where $ m $ is the dimension of the data points and $ r $ the rank of the factorization . <eos> despite apparent intractability , we provide $ - $ in the line of recent work on non-negative matrix factorization by arora et al.~ ( 2012 ) $ - $ an algorithm that provably recovers the underlying factorization in the exact case with operations of the order $ o ( m r 2^r + mnr ) $ in the worst case . <eos> to obtain that result , we invoke theory centered around a fundamental result in combinatorics , the littlewood-offord lemma .
lifted inference algorithms exploit symmetries in probabilistic models to speed up inference . <eos> they show impressive performance when calculating unconditional probabilities in relational models , but often resort to non-lifted inference when computing conditional probabilities . <eos> the reason is that conditioning on evidence breaks many of the model 's symmetries , which preempts standard lifting techniques . <eos> recent theoretical results show , for example , that conditioning on evidence which corresponds to binary relations is # p-hard , suggesting that no lifting is to be expected in the worst case . <eos> in this paper , we balance this grim result by identifying the boolean rank of the evidence as a key parameter for characterizing the complexity of conditioning in lifted inference . <eos> in particular , we show that conditioning on binary evidence with bounded boolean rank is efficient . <eos> this opens up the possibility of approximating evidence by a low-rank boolean matrix factorization , which we investigate both theoretically and empirically .
finite-state transducers ( fst ) are a standard tool for modeling paired input-output sequences and are used in numerous applications , ranging from computational biology to natural language processing . <eos> recently balle et al . presented a spectral algorithm for learning fst from samples of aligned input-output sequences . <eos> in this paper we address the more realistic , yet challenging setting where the alignments are unknown to the learning algorithm . <eos> we frame fst learning as finding a low rank hankel matrix satisfying constraints derived from observable statistics . <eos> under this formulation , we provide identifiability results for fst distributions . <eos> then , following previous work on rank minimization , we propose a regularized convex relaxation of this objective which is based on minimizing a nuclear norm penalty subject to linear constraints and can be solved efficiently .
the proximal map is the key step in gradient-type algorithms , which have become prevalent in large-scale high-dimensional problems . <eos> for simple functions this proximal map is available in closed-form while for more complicated functions it can become highly nontrivial . <eos> motivated by the need of combining regularizers to simultaneously induce different types of structures , this paper initiates a systematic investigation of when the proximal map of a sum of functions decomposes into the composition of the proximal maps of the individual summands . <eos> we not only unify a few known results scattered in the literature but also discover several new decompositions obtained almost effortlessly from our theory .
typical blur from camera shake often deviates from the standard uniform convolutional assumption , in part because of problematic rotations which create greater blurring away from some unknown center point . <eos> consequently , successful blind deconvolution for removing shake artifacts requires the estimation of a spatially-varying or non-uniform blur operator . <eos> using ideas from bayesian inference and convex analysis , this paper derives a non-uniform blind deblurring algorithm with several desirable , yet previously-unexplored attributes . <eos> the underlying objective function includes a spatially-adaptive penalty that couples the latent sharp image , non-uniform blur operator , and noise level together . <eos> this coupling allows the penalty to automatically adjust its shape based on the estimated degree of local blur and image structure such that regions with large blur or few prominent edges are discounted . <eos> remaining regions with modest blur and revealing edges therefore dominate the overall estimation process without explicitly incorporating structure-selection heuristics . <eos> the algorithm can be implemented using an optimization strategy that is virtually parameter free and simpler than existing methods . <eos> detailed theoretical analysis and empirical validation on real images serve to validate the proposed method .
sparse subspace clustering ( ssc ) and low-rank representation ( lrr ) are both considered as the state-of-the-art methods for { \em subspace clustering } . <eos> the two methods are fundamentally similar in that both are convex optimizations exploiting the intuition of self-expressiveness '' . <eos> the main difference is that ssc minimizes the vector $ \ell_1 $ norm of the representation matrix to induce sparsity while lrr minimizes nuclear norm ( aka trace norm ) to promote a low-rank structure . <eos> because the representation matrix is often simultaneously sparse and low-rank , we propose a new algorithm , termed low-rank sparse subspace clustering ( lrssc ) , by combining ssc and lrr , and develops theoretical guarantees of when the algorithm succeeds . <eos> the results reveal interesting insights into the strength and weakness of ssc and lrr and demonstrate how lrssc can take the advantages of both methods in preserving the `` self-expressiveness property '' and `` graph connectivity '' at the same time . ''
in the matrix completion problem the aim is to recover an unknown real matrix from a subset of its entries . <eos> this problem comes up in many application areas , and has received a great deal of attention in the context of the netflix prize . <eos> a central approach to this problem is to output a matrix of lowest possible complexity ( e.g . rank or trace norm ) that agrees with the partially specified matrix . <eos> the performance of this approach under the assumption that the revealed entries are sampled randomly has received considerable attention . <eos> in practice , often the set of revealed entries is not chosen at random and these results do not apply . <eos> we are therefore left with no guarantees on the performance of the algorithm we are using . <eos> we present a means to obtain performance guarantees with respect to any set of initial observations . <eos> the first step remains the same : find a matrix of lowest possible complexity that agrees with the partially specified matrix . <eos> we give a new way to interpret the output of this algorithm by next finding a probability distribution over the non-revealed entries with respect to which a bound on the generalization error can be proven . <eos> the more complex the set of revealed entries according to a certain measure , the better the bound on the generalization error .
latent variable prediction models , such as multi-layer networks , impose auxiliary latent variables between inputs and outputs to allow automatic inference of implicit features useful for prediction . <eos> unfortunately , such models are difficult to train because inference over latent variables must be performed concurrently with parameter optimization -- -creating a highly non-convex problem . <eos> instead of proposing another local training method , we develop a convex relaxation of hidden-layer conditional models that admits global training . <eos> our approach extends current convex modeling approaches to handle two nested nonlinearities separated by a non-trivial adaptive latent layer . <eos> the resulting methods are able to acquire two-layer models that can not be represented by any single-layer model over the same features , while improving training quality over local heuristics .
there are two major routes to address linear inverse problems . <eos> whereas regularization-based approaches build estimators as solutions of penalized regression optimization problems , bayesian estimators rely on the posterior distribution of the unknown , given some assumed family of priors . <eos> while these may seem radically different approaches , recent results have shown that , in the context of additive white gaussian denoising , the bayesian conditional mean estimator is always the solution of a penalized regression problem . <eos> the contribution of this paper is twofold . <eos> first , we extend the additive white gaussian denoising results to general linear inverse problems with colored gaussian noise . <eos> second , we characterize conditions under which the penalty function associated to the conditional mean estimator can satisfy certain popular properties such as convexity , separability , and smoothness . <eos> this sheds light on some tradeoff between computational efficiency and estimation accuracy in sparse regularization , and draws some connections between bayesian estimation and proximal optimization .
in this paper we focus on the principal component regression and its application to high dimension non-gaussian data . <eos> the major contributions are in two folds . <eos> first , in low dimensions and under a double asymptotic framework where both the dimension $ d $ and sample size $ n $ can increase , by borrowing the strength from recent development in minimax optimal principal component estimation , we first time sharply characterize the potential advantage of classical principal component regression over least square estimation under the gaussian model . <eos> secondly , we propose and analyze a new robust sparse principal component regression on high dimensional elliptically distributed data . <eos> the elliptical distribution is a semiparametric generalization of the gaussian , including many well known distributions such as multivariate gaussian , rank-deficient gaussian , $ t $ , cauchy , and logistic . <eos> it allows the random vector to be heavy tailed and have tail dependence . <eos> these extra flexibilities make it very suitable for modeling finance and biomedical imaging data . <eos> under the elliptical model , we prove that our method can estimate the regression coefficients in the optimal parametric rate and therefore is a good alternative to the gaussian based methods . <eos> experiments on synthetic and real world data are conducted to illustrate the empirical usefulness of the proposed method .
a successful approach to structured learning is to write the learning objective as a joint function of linear parameters and inference messages , and iterate between updates to each . <eos> this paper observes that if the inference problem is smoothedthrough the addition of entropy terms , for fixed messages , the learning objective reduces to a traditional ( non-structured ) logistic regression problem with respect to parameters . <eos> in these logistic regression problems , each training example has a bias term determined by the current set of messages . <eos> based on this insight , the structured energy function can be extended from linear factors to any function class where an oracleexists to minimize a logistic loss .
it has long been recognised that statistical dependencies in neuronal activity need to be taken into account when decoding stimuli encoded in a neural population . <eos> less studied , though equally pernicious , is the need to take account of dependencies between synaptic weights when decoding patterns previously encoded in an auto-associative memory . <eos> we show that activity-dependent learning generically produces such correlations , and failing to take them into account in the dynamics of memory retrieval leads to catastrophically poor recall . <eos> we derive optimal network dynamics for recall in the face of synaptic correlations caused by a range of synaptic plasticity rules . <eos> these dynamics involve well-studied circuit motifs , such as forms of feedback inhibition and experimentally observed dendritic nonlinearities . <eos> we therefore show how addressing the problem of synaptic correlations leads to a novel functional account of key biophysical features of the neural substrate .
an incredible gulf separates theoretical models of synapses , often described solely by a single scalar value denoting the size of a postsynaptic potential , from the immense complexity of molecular signaling pathways underlying real synapses . <eos> to understand the functional contribution of such molecular complexity to learning and memory , it is essential to expand our theoretical conception of a synapse from a single scalar to an entire dynamical system with many internal molecular functional states . <eos> moreover , theoretical considerations alone demand such an expansion ; network models with scalar synapses assuming finite numbers of distinguishable synaptic strengths have strikingly limited memory capacity . <eos> this raises the fundamental question , how does synaptic complexity give rise to memory ? <eos> to address this , we develop new mathematical theorems elucidating the relationship between the structural organization and memory properties of complex synapses that are themselves molecular networks . <eos> moreover , in proving such theorems , we uncover a framework , based on first passage time theory , to impose an order on the internal states of complex synaptic models , thereby simplifying the relationship between synaptic structure and function .
shannon 's entropy is a basic quantity in information theory , and a fundamental building block for the analysis of neural codes . <eos> estimating the entropy of a discrete distribution from samples is an important and difficult problem that has received considerable attention in statistics and theoretical neuroscience . <eos> however , neural responses have characteristic statistical structure that generic entropy estimators fail to exploit . <eos> for example , existing bayesian entropy estimators make the naive assumption that all spike words are equally likely a priori , which makes for an inefficient allocation of prior probability mass in cases where spikes are sparse . <eos> here we develop bayesian estimators for the entropy of binary spike trains using priors designed to flexibly exploit the statistical structure of simultaneously-recorded spike responses . <eos> we define two prior distributions over spike words using mixtures of dirichlet distributions centered on simple parametric models . <eos> the parametric model captures high-level statistical features of the data , such as the average spike count in a spike word , which allows the posterior over entropy to concentrate more rapidly than with standard estimators ( e.g. , in cases where the probability of spiking differs strongly from 0.5 ) . <eos> conversely , the dirichlet distributions assign prior mass to distributions far from the parametric model , ensuring consistent estimates for arbitrary distributions . <eos> we devise a compact representation of the data and prior that allow for computationally efficient implementations of bayesian least squares and empirical bayes entropy estimators with large numbers of neurons . <eos> we apply these estimators to simulated and real neural data and show that they substantially outperform traditional methods .
simultaneous recordings of the activity of large neural populations are extremely valuable as they can be used to infer the dynamics and interactions of neurons in a local circuit , shedding light on the computations performed . <eos> it is now possible to measure the activity of hundreds of neurons using 2-photon calcium imaging . <eos> however , many computations are thought to involve circuits consisting of thousands of neurons , such as cortical barrels in rodent somatosensory cortex . <eos> here we contribute a statistical method for stitching '' together sequentially imaged sets of neurons into one model by phrasing the problem as fitting a latent dynamical system with missing observations . <eos> this method allows us to substantially expand the population-sizes for which population dynamics can be characterized -- -beyond the number of simultaneously imaged neurons . <eos> in particular , we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible to predict noise correlations between non-simultaneously recorded neuron pairs . ''
recent advances in associative memory design through structured pattern sets and graph-based inference algorithms have allowed reliable learning and recall of an exponential number of patterns . <eos> although these designs correct external errors in recall , they assume neurons that compute noiselessly , in contrast to the highly variable neurons in hippocampus and olfactory cortex . <eos> here we consider associative memories with noisy internal computations and analytically characterize performance . <eos> as long as the internal noise level is below a specified threshold , the error probability in the recall phase can be made exceedingly small . <eos> more surprisingly , we show that internal noise actually improves the performance of the recall phase . <eos> computational experiments lend additional support to our theoretical analysis . <eos> this work suggests a functional benefit to noisy neurons in biological neuronal networks .
the olfactory system faces a difficult inference problem : it has to determine what odors are present based on the distributed activation of its receptor neurons . <eos> here we derive neural implementations of two approximate inference algorithms that could be used by the brain . <eos> one is a variational algorithm ( which builds on the work of beck . <eos> et al. , 2012 ) , the other is based on sampling . <eos> importantly , we use a more realistic prior distribution over odors than has been used in the past : we use a spike and slab '' prior , for which most odors have zero concentration . <eos> after mapping the two algorithms onto neural dynamics , we find that both can infer correct odors in less than 100 ms , although it takes ~500 ms to eliminate false positives . <eos> thus , at the behavioral level , the two algorithms make very similar predictions . <eos> however , they make different assumptions about connectivity and neural computations , and make different predictions about neural activity . <eos> thus , they should be distinguishable experimentally . <eos> if so , that would provide insight into the mechanisms employed by the olfactory system , and , because the two algorithms use very different coding strategies , that would also provide insight into how networks represent probabilities . ''
population neural recordings with long-range temporal structure are often best understood in terms of a shared underlying low-dimensional dynamical process . <eos> advances in recording technology provide access to an ever larger fraction of the population , but the standard computational approaches available to identify the collective dynamics scale poorly with the size of the dataset . <eos> here we describe a new , scalable approach to discovering the low-dimensional dynamics that underlie simultaneously recorded spike trains from a neural population . <eos> our method is based on recurrent linear models ( rlms ) , and relates closely to timeseries models based on recurrent neural networks . <eos> we formulate rlms for neural data by generalising the kalman-filter-based likelihood calculation for latent linear dynamical systems ( lds ) models to incorporate a generalised-linear observation process . <eos> we show that rlms describe motor-cortical population data better than either directly-coupled generalised-linear models or latent linear dynamical system models with generalised-linear observations . <eos> we also introduce the cascaded linear model ( clm ) to capture low-dimensional instantaneous correlations in neural populations . <eos> the clm describes the cortical recordings better than either ising or gaussian models and , like the rlm , can be fit exactly and quickly . <eos> the clm can also be seen as a generalization of a low-rank gaussian model , in this case factor analysis . <eos> the computational tractability of the rlm and clm allow both to scale to very high-dimensional neural data .
dropout is a relatively new algorithm for training neural networks which relies on stochastically dropping out '' neurons during training in order to avoid the co-adaptation of feature detectors . <eos> we introduce a general formalism for studying dropout on either units or connections , with arbitrary probability values , and use it to analyze the averaging and regularizing properties of dropout in both linear and non-linear networks . <eos> for deep neural networks , the averaging properties of dropout are characterized by three recursive equations , including the approximation of expectations by normalized weighted geometric means . <eos> we provide estimates and bounds for these approximations and corroborate the results with simulations . <eos> we also show in simple cases how dropout performs stochastic gradient descent on a regularized error function . ''
many powerful monte carlo techniques for estimating partition functions , such as annealed importance sampling ( ais ) , are based on sampling from a sequence of intermediate distributions which interpolate between a tractable initial distribution and an intractable target distribution . <eos> the near-universal practice is to use geometric averages of the initial and target distributions , but alternative paths can perform substantially better . <eos> we present a novel sequence of intermediate distributions for exponential families : averaging the moments of the initial and target distributions . <eos> we derive an asymptotically optimal piecewise linear schedule for the moments path and show that it performs at least as well as geometric averages with a linear schedule . <eos> moment averaging performs well empirically at estimating partition functions of restricted boltzmann machines ( rbms ) , which form the building blocks of many deep learning models , including deep belief networks and deep boltzmann machines .
for data assumed to come from a finite mixture with an unknown number of components , it has become common to use dirichlet process mixtures ( dpms ) not only for density estimation , but also for inferences about the number of components . <eos> the typical approach is to use the posterior distribution on the number of components occurring so far -- - that is , the posterior on the number of clusters in the observed data . <eos> however , it turns out that this posterior is not consistent -- - it does not converge to the true number of components . <eos> in this note , we give an elementary demonstration of this inconsistency in what is perhaps the simplest possible setting : a dpm with normal components of unit variance , applied to data from a mixture '' with one standard normal component . <eos> further , we find that this example exhibits severe inconsistency : instead of going to 1 , the posterior probability that there is one cluster goes to 0 . ''
the idea of computer vision as the bayesian inverse problem to computer graphics has a long history and an appealing elegance , but it has proved difficult to directly implement . <eos> instead , most vision tasks are approached via complex bottom-up processing pipelines . <eos> here we show that it is possible to write short , simple probabilistic graphics programs that define flexible generative models and to automatically invert them to interpret real-world images . <eos> generative probabilistic graphics programs consist of a stochastic scene generator , a renderer based on graphics software , a stochastic likelihood model linking the renderer 's output and the data , and latent variables that adjust the fidelity of the renderer and the tolerance of the likelihood model . <eos> representations and algorithms from computer graphics , originally designed to produce high-quality images , are instead used as the deterministic backbone for highly approximate and stochastic generative models . <eos> this formulation combines probabilistic programming , computer graphics , and approximate bayesian computation , and depends only on general-purpose , automatic inference techniques . <eos> we describe two applications : reading sequences of degraded and adversarially obscured alphanumeric characters , and inferring 3d road models from vehicle-mounted camera images . <eos> each of the probabilistic graphics programs we present relies on under 20 lines of probabilistic code , and supports accurate , approximately bayesian inferences about ambiguous real-world images .
dropout and other feature noising schemes control overfitting by artificially corrupting the training data . <eos> for generalized linear models , dropout performs a form of adaptive regularization . <eos> using this viewpoint , we show that the dropout regularizer is first-order equivalent to an $ \lii $ regularizer applied after scaling the features by an estimate of the inverse diagonal fisher information matrix . <eos> we also establish a connection to adagrad , an online learner , and find that a close relative of adagrad operates by repeatedly solving linear dropout-regularized problems . <eos> by casting dropout as regularization , we develop a natural semi-supervised algorithm that uses unlabeled data to create a better adaptive regularizer . <eos> we apply this idea to document classification tasks , and show that it consistently boosts the performance of dropout training , improving on state-of-the-art results on the imdb reviews dataset .
in this paper we investigate the use of langevin monte carlo methods on the probability simplex and propose a new method , stochastic gradient riemannian langevin dynamics , which is simple to implement and can be applied online . <eos> we apply this method to latent dirichlet allocation in an online setting , and demonstrate that it achieves substantial performance improvements to the state of the art online variational bayesian methods .
distributions over exchangeable matrices with infinitely many columns are useful in constructing nonparametric latent variable models . <eos> however , the distribution implied by such models over the number of features exhibited by each data point may be poorly-suited for many modeling tasks . <eos> in this paper , we propose a class of exchangeable nonparametric priors obtained by restricting the domain of existing models . <eos> such models allow us to specify the distribution over the number of features per data point , and can achieve better performance on data sets where the number of features is not well-modeled by the original distribution .
we propose an approximate inference algorithm for continuous time gaussian-markov process models with both discrete and continuous time likelihoods . <eos> we show that the continuous time limit of the expectation propagation algorithm exists and results in a hybrid fixed point iteration consisting of ( 1 ) expectation propagation updates for the discrete time terms and ( 2 ) variational updates for the continuous time term . <eos> we introduce corrections methods that improve on the marginals of the approximation . <eos> this approach extends the classical kalman-bucy smoothing procedure to non-gaussian observations , enabling continuous-time inference in a variety of models , including spiking neuronal models ( state-space models with point process observations ) and box likelihood models . <eos> experimental results on real and simulated data demonstrate high distributional accuracy and significant computational savings compared to discrete-time approaches in a neural application .
we propose a general formalism of iterated random functions with semigroup property , under which exact and approximate bayesian posterior updates can be viewed as specific instances . <eos> a convergence theory for iterated random functions is presented . <eos> as an application of the general theory we analyze convergence behaviors of exact and approximate message-passing algorithms that arise in a sequential change point detection problem formulated via a latent variable directed graphical model . <eos> the sequential inference algorithm and its supporting theory are illustrated by simulated examples .
psychologists are interested in developing instructional policies that boost student learning . <eos> an instructional policy specifies the manner and content of instruction . <eos> for example , in the domain of concept learning , a policy might specify the nature of exemplars chosen over a training sequence . <eos> traditional psychological studies compare several hand-selected policies , e.g. , contrasting a policy that selects only difficult-to-classify exemplars with a policy that gradually progresses over the training sequence from easy exemplars to more difficult ( known as { \em fading } ) . <eos> we propose an alternative to the traditional methodology in which we define a parameterized space of policies and search this space to identify the optimum policy . <eos> for example , in concept learning , policies might be described by a fading function that specifies exemplar difficulty over time . <eos> we propose an experimental technique for searching policy spaces using gaussian process surrogate-based optimization and a generative model of student performance . <eos> instead of evaluating a few experimental conditions each with many human subjects , as the traditional methodology does , our technique evaluates many experimental conditions each with a few subjects . <eos> even though individual subjects provide only a noisy estimate of the population mean , the optimization method allows us to determine the shape of the policy space and identify the global optimum , and is as efficient in its subject budget as a traditional a-b comparison . <eos> we evaluate the method via two behavioral studies , and suggest that the method has broad applicability to optimization problems involving humans in domains beyond the educational arena .
many attempts to understand the success of simple decision heuristics have examined heuristics as an approximation to a linear decision rule . <eos> this research has identified three environmental structures that aid heuristics : dominance , cumulative dominance , and noncompensatoriness . <eos> here , we further develop these ideas and examine their empirical relevance in 51 natural environments . <eos> we find that all three structures are prevalent , making it possible for some simple rules to reach the accuracy levels of the linear decision rule using less information .
we study the problem of estimating continuous quantities , such as prices , probabilities , and point spreads , using a crowdsourcing approach . <eos> a challenging aspect of combining the crowd 's answers is that workers ' reliabilities and biases are usually unknown and highly diverse . <eos> control items with known answers can be used to evaluate workers ' performance , and hence improve the combined results on the target items with unknown answers . <eos> this raises the problem of how many control items to use when the total number of items each workers can answer is limited : more control items evaluates the workers better , but leaves fewer resources for the target items that are of direct interest , and vice versa . <eos> we give theoretical results for this problem under different scenarios , and provide a simple rule of thumb for crowdsourcing practitioners . <eos> as a byproduct , we also provide theoretical analysis of the accuracy of different consensus methods .
we develop an inference and optimal design procedure for recovering synaptic weights in neural microcircuits . <eos> we base our procedure on data from an experiment in which populations of putative presynaptic neurons can be stimulated while a subthreshold recording is made from a single postsynaptic neuron . <eos> we present a realistic statistical model which accounts for the main sources of variability in this experiment and allows for large amounts of information about the biological system to be incorporated if available . <eos> we then present a simpler model to facilitate online experimental design which entails the use of efficient bayesian inference . <eos> the optimized approach results in equal quality posterior estimates of the synaptic weights in roughly half the number of experimental trials under experimentally realistic conditions , tested on synthetic data generated from the full model .
multitask learning can be effective when features useful in one task are also useful for other tasks , and the group lasso is a standard method for selecting a common subset of features . <eos> in this paper , we are interested in a less restrictive form of multitask learning , wherein ( 1 ) the available features can be organized into subsets according to a notion of similarity and ( 2 ) features useful in one task are similar , but not necessarily identical , to the features best suited for other tasks . <eos> the main contribution of this paper is a new procedure called { \em sparse overlapping sets ( sos ) lasso } , a convex optimization that automatically selects similar features for related learning tasks . <eos> error bounds are derived for soslasso and its consistency is established for squared error loss . <eos> in particular , soslasso is motivated by multi-subject fmri studies in which functional activity is classified using brain voxels as features . <eos> experiments with real and synthetic data demonstrate the advantages of soslasso compared to the lasso and group lasso .
lasso is a widely used regression technique to find sparse representations . <eos> when the dimension of the feature space and the number of samples are extremely large , solving the lasso problem remains challenging . <eos> to improve the efficiency of solving large-scale lasso problems , el ghaoui and his colleagues have proposed the safe rules which are able to quickly identify the inactive predictors , i.e. , predictors that have $ 0 $ components in the solution vector . <eos> then , the inactive predictors or features can be removed from the optimization problem to reduce its scale . <eos> by transforming the standard lasso to its dual form , it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution . <eos> in this paper , we propose an efficient and effective screening rule via dual polytope projections ( dpp ) , which is mainly based on the uniqueness and nonexpansiveness of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope . <eos> moreover , we show that our screening rule can be extended to identify inactive groups in group lasso . <eos> to the best of our knowledge , there is currently no exact '' screening rule for group lasso . <eos> we have evaluated our screening rule using many real data sets . <eos> results show that our rule is more effective to identify inactive predictors than existing state-of-the-art screening rules for lasso . ''
we introduce kernel nonparametric tests for lancaster three-variable interaction and for total independence , using embeddings of signed measures into a reproducing kernel hilbert space . <eos> the resulting test statistics are straightforward to compute , and are used in powerful three-variable interaction tests , which are consistent against all alternatives for a large family of reproducing kernels . <eos> we show the lancaster test to be sensitive to cases where two independent causes individually have weak influence on a third dependent variable , but their combined effect has a strong influence . <eos> this makes the lancaster test especially suited to finding structure in directed graphical models , where it outperforms competing nonparametric tests in detecting such v-structures .
we propose a parameter server system for distributed ml , which follows a stale synchronous parallel ( ssp ) model of computation that maximizes the time computational workers spend doing useful work on ml algorithms , while still providing correctness guarantees . <eos> the parameter server provides an easy-to-use shared interface for read/write access to an ml model 's values ( parameters and variables ) , and the ssp model allows distributed workers to read older , stale versions of these values from a local cache , instead of waiting to get them from a central storage . <eos> this significantly increases the proportion of time workers spend computing , as opposed to waiting . <eos> furthermore , the ssp model ensures ml algorithm correctness by limiting the maximum age of the stale values . <eos> we provide a proof of correctness under ssp , as well as empirical results demonstrating that the ssp model achieves faster algorithm convergence on several different ml problems , compared to fully-synchronous and asynchronous schemes .
incorporating invariance information is important for many learning problems . <eos> to exploit invariances , most existing methods resort to approximations that either lead to expensive optimization problems such as semi-definite programming , or rely on separation oracles to retain tractability . <eos> some methods further limit the space of functions and settle for non-convex models . <eos> in this paper , we propose a framework for learning in reproducing kernel hilbert spaces ( rkhs ) using local invariances that explicitly characterize the behavior of the target function around data instances . <eos> these invariances are \emph { compactly } encoded as linear functionals whose value are penalized by some loss function . <eos> based on a representer theorem that we establish , our formulation can be efficiently optimized via a convex program . <eos> for the representer theorem to hold , the linear functionals are required to be bounded in the rkhs , and we show that this is true for a variety of commonly used rkhs and invariances . <eos> experiments on learning with unlabeled data and transform invariances show that the proposed method yields better or similar results compared with the state of the art .
we use the notion of local rademacher complexity to design new algorithms for learning kernels . <eos> our algorithms thereby benefit from the sharper learning bounds based on that notion which , under certain general conditions , guarantee a faster convergence rate . <eos> we devise two new learning kernel algorithms : one based on a convex optimization problem for which we give an efficient solution using existing learning kernel techniques , and another one that can be formulated as a dc-programming problem for which we describe a solution in detail . <eos> we also report the results of experiments with both algorithms in both binary and multi-class classification tasks .
we address the problem of estimating the ratio $ \frac { q } { p } $ where $ p $ is a density function and $ q $ is another density , or , more generally an arbitrary function . <eos> knowing or approximating this ratio is needed in various problems of inference and integration , in particular , when one needs to average a function with respect to one probability distribution , given a sample from another . <eos> it is often referred as { \it importance sampling } in statistical inference and is also closely related to the problem of { \it covariate shift } in transfer learning as well as to various mcmc methods . <eos> our approach is based on reformulating the problem of estimating the ratio as an inverse problem in terms of an integral operator corresponding to a kernel , and thus reducing it to an integral equation , known as the fredholm problem of the first kind . <eos> this formulation , combined with the techniques of regularization and kernel methods , leads to a principled kernel-based framework for constructing algorithms and for analyzing them theoretically . <eos> the resulting family of algorithms ( fire , for fredholm inverse regularized estimator ) is flexible , simple and easy to implement . <eos> we provide detailed theoretical analysis including concentration bounds and convergence rates for the gaussian kernel for densities defined on $ \r^d $ and smooth $ d $ -dimensional sub-manifolds of the euclidean space . <eos> model selection for unsupervised or semi-supervised inference is generally a difficult problem . <eos> interestingly , it turns out that in the density ratio estimation setting , when samples from both distributions are available , there are simple completely unsupervised methods for choosing parameters . <eos> we call this model selection mechanism cd-cv for cross-density cross-validation . <eos> finally , we show encouraging experimental results including applications to classification within the covariate shift framework .
we consider the problem of maintaining the data-structures of a partition-based regression procedure in a setting where the training data arrives sequentially over time . <eos> we prove that it is possible to maintain such a structure in time $ o ( \log n ) $ at any time step $ n $ while achieving a nearly-optimal regression rate of $ \tilde { o } ( n^ { -2/ ( 2+d ) } ) $ in terms of the unknown metric dimension $ d $ . <eos> finally we prove a new regression lower-bound which is independent of a given data size , and hence is more appropriate for the streaming setting .
we address the problem of deciding whether there exists a consistent estimator of a given relation q , when data are missing not at random . <eos> we employ a formal representation called ` missingness graphs ' to explicitly portray the causal mechanisms responsible for missingness and to encode dependencies between these mechanisms and the variables being measured . <eos> using this representation , we define the notion of \textit { recoverability } which ensures that , for a given missingness-graph $ g $ and a given query $ q $ an algorithm exists such that in the limit of large samples , it produces an estimate of $ q $ \textit { as if } no data were missing . <eos> we further present conditions that the graph should satisfy in order for recoverability to hold and devise algorithms to detect the presence of these conditions .
we consider the stochastic approximation problem where a convex function has to be minimized , given only the knowledge of unbiased estimates of its gradients at certain points , a framework which includes machine learning methods based on the minimization of the empirical risk . <eos> we focus on problems without strong convexity , for which all previously known algorithms achieve a convergence rate for function values of $ o ( 1/\sqrt { n } ) $ . <eos> we consider and analyze two algorithms that achieve a rate of $ o ( 1/n ) $ for classical supervised learning problems . <eos> for least-squares regression , we show that averaged stochastic gradient descent with constant step-size achieves the desired rate . <eos> for logistic regression , this is achieved by a simple novel stochastic gradient algorithm that ( a ) constructs successive local quadratic approximations of the loss functions , while ( b ) preserving the same running time complexity as stochastic gradient descent . <eos> for these algorithms , we provide a non-asymptotic analysis of the generalization error ( in expectation , and also in high probability for least-squares ) , and run extensive experiments showing that they often outperform existing approaches .
recent work on molecular programming has explored new possibilities for computational abstractions with biomolecules , including logic gates , neural networks , and linear systems . <eos> in the future such abstractions might enable nanoscale devices that can sense and control the world at a molecular scale . <eos> just as in macroscale robotics , it is critical that such devices can learn about their environment and reason under uncertainty . <eos> at this small scale , systems are typically modeled as chemical reaction networks . <eos> in this work , we develop a procedure that can take arbitrary probabilistic graphical models , represented as factor graphs over discrete random variables , and compile them into chemical reaction networks that implement inference . <eos> in particular , we show that marginalization based on sum-product message passing can be implemented in terms of reactions between chemical species whose concentrations represent probabilities . <eos> we show algebraically that the steady state concentration of these species correspond to the marginal distributions of the random variables in the graph and validate the results in simulations . <eos> as with standard sum-product inference , this procedure yields exact results for tree-structured graphs , and approximate solutions for loopy graphs .
we establish minimax risk lower bounds for distributed statistical estimation given a budget $ b $ of the total number of bits that may be communicated . <eos> such lower bounds in turn reveal the minimum amount of communication required by any procedure to achieve the classical optimal rate for statistical estimation . <eos> we study two classes of protocols in which machines send messages either independently or interactively . <eos> the lower bounds are established for a variety of problems , from estimating the mean of a population to estimating parameters in linear regression or binary classification .
we present pac-bayes-empirical-bernstein inequality . <eos> the inequality is based on combination of pac-bayesian bounding technique with empirical bernstein bound . <eos> it allows to take advantage of small empirical variance and is especially useful in regression . <eos> we show that when the empirical variance is significantly smaller than the empirical loss pac-bayes-empirical-bernstein inequality is significantly tighter than pac-bayes-kl inequality of seeger ( 2002 ) and otherwise it is comparable . <eos> pac-bayes-empirical-bernstein inequality is an interesting example of application of pac-bayesian bounding technique to self-bounding functions . <eos> we provide empirical comparison of pac-bayes-empirical-bernstein inequality with pac-bayes-kl inequality on a synthetic example and several uci datasets .
we establish theoretical results concerning all local optima of various regularized m-estimators , where both loss and penalty functions are allowed to be nonconvex . <eos> our results show that as long as the loss function satisfies restricted strong convexity and the penalty function satisfies suitable regularity conditions , any local optimum of the composite objective function lies within statistical precision of the true parameter vector . <eos> our theory covers a broad class of nonconvex objective functions , including corrected versions of the lasso for errors-in-variables linear models ; regression in generalized linear models using nonconvex regularizers such as scad and mcp ; and graph and inverse covariance matrix estimation . <eos> on the optimization side , we show that a simple adaptation of composite gradient descent may be used to compute a global optimum up to the statistical precision epsilon in log ( 1/epsilon ) iterations , which is the fastest possible rate of any first-order method . <eos> we provide a variety of simulations to illustrate the sharpness of our theoretical predictions .
the increased availability of data in recent years led several authors to ask whether it is possible to use data as a { \em computational } resource . <eos> that is , if more data is available , beyond the sample complexity limit , is it possible to use the extra examples to speed up the computation time required to perform the learning task ? <eos> we give the first positive answer to this question for a { \em natural supervised learning problem } -- - we consider agnostic pac learning of halfspaces over $ 3 $ -sparse vectors in $ \ { -1,1,0\ } ^n $ . <eos> this class is inefficiently learnable using $ o\left ( n/\epsilon^2\right ) $ examples . <eos> our main contribution is a novel , non-cryptographic , methodology for establishing computational-statistical gaps , which allows us to show that , under a widely believed assumption that refuting random $ \mathrm { 3cnf } $ formulas is hard , efficiently learning this class using $ o\left ( n/\epsilon^2\right ) $ examples is impossible . <eos> we further show that under stronger hardness assumptions , even $ o\left ( n^ { 1.499 } /\epsilon^2\right ) $ examples do not suffice . <eos> on the other hand , we show a new algorithm that learns this class efficiently using $ \tilde { \omega } \left ( n^2/\epsilon^2\right ) $ examples . <eos> this formally establishes the tradeoff between sample and computational complexity for a natural supervised learning problem .
the design of convex , calibrated surrogate losses , whose minimization entails consistency with respect to a desired target loss , is an important concept to have emerged in the theory of machine learning in recent years . <eos> we give an explicit construction of a convex least-squares type surrogate loss that can be designed to be calibrated for any multiclass learning problem for which the target loss matrix has a low-rank structure ; the surrogate loss operates on a surrogate target space of dimension at most the rank of the target loss . <eos> we use this result to design convex calibrated surrogates for a variety of subset ranking problems , with target losses including the precision @ q , expected rank utility , mean average precision , and pairwise disagreement .
we investigate the relationship between three fundamental problems in machine learning : binary classification , bipartite ranking , and binary class probability estimation ( cpe ) . <eos> it is known that a good binary cpe model can be used to obtain a good binary classification model ( by thresholding at 0.5 ) , and also to obtain a good bipartite ranking model ( by using the cpe model directly as a ranking model ) ; it is also known that a binary classification model does not necessarily yield a cpe model . <eos> however , not much is known about other directions . <eos> formally , these relationships involve regret transfer bounds . <eos> in this paper , we introduce the notion of weak regret transfer bounds , where the mapping needed to transform a model from one problem to another depends on the underlying probability distribution ( and in practice , must be estimated from data ) . <eos> we then show that , in this weaker sense , a good bipartite ranking model can be used to construct a good classification model ( by thresholding at a suitable point ) , and more surprisingly , also to construct a good binary cpe model ( by calibrating the scores of the ranking model ) .
we consider the partial observability model for multi-armed bandits , introduced by mannor and shamir ( 2011 ) . <eos> our main result is a characterization of regret in the directed observability model in terms of the dominating and independence numbers of the observability graph . <eos> we also show that in the undirected case , the learner can achieve optimal regret without even accessing the observability graph before selecting an action . <eos> both results are shown using variants of the exp3 algorithm operating on the observability graph in a time-efficient manner .
this paper considers the sample complexity of the multi-armed bandit with dependencies among the arms . <eos> some of the most successful algorithms for this problem use the principle of optimism in the face of uncertainty to guide exploration . <eos> the clearest example of this is the class of upper confidence bound ( ucb ) algorithms , but recent work has shown that a simple posterior sampling algorithm , sometimes called thompson sampling , also shares a close theoretical connection with optimistic approaches . <eos> in this paper , we develop a regret bound that holds for both classes of algorithms . <eos> this bound applies broadly and can be specialized to many model classes . <eos> it depends on a new notion we refer to as the eluder dimension , which measures the degree of dependence among action rewards . <eos> compared to ucb algorithm regret bounds for specific model classes , our general bound matches the best available for linear models and is stronger than the best available for generalized linear models .
we consider the design of strategies for \emph { market making } in a market like a stock , commodity , or currency exchange . <eos> in order to obtain profit guarantees for a market maker one typically requires very particular stochastic assumptions on the sequence of price fluctuations of the asset in question . <eos> we propose a class of spread-based market making strategies whose performance can be controlled even under worst-case ( adversarial ) settings . <eos> we prove structural properties of these strategies which allows us to design a master algorithm which obtains low regret relative to the best such strategy in hindsight . <eos> we run a set of experiments showing favorable performance on real-world price data .
we investigate two new optimization problems ? <eos> minimizing a submodular function subject to a submodular lower bound constraint ( submodular cover ) and maximizing a submodular function subject to a submodular upper bound constraint ( submodular knapsack ) . <eos> we are motivated by a number of real-world applications in machine learning including sensor placement and data subset selection , which require maximizing a certain submodular function ( like coverage or diversity ) while simultaneously minimizing another ( like cooperative cost ) . <eos> these problems are often posed as minimizing the difference between submodular functions [ 9 , 23 ] which is in the worst case inapproximable . <eos> we show , however , that by phrasing these problems as constrained optimization , which is more natural for many applications , we achieve a number of bounded approximation guarantees . <eos> we also show that both these problems are closely related and , an approximation algorithm solving one can be used to obtain an approximation guarantee for the other . <eos> we provide hardness results for both problems thus showing that our approximation factors are tight up to log-factors . <eos> finally , we empirically demonstrate the performance and good scalability properties of our algorithms .
we consider a popular problem in finance , option pricing , through the lens of an online learning game between nature and an investor . <eos> in the black-scholes option pricing model from 1973 , the investor can continuously hedge the risk of an option by trading the underlying asset , assuming that the asset 's price fluctuates according to geometric brownian motion ( gbm ) . <eos> we consider a worst-case model , in which nature chooses a sequence of price fluctuations under a cumulative quadratic volatility constraint , and the investor can make a sequence of hedging decisions . <eos> our main result is to show that the value of our proposed game , which is the regret '' of hedging strategy , converges to the black-scholes option price . <eos> we use significantly weaker assumptions than previous work -- -for instance , we allow large jumps in the asset price -- -and show that the black-scholes hedging strategy is near-optimal for the investor even in this non-stochastic framework . ''
small-variance asymptotics provide an emerging technique for obtaining scalable combinatorial algorithms from rich probabilistic models . <eos> we present a small-variance asymptotic analysis of the hidden markov model and its infinite-state bayesian nonparametric extension . <eos> starting with the standard hmm , we first derive a hardinference algorithm analogous to k-means that arises when particular variances in the model tend to zero . <eos> this analysis is then extended to the bayesian nonparametric case , yielding a simple , scalable , and flexible algorithm for discrete-state sequence data with a non-fixed number of states . <eos> we also derive the corresponding combinatorial objective functions arising from our analysis , which involve a k-means-like term along with penalties based on state transitions and the number of states . <eos> a key property of such algorithms is that ? <eos> particularly in the nonparametric setting ? <eos> standard probabilistic inference algorithms lack scalability and are heavily dependent on good initialization . <eos> a number of results on synthetic and real data sets demonstrate the advantages of the proposed framework .
hypergraphs allow to encode higher-order relationships in data and are thus a very flexible modeling tool . <eos> current learning methods are either based on approximations of the hypergraphs via graphs or on tensor methods which are only applicable under special conditions . <eos> in this paper we present a new learning framework on hypergraphs which fully uses the hypergraph structure . <eos> the key element is a family of regularization functionals based on the total variation on hypergraphs .
in the mixture models problem it is assumed that there are $ k $ distributions $ \theta_ { 1 } , \ldots , \theta_ { k } $ and one gets to observe a sample from a mixture of these distributions with unknown coefficients . <eos> the goal is to associate instances with their generating distributions , or to identify the parameters of the hidden distributions . <eos> in this work we make the assumption that we have access to several samples drawn from the same $ k $ underlying distributions , but with different mixing weights . <eos> as with topic modeling , having multiple samples is often a reasonable assumption . <eos> instead of pooling the data into one sample , we prove that it is possible to use the differences between the samples to better recover the underlying structure . <eos> we present algorithms that recover the underlying structure under milder assumptions than the current state of art when either the dimensionality or the separation is high . <eos> the methods , when applied to topic modeling , allow generalization to words not present in the training data .
determinantal point processes ( dpps ) are random point processes well-suited for modeling repulsion . <eos> in machine learning , the focus of dpp-based models has been on diverse subset selection from a discrete and finite base set . <eos> this discrete setting admits an efficient algorithm for sampling based on the eigendecomposition of the defining kernel matrix . <eos> recently , there has been growing interest in using dpps defined on continuous spaces . <eos> while the discrete-dpp sampler extends formally to the continuous case , computationally , the steps required can not be directly extended except in a few restricted cases . <eos> in this paper , we present efficient approximate dpp sampling schemes based on nystrom and random fourier feature approximations that apply to a wide range of kernel functions . <eos> we demonstrate the utility of continuous dpps in repulsive mixture modeling applications and synthesizing human poses spanning activity spaces .
in many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in rewards in addition to maximizing a standard criterion . <eos> variance related risk measures are among the most common risk-sensitive criteria in finance and operations research . <eos> however , optimizing many such criteria is known to be a hard problem . <eos> in this paper , we consider both discounted and average reward markov decision processes . <eos> for each formulation , we first define a measure of variability for a policy , which in turn gives us a set of risk-sensitive criteria to optimize . <eos> for each of these criteria , we derive a formula for computing its gradient . <eos> we then devise actor-critic algorithms for estimating the gradient and updating the policy parameters in the ascent direction . <eos> we establish the convergence of our algorithms to locally risk-sensitive optimal policies . <eos> finally , we demonstrate the usefulness of our algorithms in a traffic signal control application .
we propose an approach to learning from demonstration ( lfd ) which leverages expert data , even if the expert examples are very few or inaccurate . <eos> we achieve this by integrating lfd in an approximate policy iteration algorithm . <eos> the key idea of our approach is that expert examples are used to generate linear constraints on the optimization , in a similar fashion to large-margin classification . <eos> we prove an upper bound on the true bellman error of the approximation computed by the algorithm at each iteration . <eos> we show empirically that the algorithm outperforms both pure policy iteration , as well as dagger ( a state-of-art lfd algorithm ) and supervised learning in a variety of scenarios , including when very few and/or imperfect demonstrations are available . <eos> our experiments include simulations as well as a real robotic navigation task .
we study exploration in multi-armed bandits ( mab ) in a setting where~ $ k $ players collaborate in order to identify an $ \epsilon $ -optimal arm . <eos> our motivation comes from recent employment of mab algorithms in computationally intensive , large-scale applications . <eos> our results demonstrate a non-trivial tradeoff between the number of arm pulls required by each of the players , and the amount of communication between them . <eos> in particular , our main result shows that by allowing the $ k $ players to communicate \emph { only once } , they are able to learn $ \sqrt { k } $ times faster than a single player . <eos> that is , distributing learning to $ k $ players gives rise to a factor~ $ \sqrt { k } $ parallel speed-up . <eos> we complement this result with a lower bound showing this is in general the best possible . <eos> on the other extreme , we present an algorithm that achieves the ideal factor $ k $ speed-up in learning performance , with communication only logarithmic in~ $ 1/\epsilon $ .
we present a new online learning algorithm that extends the exponentiated gradient to infinite dimensional spaces . <eos> our analysis shows that the algorithm is implicitly able to estimate the $ l_2 $ norm of the unknown competitor , $ u $ , achieving a regret bound of the order of $ o ( u \log ( u t+1 ) ) \sqrt { t } ) $ , instead of the standard $ o ( ( u^2 +1 ) \sqrt { t } ) $ , achievable without knowing $ u $ . <eos> for this analysis , we introduce novel tools for algorithms with time-varying regularizers , through the use of local smoothness . <eos> through a lower bound , we also show that the algorithm is optimal up to $ \sqrt { \log t } $ term for linear and lipschitz losses .
analytic shrinkage is a statistical technique that offers a fast alternative to cross-validation for the regularization of covariance matrices and has appealing consistency properties . <eos> we show that the proof of consistency implies bounds on the growth rates of eigenvalues and their dispersion , which are often violated in data . <eos> we prove consistency under assumptions which do not restrict the covariance structure and therefore better match real world data . <eos> in addition , we propose an extension of analytic shrinkage -- orthogonal complement shrinkage -- which adapts to the covariance structure . <eos> finally we demonstrate the superior performance of our novel approach on data from the domains of finance , spoken letter and optical character recognition , and neuroscience .
the efficiency of brain-computer interfaces ( bci ) largely depends upon a reliable extraction of informative features from the high-dimensional eeg signal . <eos> a crucial step in this protocol is the computation of spatial filters . <eos> the common spatial patterns ( csp ) algorithm computes filters that maximize the difference in band power between two conditions , thus it is tailored to extract the relevant information in motor imagery experiments . <eos> however , csp is highly sensitive to artifacts in the eeg data , i.e . few outliers may alter the estimate drastically and decrease classification performance . <eos> inspired by concepts from the field of information geometry we propose a novel approach for robustifying csp . <eos> more precisely , we formulate csp as a divergence maximization problem and utilize the property of a particular type of divergence , namely beta divergence , for robustifying the estimation of spatial filters in the presence of artifacts in the data . <eos> we demonstrate the usefulness of our method on toy data and on eeg recordings from 80 subjects .
the l1-regularized gaussian maximum likelihood estimator ( mle ) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix even under high-dimensional settings . <eos> however , it requires solving a difficult non-smooth log-determinant program with number of parameters scaling quadratically with the number of gaussian variables . <eos> state-of-the-art methods thus do not scale to problems with more than 20,000 variables . <eos> in this paper , we develop an algorithm bigquic , which can solve 1 million dimensional l1-regularized gaussian mle problems ( which would thus have 1000 billion parameters ) using a single machine , with bounded memory . <eos> in order to do so , we carefully exploit the underlying structure of the problem . <eos> our innovations include a novel block-coordinate descent method with the blocks chosen via a clustering scheme to minimize repeated computations ; and allowing for inexact computation of specific components . <eos> in spite of these modifications , we are able to theoretically analyze our procedure and show that bigquic can achieve super-linear or even quadratic convergence rates .
multiple hypothesis testing is a significant problem in nearly all neuroimaging studies . <eos> in order to correct for this phenomena , we require a reliable estimate of the family-wise error rate ( fwer ) . <eos> the well known bonferroni correction method , while being simple to implement , is quite conservative , and can substantially under-power a study because it ignores dependencies between test statistics . <eos> permutation testing , on the other hand , is an exact , non parametric method of estimating the fwer for a given threshold , but for acceptably low thresholds the computational burden can be prohibitive . <eos> in this paper , we observe that permutation testing in fact amounts to populating the columns of a very large matrix p. by analyzing the spectrum of this matrix , under certain conditions , we see that p has a low-rank plus a low-variance residual decomposition which makes it suitable for highly sub ? sampled on the order of 0.5 % matrix completion methods . <eos> thus , we propose a novel permutation testing methodology which offers a large speedup , without sacrificing the fidelity of the estimated fwer . <eos> our valuations on four different neuroimaging datasets show that a computational speedup factor of roughly 50 can be achieved while recovering the fwer distribution up to very high accuracy . <eos> further , we show that the estimated threshold is also recovered faithfully , and is stable .
graph matching is a challenging problem with very important applications in a wide range of fields , from image and video analysis to biological and biomedical problems . <eos> we propose a robust graph matching algorithm inspired in sparsity-related techniques . <eos> we cast the problem , resembling group or collaborative sparsity formulations , as a non-smooth convex optimization problem that can be efficiently solved using augmented lagrangian techniques . <eos> the method can deal with weighted or unweighted graphs , as well as multimodal data , where different graphs represent different types of data . <eos> the proposed approach is also naturally integrated with collaborative graph inference techniques , solving general network inference problems where the observed variables , possibly coming from different modalities , are not in correspondence . <eos> the algorithm is tested and compared with state-of-the-art graph matching techniques in both synthetic and real graphs . <eos> we also present results on multimodal graphs and applications to collaborative inference of brain connectivity from alignment-free functional magnetic resonance imaging ( fmri ) data .
as massively parallel computations have become broadly available with modern gpus , deep architectures trained on very large datasets have risen in popularity . <eos> discriminatively trained convolutional neural networks , in particular , were recently shown to yield state-of-the-art performance in challenging image classification benchmarks such as imagenet . <eos> however , elements of these architectures are similar to standard hand-crafted representations used in computer vision . <eos> in this paper , we explore the extent of this analogy , proposing a version of the state-of-the-art fisher vector image encoding that can be stacked in multiple layers . <eos> this architecture significantly improves on standard fisher vectors , and obtains competitive results with deep convolutional networks at a significantly smaller computational cost . <eos> our hybrid architecture allows us to measure the performance improvement brought by a deeper image classification pipeline , while staying in the realms of conventional sift features and fv encodings .
optimal transportation distances are a fundamental family of parameterized distances for histograms in the probability simplex . <eos> despite their appealing theoretical properties , excellent performance and intuitive formulation , their computation involves the resolution of a linear program whose cost is prohibitive whenever the histograms ' dimension exceeds a few hundreds . <eos> we propose in this work a new family of optimal transportation distances that look at transportation problems from a maximum-entropy perspective . <eos> we smooth the classical optimal transportation problem with an entropic regularization term , and show that the resulting optimum is also a distance which can be computed through sinkhorn 's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transportation solvers . <eos> we also report improved performance on the mnist benchmark problem over competing distances .
despite growing interest and practical use in various scientific areas , variable importances derived from tree-based ensemble methods are not well understood from a theoretical point of view . <eos> in this work we characterize the mean decrease impurity ( mdi ) variable importances as measured by an ensemble of totally randomized trees in asymptotic sample and ensemble size conditions . <eos> we derive a three-level decomposition of the information jointly provided by all input variables about the output in terms of i ) the mdi importance of each input variable , ii ) the degree of interaction of a given input variable with the other input variables , iii ) the different interaction terms of a given degree . <eos> we then show that this mdi importance of a variable is equal to zero if and only if the variable is irrelevant and that the mdi importance of a relevant variable is invariant with respect to the removal or the addition of irrelevant variables . <eos> we illustrate these properties on a simple example and discuss how they may change in the case of non-totally randomized trees such as random forests and extra-trees .
language users are remarkably good at making inferences about speakers ' intentions in context , and children learning their native language also display substantial skill in acquiring the meanings of unknown words . <eos> these two cases are deeply related : language users invent new terms in conversation , and language learners learn the literal meanings of words based on their pragmatic inferences about how those words are used . <eos> while pragmatic inference and word learning have both been independently characterized in probabilistic terms , no current work unifies these two . <eos> we describe a model in which language learners assume that they jointly approximate a shared , external lexicon and reason recursively about the goals of others in using this lexicon . <eos> this model captures phenomena in word learning and pragmatic inference ; it additionally leads to insights about the emergence of communicative systems in conversation and the mechanisms by which pragmatic inferences become incorporated into word meanings .
in the high-dimensional regression model a response variable is linearly related to $ p $ covariates , but the sample size $ n $ is smaller than $ p $ . <eos> we assume that only a small subset of covariates is ` active ' ( i.e. , the corresponding coefficients are non-zero ) , and consider the model-selection problem of identifying the active covariates . <eos> a popular approach is to estimate the regression coefficients through the lasso ( $ \ell_1 $ -regularized least squares ) . <eos> this is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones , as quantified through the so called ` irrepresentability ' condition . <eos> in this paper we study the ` gauss-lasso ' selector , a simple two-stage method that first solves the lasso , and then performs ordinary least squares restricted to the lasso active set . <eos> we formulate ` generalized irrepresentability condition ' ( gic ) , an assumption that is substantially weaker than irrepresentability . <eos> we prove that , under gic , the gauss-lasso correctly recovers the active set .
fitting high-dimensional statistical models often requires the use of non-linear parameter estimation procedures . <eos> as a consequence , it is generally impossible to obtain an exact characterization of the probability distribution of the parameter estimates . <eos> this in turn implies that it is extremely challenging to quantify the ` uncertainty ' associated with a certain parameter estimate . <eos> concretely , no commonly accepted procedure exists for computing classical measures of uncertainty and statistical significance as confidence intervals or p-values . <eos> we consider here a broad class of regression problems , and propose an efficient algorithm for constructing confidence intervals and p-values . <eos> the resulting confidence intervals have nearly optimal size . <eos> when testing for the null hypothesis that a certain parameter is vanishing , our method has nearly optimal power . <eos> our approach is based on constructing a ` de-biased ' version of regularized m-estimators . <eos> the new construction improves over recent work in the field in that it does not assume a special structure on the design matrix . <eos> furthermore , proofs are remarkably simple . <eos> we test our method on a diabetes prediction problem .
this paper addresses the problem of unsupervised feature learning for text data . <eos> our method is grounded in the principle of minimum description length and uses a dictionary-based compression scheme to extract a succinct feature set . <eos> specifically , our method finds a set of word $ k $ -grams that minimizes the cost of reconstructing the text losslessly . <eos> we formulate document compression as a binary optimization task and show how to solve it approximately via a sequence of reweighted linear programs that are efficient to solve and parallelizable . <eos> as our method is unsupervised , features may be extracted once and subsequently used in a variety of tasks . <eos> we demonstrate the performance of these features over a range of scenarios including unsupervised exploratory analysis and supervised text categorization . <eos> our compressed feature space is two orders of magnitude smaller than the full $ k $ -gram space and matches the text categorization accuracy achieved in the full feature space . <eos> this dimensionality reduction not only results in faster training times , but it can also help elucidate structure in unsupervised learning tasks and reduce the amount of training data necessary for supervised learning .
the goal of unsupervised feature selection is to identify a small number of important features that can represent the data . <eos> we propose a new algorithm , a modification of the classical pivoted qr algorithm of businger and golub , that requires a small number of passes over the data . <eos> the improvements are based on two ideas : keeping track of multiple features in each pass , and skipping calculations that can be shown not to affect the final selection . <eos> our algorithm selects the exact same features as the classical pivoted qr algorithm , and has the same favorable numerical stability . <eos> we describe experiments on real-world datasets which sometimes show improvements of { \em several orders of magnitude } over the classical algorithm . <eos> these results appear to be competitive with recently proposed randomized algorithms in terms of pass efficiency and run time . <eos> on the other hand , the randomized algorithms may produce better features , at the cost of small probability of failure .
it is a common practice to approximate complicated '' functions with more friendly ones . <eos> in large-scale machine learning applications , nonsmooth losses/regularizers that entail great computational challenges are usually approximated by smooth functions . <eos> we re-examine this powerful methodology and point out a nonsmooth approximation which simply pretends the linearity of the proximal map . <eos> the new approximation is justified using a recent convex analysis tool -- -proximal average , and yields a novel proximal gradient algorithm that is strictly better than the one based on smoothing , without incurring any extra overhead . <eos> numerical experiments conducted on two important applications , overlapping group lasso and graph-guided fused lasso , corroborate the theoretical claims . ''
structured sparse estimation has become an important technique in many areas of data analysis . <eos> unfortunately , these estimators normally create computational difficulties that entail sophisticated algorithms . <eos> our first contribution is to uncover a rich class of structured sparse regularizers whose polar operator can be evaluated efficiently . <eos> with such an operator , a simple conditional gradient method can then be developed that , when combined with smoothing and local optimization , significantly reduces training time vs. the state of the art . <eos> we also demonstrate a new reduction of polar to proximal maps that enables more efficient latent fused lasso .
motivated by various applications in machine learning , the problem of minimizing a convex smooth loss function with trace norm regularization has received much attention lately . <eos> currently , a popular method for solving such problem is the proximal gradient method ( pgm ) , which is known to have a sublinear rate of convergence . <eos> in this paper , we show that for a large class of loss functions , the convergence rate of the pgm is in fact linear . <eos> our result is established without any strong convexity assumption on the loss function . <eos> a key ingredient in our proof is a new lipschitzian error bound for the aforementioned trace norm-regularized problem , which may be of independent interest .
stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance . <eos> to remedy this problem , we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient ( svrg ) . <eos> for smooth and strongly convex functions , we prove that this method enjoys the same fast convergence rate as those of stochastic dual coordinate ascent ( sdca ) and stochastic average gradient ( sag ) . <eos> however , our analysis is significantly simpler and more intuitive . <eos> moreover , unlike sdca or sag , our method does not require the storage of gradients , and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning .
stochastic dual coordinate ascent ( sdca ) is an effective technique for solving regularized loss minimization problems in machine learning . <eos> this paper considers an extension of sdca under the mini-batch setting that is often used in practice . <eos> our main contribution is to introduce an accelerated mini-batch version of sdca and prove a fast convergence rate for this method . <eos> we discuss an implementation of our method over a parallel computing system , and compare the results to both the vanilla stochastic dual coordinate ascent and to the accelerated deterministic gradient descent method of nesterov [ 2007 ] .
we study stochastic optimization problems when the \emph { data } is sparse , which is in a sense dual to the current understanding of high-dimensional statistical learning and optimization . <eos> we highlight both the difficulties -- -in terms of increased sample complexity that sparse data necessitates -- -and the potential benefits , in terms of allowing parallelism and asynchrony in the design of algorithms . <eos> concretely , we derive matching upper and lower bounds on the minimax rate for optimization and learning with sparse data , and we exhibit algorithms achieving these rates . <eos> our algorithms are adaptive : they achieve the best possible rate for the data observed . <eos> we also show how leveraging sparsity leads to ( still minimax optimal ) parallel and asynchronous algorithms , providing experimental evidence complementing our theoretical results on medium to large-scale learning tasks .
for smooth and strongly convex optimization , the optimal iteration complexity of the gradient-based algorithm is $ o ( \sqrt { \kappa } \log 1/\epsilon ) $ , where $ \kappa $ is the conditional number . <eos> in the case that the optimization problem is ill-conditioned , we need to evaluate a larger number of full gradients , which could be computationally expensive . <eos> in this paper , we propose to reduce the number of full gradient required by allowing the algorithm to access the stochastic gradients of the objective function . <eos> to this end , we present a novel algorithm named epoch mixed gradient descent ( emgd ) that is able to utilize two kinds of gradients . <eos> a distinctive step in emgd is the mixed gradient descent , where we use an combination of the gradient and the stochastic gradient to update the intermediate solutions . <eos> by performing a fixed number of mixed gradient descents , we are able to improve the sub-optimality of the solution by a constant factor , and thus achieve a linear convergence rate . <eos> theoretical analysis shows that emgd is able to find an $ \epsilon $ -optimal solution by computing $ o ( \log 1/\epsilon ) $ full gradients and $ o ( \kappa^2\log 1/\epsilon ) $ stochastic gradients .
it is well known that the optimal convergence rate for stochastic optimization of smooth functions is $ [ o ( 1/\sqrt { t } ) ] $ , which is same as stochastic optimization of lipschitz continuous convex functions . <eos> this is in contrast to optimizing smooth functions using full gradients , which yields a convergence rate of $ [ o ( 1/t^2 ) ] $ . <eos> in this work , we consider a new setup for optimizing smooth functions , termed as { \bf mixed optimization } , which allows to access both a stochastic oracle and a full gradient oracle . <eos> our goal is to significantly improve the convergence rate of stochastic optimization of smooth functions by having an additional small number of accesses to the full gradient oracle . <eos> we show that , with an $ [ o ( \ln t ) ] $ calls to the full gradient oracle and an $ o ( t ) $ calls to the stochastic oracle , the proposed mixed optimization algorithm is able to achieve an optimization error of $ [ o ( 1/t ) ] $ .
in this paper , we are interested in the development of efficient algorithms for convex optimization problems in the simultaneous presence of multiple objectives and stochasticity in the first-order information . <eos> we cast the stochastic multiple objective optimization problem into a constrained optimization problem by choosing one function as the objective and try to bound other objectives by appropriate thresholds . <eos> we first examine a two stages exploration-exploitation based algorithm which first approximates the stochastic objectives by sampling and then solves a constrained stochastic optimization problem by projected gradient method . <eos> this method attains a suboptimal convergence rate even under strong assumption on the objectives . <eos> our second approach is an efficient primal-dual stochastic algorithm . <eos> it leverages on the theory of lagrangian method in constrained optimization and attains the optimal convergence rate of $ [ o ( 1/ \sqrt { t } ) ] $ in high probability for general lipschitz continuous objectives .
we consider robust optimization for polynomial optimization problems where the uncertainty set is a set of candidate probability density functions . <eos> this set is a ball around a density function estimated from data samples , i.e. , it is data-driven and random . <eos> polynomial optimization problems are inherently hard due to nonconvex objectives and constraints . <eos> however , we show that by employing polynomial and histogram density estimates , we can introduce robustness with respect to distributional uncertainty sets without making the problem harder . <eos> we show that the solution to the distributionally robust problem is the limit of a sequence of tractable semidefinite programming relaxations . <eos> we also give finite-sample consistency guarantees for the data-driven uncertainty sets . <eos> finally , we apply our model and solution method in a water network problem .
nonparametric estimation of the conditional distribution of a response given high-dimensional features is a challenging problem . <eos> it is important to allow not only the mean but also the variance and shape of the response density to change flexibly with features , which are massive-dimensional . <eos> we propose a multiscale dictionary learning model , which expresses the conditional response density as a convex combination of dictionary densities , with the densities used and their weights dependent on the path through a tree decomposition of the feature space . <eos> a fast graph partitioning algorithm is applied to obtain the tree decomposition , with bayesian methods then used to adaptively prune and average over different sub-trees in a soft probabilistic manner . <eos> the algorithm scales efficiently to approximately one million features . <eos> state of the art predictive performance is demonstrated for toy examples and two neuroscience applications including up to a million features .
a large number of algorithms in machine learning , from principal component analysis ( pca ) , and its non-linear ( kernel ) extensions , to more recent spectral embedding and support estimation methods , rely on estimating a linear subspace from samples . <eos> in this paper we introduce a general formulation of this problem and derive novel learning error estimates . <eos> our results rely on natural assumptions on the spectral properties of the covariance operator associated to the data distribution , and hold for a wide class of metrics between subspaces . <eos> as special cases , we discuss sharp error estimates for the reconstruction properties of pca and spectral support estimation . <eos> key to our analysis is an operator theoretic approach that has broad applicability to spectral learning methods .
we present a novel non-parametric method for finding a subspace of stimulus features that contains all information about the response of a system . <eos> our method generalizes similar approaches to this problem such as spike triggered average , spike triggered covariance , or maximally informative dimensions . <eos> instead of maximizing the mutual information between features and responses directly , we use integral probability metrics in kernel hilbert spaces to minimize the information between uninformative features and the combination of informative features and responses . <eos> since estimators of these metrics access the data via kernels , are easy to compute , and exhibit good theoretical convergence properties , our method can easily be generalized to populations of neurons or spike patterns . <eos> by using a particular expansion of the mutual information , we can show that the informative features must contain all information if we can make the uninformative features independent of the rest .
compressed sensing ( cs ) is a concept that allows to acquire compressible signals with a small number of measurements . <eos> as such , it is very attractive for hardware implementations . <eos> therefore , correct calibration of the hardware is a central issue . <eos> in this paper we study the so-called blind calibration , i.e . when the training signals that are available to perform the calibration are sparse but unknown . <eos> we extend the approximate message passing ( amp ) algorithm used in cs to the case of blind calibration . <eos> in the calibration-amp , both the gains on the sensors and the elements of the signals are treated as unknowns . <eos> our algorithm is also applicable to settings in which the sensors distort the measurements in other ways than multiplication by a gain , unlike previously suggested blind calibration algorithms based on convex relaxations . <eos> we study numerically the phase diagram of the blind calibration problem , and show that even in cases where convex relaxation is possible , our algorithm requires a smaller number of measurements and/or signals in order to perform well .
we study the fundamental problems of variance and risk estimation in high dimensional statistical modeling . <eos> in particular , we consider the problem of learning a coefficient vector $ \theta_0\in r^p $ from noisy linear observation $ y=x\theta_0+w\in r^n $ and the popular estimation procedure of solving an $ \ell_1 $ -penalized least squares objective known as the lasso or basis pursuit denoising ( bpdn ) . <eos> in this context , we develop new estimators for the $ \ell_2 $ estimation risk $ \|\hat { \theta } -\theta_0\|_2 $ and the variance of the noise . <eos> these can be used to select the regularization parameter optimally . <eos> our approach combines stein unbiased risk estimate ( stein'81 ) and recent results of ( bayati and montanari'11-12 ) on the analysis of approximate message passing and risk of lasso . <eos> we establish high-dimensional consistency of our estimators for sequences of matrices $ x $ of increasing dimensions , with independent gaussian entries . <eos> we establish validity for a broader class of gaussian designs , conditional on the validity of a certain conjecture from statistical physics . <eos> our approach is the first that provides an asymptotically consistent risk estimator . <eos> in addition , we demonstrate through simulation that our variance estimation outperforms several existing methods in the literature .
max-product ? belief propagation ( bp ) is a popular distributed heuristic for finding the maximum a posteriori ( map ) assignment in a joint probability distribution represented by a graphical model ( gm ) . <eos> it was recently shown that bp converges to the correct map assignment for a class of loopy gms with the following common feature : the linear programming ( lp ) relaxation to the map problem is tight ( has no integrality gap ) . <eos> unfortunately , tightness of the lp relaxation does not , in general , guarantee convergence and correctness of the bp algorithm . <eos> the failure of bp in such cases motivates reverse engineering a solution namely , given a tight lp , can we design a ? good bp algorithm . <eos> in this paper , we design a bp algorithm for the maximum weight matching ( mwm ) problem over general graphs . <eos> we prove that the algorithm converges to the correct optimum if the respective lp relaxation , which may include inequalities associated with non-intersecting odd-sized cycles , is tight . <eos> the most significant part of our approach is the introduction of a novel graph transformation designed to force convergence of bp . <eos> our theoretical result suggests an efficient bp-based heuristic for the mwm problem , which consists of making sequential , ? cutting plane ? , modifications to the underlying gm . <eos> our experiments show that this heuristic performs as well as traditional cutting-plane algorithms using lp solvers on mwm problems .
we consider the sensor selection problem on multivariate gaussian distributions where only a \emph { subset } of latent variables is of inferential interest . <eos> for pairs of vertices connected by a unique path in the graph , we show that there exist decompositions of nonlocal mutual information into local information measures that can be computed efficiently from the output of message passing algorithms . <eos> we integrate these decompositions into a computationally efficient greedy selector where the computational expense of quantification can be distributed across nodes in the network . <eos> experimental results demonstrate the comparative efficiency of our algorithms for sensor selection in high-dimensional distributions . <eos> we additionally derive an online-computable performance bound based on augmentations of the relevant latent variable set that , when such a valid augmentation exists , is applicable for \emph { any } distribution with nuisances .
a common classifier for unlabeled nodes on undirected graphs uses label propagation from the labeled nodes , equivalent to the harmonic predictor on gaussian random fields ( grfs ) . <eos> for active learning on grfs , the commonly used v-optimality criterion queries nodes that reduce the l2 ( regression ) loss . <eos> v-optimality satisfies a submodularity property showing that greedy reduction produces a ( 1 ? <eos> 1/e ) globally optimal solution . <eos> however , l2 loss may not characterise the true nature of 0/1 loss in classification problems and thus may not be the best choice for active learning . <eos> we consider a new criterion we call ? -optimality , which queries the node that minimizes the sum of the elements in the predictive covariance . <eos> ? -optimality directly optimizes the risk of the surveying problem , which is to determine the proportion of nodes belonging to one class . <eos> in this paper we extend submodularity guarantees from v-optimality to ? -optimality using properties specific to grfs . <eos> we further show that grfs satisfy the suppressor-free condition in addition to the conditional independence inherited from markov random fields . <eos> we test ? -optimality on real-world graphs with both synthetic and real data and show that it outperforms v-optimality and other related methods on classification .
many real-world problems have complicated objective functions . <eos> to optimize such functions , humans utilize sophisticated sequential decision-making strategies . <eos> many optimization algorithms have also been developed for this same purpose , but how do they compare to humans in terms of both performance and behavior ? <eos> we try to unravel the general underlying algorithm people may be using while searching for the maximum of an invisible 1d function . <eos> subjects click on a blank screen and are shown the ordinate of the function at each clicked abscissa location . <eos> their task is to find the function ? s maximum in as few clicks as possible . <eos> subjects win if they get close enough to the maximum location . <eos> analysis over 23 non-maths undergraduates , optimizing 25 functions from different families , shows that humans outperform 24 well-known optimization algorithms . <eos> bayesian optimization based on gaussian processes , which exploit all the x values tried and all the f ( x ) values obtained so far to pick the next x , predicts human performance and searched locations better . <eos> in 6 follow-up controlled experiments over 76 subjects , covering interpolation , extrapolation , and optimization tasks , we further confirm that gaussian processes provide a general and unified theoretical account to explain passive and active function learning and search in humans .
in this paper we present active learning algorithms in the context of structured prediction problems . <eos> to reduce the amount of labeling necessary to learn good models , our algorithms only label subsets of the output . <eos> to this end , we query examples using entropies of local marginals , which are a good surrogate for uncertainty . <eos> we demonstrate the effectiveness of our approach in the task of 3d layout prediction from single images , and show that good models are learned when labeling only a handful of random variables . <eos> in particular , the same performance as using the full training set can be obtained while only labeling ~10\ % of the random variables .
we study low rank matrix and tensor completion and propose novel algorithms that employ adaptive sampling schemes to obtain strong performance guarantees for these problems . <eos> our algorithms exploit adaptivity to identify entries that are highly informative for identifying the column space of the matrix ( tensor ) and consequently , our results hold even when the row space is highly coherent , in contrast with previous analysis of matrix completion . <eos> in the absence of noise , we show that one can exactly recover a $ n \times n $ matrix of rank $ r $ using $ o ( r^2 n \log ( r ) ) $ observations , which is better than the best known bound under random sampling . <eos> we also show that one can recover an order $ t $ tensor using $ o ( r^ { 2 ( t-1 ) } t^2 n \log ( r ) ) $ . <eos> for noisy recovery , we show that one can consistently estimate a low rank matrix corrupted with noise using $ o ( nr \textrm { polylog } ( n ) ) $ observations . <eos> we complement our study with simulations that verify our theoretical guarantees and demonstrate the scalability of our algorithms .
maximization of submodular functions has wide applications in machine learning and artificial intelligence . <eos> adaptive submodular maximization has been traditionally studied under the assumption that the model of the world , the expected gain of choosing an item given previously selected items and their states , is known . <eos> in this paper , we study the scenario where the expected gain is initially unknown and it is learned by interacting repeatedly with the optimized function . <eos> we propose an efficient algorithm for solving our problem and prove that its expected cumulative regret increases logarithmically with time . <eos> our regret bound captures the inherent property of submodular maximization , earlier mistakes are more costly than later ones . <eos> we refer to our approach as optimistic adaptive submodular maximization ( oasm ) because it trades off exploration and exploitation based on the optimism in the face of uncertainty principle . <eos> we evaluate our method on a preference elicitation problem and show that non-trivial k-step policies can be learned from just a few hundred interactions with the problem .
we propose a learning setting in which unlabeled data is free , and the cost of a label depends on its value , which is not known in advance . <eos> we study binary classification in an extreme case , where the algorithm only pays for negative labels . <eos> our motivation are applications such as fraud detection , in which investigating an honest transaction should be avoided if possible . <eos> we term the setting auditing , and consider the auditing complexity of an algorithm : the number of negative points it labels to learn a hypothesis with low relative error . <eos> we design auditing algorithms for thresholds on the line and axis-aligned rectangles , and show that with these algorithms , the auditing complexity can be significantly lower than the active label complexity . <eos> we discuss a general approach for auditing for a general hypothesis class , and describe several interesting directions for future work .
in many practical applications of active learning , it is more cost-effective to request labels in large batches , rather than one-at-a-time . <eos> this is because the cost of labeling a large batch of examples at once is often sublinear in the number of examples in the batch . <eos> in this work , we study the label complexity of active learning algorithms that request labels in a given number of batches , as well as the tradeoff between the total number of queries and the number of rounds allowed . <eos> we additionally study the total cost sufficient for learning , for an abstract notion of the cost of requesting the labels of a given number of examples at once . <eos> in particular , we find that for sublinear cost functions , it is often desirable to request labels in large batches ( i.e. , buying in bulk ) ; although this may increase the total number of labels requested , it reduces the total cost required for learning .
we introduce a new objective function for pool-based bayesian active learning with probabilistic hypotheses . <eos> this objective function , called the policy gibbs error , is the expected error rate of a random classifier drawn from the prior distribution on the examples adaptively selected by the active learning policy . <eos> exact maximization of the policy gibbs error is hard , so we propose a greedy strategy that maximizes the gibbs error at each iteration , where the gibbs error on an instance is the expected error of a random classifier selected from the posterior label distribution on that instance . <eos> we apply this maximum gibbs error criterion to three active learning scenarios : non-adaptive , adaptive , and batch active learning . <eos> in each scenario , we prove that the criterion achieves near-maximal policy gibbs error when constrained to a fixed budget . <eos> for practical implementations , we provide approximations to the maximum gibbs error criterion for bayesian conditional random fields and transductive naive bayes . <eos> our experimental results on a named entity recognition task and a text classification task show that the maximum gibbs error criterion is an effective active learning criterion for noisy models .
we consider a number of classical and new computational problems regarding marginal distributions , and inference in models specifying a full joint distribution . <eos> we prove general and efficient reductions between a number of these problems , which demonstrate that algorithmic progress in inference automatically yields progress for ? pure data problems . <eos> our main technique involves formulating the problems as linear programs , and proving that the dual separation oracle for the ellipsoid method is provided by the target problem . <eos> this technique may be of independent interest in probabilistic inference .
we investigate the problem of learning the structure of a markov network from data . <eos> it is shown that the structure of such networks can be described in terms of constraints which enables the use of existing solver technology with optimization capabilities to compute optimal networks starting from initial scores computed from the data . <eos> to achieve efficient encodings , we develop a novel characterization of markov network structure using a balancing condition on the separators between cliques forming the network . <eos> the resulting translations into propositional satisfiability and its extensions such as maximum satisfiability , satisfiability modulo theories , and answer set programming , enable us to prove the optimality of networks which have been previously found by stochastic search .
in large-scale applications of undirected graphical models , such as social networks and biological networks , similar patterns occur frequently and give rise to similar parameters . <eos> in this situation , it is beneficial to group the parameters for more efficient learning . <eos> we show that even when the grouping is unknown , we can infer these parameter groups during learning via a bayesian approach . <eos> we impose a dirichlet process prior on the parameters . <eos> posterior inference usually involves calculating intractable terms , and we propose two approximation algorithms , namely a metropolis-hastings algorithm with auxiliary variables and a gibbs sampling algorithm with stripped beta approximation ( gibbs_sba ) . <eos> simulations show that both algorithms outperform conventional maximum likelihood estimation ( mle ) . <eos> gibbs_sba 's performance is close to gibbs sampling with exact likelihood calculation . <eos> models learned with gibbs_sba also generalize better than the models learned by mle on real-world senate voting data .
in this paper we describe how map inference can be used to sample efficiently from gibbs distributions . <eos> specifically , we provide means for drawing either approximate or unbiased samples from gibbs ' distributions by introducing low dimensional perturbations and solving the corresponding map assignments . <eos> our approach also leads to new ways to derive lower bounds on partition functions . <eos> we demonstrate empirically that our method excels in the typical high signal - high coupling '' regime . <eos> the setting results in ragged energy landscapes that are challenging for alternative approaches to sampling and/or lower bounds. ``
edml is a recently proposed algorithm for learning parameters in bayesian networks . <eos> it was originally derived in terms of approximate inference on a meta-network , which underlies the bayesian approach to parameter estimation . <eos> while this initial derivation helped discover edml in the first place and provided a concrete context for identifying some of its properties ( e.g. , in contrast to em ) , the formal setting was somewhat tedious in the number of concepts it drew on . <eos> in this paper , we propose a greatly simplified perspective on edml , which casts it as a general approach to continuous optimization . <eos> the new perspective has several advantages . <eos> first , it makes immediate some results that were non-trivial to prove initially . <eos> second , it facilitates the design of edml algorithms for new graphical models , leading to a new algorithm for learning parameters in markov networks . <eos> we derive this algorithm in this paper , and show , empirically , that it can sometimes learn better estimates from complete data , several times faster than commonly used optimization methods , such as conjugate gradient and l-bfgs .
inference in general ising models is difficult , due to high treewidth making tree-based algorithms intractable . <eos> moreover , when interactions are strong , gibbs sampling may take exponential time to converge to the stationary distribution . <eos> we present an algorithm to project ising model parameters onto a parameter set that is guaranteed to be fast mixing , under several divergences . <eos> we find that gibbs sampling using the projected parameters is more accurate than with the original parameters when interaction strengths are strong and when limited time is available for sampling .
we consider the problem of sampling from a probability distribution defined over a high-dimensional discrete set , specified for instance by a graphical model . <eos> we propose a sampling algorithm , called paws , based on embedding the set into a higher-dimensional space which is then randomly projected using universal hash functions to a lower-dimensional subspace and explored using combinatorial search methods . <eos> our scheme can leverage fast combinatorial optimization tools as a blackbox and , unlike mcmc methods , samples produced are guaranteed to be within an ( arbitrarily small ) constant factor of the true probability distribution . <eos> we demonstrate that by using state-of-the-art combinatorial search tools , paws can efficiently sample from ising grids with strong interactions and from software verification instances , while mcmc and variational methods fail in both cases .
we describe a class of algorithms for amortized inference in bayesian networks . <eos> in this setting , we invest computation upfront to support rapid online inference for a wide range of queries . <eos> our approach is based on learning an inverse factorization of a model 's joint distribution : a factorization that turns observations into root nodes . <eos> our algorithms accumulate information to estimate the local conditional distributions that constitute such a factorization . <eos> these stochastic inverses can be used to invert each of the computation steps leading to an observation , sampling backwards in order to quickly find a likely explanation . <eos> we show that estimated inverses converge asymptotically in number of ( prior or posterior ) training samples . <eos> to make use of inverses before convergence , we describe the inverse mcmc algorithm , which uses stochastic inverses to make block proposals for a metropolis-hastings sampler . <eos> we explore the efficiency of this sampler for a variety of parameter regimes and bayes nets .
we introduce a nonparametric approach for estimating drift functions in systems of stochastic differential equations from incomplete observations of the state vector . <eos> using a gaussian process prior over the drift as a function of the state vector , we develop an approximate em algorithm to deal with the unobserved , latent dynamics between observations . <eos> the posterior over states is approximated by a piecewise linearized process and the map estimation of the drift is facilitated by a sparse gaussian process regression .
reliance on computationally expensive algorithms for inference has been limiting the use of bayesian nonparametric models in large scale applications . <eos> to tackle this problem , we propose a bayesian learning algorithm for dp mixture models . <eos> instead of following the conventional paradigm -- random initialization plus iterative update , we take an progressive approach . <eos> starting with a given prior , our method recursively transforms it into an approximate posterior through sequential variational approximation . <eos> in this process , new components will be incorporated on the fly when needed . <eos> the algorithm can reliably estimate a dp mixture model in one pass , making it particularly suited for applications with massive data . <eos> experiments on both synthetic data and real datasets demonstrate remarkable improvement on efficiency -- orders of magnitude speed-up compared to the state-of-the-art .
variational inference algorithms provide the most effective framework for large-scale training of bayesian nonparametric models . <eos> stochastic online approaches are promising , but are sensitive to the chosen learning rate and often converge to poor local optima . <eos> we present a new algorithm , memoized online variational inference , which scales to very large ( yet finite ) datasets while avoiding the complexities of stochastic gradient . <eos> our algorithm maintains finite-dimensional sufficient statistics from batches of the full dataset , requiring some additional memory but still scaling to millions of examples . <eos> exploiting nested families of variational bounds for infinite nonparametric models , we develop principled birth and merge moves allowing non-local optimization . <eos> births adaptively add components to the model to escape local optima , while merges remove redundancy and improve speed . <eos> using dirichlet process mixture models for image clustering and denoising , we demonstrate major improvements in robustness and accuracy .
in this paper , we seek robust policies for uncertain markov decision processes ( mdps ) . <eos> most robust optimization approaches for these problems have focussed on the computation of { \em maximin } policies which maximize the value corresponding to the worst realization of the uncertainty . <eos> recent work has proposed { \em minimax } regret as a suitable alternative to the { \em maximin } objective for robust optimization . <eos> however , existing algorithms for handling { \em minimax } regret are restricted to models with uncertainty over rewards only . <eos> we provide algorithms that employ sampling to improve across multiple dimensions : ( a ) handle uncertainties over both transition and reward models ; ( b ) dependence of model uncertainties across state , action pairs and decision epochs ; ( c ) scalability and quality bounds . <eos> finally , to demonstrate the empirical effectiveness of our sampling approaches , we provide comparisons against benchmark algorithms on two domains from literature . <eos> we also provide a sample average approximation ( saa ) analysis to compute a posteriori error bounds .
given a markov decision process ( mdp ) with $ n $ states and $ m $ actions per state , we study the number of iterations needed by policy iteration ( pi ) algorithms to converge to the optimal $ \gamma $ -discounted optimal policy . <eos> we consider two variations of pi : howard 's pi that changes the actions in all states with a positive advantage , and simplex-pi that only changes the action in the state with maximal advantage . <eos> we show that howard 's pi terminates after at most $ o \left ( \frac { n m } { 1-\gamma } \log \left ( \frac { 1 } { 1-\gamma } \right ) \right ) $ iterations , improving by a factor $ o ( \log n ) $ a result by hansen et al . ( 2013 ) , while simplex-pi terminates after at most $ o \left ( \frac { n^2 m } { 1-\gamma } \log \left ( \frac { 1 } { 1-\gamma } \right ) \right ) $ iterations , improving by a factor $ o ( \log n ) $ a result by ye ( 2011 ) . <eos> under some structural assumptions of the mdp , we then consider bounds that are independent of the discount factor~ $ \gamma $ : given a measure of the maximal transient time $ \tau_t $ and the maximal time $ \tau_r $ to revisit states in recurrent classes under all policies , we show that simplex-pi terminates after at most $ \tilde o \left ( n^3 m^2 \tau_t \tau_r \right ) $ iterations . <eos> this generalizes a recent result for deterministic mdps by post & ye ( 2012 ) , in which $ \tau_t \le n $ and $ \tau_r \le n $ . <eos> we explain why similar results seem hard to derive for howard 's pi . <eos> finally , under the additional ( restrictive ) assumption that the state space is partitioned in two sets , respectively states that are transient and recurrent for all policies , we show that simplex-pi and howard 's pi terminate after at most $ \tilde o ( nm ( \tau_t+\tau_r ) ) $ iterations .
we consider the problem of reinforcement learning over episodes of a finite-horizon deterministic system and as a solution propose optimistic constraint propagation ( ocp ) , an algorithm designed to synthesize efficient exploration and value function generalization . <eos> we establish that when the true value function lies within the given hypothesis class , ocp selects optimal actions over all but at most k episodes , where k is the eluder dimension of the given hypothesis class . <eos> we establish further efficiency and asymptotic performance guarantees that apply even if the true value function does not lie in the given hypothesis space , for the special case where the hypothesis space is the span of pre-specified indicator functions over disjoint sets .
this paper addresses the problem of online planning in markov decision processes using only a generative model . <eos> we propose a new algorithm which is based on the construction of a forest of single successor state planning trees . <eos> for every explored state-action , such a tree contains exactly one successor state , drawn from the generative model . <eos> the trees are built using a planning algorithm which follows the optimism in the face of uncertainty principle , in assuming the most favorable outcome in the absence of further information . <eos> in the decision making step of the algorithm , the individual trees are combined . <eos> we discuss the approach , prove that our proposed algorithm is consistent , and empirically show that it performs better than a related algorithm which additionally assumes the knowledge of all transition distributions .
we study the problem of online learning in finite episodic markov decision processes where the loss function is allowed to change between episodes . <eos> the natural performance measure in this learning problem is the regret defined as the difference between the total loss of the best stationary policy and the total loss suffered by the learner . <eos> we assume that the learner is given access to a finite action space $ \a $ and the state space $ \x $ has a layered structure with $ l $ layers , so that state transitions are only possible between consecutive layers . <eos> we describe a variant of the recently proposed relative entropy policy search algorithm and show that its regret after $ t $ episodes is $ 2\sqrt { l\nx\na t\log ( \nx\na/l ) } $ in the bandit setting and $ 2l\sqrt { t\log ( \nx\na/l ) } $ in the full information setting . <eos> these guarantees largely improve previously known results under much milder assumptions and can not be significantly improved under general assumptions .
we study the problem of online learning markov decision processes ( mdps ) when both the transition distributions and loss functions are chosen by an adversary . <eos> we present an algorithm that , under a mixing assumption , achieves $ o ( \sqrt { t\log|\pi| } +\log|\pi| ) $ regret with respect to a comparison set of policies $ \pi $ . <eos> the regret is independent of the size of the state and action spaces . <eos> when expectations over sample paths can be computed efficiently and the comparison set $ \pi $ has polynomial size , this algorithm is efficient . <eos> we also consider the episodic adversarial online shortest path problem . <eos> here , in each episode an adversary may choose a weighted directed acyclic graph with an identified start and finish node . <eos> the goal of the learning algorithm is to choose a path that minimizes the loss while traversing from the start to finish node . <eos> at the end of each episode the loss function ( given by weights on the edges ) is revealed to the learning algorithm . <eos> the goal is to minimize regret with respect to a fixed policy for selecting paths . <eos> this problem is a special case of the online mdp problem . <eos> for randomly chosen graphs and adversarial losses , this problem can be efficiently solved . <eos> we show that it also can be efficiently solved for adversarial graphs and randomly chosen losses . <eos> when both graphs and losses are adversarially chosen , we present an efficient algorithm whose regret scales linearly with the number of distinct graphs . <eos> finally , we show that designing efficient algorithms for the adversarial online shortest path problem ( and hence for the adversarial mdp problem ) is as hard as learning parity with noise , a notoriously difficult problem that has been used to design efficient cryptographic schemes .
this paper addresses the problem of online learning in a dynamic setting . <eos> we consider a social network in which each individual observes a private signal about the underlying state of the world and communicates with her neighbors at each time period . <eos> unlike many existing approaches , the underlying state is dynamic , and evolves according to a geometric random walk . <eos> we view the scenario as an optimization problem where agents aim to learn the true state while suffering the smallest possible loss . <eos> based on the decomposition of the global loss function , we introduce two update mechanisms , each of which generates an estimate of the true state . <eos> we establish a tight bound on the rate of change of the underlying state , under which individuals can track the parameter with a bounded variance . <eos> then , we characterize explicit expressions for the steady state mean-square deviation ( msd ) of the estimates from the truth , per individual . <eos> we observe that only one of the estimators recovers the optimal msd , which underscores the impact of the objective function decomposition on the learning quality . <eos> finally , we provide an upper bound on the regret of the proposed methods , measured as an average of errors in estimating the parameter in a finite time .
we develop a probabilistic approach for accurate network modeling using node popularities within the framework of the mixed-membership stochastic blockmodel ( mmsb ) . <eos> our model integrates some of the basic properties of nodes in social networks : homophily and preferential connection to popular nodes . <eos> we develop a scalable algorithm for posterior inference , based on a novel nonconjugate variant of stochastic variational inference . <eos> we evaluate the link prediction accuracy of our algorithm on eight real-world networks with up to 60,000 nodes , and 24 benchmark networks . <eos> we demonstrate that our algorithm predicts better than the mmsb . <eos> further , using benchmark networks we show that node popularities are essential to achieving high accuracy in the presence of skewed degree distribution and noisy links -- -both characteristics of real networks .
we propose a scalable approach for making inference about latent spaces of large networks . <eos> with a succinct representation of networks as a bag of triangular motifs , a parsimonious statistical model , and an efficient stochastic variational inference algorithm , we are able to analyze real networks with over a million vertices and hundreds of latent roles on a single machine in a matter of hours , a setting that is out of reach for many existing methods . <eos> when compared to the state-of-the-art probabilistic approaches , our method is several orders of magnitude faster , with competitive or improved accuracy for latent space recovery and link prediction .
unstructured social group activity recognition in web videos is a challenging task due to 1 ) the semantic gap between class labels and low-level visual features and 2 ) the lack of labeled training data . <eos> to tackle this problem , we propose a relevance topic model '' for jointly learning meaningful mid-level representations upon bag-of-words ( bow ) video representations and a classifier with sparse weights . <eos> in our approach , sparse bayesian learning is incorporated into an undirected topic model ( i.e. , replicated softmax ) to discover topics which are relevant to video classes and suitable for prediction . <eos> rectified linear units are utilized to increase the expressive power of topics so as to explain better video data containing complex contents and make variational inference tractable for the proposed model . <eos> an efficient variational em algorithm is presented for model parameter estimation and inference . <eos> experimental results on the unstructured social activity attribute dataset show that our model achieves state of the art performance and outperforms other supervised topic model in terms of classification accuracy , particularly in the case of a very small number of labeled training videos . ''
we present sda-bayes , a framework for ( s ) treaming , ( d ) istributed , ( a ) synchronous computation of a bayesian posterior . <eos> the framework makes streaming updates to the estimated posterior according to a user-specified approximation primitive function . <eos> we demonstrate the usefulness of our framework , with variational bayes ( vb ) as the primitive , by fitting the latent dirichlet allocation model to two large-scale document collections . <eos> we demonstrate the advantages of our algorithm over stochastic variational inference ( svi ) , both in the single-pass setting svi was designed for and in the streaming setting , to which svi does not apply .
logistic-normal topic models can effectively discover correlation structures among latent topics . <eos> however , their inference remains a challenge because of the non-conjugacy between the logistic-normal prior and multinomial topic mixing proportions . <eos> existing algorithms either make restricting mean-field assumptions or are not scalable to large-scale applications . <eos> this paper presents a partially collapsed gibbs sampling algorithm that approaches the provably correct distribution by exploring the ideas of data augmentation . <eos> to improve time efficiency , we further present a parallel implementation that can deal with large-scale applications and learn the correlation structures of thousands of topics from millions of documents . <eos> extensive empirical results demonstrate the promise .
overcomplete latent representations have been very popular for unsupervised feature learning in recent years . <eos> in this paper , we specify which overcomplete models can be identified given observable moments of a certain order . <eos> we consider probabilistic admixture or topic models in the overcomplete regime , where the number of latent topics can greatly exceed the size of the observed word vocabulary . <eos> while general overcomplete topic models are not identifiable , we establish { \em generic } identifiability under a constraint , referred to as { \em topic persistence } . <eos> our sufficient conditions for identifiability involve a novel set of higher order '' expansion conditions on the { \em topic-word matrix } or the { \em population structure } of the model . <eos> this set of higher-order expansion conditions allow for overcomplete models , and require the existence of a perfect matching from latent topics to higher order observed words . <eos> we establish that random structured topic models are identifiable w.h.p . <eos> in the overcomplete regime . <eos> our identifiability results allow for general ( non-degenerate ) distributions for modeling the topic proportions , and thus , we can handle arbitrarily correlated topics in our framework . <eos> our identifiability results imply uniqueness of a class of tensor decompositions with structured sparsity which is contained in the class of { \em tucker } decompositions , but is more general than the { \em candecomp/parafac } ( cp ) decomposition . ''
while several papers have investigated computationally and statistically efficient methods for learning gaussian mixtures , precise minimax bounds for their statistical performance as well as fundamental limits in high-dimensional settings are not well-understood . <eos> in this paper , we provide precise information theoretic bounds on the clustering accuracy and sample complexity of learning a mixture of two isotropic gaussians in high dimensions under small mean separation . <eos> if there is a sparse subset of relevant dimensions that determine the mean separation , then the sample complexity only depends on the number of relevant dimensions and mean separation , and can be achieved by a simple computationally efficient procedure . <eos> our results provide the first step of a theoretical basis for recent methods that combine feature selection and clustering .
we propose a new class of structured schatten norms for tensors that includes two recently proposed norms ( overlapped '' and `` latent '' ) for convex-optimization-based tensor decomposition . <eos> based on the properties of the structured schatten norms , we mathematically analyze the performance of `` latent '' approach for tensor decomposition , which was empirically found to perform better than the `` overlapped '' approach in some settings . <eos> we show theoretically that this is indeed the case . <eos> in particular , when the unknown true tensor is low-rank in a specific mode , this approach performs as well as knowing the mode with the smallest rank . <eos> along the way , we show a novel duality result for structures schatten norms , which is also interesting in the general context of structured sparsity . <eos> we confirm through numerical simulations that our theory can precisely predict the scaling behaviour of the mean squared error. ``
seriation seeks to reconstruct a linear order between variables using unsorted similarity information . <eos> it has direct applications in archeology and shotgun gene sequencing for example . <eos> we prove the equivalence between the seriation and the combinatorial 2-sum problem ( a quadratic minimization problem over permutations ) over a class of similarity matrices . <eos> the seriation problem can be solved exactly by a spectral algorithm in the noiseless case and we produce a convex relaxation for the 2-sum problem to improve the robustness of solutions in a noisy setting . <eos> this relaxation also allows us to impose additional structural constraints on the solution , to solve semi-supervised seriation problems . <eos> we present numerical experiments on archeological data , markov chains and gene sequences .
the problem of matching not just two , but m different sets of objects to each other arises in a variety of contexts , including finding the correspondence between feature points across multiple images in computer vision . <eos> at present it is usually solved by matching the sets pairwise , in series . <eos> in contrast , we propose a new method , permutation synchronization , which finds all the matchings jointly , in one shot , via a relaxation to eigenvector decomposition . <eos> the resulting algorithm is both computationally efficient , and , as we demonstrate with theoretical arguments as well as experimental results , much more stable to noise than previous methods .
recently , it has become evident that submodularity naturally captures widely occurring concepts in machine learning , signal processing and computer vision . <eos> in consequence , there is need for efficient optimization procedures for submodular functions , in particular for minimization problems . <eos> while general submodular minimization is challenging , we propose a new approach that exploits existing decomposability of submodular functions . <eos> in contrast to previous approaches , our method is neither approximate , nor impractical , nor does it need any cumbersome parameter tuning . <eos> moreover , it is easy to implement and parallelize . <eos> a key component of our approach is a formulation of the discrete submodular minimization problem as a continuous best approximation problem . <eos> it is solved through a sequence of reflections and its solution can be automatically thresholded to obtain an optimal discrete solution . <eos> our method solves both the continuous and discrete formulations of the problem , and therefore has applications in learning , inference , and reconstruction . <eos> in our experiments , we show the benefits of our new algorithms for two image segmentation tasks .
we investigate three related and important problems connected to machine learning , namely approximating a submodular function everywhere , learning a submodular function ( in a pac like setting [ 26 ] ) , and constrained minimization of submodular functions . <eos> in all three problems , we provide improved bounds which depend on the curvatureof a submodular function and improve on the previously known best results for these problems [ 9 , 3 , 7 , 25 ] when the function is not too curved ? <eos> a property which is true of many real-world submodular functions . <eos> in the former two problems , we obtain these bounds through a generic black-box transformation ( which can potentially work for any algorithm ) , while in the case of submodular minimization , we propose a framework of algorithms which depend on choosing an appropriate surrogate for the submodular function . <eos> in all these cases , we provide almost matching lower bounds . <eos> while improved curvature-dependent bounds were shown for monotone submodular maximization [ 4 , 27 ] , the existence of similar improved bounds for the aforementioned problems has been open . <eos> we resolve this question in this paper by showing that the same notion of curvature provides these improved results . <eos> empirical experiments add further support to our claims .
many problems in machine learning can be solved by rounding the solution of an appropriate linear program . <eos> we propose a scheme that is based on a quadratic program relaxation which allows us to use parallel stochastic-coordinate-descent to approximately solve large linear programs efficiently . <eos> our software is an order of magnitude faster than cplex ( a commercial linear programming solver ) and yields similar solution quality . <eos> our results include a novel perturbation analysis of a quadratic-penalty formulation of linear programming and a convergence result , which we use to derive running time and quality guarantees .
humans recognize visually-presented objects rapidly and accurately . <eos> to understand this ability , we seek to construct models of the ventral stream , the series of cortical areas thought to subserve object recognition . <eos> one tool to assess the quality of a model of the ventral stream is the representation dissimilarity matrix ( rdm ) , which uses a set of visual stimuli and measures the distances produced in either the brain ( i.e . fmri voxel responses , neural firing rates ) or in models ( features ) . <eos> previous work has shown that all known models of the ventral stream fail to capture the rdm pattern observed in either it cortex , the highest ventral area , or in the human ventral stream . <eos> in this work , we construct models of the ventral stream using a novel optimization procedure for category-level object recognition problems , and produce rdms resembling both macaque it and human ventral stream . <eos> the model , while novel in the optimization procedure , further develops a long-standing functional hypothesis that the ventral visual stream is a hierarchically arranged series of processing stages optimized for visual object recognition .
the receptive field ( rf ) of a sensory neuron describes how the neuron integrates sensory stimuli over time and space . <eos> in typical experiments with naturalistic or flickering spatiotemporal stimuli , rfs are very high-dimensional , due to the large number of coefficients needed to specify an integration profile across time and space . <eos> estimating these coefficients from small amounts of data poses a variety of challenging statistical and computational problems . <eos> here we address these challenges by developing bayesian reduced rank regression methods for rf estimation . <eos> this corresponds to modeling the rf as a sum of several space-time separable ( i.e. , rank-1 ) filters , which proves accurate even for neurons with strongly oriented space-time rfs . <eos> this approach substantially reduces the number of parameters needed to specify the rf , from 1k-100k down to mere 100s in the examples we consider , and confers substantial benefits in statistical power and computational efficiency . <eos> in particular , we introduce a novel prior over low-rank rfs using the restriction of a matrix normal prior to the manifold of low-rank matrices . <eos> we then use a localized '' prior over row and column covariances to obtain sparse , smooth , localized estimates of the spatial and temporal rf components . <eos> we develop two methods for inference in the resulting hierarchical model : ( 1 ) a fully bayesian method using blocked-gibbs sampling ; and ( 2 ) a fast , approximate method that employs alternating coordinate ascent of the conditional marginal likelihood . <eos> we develop these methods under gaussian and poisson noise models , and show that low-rank estimates substantially outperform full rank estimates in accuracy and speed using neural data from retina and v1 . ''
we describe a set of fast , tractable methods for characterizing neural responses to high-dimensional sensory stimuli using a model we refer to as the generalized quadratic model ( gqm ) . <eos> the gqm consists of a low-rank quadratic form followed by a point nonlinearity and exponential-family noise . <eos> the quadratic form characterizes the neuron 's stimulus selectivity in terms of a set linear receptive fields followed by a quadratic combination rule , and the invertible nonlinearity maps this output to the desired response range . <eos> special cases of the gqm include the 2nd-order volterra model ( marmarelis and marmarelis 1978 , koh and powers 1985 ) and the elliptical linear-nonlinear-poisson model ( park and pillow 2011 ) . <eos> here we show that for canonical form '' gqms , spectral decomposition of the first two response-weighted moments yields approximate maximum-likelihood estimators via a quantity called the expected log-likelihood . <eos> the resulting theory generalizes moment-based estimators such as the spike-triggered covariance , and , in the gaussian noise case , provides closed-form estimators under a large class of non-gaussian stimulus distributions . <eos> we show that these estimators are fast and provide highly accurate estimates with far lower computational cost than full maximum likelihood . <eos> moreover , the gqm provides a natural framework for combining multi-dimensional stimulus sensitivity and spike-history dependencies within a single model . <eos> we show applications to both analog and spiking data using intracellular recordings of v1 membrane potential and extracellular recordings of retinal spike trains . ''
how does neural population process sensory information ? <eos> optimal coding theories assume that neural tuning curves are adapted to the prior distribution of the stimulus variable . <eos> most of the previous work has discussed optimal solutions for only one-dimensional stimulus variables . <eos> here , we expand some of these ideas and present new solutions that define optimal tuning curves for high-dimensional stimulus variables . <eos> we consider solutions for a minimal case where the number of neurons in the population is equal to the number of stimulus dimensions ( diffeomorphic ) . <eos> in the case of two-dimensional stimulus variables , we analytically derive optimal solutions for different optimal criteria such as minimal l2 reconstruction error or maximal mutual information . <eos> for higher dimensional case , the learning rule to improve the population code is provided .
recordings from large populations of neurons make it possible to search for hypothesized low-dimensional dynamics . <eos> finding these dynamics requires models that take into account biophysical constraints and can be fit efficiently and robustly . <eos> here , we present an approach to dimensionality reduction for neural data that is convex , does not make strong assumptions about dynamics , does not require averaging over many trials and is extensible to more complex statistical models that combine local and global influences . <eos> the results can be combined with spectral methods to learn dynamical systems models . <eos> the basic method can be seen as an extension of pca to the exponential family using nuclear norm minimization . <eos> we evaluate the effectiveness of this method using an exact decomposition of the bregman divergence that is analogous to variance explained for pca . <eos> we show on model data that the parameters of latent linear dynamical systems can be recovered , and that even if the dynamics are not stationary we can still recover the true latent subspace . <eos> we also demonstrate an extension of nuclear norm minimization that can separate sparse local connections from global latent dynamics . <eos> finally , we demonstrate improved prediction on real neural data from monkey motor cortex compared to fitting linear dynamical models without nuclear norm smoothing .
we propose a compressed sensing ( cs ) calcium imaging framework for monitoring large neuronal populations , where we image randomized projections of the spatial calcium concentration at each timestep , instead of measuring the concentration at individual locations . <eos> we develop scalable nonnegative deconvolution methods for extracting the neuronal spike time series from such observations . <eos> we also address the problem of demixing the spatial locations of the neurons using rank-penalized matrix factorization methods . <eos> by exploiting the sparsity of neural spiking we demonstrate that the number of measurements needed per timestep is significantly smaller than the total number of neurons , a result that can potentially enable imaging of larger populations at considerably faster rates compared to traditional raster-scanning techniques . <eos> unlike traditional cs setups , our problem involves a block-diagonal sensing matrix and a non-orthogonal sparse basis that spans multiple timesteps . <eos> we study the effect of these distinctive features in a noiseless setup using recent results relating conic geometry to cs . <eos> we provide tight approximations to the number of measurements needed for perfect deconvolution for certain classes of spiking processes , and show that this number displays a phase transition , '' similar to phenomena observed in more standard cs settings ; however , in this case the required measurement rate depends not just on the mean sparsity level but also on other details of the underlying spiking process . ''
in this paper we propose a class of efficient generalized method-of-moments ( gmm ) algorithms for computing parameters of the plackett-luce model , where the data consists of full rankings over alternatives . <eos> our technique is based on breaking the full rankings into pairwise comparisons , and then computing parameters that satisfy a set of generalized moment conditions . <eos> we identify conditions for the output of gmm to be unique , and identify a general class of consistent and inconsistent breakings . <eos> we then show by theory and experiments that our algorithms run significantly faster than the classical minorize-maximization ( mm ) algorithm , while achieving competitive statistical efficiency .
we propose a model for demand estimation in multi-agent , differentiated product settings and present an estimation algorithm that uses reversible jump mcmc techniques to classify agents ' types . <eos> our model extends the popular setup in berry , levinsohn and pakes ( 1995 ) to allow for the data-driven classification of agents ' types using agent-level data . <eos> we focus on applications involving data on agents ' ranking over alternatives , and present theoretical conditions that establish the identifiability of the model and uni-modality of the likelihood/posterior . <eos> results on both real and simulated data provide support for the scalability of our approach .
in standard matrix completion theory , it is required to have at least $ o ( n\ln^2 n ) $ observed entries to perfectly recover a low-rank matrix $ m $ of size $ n\times n $ , leading to a large number of observations when $ n $ is large . <eos> in many real tasks , side information in addition to the observed entries is often available . <eos> in this work , we develop a novel theory of matrix completion that explicitly explore the side information to reduce the requirement on the number of observed entries . <eos> we show that , under appropriate conditions , with the assistance of side information matrices , the number of observed entries needed for a perfect recovery of matrix $ m $ can be dramatically reduced to $ o ( \ln n ) $ . <eos> we demonstrate the effectiveness of the proposed approach for matrix completion in transductive incomplete multi-label learning .
this paper presents correlated nystrom views ( xnv ) , a fast semi-supervised algorithm for regression and classification . <eos> the algorithm draws on two main ideas . <eos> first , it generates two views consisting of computationally inexpensive random features . <eos> second , multiview regression , using canonical correlation analysis ( cca ) on unlabeled data , biases the regression towards useful features . <eos> it has been shown that cca regression can substantially reduce variance with a minimal increase in bias if the views contains accurate estimators . <eos> recent theoretical and empirical work shows that regression with random features closely approximates kernel regression , implying that the accuracy requirement holds for random views . <eos> we show that xnv consistently outperforms a state-of-the-art algorithm for semi-supervised learning : substantially improving predictive performance and reducing the variability of performance on a wide variety of real-world datasets , whilst also reducing runtime by orders of magnitude .
label propagation is one of the state-of-the-art methods for semi-supervised learning , which estimates labels by propagating label information through a graph . <eos> label propagation assumes that data points ( nodes ) connected in a graph should have similar labels . <eos> consequently , the label estimation heavily depends on edge weights in a graph which represent similarity of each node pair . <eos> we propose a method for a graph to capture the manifold structure of input features using edge weights parameterized by a similarity function . <eos> in this approach , edge weights represent both similarity and local reconstruction weight simultaneously , both being reasonable for label propagation . <eos> for further justification , we provide analytical considerations including an interpretation as a cross-validation of a propagation model in the feature space , and an error analysis based on a low dimensional manifold model . <eos> experimental results demonstrated the effectiveness of our approach both in synthetic and real datasets .
in this paper , we propose a new and computationally efficient framework for learning sparse models . <eos> we formulate a unified approach that contains as particular cases models promoting sparse synthesis and analysis type of priors , and mixtures thereof . <eos> the supervised training of the proposed model is formulated as a bilevel optimization problem , in which the operators are optimized to achieve the best possible performance on a specific task , e.g. , reconstruction or classification . <eos> by restricting the operators to be shift invariant , our approach can be thought as a way of learning analysis+synthesis sparsity-promoting convolutional operators . <eos> leveraging recent ideas on fast trainable regressors designed to approximate exact sparse codes , we propose a way of constructing feed-forward neural networks capable of approximating the learned models at a fraction of the computational cost of exact solvers . <eos> in the shift-invariant case , this leads to a principled way of constructing task-specific convolutional networks . <eos> we illustrate the proposed models on several experiments in music analysis and image processing applications .
we consider the problem of accurately estimating a high-dimensional sparse vector using a small number of linear measurements that are contaminated by noise . <eos> it is well known that standard computationally tractable sparse recovery algorithms , such as the lasso , omp , and their various extensions , perform poorly when the measurement matrix contains highly correlated columns . <eos> we develop a simple greedy algorithm , called swap , that iteratively swaps variables until a desired loss function can not be decreased any further . <eos> swap is surprisingly effective in handling measurement matrices with high correlations . <eos> we prove that swap can be easily used as a wrapper around standard sparse recovery algorithms for improved performance . <eos> we theoretically quantify the statistical guarantees of swap and complement our analysis with numerical results on synthetic and real data .
automatic music recommendation has become an increasingly relevant problem in recent years , since a lot of music is now sold and consumed digitally . <eos> most recommender systems rely on collaborative filtering . <eos> however , this approach suffers from the cold start problem : it fails when no usage data is available , so it is not effective for recommending new and unpopular songs . <eos> in this paper , we propose to use a latent factor model for recommendation , and predict the latent factors from music audio when they can not be obtained from usage data . <eos> we compare a traditional approach using a bag-of-words representation of the audio signals with deep convolutional neural networks , and evaluate the predictions quantitatively and qualitatively on the million song dataset . <eos> we show that using predicted latent factors produces sensible recommendations , despite the fact that there is a large semantic gap between the characteristics of a song that affect user preference and the corresponding audio signal . <eos> we also show that recent advances in deep learning translate very well to the music recommendation setting , with deep convolutional neural networks significantly outperforming the traditional approach .
we propose a novel class of algorithms for low rank matrix completion . <eos> our approach builds on novel penalty functions on the singular values of the low rank matrix . <eos> by exploiting a mixture model representation of this penalty , we show that a suitably chosen set of latent variables enables to derive an expectation-maximization algorithm to obtain a maximum a posteriori estimate of the completed low rank matrix . <eos> the resulting algorithm is an iterative soft-thresholded algorithm which iteratively adapts the shrinkage coefficients associated to the singular values . <eos> the algorithm is simple to implement and can scale to large matrices . <eos> we provide numerical comparisons between our approach and recent alternatives showing the interest of the proposed approach for low rank matrix completion .
multi-armed bandit problems are receiving a great deal of attention because they adequately formalize the exploration-exploitation trade-offs arising in several industrially relevant applications , such as online advertisement and , more generally , recommendation systems . <eos> in many cases , however , these applications have a strong social component , whose integration in the bandit algorithm could lead to a dramatic performance increase . <eos> for instance , we may want to serve content to a group of users by taking advantage of an underlying network of social relationships among them . <eos> in this paper , we introduce novel algorithmic approaches to the solution of such networked bandit problems . <eos> more specifically , we design and analyze a global strategy which allocates a bandit algorithm to each network node ( user ) and allows it to sharesignals ( contexts and payoffs ) with the neghboring nodes . <eos> we then derive two more scalable variants of this strategy based on different ways of clustering the graph nodes . <eos> we experimentally compare the algorithm and its variants to state-of-the-art methods for contextual bandits that do not use the relational information . <eos> our experiments , carried out on synthetic and real-world datasets , show a marked increase in prediction performance obtained by exploiting the network structure .
in many natural settings , the analysis goal is not to characterize a single data set in isolation , but rather to understand the difference between one set of observations and another . <eos> for example , given a background corpus of news articles together with writings of a particular author , one may want a topic model that explains word patterns and themes specific to the author . <eos> another example comes from genomics , in which biological signals may be collected from different regions of a genome , and one wants a model that captures the differential statistics observed in these regions . <eos> this paper formalizes this notion of contrastive learning for mixture models , and develops spectral algorithms for inferring mixture components specific to a foreground data set when contrasted with a background data set . <eos> the method builds on recent moment-based estimators and tensor decompositions for latent variable models , and has the intuitive feature of using background data statistics to appropriately modify moments estimated from foreground data . <eos> a key advantage of the method is that the background data need only be coarsely modeled , which is important when the background is too complex , noisy , or not of interest . <eos> the method is demonstrated on applications in contrastive topic modeling and genomic sequence analysis .
determinantal point process ( dpp ) has gained much popularity for modeling sets of diverse items . <eos> the gist of dpp is that the probability of choosing a particular set of items is proportional to the determinant of a positive definite matrix that defines the similarity of those items . <eos> however , computing the determinant requires time cubic in the number of items , and is hence impractical for large sets . <eos> in this paper , we address this problem by constructing a rapidly mixing markov chain , from which we can acquire a sample from the given dpp in sub-cubic time . <eos> in addition , we show that this framework can be extended to sampling from cardinality-constrained dpps . <eos> as an application , we show how our sampling algorithm can be used to provide a fast heuristic for determining the number of clusters , resulting in better clustering .
computing the stationary distribution of a large finite or countably infinite state space markov chain ( mc ) has become central in many problems such as statistical inference and network analysis . <eos> standard methods involve large matrix multiplications as in power iteration , or simulations of long random walks to sample states from the stationary distribution , as in markov chain monte carlo ( mcmc ) . <eos> however these methods are computationally costly ; either they involve operations at every state or they scale ( in computation time ) at least linearly in the size of the state space . <eos> in this paper , we provide a novel algorithm that answers whether a chosen state in a mc has stationary probability larger than some $ \delta \in ( 0,1 ) $ . <eos> if so , it estimates the stationary probability . <eos> our algorithm uses information from a local neighborhood of the state on the graph induced by the mc , which has constant size relative to the state space . <eos> we provide correctness and convergence guarantees that depend on the algorithm parameters and mixing properties of the mc . <eos> simulation results show mcs for which this method gives tight estimates .
inspired by real-time ad exchanges for online display advertising , we consider the problem of inferring a buyer 's value distribution for a good when the buyer is repeatedly interacting with a seller through a posted-price mechanism . <eos> we model the buyer as a strategic agent , whose goal is to maximize her long-term surplus , and we are interested in mechanisms that maximize the seller 's long-term revenue . <eos> we present seller algorithms that are no-regret when the buyer discounts her future surplus -- - i.e . the buyer prefers showing advertisements to users sooner rather than later . <eos> we also give a lower bound on regret that increases as the buyer 's discounting weakens and shows , in particular , that any seller algorithm will suffer linear regret if there is no discounting .
we study differentially private mechanisms for answering \emph { smooth } queries on databases consisting of data points in $ \mathbb { r } ^d $ . <eos> a $ k $ -smooth query is specified by a function whose partial derivatives up to order $ k $ are all bounded . <eos> we develop an $ \epsilon $ -differentially private mechanism which for the class of $ k $ -smooth queries has accuracy $ o ( \left ( \frac { 1 } { n } \right ) ^ { \frac { k } { 2d+k } } /\epsilon ) $ . <eos> the mechanism first outputs a summary of the database . <eos> to obtain an answer of a query , the user runs a public evaluation algorithm which contains no information of the database . <eos> outputting the summary runs in time $ o ( n^ { 1+\frac { d } { 2d+k } } ) $ , and the evaluation algorithm for answering a query runs in time $ \tilde o ( n^ { \frac { d+2+\frac { 2d } { k } } { 2d+k } } ) $ . <eos> our mechanism is based on $ l_ { \infty } $ -approximation of ( transformed ) smooth functions by low degree even trigonometric polynomials with small and efficiently computable coefficients .
we provide a general technique for making online learning algorithms differentially private , in both the full information and bandit settings . <eos> our technique applies to algorithms that aim to minimize a \emph { convex } loss function which is a sum of smaller convex loss terms , one for each data point . <eos> we modify the popular \emph { mirror descent } approach , or rather a variant called \emph { follow the approximate leader } . <eos> the technique leads to the first nonprivate algorithms for private online learning in the bandit setting . <eos> in the full information setting , our algorithms improve over the regret bounds of previous work . <eos> in many cases , our algorithms ( in both settings ) matching the dependence on the input length , $ t $ , of the \emph { optimal nonprivate } regret bounds up to logarithmic factors in $ t $ . <eos> our algorithms require logarithmic space and update time .
we provide a detailed study of the estimation of probability distributions -- -discrete and continuous -- -in a stringent setting in which data is kept private even from the statistician . <eos> we give sharp minimax rates of convergence for estimation in these locally private settings , exhibiting fundamental tradeoffs between privacy and convergence rate , as well as providing tools to allow movement along the privacy-statistical efficiency continuum . <eos> one of the consequences of our results is that warner 's classical work on randomized response is an optimal way to perform survey sampling while maintaining privacy of the respondents .
differential privacy is a cryptographically motivated definition of privacy which has gained considerable attention in the algorithms , machine-learning and data-mining communities . <eos> while there has been an explosion of work on differentially private machine learning algorithms , a major barrier to achieving end-to-end differential privacy in practical machine learning applications is the lack of an effective procedure for differentially private parameter tuning , or , determining the parameter value , such as a bin size in a histogram , or a regularization parameter , that is suitable for a particular application . <eos> in this paper , we introduce a generic validation procedure for differentially private machine learning algorithms that apply when a certain stability condition holds on the training algorithm and the validation performance metric . <eos> the training data size and the privacy budget used for training in our procedure is independent of the number of parameter values searched over . <eos> we apply our generic procedure to two fundamental tasks in statistics and machine-learning -- training a regularized linear classifier and building a histogram density estimator that result in end-to-end differentially private solutions for these problems .
measuring similarity is crucial to many learning tasks . <eos> it is also a richer and broader notion than what most metric learning algorithms can model . <eos> for example , similarity can arise from the process of aggregating the decisions of multiple latent components , where each latent component compares data in its own way by focusing on a different subset of features . <eos> in this paper , we propose similarity component analysis ( sca ) , a probabilistic graphical model that discovers those latent components from data . <eos> in sca , a latent component generates a local similarity value , computed with its own metric , independently of other components . <eos> the final similarity measure is then obtained by combining the local similarity values with a ( noisy- ) or gate . <eos> we derive an em-based algorithm for fitting the model parameters with similarity-annotated data from pairwise comparisons . <eos> we validate the sca model on synthetic datasets where sca discovers the ground-truth about the latent components . <eos> we also apply sca to a multiway classification task and a link prediction task . <eos> for both tasks , sca attains significantly better prediction accuracies than competing methods . <eos> moreover , we show how sca can be instrumental in exploratory analysis of data , where we gain insights about the data by examining patterns hidden in its latent components ' local similarity values .
we describe a novel approach for computing collision-free \emph { global } trajectories for $ p $ agents with specified initial and final configurations , based on an improved version of the alternating direction method of multipliers ( admm ) algorithm . <eos> compared with existing methods , our approach is naturally parallelizable and allows for incorporating different cost functionals with only minor adjustments . <eos> we apply our method to classical challenging instances and observe that its computational requirements scale well with $ p $ for several cost functionals . <eos> we also show that a specialization of our algorithm can be used for { \em local } motion planning by solving the problem of joint optimization in velocity space .
when approximating binary similarity using the hamming distance between short binary hashes , we shown that even if the similarity is symmetric , we can have shorter and more accurate hashes by using two distinct code maps . <eos> i.e.~by approximating the similarity between $ x $ and $ x ' $ as the hamming distance between $ f ( x ) $ and $ g ( x ' ) $ , for two distinct binary codes $ f , g $ , rather than as the hamming distance between $ f ( x ) $ and $ f ( x ' ) $ .
our focus is on approximate nearest neighbor retrieval in metric and non-metric spaces . <eos> we employ a vp-tree and explore two simple yet effective learning-to prune approaches : density estimation through sampling and ? stretching of the triangle inequality . <eos> both methods are evaluated using data sets with metric ( euclidean ) and non-metric ( kl-divergence and itakura-saito ) distance functions . <eos> conditions on spaces where the vp-tree is applicable are discussed . <eos> the vp-tree with a learned pruner is compared against the recently proposed state-of-the-art approaches : the bbtree , the multi-probe locality sensitive hashing ( lsh ) , and permutation methods . <eos> our method was competitive against state-of-the-art methods and , in most cases , was more efficient for the same rank approximation quality .
many machine learning problems can be interpreted as learning for matching two types of objects ( e.g. , images and captions , users and products , queries and documents ) . <eos> the matching level of two objects is usually measured as the inner product in a certain feature space , while the modeling effort focuses on mapping of objects from the original space to the feature space . <eos> this schema , although proven successful on a range of matching tasks , is insufficient for capturing the rich structure in the matching process of more complicated objects . <eos> in this paper , we propose a new deep architecture to more effectively model the complicated matching relations between two objects from heterogeneous domains . <eos> more specifically , we apply this model to matching tasks in natural language , e.g. , finding sensible responses for a tweet , or relevant answers to a given question . <eos> this new architecture naturally combines the localness and hierarchy intrinsic to the natural language problems , and therefore greatly improves upon the state-of-the-art models .
this paper examines the question : what kinds of distributions can be efficiently represented by restricted boltzmann machines ( rbms ) ? <eos> we characterize the rbm 's unnormalized log-likelihood function as a type of neural network ( called an rbm network ) , and through a series of simulation results relate these networks to types that are better understood . <eos> we show the surprising result that rbm networks can efficiently compute any function that depends on the number of 1 's in the input , such as parity . <eos> we also provide the first known example of a particular type of distribution which provably can not be efficiently represented by an rbm ( or equivalently , can not be efficiently computed by an rbm network ) , assuming a realistic exponential upper bound on the size of the weights . <eos> by formally demonstrating that a relatively simple distribution can not be represented efficiently by an rbm our results provide a new rigorous justification for the use of potentially more expressive generative models , such as deeper ones .
the recently introduced continuous skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships . <eos> in this paper we present several improvements that make the skip-gram model more expressive and enable it to learn higher quality vectors more rapidly . <eos> we show that by subsampling frequent words we obtain significant speedup , and also learn higher quality representations as measured by our tasks . <eos> we also introduce negative sampling , a simplified variant of noise contrastive estimation ( nce ) that learns more accurate vectors for frequent words compared to the hierarchical softmax . <eos> an inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases . <eos> for example , the meanings of canada '' and `` air '' can not be easily combined to obtain `` air canada '' . <eos> motivated by this example , we present a simple and efficient method for finding phrases , and show that their vector representations can be accurately learned by the skip-gram model. ``
sparse high-dimensional data vectors are common in many application domains where a very large number of rarely non-zero features can be devised . <eos> unfortunately , this creates a computational bottleneck for unsupervised feature learning algorithms such as those based on auto-encoders and rbms , because they involve a reconstruction step where the whole input vector is predicted from the current feature values . <eos> an algorithm was recently developed to successfully handle the case of auto-encoders , based on an importance sampling scheme stochastically selecting which input elements to actually reconstruct during training for each particular example . <eos> to generalize this idea to rbms , we propose a stochastic ratio-matching algorithm that inherits all the computational advantages and unbiasedness of the importance sampling scheme . <eos> we show that stochastic ratio matching is a good estimator , allowing the approach to beat the state-of-the-art on two bag-of-word text classification benchmarks ( 20 newsgroups and rcv1 ) , while keeping computational cost linear in the number of non-zeros .
recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data generating density , in the case where the corruption noise is gaussian , the reconstruction error is the squared error , and the data is continuous-valued . <eos> this has led to various proposals for sampling from this implicitly learned density function , using langevin and metropolis-hastings mcmc . <eos> however , it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying data generating distribution when the data are discrete , or using other forms of corruption process and reconstruction errors . <eos> another issue is the mathematical justification which is only valid in the limit of small corruption noise . <eos> we propose here a different attack on the problem , which deals with all these issues : arbitrary ( but noisy enough ) corruption , arbitrary reconstruction loss ( seen as a log-likelihood ) , handling both discrete and continuous-valued variables , and removing the bias due to non-infinitesimal corruption noise ( or non-infinitesimal contractive penalty ) .
we introduce the multi-prediction deep boltzmann machine ( mp-dbm ) . <eos> the mp-dbm can be seen as a single probabilistic model trained to maximize a variational approximation to the generalized pseudolikelihood , or as a family of recurrent nets that share parameters and approximately solve different inference problems . <eos> prior methods of training dbms either do not perform well on classification tasks or require an initial learning pass that trains the dbm greedily , one layer at a time . <eos> the mp-dbm does not require greedy layerwise pretraining , and outperforms the standard dbm at classification , classification with missing inputs , and mean field prediction tasks .
we demonstrate that there is significant redundancy in the parameterization of several deep learning models . <eos> given only a few weight values for each feature it is possible to accurately predict the remaining values . <eos> moreover , we show that not only can the parameter values be predicted , but many of them need not be learned at all . <eos> we train several different architectures by learning only a small number of weights and predicting the rest . <eos> in the best case we are able to predict more than 95 % of the weights of a network without any drop in accuracy .
multilayer perceptrons ( mlps ) or neural networks are popular models used for nonlinear regression and classification tasks . <eos> as regressors , mlps model the conditional distribution of the predictor variables y given the input variables x . <eos> however , this predictive distribution is assumed to be unimodal ( e.g . gaussian ) . <eos> for tasks such as structured prediction problems , the conditional distribution should be multimodal , forming one-to-many mappings . <eos> by using stochastic hidden variables rather than deterministic ones , sigmoid belief nets ( sbns ) can induce a rich multimodal distribution in the output space . <eos> however , previously proposed learning algorithms for sbns are very slow and do not work well for real-valued data . <eos> in this paper , we propose a stochastic feedforward network with hidden layers having \emph { both deterministic and stochastic } variables . <eos> a new generalized em training procedure using importance sampling allows us to efficiently learn complicated conditional distributions . <eos> we demonstrate the superiority of our model to conditional restricted boltzmann machines and mixture density networks on synthetic datasets and on modeling facial expressions . <eos> moreover , we show that latent features of our model improves classification and provide additional qualitative results on color images .
this work introduces a model that can recognize objects in images even if no training data is available for the object class . <eos> the only necessary knowledge about unseen categories comes from unsupervised text corpora . <eos> unlike previous zero-shot learning models , which can only differentiate between unseen classes , our model can operate on a mixture of objects , simultaneously obtaining state of the art performance on classes with thousands of training images and reasonable performance on unseen classes . <eos> this is achieved by seeing the distributions of words in texts as a semantic space for understanding what objects look like . <eos> our deep learning model does not require any manually defined semantic or visual features for either words or images . <eos> images are mapped to be close to semantic word vectors corresponding to their classes , and the resulting image embeddings can be used to distinguish whether an image is of a seen or unseen class . <eos> then , a separate recognition model can be employed for each type . <eos> we demonstrate two strategies , the first gives high accuracy on unseen classes , while the second is conservative in its prediction of novelty and keeps the seen classes ' accuracy high .
a common problem in knowledge representation and related fields is reasoning over a large joint knowledge graph , represented as triples of a relation between two entities . <eos> the goal of this paper is to develop a more powerful neural network model suitable for inference over these relationships . <eos> previous models suffer from weak interaction between entities or simple linear projection of the vector space . <eos> we address these problems by introducing a neural tensor network ( ntn ) model which allow the entities and relations to interact multiplicatively . <eos> additionally , we observe that such knowledge base models can be further improved by representing each entity as the average of vectors for the words in the entity name , giving an additional dimension of similarity by which entities can share statistical strength . <eos> we assess the model by considering the problem of predicting additional true relations between entities given a partial knowledge base . <eos> our model outperforms previous models and can classify unseen relationships in wordnet and freebase with an accuracy of 86.2 % and 90.0 % , respectively .
this paper proposes a way of improving classification performance for classes which have very few training examples . <eos> the key idea is to discover classes which are similar and transfer knowledge among them . <eos> our method organizes the classes into a tree hierarchy . <eos> the tree structure can be used to impose a generative prior over classification parameters . <eos> we show that these priors can be combined with discriminative models such as deep neural networks . <eos> our method benefits from the power of discriminative training of deep neural networks , at the same time using tree-based generative priors over classification parameters . <eos> we also propose an algorithm for learning the underlying tree structure . <eos> this gives the model some flexibility to tune the tree so that the tree is pertinent to task being solved . <eos> we show that the model can transfer knowledge across related classes using fixed semantic trees . <eos> moreover , it can learn new meaningful trees usually leading to improved performance . <eos> our method achieves state-of-the-art classification results on the cifar-100 image data set and the mir flickr multimodal data set .
stacked sparse denoising auto-encoders ( ssdas ) have recently been shown to be successful at removing noise from corrupted images . <eos> however , like most denoising techniques , the ssda is not robust to variation in noise types beyond what it has seen during training . <eos> we present the multi-column stacked sparse denoising autoencoder , a novel technique of combining multiple ssdas into a multi-column ssda ( mc-ssda ) by combining the outputs of each ssda . <eos> we eliminate the need to determine the type of noise , let alone its statistics , at test time . <eos> we show that good denoising performance can be achieved with a single system on a variety of different noise types , including ones not seen in the training set . <eos> additionally , we experimentally demonstrate the efficacy of mc-ssda denoising by achieving mnist digit error rates on denoised images at close to that of the uncorrupted images .
designing a principled and effective algorithm for learning deep architectures is a challenging problem . <eos> the current approach involves two training phases : a fully unsupervised learning followed by a strongly discriminative optimization . <eos> we suggest a deep learning strategy that bridges the gap between the two phases , resulting in a three-phase learning procedure . <eos> we propose to implement the scheme using a method to regularize deep belief networks with top-down information . <eos> the network is constructed from building blocks of restricted boltzmann machines learned by combining bottom-up and top-down sampled signals . <eos> a global optimization procedure that merges samples from a forward bottom-up pass and a top-down pass is used . <eos> experiments on the mnist dataset show improvements over the existing algorithms for deep belief networks . <eos> object recognition results on the caltech-101 dataset also yield competitive results .
recently , it was shown that by dropping out hidden activities with a probability of 0.5 , deep neural networks can perform very well . <eos> we describe a model in which a binary belief network is overlaid on a neural network and is used to decrease the information content of its hidden units by selectively setting activities to zero . <eos> this `` dropout network can be trained jointly with the neural network by approximately computing local expectations of binary dropout variables , computing derivatives using back-propagation , and using stochastic gradient descent . <eos> interestingly , experiments show that the learnt dropout network parameters recapitulate the neural network parameters , suggesting that a good dropout network regularizes activities according to magnitude . <eos> when evaluated on the mnist and norb datasets , we found our method can be used to achieve lower classification error rates than other feather learning methods , including standard dropout , denoising auto-encoders , and restricted boltzmann machines . <eos> for example , our model achieves 5.8 % error on the norb test set , which is better than state-of-the-art results obtained using convolutional architectures. ``
we study pca as a stochastic optimization problem and propose a novel stochastic approximation algorithm which we refer to as matrix stochastic gradient '' ( msg ) , as well as a practical variant , capped msg . <eos> we study the method both theoretically and empirically. ``
stochastic gradient optimization is a class of widely used algorithms for training machine learning models . <eos> to optimize an objective , it uses the noisy gradient computed from the random data samples instead of the true gradient computed from the entire dataset . <eos> however , when the variance of the noisy gradient is large , the algorithm might spend much time bouncing around , leading to slower convergence and worse performance . <eos> in this paper , we develop a general approach of using control variate for variance reduction in stochastic gradient . <eos> data statistics such as low-order moments ( pre-computed or estimated online ) is used to form the control variate . <eos> we demonstrate how to construct the control variate for two practical problems using stochastic gradient optimization . <eos> one is convex -- -the map estimation for logistic regression , and the other is non-convex -- -stochastic variational inference for latent dirichlet allocation . <eos> on both problems , our approach shows faster convergence and better performance than the classical approach .
we consider streaming , one-pass principal component analysis ( pca ) , in the high-dimensional regime , with limited memory . <eos> here , $ p $ -dimensional samples are presented sequentially , and the goal is to produce the $ k $ -dimensional subspace that best approximates these points . <eos> standard algorithms require $ o ( p^2 ) $ memory ; meanwhile no algorithm can do better than $ o ( kp ) $ memory , since this is what the output itself requires . <eos> memory ( or storage ) complexity is most meaningful when understood in the context of computational and sample complexity . <eos> sample complexity for high-dimensional pca is typically studied in the setting of the { \em spiked covariance model } , where $ p $ -dimensional points are generated from a population covariance equal to the identity ( white noise ) plus a low-dimensional perturbation ( the spike ) which is the signal to be recovered . <eos> it is now well-understood that the spike can be recovered when the number of samples , $ n $ , scales proportionally with the dimension , $ p $ . <eos> yet , all algorithms that provably achieve this , have memory complexity $ o ( p^2 ) $ . <eos> meanwhile , algorithms with memory-complexity $ o ( kp ) $ do not have provable bounds on sample complexity comparable to $ p $ . <eos> we present an algorithm that achieves both : it uses $ o ( kp ) $ memory ( meaning storage of any kind ) and is able to compute the $ k $ -dimensional spike with $ o ( p \log p ) $ sample-complexity -- the first algorithm of its kind . <eos> while our theoretical analysis focuses on the spiked covariance model , our simulations show that our algorithm is successful on much more general models for the data .
we consider the problem of independently sampling $ s $ non-zero entries of a matrix $ a $ in order to produce a sparse sketch of it , $ b $ , that minimizes $ \|a-b\|_2 $ . <eos> for large $ m \times n $ matrices , such that $ n \gg m $ ( for example , representing $ n $ observations over $ m $ attributes ) we give distributions exhibiting four important properties . <eos> first , they have closed forms for the probability of sampling each item which are computable from minimal information regarding $ a $ . <eos> second , they allow sketching of matrices whose non-zeros are presented to the algorithm in arbitrary order as a stream , with $ o ( 1 ) $ computation per non-zero . <eos> third , the resulting sketch matrices are not only sparse , but their non-zero entries are highly compressible . <eos> lastly , and most importantly , under mild assumptions , our distributions are provably competitive with the optimal offline distribution . <eos> note that the probabilities in the optimal offline distribution may be complex functions of all the entries in the matrix . <eos> therefore , regardless of computational complexity , the optimal distribution might be impossible to compute in the streaming model .
we consider the problem of sparse precision matrix estimation in high dimensions using the clime estimator , which has several desirable theoretical properties . <eos> we present an inexact alternating direction method of multiplier ( admm ) algorithm for clime , and establish rates of convergence for both the objective and optimality conditions . <eos> further , we develop a large scale distributed framework for the computations , which scales to millions of dimensions and trillions of parameters , using hundreds of cores . <eos> the proposed framework solves clime in column-blocks and only involves elementwise operations and parallel matrix multiplications . <eos> we evaluate our algorithm on both shared-memory and distributed-memory architectures , which can use block cyclic distribution of data and parameters to achieve load balance and improve the efficiency in the use of memory hierarchies . <eos> experimental results show that our algorithm is substantially more scalable than state-of-the-art methods and scales almost linearly with the number of cores .
research on distributed machine learning algorithms has focused primarily on one of two extremes -- -algorithms that obey strict concurrency constraints or algorithms that obey few or no such constraints . <eos> we consider an intermediate alternative in which algorithms optimistically assume that conflicts are unlikely and if conflicts do arise a conflict-resolution protocol is invoked . <eos> we view this optimistic concurrency control '' paradigm as particularly appropriate for large-scale machine learning algorithms , particularly in the unsupervised setting . <eos> we demonstrate our approach in three problem areas : clustering , feature learning and online facility location . <eos> we evaluate our methods via large-scale experiments in a cluster computing environment. ``
many large-scale machine learning problems ( such as clustering , non-parametric learning , kernel machines , etc . ) <eos> require selecting , out of a massive data set , a manageable , representative subset . <eos> such problems can often be reduced to maximizing a submodular set function subject to cardinality constraints . <eos> classical approaches require centralized access to the full data set ; but for truly large-scale problems , rendering the data centrally is often impractical . <eos> in this paper , we consider the problem of submodular function maximization in a distributed fashion . <eos> we develop a simple , two-stage protocol greedi , that is easily implemented using mapreduce style computations . <eos> we theoretically analyze our approach , and show , that under certain natural conditions , performance close to the ( impractical ) centralized approach can be achieved . <eos> in our extensive experiments , we demonstrate the effectiveness of our approach on several applications , including sparse gaussian process inference on tens of millions of examples using hadoop .
in this work , we propose a general method for recovering low-rank three-order tensors , in which the data can be deformed by some unknown transformation and corrupted by arbitrary sparse errors . <eos> since the unfolding matrices of a tensor are interdependent , we introduce auxiliary variables and relax the hard equality constraints by the augmented lagrange multiplier method . <eos> to improve the computational efficiency , we introduce a proximal gradient step to the alternating direction minimization method . <eos> we have provided proof for the convergence of the linearized version of the problem which is the inner loop of the overall algorithm . <eos> both simulations and experiments show that our methods are more efficient and effective than previous work . <eos> the proposed method can be easily applied to simultaneously rectify and align multiple images or videos frames . <eos> in this context , the state-of-the-art algorithms rasl '' and `` tilt '' can be viewed as two special cases of our work , and yet each only performs part of the function of our method . ''
phase retrieval problems involve solving linear equations , but with missing sign ( or phase , for complex numbers ) information . <eos> over the last two decades , a popular generic empirical approach to the many variants of this problem has been one of alternating minimization ; i.e . alternating between estimating the missing phase information , and the candidate solution . <eos> in this paper , we show that a simple alternating minimization algorithm geometrically converges to the solution of one such problem -- finding a vector $ x $ from $ y , a $ , where $ y = |a'x| $ and $ |z| $ denotes a vector of element-wise magnitudes of $ z $ -- under the assumption that $ a $ is gaussian . <eos> empirically , our algorithm performs similar to recently proposed convex techniques for this variant ( which are based on lifting '' to a convex matrix problem ) in sample complexity and robustness to noise . <eos> however , our algorithm is much more efficient and can scale to large problems . <eos> analytically , we show geometric convergence to the solution , and sample complexity that is off by log factors from obvious lower bounds . <eos> we also establish close to optimal scaling for the case when the unknown vector is sparse . <eos> our work represents the only known proof of alternating minimization for any variant of phase retrieval problems in the non-convex setting . ''
what if there is a teacher who knows the learning goal and wants to design good training data for a machine learner ? <eos> we propose an optimal teaching framework aimed at learners who employ bayesian models . <eos> our framework is expressed as an optimization problem over teaching examples that balance the future loss of the learner and the effort of the teacher . <eos> this optimization problem is in general hard . <eos> in the case where the learner employs conjugate exponential family models , we present an approximate algorithm for finding the optimal teaching set . <eos> our algorithm optimizes the aggregate sufficient statistics , then unpacks them into actual teaching examples . <eos> we give several examples to illustrate our framework .
sampling inference methods are computationally difficult to scale for many models in part because global dependencies can reduce opportunities for parallel computation . <eos> without strict conditional independence structure among variables , standard gibbs sampling theory requires sample updates to be performed sequentially , even if dependence between most variables is not strong . <eos> empirical work has shown that some models can be sampled effectively by going hogwild '' and simply running gibbs updates in parallel with only periodic global communication , but the successes and limitations of such a strategy are not well understood . <eos> as a step towards such an understanding , we study the hogwild gibbs sampling strategy in the context of gaussian distributions . <eos> we develop a framework which provides convergence conditions and error bounds along with simple proofs and connections to methods in numerical linear algebra . <eos> in particular , we show that if the gaussian precision matrix is generalized diagonally dominant , then any hogwild gibbs sampler , with any update schedule or allocation of variables to processors , yields a stable sampling process with the correct sample mean. ``
learning the joint dependence of discrete variables is a fundamental problem in machine learning , with many applications including prediction , clustering and dimensionality reduction . <eos> more recently , the framework of copula modeling has gained popularity due to its modular parametrization of joint distributions . <eos> among other properties , copulas provide a recipe for combining flexible models for univariate marginal distributions with parametric families suitable for potentially high dimensional dependence structures . <eos> more radically , the extended rank likelihood approach of hoff ( 2007 ) bypasses learning marginal models completely when such information is ancillary to the learning task at hand as in , e.g. , standard dimensionality reduction problems or copula parameter estimation . <eos> the main idea is to represent data by their observable rank statistics , ignoring any other information from the marginals . <eos> inference is typically done in a bayesian framework with gaussian copulas , and it is complicated by the fact this implies sampling within a space where the number of constraints increase quadratically with the number of data points . <eos> the result is slow mixing when using off-the-shelf gibbs sampling . <eos> we present an efficient algorithm based on recent advances on constrained hamiltonian markov chain monte carlo that is simple to implement and does not require paying for a quadratic cost in sample size .
we present a new approach to sample from generic binary distributions , based on an exact hamiltonian monte carlo algorithm applied to a piecewise continuous augmentation of the binary distribution of interest . <eos> an extension of this idea to distributions over mixtures of binary and continuous variables allows us to sample from posteriors of linear and probit regression models with spike-and-slab priors and truncated parameters . <eos> we illustrate the advantages of these algorithms in several examples in which they outperform the metropolis or gibbs samplers .
an increasing number of applications require processing of signals defined on weighted graphs . <eos> while wavelets provide a flexible tool for signal processing in the classical setting of regular domains , the existing graph wavelet constructions are less flexible -- they are guided solely by the structure of the underlying graph and do not take directly into consideration the particular class of signals to be processed . <eos> this paper introduces a machine learning framework for constructing graph wavelets that can sparsely represent a given class of signals . <eos> our construction uses the lifting scheme , and is based on the observation that the recurrent nature of the lifting scheme gives rise to a structure resembling a deep auto-encoder network . <eos> particular properties that the resulting wavelets must satisfy determine the training objective and the structure of the involved neural networks . <eos> the training is unsupervised , and is conducted similarly to the greedy pre-training of a stack of auto-encoders . <eos> after training is completed , we obtain a linear wavelet transform that can be applied to any graph signal in time and memory linear in the size of the graph . <eos> improved sparsity of our wavelet transform for the test signals is confirmed via experiments both on synthetic and real data .
given a convergent sequence of graphs , there exists a limit object called the graphon from which random graphs are generated . <eos> this nonparametric perspective of random graphs opens the door to study graphs beyond the traditional parametric models , but at the same time also poses the challenging question of how to estimate the graphon underlying observed graphs . <eos> in this paper , we propose a computationally efficient algorithm to estimate a graphon from a set of observed graphs generated from it . <eos> we show that , by approximating the graphon with stochastic block models , the graphon can be consistently estimated , that is , the estimation error vanishes as the size of the graph approaches infinity .
we propose an efficient bayesian nonparametric model for discovering hierarchical community structure in social networks . <eos> our model is a tree-structured mixture of potentially exponentially many stochastic blockmodels . <eos> we describe a family of greedy agglomerative model selection algorithms whose worst case scales quadratically in the number of vertices of the network , but independent of the number of communities . <eos> our algorithms are two orders of magnitude faster than the infinite relational model , achieving comparable or better accuracy .
probabilistic models for binary spike patterns provide a powerful tool for understanding the statistical dependencies in large-scale neural recordings . <eos> maximum entropy ( or maxent '' ) models , which seek to explain dependencies in terms of low-order interactions between neurons , have enjoyed remarkable success in modeling such patterns , particularly for small groups of neurons . <eos> however , these models are computationally intractable for large populations , and low-order maxent models have been shown to be inadequate for some datasets . <eos> to overcome these limitations , we propose a family of `` universal '' models for binary spike patterns , where universality refers to the ability to model arbitrary distributions over all $ 2^m $ binary patterns . <eos> we construct universal models using a dirichlet process centered on a well-behaved parametric base measure , which naturally combines the flexibility of a histogram and the parsimony of a parametric model . <eos> we derive computationally efficient inference methods using bernoulli and cascade-logistic base measures , which scale tractably to large populations . <eos> we also establish a condition for equivalence between the cascade-logistic and the 2nd-order maxent or `` ising '' model , making cascade-logistic a reasonable choice for base measure in a universal model . <eos> we illustrate the performance of these models using neural data . ''
point processes are popular models of neural spiking behavior as they provide a statistical distribution over temporal sequences of spikes and help to reveal the complexities underlying a series of recorded action potentials . <eos> however , the most common neural point process models , the poisson process and the gamma renewal process , do not capture interactions and correlations that are critical to modeling populations of neurons . <eos> we develop a novel model based on a determinantal point process over latent embeddings of neurons that effectively captures and helps visualize complex inhibitory and competitive interaction . <eos> we show that this model is a natural extension of the popular generalized linear model to sets of interacting neurons . <eos> the model is extended to incorporate gain control or divisive normalization , and the modulation of neural spiking based on periodic phenomena . <eos> applied to neural spike recordings from the rat hippocampus , we see that the model captures inhibitory relationships , a dichotomy of classes of neurons , and a periodic modulation by the theta rhythm known to be present in the data .
the macaque superior temporal sulcus ( sts ) is a brain area that receives and integrates inputs from both the ventral and dorsal visual processing streams ( thought to specialize in form and motion processing respectively ) . <eos> for the processing of articulated actions , prior work has shown that even a small population of sts neurons contains sufficient information for the decoding of actor invariant to action , action invariant to actor , as well as the specific conjunction of actor and action . <eos> this paper addresses two questions . <eos> first , what are the invariance properties of individual neural representations ( rather than the population representation ) in sts ? <eos> second , what are the neural encoding mechanisms that can produce such individual neural representations from streams of pixel images ? <eos> we find that a baseline model , one that simply computes a linear weighted sum of ventral and dorsal responses to short action ? snippets ? , produces surprisingly good fits to the neural data . <eos> interestingly , even using inputs from a single stream , both actor-invariance and action-invariance can be produced simply by having different linear weights .
how are firing rates in a spiking network related to neural input , connectivity and network function ? <eos> this is an important problem because firing rates are one of the most important measures of network activity , in both the study of neural computation and neural network dynamics . <eos> however , it is a difficult problem , because the spiking mechanism of individual neurons is highly non-linear , and these individual neurons interact strongly through connectivity . <eos> we develop a new technique for calculating firing rates in optimal balanced networks . <eos> these are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition . <eos> we can calculate firing rates by treating balanced network dynamics as an algorithm for optimizing signal representation . <eos> we identify this algorithm and then calculate firing rates by finding the solution to the algorithm . <eos> our firing rate calculation relates network firing rates directly to network input , connectivity and function . <eos> this allows us to explain the function and underlying mechanism of tuning curves in a variety of systems .
recent extensions of the perceptron , as e.g . the tempotron , suggest that this theoretical concept is highly relevant also for understanding networks of spiking neurons in the brain . <eos> it is not known , however , how the computational power of the perceptron and of its variants might be accomplished by the plasticity mechanisms of real synapses . <eos> here we prove that spike-timing-dependent plasticity having an anti-hebbian form for excitatory synapses as well as a spike-timing-dependent plasticity of hebbian shape for inhibitory synapses are sufficient for realizing the original perceptron learning rule if the respective plasticity mechanisms act in concert with the hyperpolarisation of the post-synaptic neurons . <eos> we also show that with these simple yet biologically realistic dynamics tempotrons are efficiently learned . <eos> the proposed mechanism might underly the acquisition of mappings of spatio-temporal activity patterns in one area of the brain onto other spatio-temporal spike patterns in another region and of long term memories in cortex . <eos> our results underline that learning processes in realistic networks of spiking neurons depend crucially on the interactions of synaptic plasticity mechanisms with the dynamics of participating neurons .
psychophysical experiments have demonstrated that the brain integrates information from multiple sensory cues in a near bayesian optimal manner . <eos> the present study proposes a novel mechanism to achieve this . <eos> we consider two reciprocally connected networks , mimicking the integration of heading direction information between the dorsal medial superior temporal ( mstd ) and the ventral intraparietal ( vip ) areas . <eos> each network serves as a local estimator and receives an independent cue , either the visual or the vestibular , as direct input for the external stimulus . <eos> we find that positive reciprocal interactions can improve the decoding accuracy of each individual network as if it implements bayesian inference from two cues . <eos> our model successfully explains the experimental finding that both mstd and vip achieve bayesian multisensory integration , though each of them only receives a single cue as direct external input . <eos> our result suggests that the brain may implement optimal information integration distributively at each local estimator through the reciprocal connections between cortical regions .
we investigate a spiking neuron model of multisensory integration . <eos> multiple stimuli from different sensory modalities are encoded by a single neural circuit comprised of a multisensory bank of receptive fields in cascade with a population of biophysical spike generators . <eos> we demonstrate that stimuli of different dimensions can be faithfully multiplexed and encoded in the spike domain and derive tractable algorithms for decoding each stimulus from the common pool of spikes . <eos> we also show that the identification of multisensory processing in a single neuron is dual to the recovery of stimuli encoded with a population of multisensory neurons , and prove that only a projection of the circuit onto input stimuli can be identified . <eos> we provide an example of multisensory integration using natural audio and video and discuss the performance of the proposed decoding and identification algorithms .
we present a recurrent neuronal network , modeled as a continuous-time dynamical system , that can solve constraint satisfaction problems . <eos> discrete variables are represented by coupled winner-take-all ( wta ) networks , and their values are encoded in localized patterns of oscillations that are learned by the recurrent weights in these networks . <eos> constraints over the variables are encoded in the network connectivity . <eos> although there are no sources of noise , the network can escape from local optima in its search for solutions that satisfy all constraints by modifying the effective network connectivity through oscillations . <eos> if there is no solution that satisfies all constraints , the network state changes in a pseudo-random manner and its trajectory approximates a sampling procedure that selects a variable assignment with a probability that increases with the fraction of constraints satisfied by this assignment . <eos> external evidence , or input to the network , can force variables to specific values . <eos> when new inputs are applied , the network re-evaluates the entire set of variables in its search for the states that satisfy the maximum number of constraints , while being consistent with the external input . <eos> our results demonstrate that the proposed network architecture can perform a deterministic search for the optimal solution to problems with non-convex cost functions . <eos> the network is inspired by canonical microcircuit models of the cortex and suggests possible dynamical mechanisms to solve constraint satisfaction problems that can be present in biological networks , or implemented in neuromorphic electronic circuits .
we solve the mean field equations for a stochastic hopfield network with temperature ( noise ) in the presence of strong , i.e. , multiply stored patterns , and use this solution to obtain the storage capacity of such a network . <eos> our result provides for the first time a rigorous solution of the mean field equations for the standard hopfield model and is in contrast to the mathematically unjustifiable replica technique that has been hitherto used for this derivation . <eos> we show that the critical temperature for stability of a strong pattern is equal to its degree or multiplicity , when sum of the cubes of degrees of all stored patterns is negligible compared to the network size . <eos> in the case of a single strong pattern in the presence of simple patterns , when the ratio of the number of all stored patterns and the network size is a positive constant , we obtain the distribution of the overlaps of the patterns with the mean field and deduce that the storage capacity for retrieving a strong pattern exceeds that for retrieving a simple pattern by a multiplicative factor equal to the square of the degree of the strong pattern . <eos> this square law property provides justification for using strong patterns to model attachment types and behavioural prototypes in psychology and psychotherapy .
local competition among neighboring neurons is common in biological neural networks ( nns ) . <eos> we apply the concept to gradient-based , backprop-trained artificial multilayer nns . <eos> nns with competing linear units tend to outperform those with non-competing nonlinear units , and avoid catastrophic forgetting when training sets change over time .
we introduce rnade , a new model for joint density estimation of real-valued vectors . <eos> our model calculates the density of a datapoint as the product of one-dimensional conditionals modeled using mixture density networks with shared parameters . <eos> rnade learns a distributed representation of the data , while having a tractable expression for the calculation of densities . <eos> a tractable likelihood allows direct comparison with other methods and training by standard gradient-based optimizers . <eos> we compare the performance of rnade on several datasets of heterogeneous and perceptual data , finding it outperforms mixture models in all but one case .
with simultaneous measurements from ever increasing populations of neurons , there is a growing need for sophisticated tools to recover signals from individual neurons . <eos> in electrophysiology experiments , this classically proceeds in a two-step process : ( i ) threshold the waveforms to detect putative spikes and ( ii ) cluster the waveforms into single units ( neurons ) . <eos> we extend previous bayesian nonparamet- ric models of neural spiking to jointly detect and cluster neurons using a gamma process model . <eos> importantly , we develop an online approximate inference scheme enabling real-time analysis , with performance exceeding the previous state-of-the- art . <eos> via exploratory data analysis ? using data with partial ground truth as well as two novel data sets ? we find several features of our model collectively contribute to our improved performance including : ( i ) accounting for colored noise , ( ii ) de- tecting overlapping spikes , ( iii ) tracking waveform dynamics , and ( iv ) using mul- tiple channels . <eos> we hope to enable novel experiments simultaneously measuring many thousands of neurons and possibly adapting stimuli dynamically to probe ever deeper into the mysteries of the brain .
this paper considers the problem of transferring experimental findings learned from multiple heterogeneous domains to a target environment , in which only limited experiments can be performed . <eos> we reduce questions of transportability from multiple domains and with limited scope to symbolic derivations in the do-calculus , thus extending the treatment of transportability from full experiments introduced in pearl and bareinboim ( 2011 ) . <eos> we further provide different graphical and algorithmic conditions for computing the transport formula for this setting , that is , a way of fusing the observational and experimental information scattered throughout different domains to synthesize a consistent estimate of the desired effects .
causal inference uses observational data to infer the causal structure of the data generating system . <eos> we study a class of restricted structural equation models for time series that we call time series models with independent noise ( timino ) . <eos> these models require independent residual time series , whereas traditional methods like granger causality exploit the variance of residuals . <eos> this work contains two main contributions : ( 1 ) theoretical : by restricting the model class ( e.g . to additive noise ) we provide more general identifiability results than existing ones . <eos> the results cover lagged and instantaneous effects that can be nonlinear and unfaithful , and non-instantaneous feedbacks between the time series . <eos> ( 2 ) practical : if there are no feedback loops between time series , we propose an algorithm based on non-linear independence tests of time series . <eos> when the data are causally insufficient , or the data generating process does not satisfy the model assumptions , this algorithm may still give partial results , but mostly avoids incorrect answers . <eos> the structural equation model point of view allows us to extend both the theoretical and the algorithmic part to situations in which the time series have been measured with different time delays ( as may happen for fmri data , for example ) . <eos> timino outperforms existing methods on artificial and real data . <eos> code is provided .
we give a polynomial-time algorithm for provably learning the structure and parameters of bipartite noisy-or bayesian networks of binary variables where the top layer is completely hidden . <eos> unsupervised learning of these models is a form of discrete factor analysis , enabling the discovery of hidden variables and their causal relationships with observed data . <eos> we obtain an efficient learning algorithm for a family of bayesian networks that we call quartet-learnable , meaning that every latent variable has four children that do not have any other parents in common . <eos> we show that the existence of such a quartet allows us to uniquely identify each latent variable and to learn all parameters involving that latent variable . <eos> underlying our algorithm are two new techniques for structure learning : a quartet test to determine whether a set of binary variables are singly coupled , and a conditional mutual information test that we use to learn parameters . <eos> we also show how to subtract already learned latent variables from the model to create new singly-coupled quartets , which substantially expands the class of structures that we can learn . <eos> finally , we give a proof of the polynomial sample complexity of our learning algorithm , and experimentally compare it to variational em .
learning dynamic models from observed data has been a central issue in many scientific studies or engineering tasks . <eos> the usual setting is that data are collected sequentially from trajectories of some dynamical system operation . <eos> in quite a few modern scientific modeling tasks , however , it turns out that reliable sequential data are rather difficult to gather , whereas out-of-order snapshots are much easier to obtain . <eos> examples include the modeling of galaxies , chronic diseases such alzheimer 's , or certain biological processes . <eos> existing methods for learning dynamic model from non-sequence data are mostly based on expectation-maximization , which involves non-convex optimization and is thus hard to analyze . <eos> inspired by recent advances in spectral learning methods , we propose to study this problem from a different perspective : moment matching and spectral decomposition . <eos> under that framework , we identify reasonable assumptions on the generative process of non-sequence data , and propose learning algorithms based on the tensor decomposition method \cite { anandkumar2012tensor } to \textit { provably } recover first-order markov models and hidden markov models . <eos> to the best of our knowledge , this is the first formal guarantee on learning from non-sequence data . <eos> preliminary simulation results confirm our theoretical findings .
in this work we develop efficient methods for learning random map predictors for structured label problems . <eos> in particular , we construct posterior distributions over perturbations that can be adjusted via stochastic gradient methods . <eos> we show that every smooth posterior distribution would suffice to define a smooth pac-bayesian risk bound suitable for gradient methods . <eos> in addition , we relate the posterior distributions to computational properties of the map predictors . <eos> we suggest multiplicative posteriors to learn super-modular potential functions that accompany specialized map predictors such as graph-cuts . <eos> we also describe label-augmented posterior models that can use efficient map approximations , such as those arising from linear program relaxations .
markov decision processes ( mdps ) are extremely useful for modeling and solving sequential decision making problems . <eos> graph-based mdps provide a compact representation for mdps with large numbers of random variables . <eos> however , the complexity of exactly solving a graph-based mdp usually grows exponentially in the number of variables , which limits their application . <eos> we present a new variational framework to describe and solve the planning problem of mdps , and derive both exact and approximate planning algorithms . <eos> in particular , by exploiting the graph structure of graph-based mdps , we propose a factored variational value iteration algorithm in which the value function is first approximated by the multiplication of local-scope value functions , then solved by minimizing a kullback-leibler ( kl ) divergence . <eos> the kl divergence is optimized using the belief propagation algorithm , with complexity exponential in only the cluster size of the graph . <eos> experimental comparison on different models shows that our algorithm outperforms existing approximation algorithms at finding good policies .
we present a non-factorized variational method for full posterior inference in bayesian hierarchical models , with the goal of capturing the posterior variable dependencies via efficient and possibly parallel computation . <eos> our approach unifies the integrated nested laplace approximation ( inla ) under the variational framework . <eos> the proposed method is applicable in more challenging scenarios than typically assumed by inla , such as bayesian lasso , which is characterized by the non-differentiability of the $ \ell_ { 1 } $ norm arising from independent laplace priors . <eos> we derive an upper bound for the kullback-leibler divergence , which yields a fast closed-form solution via decoupled optimization . <eos> our method is a reliable analytic alternative to markov chain monte carlo ( mcmc ) , and it results in a tighter evidence lower bound than that of mean-field variational bayes ( vb ) method .
when a probabilistic model and its prior are given , bayesian learning offers inference with automatic parameter tuning . <eos> however , bayesian learning is often obstructed by computational difficulty : the rigorous bayesian learning is intractable in many models , and its variational bayesian ( vb ) approximation is prone to suffer from local minima . <eos> in this paper , we overcome this difficulty for low-rank subspace clustering ( lrsc ) by providing an exact global solver and its efficient approximation . <eos> lrsc extracts a low-dimensional structure of data by embedding samples into the union of low-dimensional subspaces , and its variational bayesian variant has shown good performance . <eos> we first prove a key property that the vb-lrsc model is highly redundant . <eos> thanks to this property , the optimization problem of vb-lrsc can be separated into small subproblems , each of which has only a small number of unknown variables . <eos> our exact global solver relies on another key property that the stationary condition of each subproblem is written as a set of polynomial equations , which is solvable with the homotopy method . <eos> for further computational efficiency , we also propose an efficient approximate variant , of which the stationary condition can be written as a polynomial equation with a single variable . <eos> experimental results show the usefulness of our approach .
expectation propagation ( ep ) is a popular approximate posterior inference algorithm that often provides a fast and accurate alternative to sampling-based methods . <eos> however , while the ep framework in theory allows for complex non-gaussian factors , there is still a significant practical barrier to using them within ep , because doing so requires the implementation of message update operators , which can be difficult and require hand-crafted approximations . <eos> in this work , we study the question of whether it is possible to automatically derive fast and accurate ep updates by learning a discriminative model e.g. , a neural network or random forest ) to map ep message inputs to ep message outputs . <eos> we address the practical concerns that arise in the process , and we provide empirical analysis on several challenging and diverse factors , indicating that there is a space of factors where this approach appears promising .
we consider the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces . <eos> our objective is to propose a canonical model which is easy to train , contains a reduced number of parameters and can scale up to very large databases . <eos> hence , we propose , transe , a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities . <eos> despite its simplicity , this assumption proves to be powerful since extensive experiments show that transe significantly outperforms state-of-the-art methods in link prediction on two knowledge bases . <eos> besides , it can be successfully trained on a large scale data set with 1m entities , 25k relationships and more than 17m training samples .
stochastic block models characterize observed network relationships via latent community memberships . <eos> in large social networks , we expect entities to participate in multiple communities , and the number of communities to grow with the network size . <eos> we introduce a new model for these phenomena , the hierarchical dirichlet process relational model , which allows nodes to have mixed membership in an unbounded set of communities . <eos> to allow scalable learning , we derive an online stochastic variational inference algorithm . <eos> focusing on assortative models of undirected networks , we also propose an efficient structured mean field variational bound , and online methods for automatically pruning unused communities . <eos> compared to state-of-the-art online learning methods for parametric relational models , we show significantly improved perplexity and link prediction accuracy for sparse networks with tens of thousands of nodes . <eos> we also showcase an analysis of littlesis , a large network of who-knows-who at the heights of business and government .
in this paper , we theoretically study the problem of binary classification in the presence of random classification noise -- - the learner , instead of seeing the true labels , sees labels that have independently been flipped with some small probability . <eos> moreover , random label noise is \emph { class-conditional } -- - the flip probability depends on the class . <eos> we provide two approaches to suitably modify any given surrogate loss function . <eos> first , we provide a simple unbiased estimator of any loss , and obtain performance bounds for empirical risk minimization in the presence of iid data with noisy labels . <eos> if the loss function satisfies a simple symmetry condition , we show that the method leads to an efficient algorithm for empirical minimization . <eos> second , by leveraging a reduction of risk minimization under noisy labels to classification with weighted 0-1 loss , we suggest the use of a simple weighted surrogate loss , for which we are able to obtain strong empirical risk bounds . <eos> this approach has a very remarkable consequence -- - methods used in practice such as biased svm and weighted logistic regression are provably noise-tolerant . <eos> on a synthetic non-separable dataset , our methods achieve over 88\ % accuracy even when 40\ % of the labels are corrupted , and are competitive with respect to recently proposed methods for dealing with label noise in several benchmark datasets .
we study the problem of reconstructing low-rank matrices from their noisy observations . <eos> we formulate the problem in the bayesian framework , which allows us to exploit structural properties of matrices in addition to low-rankedness , such as sparsity . <eos> we propose an efficient approximate message passing algorithm , derived from the belief propagation algorithm , to perform the bayesian inference for matrix reconstruction . <eos> we have also successfully applied the proposed algorithm to a clustering problem , by formulating the problem of clustering as a low-rank matrix reconstruction problem with an additional structural property . <eos> numerical experiments show that the proposed algorithm outperforms lloyd 's k-means algorithm .
bilinear approximation of a matrix is a powerful paradigm of unsupervised learning . <eos> in some applications , however , there is a natural hierarchy of concepts that ought to be reflected in the unsupervised analysis . <eos> for example , in the neurosciences image sequence considered here , there are the semantic concepts of pixel $ \rightarrow $ neuron $ \rightarrow $ assembly that should find their counterpart in the unsupervised analysis . <eos> driven by this concrete problem , we propose a decomposition of the matrix of observations into a product of more than two sparse matrices , with the rank decreasing from lower to higher levels . <eos> in contrast to prior work , we allow for both hierarchical and heterarchical relations of lower-level to higher-level concepts . <eos> in addition , we learn the nature of these relations rather than imposing them . <eos> finally , we describe an optimization scheme that allows to optimize the decomposition over all levels jointly , rather than in a greedy level-by-level fashion . <eos> the proposed bilevel shmf ( sparse heterarchical matrix factorization ) is the first formalism that allows to simultaneously interpret a calcium imaging sequence in terms of the constituent neurons , their membership in assemblies , and the time courses of both neurons and assemblies . <eos> experiments show that the proposed model fully recovers the structure from difficult synthetic data designed to imitate the experimental data . <eos> more importantly , bilevel shmf yields plausible interpretations of real-world calcium imaging data .
we study the problem of learning a tensor from a set of linear measurements . <eos> a prominent methodology for this problem is based on the extension of trace norm regularization , which has been used extensively for learning low rank matrices , to the tensor setting . <eos> in this paper , we highlight some limitations of this approach and propose an alternative convex relaxation on the euclidean unit ball . <eos> we then describe a technique to solve the associated regularization problem , which builds upon the alternating direction method of multipliers . <eos> experiments on one synthetic dataset and two real datasets indicate that the proposed method improves significantly over tensor trace norm regularization in terms of estimation error , while remaining computationally tractable .
we present a maximum margin framework that clusters data using latent variables . <eos> using latent representations enables our framework to model unobserved information embedded in the data . <eos> we implement our idea by large margin learning , and develop an alternating descent algorithm to effectively solve the resultant non-convex optimization problem . <eos> we instantiate our latent maximum margin clustering framework with tag-based video clustering tasks , where each video is represented by a latent tag model describing the presence or absence of video tags . <eos> experimental results obtained on three standard datasets show that the proposed method outperforms non-latent maximum margin clustering as well as conventional clustering approaches .
many applications require the analysis of complex interactions between time series . <eos> these interactions can be non-linear and involve vector valued as well as complex data structures such as graphs or strings . <eos> here we provide a general framework for the statistical analysis of these interactions when random variables are sampled from stationary time-series of arbitrary objects . <eos> to achieve this goal we analyze the properties of the kernel cross-spectral density operator induced by positive definite kernels on arbitrary input domains . <eos> this framework enables us to develop an independence test between time series as well as a similarity measure to compare different types of coupling . <eos> the performance of our test is compared to the hsic test using i.i.d . <eos> assumptions , showing improvement in terms of detection errors as well as the suitability of this approach for testing dependency in complex dynamical systems . <eos> finally , we use this approach to characterize complex interactions in electrophysiological neural time series .
kernel embedding of distributions has led to many recent advances in machine learning . <eos> however , latent and low rank structures prevalent in real world distributions have rarely been taken into account in this setting . <eos> furthermore , no prior work in kernel embedding literature has addressed the issue of robust embedding when the latent and low rank information are misspecified . <eos> in this paper , we propose a hierarchical low rank decomposition of kernels embeddings which can exploit such low rank structures in data while being robust to model misspecification . <eos> we also illustrate with empirical evidence that the estimated low rank embeddings lead to improved performance in density estimation .
we propose a family of maximum mean discrepancy ( mmd ) kernel two-sample tests that have low sample complexity and are consistent . <eos> the test has a hyperparameter that allows one to control the tradeoff between sample complexity and computational time . <eos> our family of tests , which we denote as b-tests , is both computationally and statistically efficient , combining favorable properties of previously proposed mmd two-sample tests . <eos> it does so by better leveraging samples to produce low variance estimates in the finite sample case , while avoiding a quadratic number of kernel evaluations and complex null-hypothesis approximation as would be required by tests relying on one sample u-statistics . <eos> the b-test uses a smaller than quadratic number of kernel evaluations and avoids completely the computational burden of complex null-hypothesis approximation while maintaining consistency and probabilistically conservative thresholds on type i error . <eos> finally , recent results of combining multiple kernels transfer seamlessly to our hypothesis test , allowing a further increase in discriminative power and decrease in sample complexity .
we study in this paper flat and hierarchical classification strategies in the context of large-scale taxonomies . <eos> to this end , we first propose a multiclass , hierarchical data dependent bound on the generalization error of classifiers deployed in large-scale taxonomies . <eos> this bound provides an explanation to several empirical results reported in the literature , related to the performance of flat and hierarchical classifiers . <eos> we then introduce another type of bounds targeting the approximation error of a family of classifiers , and derive from it features used in a meta-classifier to decide which nodes to prune ( or flatten ) in a large-scale taxonomy . <eos> we finally illustrate the theoretical developments through several experiments conducted on two widely used taxonomies .
this paper presents an approach to multilabel classification ( mlc ) with a large number of labels . <eos> our approach is a reduction to binary classification in which label sets are represented by low dimensional binary vectors . <eos> this representation follows the principle of bloom filters , a space-efficient data structure originally designed for approximate membership testing . <eos> we show that a naive application of bloom filters in mlc is not robust to individual binary classifiers ' errors . <eos> we then present an approach that exploits a specific feature of real-world datasets when the number of labels is large : many labels ( almost ) never appear together . <eos> our approch is provably robust , has sublinear training and inference complexity with respect to the number of labels , and compares favorably to state-of-the-art algorithms on two large scale multilabel datasets .
the estimation of dependencies between multiple variables is a central problem in the analysis of financial time series . <eos> a common approach is to express these dependencies in terms of a copula function . <eos> typically the copula function is assumed to be constant but this may be innacurate when there are covariates that could have a large influence on the dependence structure of the data . <eos> to account for this , a bayesian framework for the estimation of conditional copulas is proposed . <eos> in this framework the parameters of a copula are non-linearly related to some arbitrary conditioning variables . <eos> we evaluate the ability of our method to predict time-varying dependencies on several equities and currencies and observe consistent performance gains compared to static copula models and other time-varying copula methods .
state-space models are successfully used in many areas of science , engineering and economics to model time series and dynamical systems . <eos> we present a fully bayesian approach to inference and learning in nonlinear nonparametric state-space models . <eos> we place a gaussian process prior over the transition dynamics , resulting in a flexible model able to capture complex dynamical phenomena . <eos> however , to enable efficient inference , we marginalize over the dynamics of the model and instead infer directly the joint smoothing distribution through the use of specially tailored particle markov chain monte carlo samplers . <eos> once an approximation of the smoothing distribution is computed , the state transition predictive distribution can be formulated analytically . <eos> we make use of sparse gaussian process models to greatly reduce the computational complexity of the approach .
bayesian optimization has recently been proposed as a framework for automatically tuning the hyperparameters of machine learning models and has been shown to yield state-of-the-art performance with impressive ease and efficiency . <eos> in this paper , we explore whether it is possible to transfer the knowledge gained from previous optimizations to new tasks in order to find optimal hyperparameter settings more efficiently . <eos> our approach is based on extending multi-task gaussian processes to the framework of bayesian optimization . <eos> we show that this method significantly speeds up the optimization process when compared to the standard single-task approach . <eos> we further propose a straightforward extension of our algorithm in order to jointly minimize the average error across multiple tasks and demonstrate how this can be used to greatly speed up $ k $ -fold cross-validation . <eos> lastly , our most significant contribution is an adaptation of a recently proposed acquisition function , entropy search , to the cost-sensitive and multi-task settings . <eos> we demonstrate the utility of this new acquisition function by utilizing a small dataset in order to explore hyperparameter settings for a large dataset . <eos> our algorithm dynamically chooses which dataset to query in order to yield the most information per unit cost .
we propose an efficient discrete optimization algorithm for selecting a subset of training data to induce sparsity for gaussian process regression . <eos> the algorithm estimates this inducing set and the hyperparameters using a single objective , either the marginal likelihood or a variational free energy . <eos> the space and time complexity are linear in the training set size , and the algorithm can be applied to large regression problems on discrete or continuous domains . <eos> empirical evaluation shows state-of-art performance in the discrete case and competitive results in the continuous case .
we introduce a novel variational method that allows to approximately integrate out kernel hyperparameters , such as length-scales , in gaussian process regression . <eos> this approach consists of a novel variant of the variational framework that has been recently developed for the gaussian process latent variable model which additionally makes use of a standardised representation of the gaussian process . <eos> we consider this technique for learning mahalanobis distance metrics in a gaussian process regression setting and provide experimental evaluations and comparisons with existing methods by considering datasets with high-dimensional inputs .
multi-task prediction models are widely being used to couple regressors or classification models by sharing information across related tasks . <eos> a common pitfall of these models is that they assume that the output tasks are independent conditioned on the inputs . <eos> here , we propose a multi-task gaussian process approach to model both the relatedness between regressors as well as the task correlations in the residuals , in order to more accurately identify true sharing between regressors . <eos> the resulting gaussian model has a covariance term that is the sum of kronecker products , for which efficient parameter inference and out of sample prediction are feasible . <eos> on both synthetic examples and applications to phenotype prediction in genetics , we find substantial benefits of modeling structured noise compared to established alternatives .
entropy rate quantifies the amount of disorder in a stochastic process . <eos> for spiking neurons , the entropy rate places an upper bound on the rate at which the spike train can convey stimulus information , and a large literature has focused on the problem of estimating entropy rate from spike train data . <eos> here we present bayes least squares and empirical bayesian entropy rate estimators for binary spike trains using hierarchical dirichlet process ( hdp ) priors . <eos> our estimator leverages the fact that the entropy rate of an ergodic markov chain with known transition probabilities can be calculated analytically , and many stochastic processes that are non-markovian can still be well approximated by markov processes of sufficient depth . <eos> choosing an appropriate depth of markov model presents challenges due to possibly long time dependencies and short data sequences : a deeper model can better account for long time-dependencies , but is more difficult to infer from limited data . <eos> our approach mitigates this difficulty by using a hierarchical prior to share statistical power across markov chains of different depths . <eos> we present both a fully bayesian and empirical bayes entropy rate estimator based on this model , and demonstrate their performance on simulated and real neural spike train data .
we consider design of linear projection measurements for a vector poisson signal model . <eos> the projections are performed on the vector poisson rate , $ x\in\mathbb { r } _+^n $ , and the observed data are a vector of counts , $ y\in\mathbb { z } _+^m $ . <eos> the projection matrix is designed by maximizing mutual information between $ y $ and $ x $ , $ i ( y ; x ) $ . <eos> when there is a latent class label $ c\in\ { 1 , \dots , l\ } $ associated with $ x $ , we consider the mutual information with respect to $ y $ and $ c $ , $ i ( y ; c ) $ . <eos> new analytic expressions for the gradient of $ i ( y ; x ) $ and $ i ( y ; c ) $ are presented , with gradient performed with respect to the measurement matrix . <eos> connections are made to the more widely studied gaussian measurement model . <eos> example results are presented for compressive topic modeling of a document corpora ( word counting ) , and hyperspectral compressive sensing for chemical classification ( photon counting ) .
we provide a unified framework for the high-dimensional analysis of ? superposition-structured or ? dirty statistical models : where the model parameters are a ? superposition of structurally constrained parameters . <eos> we allow for any number and types of structures , and any statistical model . <eos> we consider the general class of $ m $ -estimators that minimize the sum of any loss function , and an instance of what we call a ? hybrid regularization , that is the infimal convolution of weighted regularization functions , one for each structural component . <eos> we provide corollaries showcasing our unified framework for varied statistical models such as linear regression , multiple regression and principal component analysis , over varied superposition structures .
infinite mixture models are commonly used for clustering . <eos> one can sample from the posterior of mixture assignments by monte carlo methods or find its maximum a posteriori solution by optimization . <eos> however , in some problems the posterior is diffuse and it is hard to interpret the sampled partitionings . <eos> in this paper , we introduce novel statistics based on block sizes for representing sample sets of partitionings and feature allocations . <eos> we develop an element-based definition of entropy to quantify segmentation among their elements . <eos> then we propose a simple algorithm called entropy agglomeration ( ea ) to summarize and visualize this information . <eos> experiments on various infinite mixture posteriors as well as a feature allocation dataset demonstrate that the proposed statistics are useful in practice .
this paper presents a novel algorithm , based upon the dependent dirichlet process mixture model ( ddpmm ) , for clustering batch-sequential data containing an unknown number of evolving clusters . <eos> the algorithm is derived via a low-variance asymptotic analysis of the gibbs sampling algorithm for the ddpmm , and provides a hard clustering with convergence guarantees similar to those of the k-means algorithm . <eos> empirical results from a synthetic test with moving gaussian clusters and a test with real ads-b aircraft trajectory data demonstrate that the algorithm requires orders of magnitude less computational time than contemporary probabilistic and hard clustering algorithms , while providing higher accuracy on the examined datasets .
in this paper , we study the following new variant of prototype learning , called { \em $ k $ -prototype learning problem for 3d rigid structures } : given a set of 3d rigid structures , find a set of $ k $ rigid structures so that each of them is a prototype for a cluster of the given rigid structures and the total cost ( or dissimilarity ) is minimized . <eos> prototype learning is a core problem in machine learning and has a wide range of applications in many areas . <eos> existing results on this problem have mainly focused on the graph domain . <eos> in this paper , we present the first algorithm for learning multiple prototypes from 3d rigid structures . <eos> our result is based on a number of new insights to rigid structures alignment , clustering , and prototype reconstruction , and is practically efficient with quality guarantee . <eos> we validate our approach using two type of data sets , random data and biological data of chromosome territories . <eos> experiments suggest that our approach can effectively learn prototypes in both types of data .
this paper provides new algorithms for distributed clustering for two popular center-based objectives , $ k $ -median and $ k $ -means . <eos> these algorithms have provable guarantees and improve communication complexity over existing approaches . <eos> following a classic approach in clustering by \cite { har2004coresets } , we reduce the problem of finding a clustering with low cost to the problem of finding a ` coreset ' of small size . <eos> we provide a distributed method for constructing a global coreset which improves over the previous methods by reducing the communication complexity , and which works over general communication topologies . <eos> we provide experimental evidence for this approach on both synthetic and real data sets .
ideas from the image processing literature have recently motivated a new set of clustering algorithms that rely on the concept of total variation . <eos> while these algorithms perform well for bi-partitioning tasks , their recursive extensions yield unimpressive results for multiclass clustering tasks . <eos> this paper presents a general framework for multiclass total variation clustering that does not rely on recursion . <eos> the results greatly outperform previous total variation algorithms and compare well with state-of-the-art nmf approaches .
we consider the general problem of multiple model learning ( mml ) from data , from the statistical and algorithmic perspectives ; this problem includes clustering , multiple regression and subspace clustering as special cases . <eos> a common approach to solving new mml problems is to generalize lloyd 's algorithm for clustering ( or expectation-maximization for soft clustering ) . <eos> however this approach is unfortunately sensitive to outliers and large noise : a single exceptional point may take over one of the models . <eos> we propose a different general formulation that seeks for each model a distribution over data points ; the weights are regularized to be sufficiently spread out . <eos> this enhances robustness by making assumptions on class balance . <eos> we further provide generalization bounds and explain how the new iterations may be computed efficiently . <eos> we demonstrate the robustness benefits of our approach with some experimental results and prove for the important case of clustering that our approach has a non-trivial breakdown point , i.e. , is guaranteed to be robust to a fixed percentage of adversarial unbounded outliers .
spectral clustering is a fast and popular algorithm for finding clusters in networks . <eos> recently , chaudhuri et al . and amini et al . proposed variations on the algorithm that artificially inflate the node degrees for improved statistical performance . <eos> the current paper extends the previous theoretical results to the more canonical spectral clustering algorithm in a way that removes any assumption on the minimum degree and provides guidance on the choice of tuning parameter . <eos> moreover , our results show how the star shape '' in the eigenvectors -- which are consistently observed in empirical networks -- can be explained by the degree-corrected stochastic blockmodel and the extended planted partition model , two statistical model that allow for highly heterogeneous degrees . <eos> throughout , the paper characterizes and justifies several of the variations of the spectral clustering algorithm in terms of these models. ``
suppose $ k $ centers are fit to $ m $ points by heuristically minimizing the $ k $ -means cost ; what is the corresponding fit over the source distribution ? <eos> this question is resolved here for distributions with $ p\geq 4 $ bounded moments ; in particular , the difference between the sample cost and distribution cost decays with $ m $ and $ p $ as $ m^ { \min\ { -1/4 , -1/2+2/p\ } } $ . <eos> the essential technical contribution is a mechanism to uniformly control deviations in the face of unbounded parameter sets , cost functions , and source distributions . <eos> to further demonstrate this mechanism , a soft clustering variant of $ k $ -means cost is also considered , namely the log likelihood of a gaussian mixture , subject to the constraint that all covariance matrices have bounded spectrum . <eos> lastly , a rate with refined constants is provided for $ k $ -means instances possessing some cluster structure .
we describe a framework for designing efficient active learning algorithms that are tolerant to random classification noise . <eos> the framework is based on active learning algorithms that are statistical in the sense that they rely on estimates of expectations of functions of filtered random examples . <eos> it builds on the powerful statistical query framework of kearns ( 1993 ) . <eos> we show that any efficient active statistical learning algorithm can be automatically converted to an efficient active learning algorithm which is tolerant to random classification noise as well as other forms of uncorrelated '' noise . <eos> the complexity of the resulting algorithms has information-theoretically optimal quadratic dependence on $ 1/ ( 1-2\eta ) $ , where $ \eta $ is the noise rate . <eos> we demonstrate the power of our framework by showing that commonly studied concept classes including thresholds , rectangles , and linear separators can be efficiently actively learned in our framework . <eos> these results combined with our generic conversion lead to the first known computationally-efficient algorithms for actively learning some of these concept classes in the presence of random classification noise that provide exponential improvement in the dependence on the error $ \epsilon $ over their passive counterparts . <eos> in addition , we show that our algorithms can be automatically converted to efficient active differentially-private algorithms . <eos> this leads to the first differentially-private active learning algorithms with exponential label savings over the passive case . ''
we informally call a stochastic process learnable if it admits a generalization error approaching zero in probability for any concept class with finite vc-dimension ( iid processes are the simplest example ) . <eos> a mixture of learnable processes need not be learnable itself , and certainly its generalization error need not decay at the same rate . <eos> in this paper , we argue that it is natural in predictive pac to condition not on the past observations but on the mixture component of the sample path . <eos> this definition not only matches what a realistic learner might demand , but also allows us to sidestep several otherwise grave problems in learning from dependent data . <eos> in particular , we give a novel pac generalization bound for mixtures of learnable processes with a generalization error that is not worse than that of each mixture component . <eos> we also provide a characterization of mixtures of absolutely regular ( $ \beta $ -mixing ) processes , of independent interest .
we present the first result for kernel regression where the procedure adapts locally at a point $ x $ to both the unknown local dimension of the metric and the unknown h\ { o } lder-continuity of the regression function at $ x $ . <eos> the result holds with high probability simultaneously at all points $ x $ in a metric space of unknown structure . ''
the lasso is a cornerstone of modern multivariate data analysis , yet its performance suffers in the common situation in which covariates are correlated . <eos> this limitation has led to a growing number of \emph { preconditioned lasso } algorithms that pre-multiply $ x $ and $ y $ by matrices $ p_x $ , $ p_y $ prior to running the standard lasso . <eos> a direct comparison of these and similar lasso-style algorithms to the original lasso is difficult because the performance of all of these methods depends critically on an auxiliary penalty parameter $ \lambda $ . <eos> in this paper we propose an agnostic , theoretical framework for comparing preconditioned lasso algorithms to the lasso without having to choose $ \lambda $ . <eos> we apply our framework to three preconditioned lasso instances and highlight when they will outperform the lasso . <eos> additionally , our theory offers insights into the fragilities of these algorithms to which we provide partial solutions .
we address the problem of fast estimation of ordinary least squares ( ols ) from large amounts of data ( $ n \gg p $ ) . <eos> we propose three methods which solve the big data problem by subsampling the covariance matrix using either a single or two stage estimation . <eos> all three run in the order of size of input i.e . o ( $ np $ ) and our best method , { \it uluru } , gives an error bound of $ o ( \sqrt { p/n } ) $ which is independent of the amount of subsampling as long as it is above a threshold . <eos> we provide theoretical bounds for our algorithms in the fixed design ( with randomized hadamard preconditioning ) as well as sub-gaussian random design setting . <eos> we also compare the performance of our methods on synthetic and real-world datasets and show that if observations are i.i.d. , sub-gaussian then one can directly subsample without the expensive randomized hadamard preconditioning without loss of accuracy .
we propose a fast algorithm for ridge regression when the number of features is much larger than the number of observations ( $ p \gg n $ ) . <eos> the standard way to solve ridge regression in this setting works in the dual space and gives a running time of $ o ( n^2p ) $ . <eos> our algorithm ( srht-drr ) runs in time $ o ( np\log ( n ) ) $ and works by preconditioning the design matrix by a randomized walsh-hadamard transform with a subsequent subsampling of features . <eos> we provide risk bounds for our srht-drr algorithm in the fixed design setting and show experimental results on synthetic and real datasets .
learning from prior tasks and transferring that experience to improve future performance is critical for building lifelong learning agents . <eos> although results in supervised and reinforcement learning show that transfer may significantly improve the learning performance , most of the literature on transfer is focused on batch learning tasks . <eos> in this paper we study the problem of sequential transfer in online learning , notably in the multi-arm bandit framework , where the objective is to minimize the cumulative regret over a sequence of tasks by incrementally transferring knowledge from prior tasks . <eos> we introduce a novel bandit algorithm based on a method-of-moments approach for the estimation of the possible tasks and derive regret bounds for it .
we consider the stochastic multi-armed bandit problem with a prior distribution on the reward distributions . <eos> we are interested in studying prior-free and prior-dependent regret bounds , very much in the same spirit than the usual distribution-free and distribution-dependent bounds for the non-bayesian stochastic bandit . <eos> we first show that thompson sampling attains an optimal prior-free bound in the sense that for any prior distribution its bayesian regret is bounded from above by $ 14 \sqrt { n k } $ . <eos> this result is unimprovable in the sense that there exists a prior distribution such that any algorithm has a bayesian regret bounded from below by $ \frac { 1 } { 20 } \sqrt { n k } $ . <eos> we also study the case of priors for the setting of bubeck et al . [ 2013 ] ( where the optimal mean is known as well as a lower bound on the smallest gap ) and we show that in this case the regret of thompson sampling is in fact uniformly bounded over time , thus showing that thompson sampling can greatly take advantage of the nice properties of these priors .
we consider an infinite-armed bandit problem with bernoulli rewards . <eos> the mean rewards are independent , uniformly distributed over $ [ 0,1 ] $ . <eos> rewards 0 and 1 are referred to as a success and a failure , respectively . <eos> we propose a novel algorithm where the decision to exploit any arm is based on two successive targets , namely , the total number of successes until the first failure and the first $ m $ failures , respectively , where $ m $ is a fixed parameter . <eos> this two-target algorithm achieves a long-term average regret in $ \sqrt { 2n } $ for a large parameter $ m $ and a known time horizon $ n $ . <eos> this regret is optimal and strictly less than the regret achieved by the best known algorithms , which is in $ 2\sqrt { n } $ . <eos> the results are extended to any mean-reward distribution whose support contains 1 and to unknown time horizons . <eos> numerical experiments show the performance of the algorithm for finite time horizons .
thompson sampling has been demonstrated in many complex bandit models , however the theoretical guarantees available for the parametric multi-armed bandit are still limited to the bernoulli case . <eos> here we extend them by proving asymptotic optimality of the algorithm using the jeffreys prior for $ 1 $ -dimensional exponential family bandits . <eos> our proof builds on previous work , but also makes extensive use of closed forms for kullback-leibler divergence and fisher information ( and thus jeffreys prior ) available in an exponential family . <eos> this allow us to give a finite time exponential concentration inequality for posterior distributions on exponential families that may be of interest in its own right . <eos> moreover our analysis covers some distributions for which no optimistic algorithm has yet been proposed , including heavy-tailed exponential families .
monte-carlo tree search is drawing great interest in the domain of planning under uncertainty , particularly when little or no domain knowledge is available . <eos> one of the central problems is the trade-off between exploration and exploitation . <eos> in this paper we present a novel bayesian mixture modelling and inference based thompson sampling approach to addressing this dilemma . <eos> the proposed dirichlet-normalgamma mcts ( dng-mcts ) algorithm represents the uncertainty of the accumulated reward for actions in the mcts search tree as a mixture of normal distributions and inferences on it in bayesian settings by choosing conjugate priors in the form of combinations of dirichlet and normalgamma distributions . <eos> thompson sampling is used to select the best action at each decision node . <eos> experimental results show that our proposed algorithm has achieved the state-of-the-art comparing with popular uct algorithm in the context of online planning for general markov decision processes .
consider an unweighted k-nearest neighbor graph on n points that have been sampled i.i.d . <eos> from some unknown density p on r^d . <eos> we prove how one can estimate the density p just from the unweighted adjacency matrix of the graph , without knowing the points themselves or their distance or similarity scores . <eos> the key insights are that local differences in link numbers can be used to estimate some local function of p , and that integrating this function along shortest paths leads to an estimate of the underlying density .
motivated by the desire to extend fast randomized techniques to nonlinear $ l_p $ regression , we consider a class of structured regression problems . <eos> these problems involve vandermonde matrices which arise naturally in various statistical modeling settings , including classical polynomial fitting problems and recently developed randomized techniques for scalable kernel methods . <eos> we show that this structure can be exploited to further accelerate the solution of the regression problem , achieving running times that are faster than input sparsity '' . <eos> we present empirical results confirming both the practical value of our modeling framework , as well as speedup benefits of randomized regression . ''
we present and study a distributed optimization algorithm by employing a stochastic dual coordinate ascent method . <eos> stochastic dual coordinate ascent methods enjoy strong theoretical guarantees and often have better performances than stochastic gradient descent methods in optimizing regularized loss minimization problems . <eos> it still lacks of efforts in studying them in a distributed framework . <eos> we make a progress along the line by presenting a distributed stochastic dual coordinate ascent algorithm in a star network , with an analysis of the tradeoff between computation and communication . <eos> we verify our analysis by experiments on real data sets . <eos> moreover , we compare the proposed algorithm with distributed stochastic gradient descent methods and distributed alternating direction methods of multipliers for optimizing svms in the same distributed framework , and observe competitive performances .
in modeling multivariate time series , it is important to allow time-varying smoothness in the mean and covariance process . <eos> in particular , there may be certain time intervals exhibiting rapid changes and others in which changes are slow . <eos> if such locally adaptive smoothness is not accounted for , one can obtain misleading inferences and predictions , with over-smoothing across erratic time intervals and under-smoothing across times exhibiting slow variation . <eos> this can lead to miscalibration of predictive intervals , which can be substantially too narrow or wide depending on the time . <eos> we propose a continuous multivariate stochastic process for time series having locally varying smoothness in both the mean and covariance matrix . <eos> this process is constructed utilizing latent dictionary functions in time , which are given nested gaussian process priors and linearly related to the observed data through a sparse mapping . <eos> using a differential equation representation , we bypass usual computational bottlenecks in obtaining mcmc and online algorithms for approximate bayesian inference . <eos> the performance is assessed in simulations and illustrated in a financial application .
for classifying time series , a nearest-neighbor approach is widely used in practice with performance often competitive with or better than more elaborate methods such as neural networks , decision trees , and support vector machines . <eos> we develop theoretical justification for the effectiveness of nearest-neighbor-like classification of time series . <eos> our guiding hypothesis is that in many applications , such as forecasting which topics will become trends on twitter , there are n't actually that many prototypical time series to begin with , relative to the number of time series we have access to , e.g. , topics become trends on twitter only in a few distinct manners whereas we can collect massive amounts of twitter data . <eos> to operationalize this hypothesis , we propose a latent source model for time series , which naturally leads to a weighted majority voting '' classification rule that can be approximated by a nearest-neighbor classifier . <eos> we establish nonasymptotic performance guarantees of both weighted majority voting and nearest-neighbor classification under our model accounting for how much of the time series we observe and the model complexity . <eos> experimental results on synthetic data show weighted majority voting achieving the same misclassification rate as nearest-neighbor classification while observing less of the time series . <eos> we then use weighted majority to forecast which news topics on twitter become trends , where we are able to detect such `` trending topics '' in advance of twitter 79 % of the time , with a mean early advantage of 1 hour and 26 minutes , a true positive rate of 95 % , and a false positive rate of 4 % . ''
many scientific data occur as sequences of multidimensional arrays called tensors . <eos> how can hidden , evolving trends in such data be extracted while preserving the tensor structure ? <eos> the model that is traditionally used is the linear dynamical system ( lds ) , which treats the observation at each time slice as a vector . <eos> in this paper , we propose the multilinear dynamical system ( mlds ) for modeling tensor time series and an expectation-maximization ( em ) algorithm to estimate the parameters . <eos> the mlds models each time slice of the tensor time series as the multilinear projection of a corresponding member of a sequence of latent , low-dimensional tensors . <eos> compared to the lds with an equal number of parameters , the mlds achieves higher prediction accuracy and marginal likelihood for both simulated and real datasets .
numerous datasets ranging from group memberships within social networks to purchase histories on e-commerce sites are represented by binary matrices . <eos> while this data is often either proprietary or sensitive , aggregated data , notably row and column marginals , is often viewed as much less sensitive , and may be furnished for analysis . <eos> here , we investigate how these data can be exploited to make inferences about the underlying matrix h. instead of assuming a generative model for h , we view the input marginals as constraints on the dataspace of possible realizations of h and compute the probability density function of particular entries h ( i , j ) of interest . <eos> we do this , for all the cells of h simultaneously , without generating realizations but rather via implicitly sampling the datasets that satisfy the input marginals . <eos> the end result is an efficient algorithm with running time equal to the time required by standard sampling techniques to generate a single dataset from the same dataspace . <eos> our experimental evaluation demonstrates the efficiency and the efficacy of our framework in multiple settings .
we propose a general framework for reconstructing and denoising single entries of incomplete and noisy entries . <eos> we describe : effective algorithms for deciding if and entry can be reconstructed and , if so , for reconstructing and denoising it ; and a priori bounds on the error of each entry , individually . <eos> in the noiseless case our algorithm is exact . <eos> for rank-one matrices , the new algorithm is fast , admits a highly-parallel implementation , and produces an error minimizing estimate that is qualitatively close to our theoretical and the state-of-the-art nuclear norm and optspace methods .
most current planners assume complete domain models and focus on generating correct plans . <eos> unfortunately , domain modeling is a laborious and error-prone task , thus real world agents have to plan with incomplete domain models . <eos> while domain experts can not guarantee completeness , often they are able to circumscribe the incompleteness of the model by providing annotations as to which parts of the domain model may be incomplete . <eos> in such cases , the goal should be to synthesize plans that are robust with respect to any known incompleteness of the domain . <eos> in this paper , we first introduce annotations expressing the knowledge of the domain incompleteness and formalize the notion of plan robustness with respect to an incomplete domain model . <eos> we then show an approach to compiling the problem of finding robust plans to the conformant probabilistic planning problem , and present experimental results with probabilistic-ff planner .
we consider the task of nearest-neighbor search with the class of binary-space-partitioning trees , which includes kd-trees , principal axis trees and random projection trees , and try to rigorously answer the question which tree to use for nearest-neighbor search ? '' <eos> to this end , we present the theoretical results which imply that trees with better vector quantization performance have better search performance guarantees . <eos> we also explore another factor affecting the search performance -- margins of the partitions in these trees . <eos> we demonstrate , both theoretically and empirically , that large margin partitions can improve the search performance of a space-partitioning tree. ``
the markov chain is a convenient tool to represent the dynamics of complex systems such as traffic and social systems , where probabilistic transition takes place between internal states . <eos> a markov chain is characterized by initial-state probabilities and a state-transition probability matrix . <eos> in the traditional setting , a major goal is to figure out properties of a markov chain when those probabilities are known . <eos> this paper tackles an inverse version of the problem : we find those probabilities from partial observations at a limited number of states . <eos> the observations include the frequency of visiting a state and the rate of reaching a state from another . <eos> practical examples of this task include traffic monitoring systems in cities , where we need to infer the traffic volume on every single link on a road network from a very limited number of observation points . <eos> we formulate this task as a regularized optimization problem for probability functions , which is efficiently solved using the notion of natural gradient . <eos> using synthetic and real-world data sets including city traffic monitoring data , we demonstrate the effectiveness of our method .
in stochastic optimal control the distribution of the exogenous noise is typically unknown and must be inferred from limited data before dynamic programming ( dp ) -based solution schemes can be applied . <eos> if the conditional expectations in the dp recursions are estimated via kernel regression , however , the historical sample paths enter the solution procedure directly as they determine the evaluation points of the cost-to-go functions . <eos> the resulting data-driven dp scheme is asymptotically consistent and admits efficient computational solution when combined with parametric value function approximations . <eos> if training data is sparse , however , the estimated cost-to-go functions display a high variability and an optimistic bias , while the corresponding control policies perform poorly in out-of-sample tests . <eos> to mitigate these small sample effects , we propose a robust data-driven dp scheme , which replaces the expectations in the dp recursions with worst-case expectations over a set of distributions close to the best estimate . <eos> we show that the arising min-max problems in the dp recursions reduce to tractable conic programs . <eos> we also demonstrate that this robust algorithm dominates state-of-the-art benchmark algorithms in out-of-sample tests across several application domains .
the bayesian online change point detection ( bocpd ) algorithm provides an efficient way to do exact inference when the parameters of an underlying model may suddenly change over time . <eos> bocpd requires computation of the underlying model 's posterior predictives , which can only be computed online in $ o ( 1 ) $ time and memory for exponential family models . <eos> we develop variational approximations to the posterior on change point times ( formulated as run lengths ) for efficient inference when the underlying model is not in the exponential family , and does not have tractable posterior predictive distributions . <eos> in doing so , we develop improvements to online variational inference . <eos> we apply our methodology to a tracking problem using radar data with a signal-to-noise feature that is rice distributed . <eos> we also develop a variational method for inferring the parameters of the ( non-exponential family ) rice distribution .
in this paper we introduce a novel method that can efficiently estimate a family of hierarchical dense sets in high-dimensional distributions . <eos> our method can be regarded as a natural extension of the one-class svm ( ocsvm ) algorithm that finds multiple parallel separating hyperplanes in a reproducing kernel hilbert space . <eos> we call our method q-ocsvm , as it can be used to estimate $ q $ quantiles of a high-dimensional distribution . <eos> for this purpose , we introduce a new global convex optimization program that finds all estimated sets at once and show that it can be solved efficiently . <eos> we prove the correctness of our method and present empirical results that demonstrate its superiority over existing methods .
stochastic and-or grammars compactly represent both compositionality and reconfigurability and have been used to model different types of data such as images and events . <eos> we present a unified formalization of stochastic and-or grammars that is agnostic to the type of the data being modeled , and propose an unsupervised approach to learning the structures as well as the parameters of such grammars . <eos> starting from a trivial initial grammar , our approach iteratively induces compositions and reconfigurations in a unified manner and optimizes the posterior probability of the grammar . <eos> in our empirical evaluation , we applied our approach to learning event grammars and image grammars and achieved comparable or better performance than previous approaches .
distance-based approaches to outlier detection are popular in data mining , as they do not require to model the underlying probability distribution , which is particularly challenging for high-dimensional data . <eos> we present an empirical comparison of various approaches to distance-based outlier detection across a large number of datasets . <eos> we report the surprising observation that a simple , sampling-based scheme outperforms state-of-the-art techniques in terms of both efficiency and effectiveness . <eos> to better understand this phenomenon , we provide a theoretical analysis why the sampling-based approach outperforms alternative methods based on k-nearest neighbor search .
people can learn a new visual class from just one example , yet machine learning algorithms typically require hundreds or thousands of examples to tackle the same problems . <eos> here we present a hierarchical bayesian model based on compositionality and causality that can learn a wide range of natural ( although simple ) visual concepts , generalizing in human-like ways from just one image . <eos> we evaluated performance on a challenging one-shot classification task , where our model achieved a human-level error rate while substantially outperforming two deep learning models . <eos> we also used a visual turing test '' to show that our model produces human-like performance on other conceptual tasks , including generating new examples and parsing . ''
majorization-minimization algorithms consist of iteratively minimizing a majorizing surrogate of an objective function . <eos> because of its simplicity and its wide applicability , this principle has been very popular in statistics and in signal processing . <eos> in this paper , we intend to make this principle scalable . <eos> we introduce a stochastic majorization-minimization scheme which is able to deal with large-scale or possibly infinite data sets . <eos> when applied to convex optimization problems under suitable assumptions , we show that it achieves an expected convergence rate of $ o ( 1/\sqrt { n } ) $ after~ $ n $ iterations , and of $ o ( 1/n ) $ for strongly convex functions . <eos> equally important , our scheme almost surely converges to stationary points for a large class of non-convex problems . <eos> we develop several efficient algorithms based on our framework . <eos> first , we propose a new stochastic proximal gradient method , which experimentally matches state-of-the-art solvers for large-scale $ \ell_1 $ -logistic regression . <eos> second , we develop an online dc programming algorithm for non-convex sparse estimation . <eos> finally , we demonstrate the effectiveness of our technique for solving large-scale structured matrix factorization problems .
principal component analysis ( pca ) , a well-established technique for data analysis and processing , provides a convenient form of dimensionality reduction that is effective for cleaning small gaussian noises presented in the data . <eos> however , the applicability of standard principal component analysis in real scenarios is limited by its sensitivity to large errors . <eos> in this paper , we tackle the challenge problem of recovering data corrupted with errors of high magnitude by developing a novel robust transfer principal component analysis method . <eos> our method is based on the assumption that useful information for the recovery of a corrupted data matrix can be gained from an uncorrupted related data matrix . <eos> speci ? cally , we formulate the data recovery problem as a joint robust principal component analysis problem on the two data matrices , with shared common principal components across matrices and individual principal components speci ? c to each data matrix . <eos> the formulated optimization problem is a minimization problem over a convex objective function but with non-convex rank constraints . <eos> we develop an ef ? cient proximal projected gradient descent algorithm to solve the proposed optimization problem with convergence guarantees . <eos> our empirical results over image denoising tasks show the proposed method can effectively recover images with random large errors , and signi ? cantly outperform both standard pca and robust pca .
robust pca methods are typically based on batch optimization and have to load all the samples into memory . <eos> this prevents them from efficiently processing big data . <eos> in this paper , we develop an online robust principal component analysis ( or-pca ) that processes one sample per time instance and hence its memory cost is independent of the data size , significantly enhancing the computation and storage efficiency . <eos> the proposed method is based on stochastic optimization of an equivalent reformulation of the batch rpca method . <eos> indeed , we show that or-pca provides a sequence of subspace estimations converging to the optimum of its batch counterpart and hence is provably robust to sparse corruption . <eos> moreover , or-pca can naturally be applied for tracking dynamic subspace . <eos> comprehensive simulations on subspace recovering and tracking demonstrate the robustness and efficiency advantages of the or-pca over online pca and batch rpca methods .
we prove the first finite-sample convergence rates for any incremental pca algorithm using sub-quadratic time and memory per iteration . <eos> the algorithm analyzed is oja 's learning rule , an efficient and well-known scheme for estimating the top principal component . <eos> our analysis of this non-convex problem yields expected and high-probability convergence rates of $ \tilde { o } ( 1/n ) $ through a novel technique . <eos> we relate our guarantees to existing rates for stochastic gradient descent on strongly convex functions , and extend those results . <eos> we also include experiments which demonstrate convergence behaviors predicted by our analysis .
principal geodesic analysis ( pga ) is a generalization of principal component analysis ( pca ) for dimensionality reduction of data on a riemannian manifold . <eos> currently pga is defined as a geometric fit to the data , rather than as a probabilistic model . <eos> inspired by probabilistic pca , we present a latent variable model for pga that provides a probabilistic framework for factor analysis on manifolds . <eos> to compute maximum likelihood estimates of the parameters in our model , we develop a monte carlo expectation maximization algorithm , where the expectation is approximated by hamiltonian monte carlo sampling of the latent variables . <eos> we demonstrate the ability of our method to recover the ground truth parameters in simulated sphere data , as well as its effectiveness in analyzing shape variability of a corpus callosum data set from human brain images .
the performance of standard algorithms for independent component analysis quickly deteriorates under the addition of gaussian noise . <eos> this is partially due to a common first step that typically consists of whitening , i.e. , applying principal component analysis ( pca ) and rescaling the components to have identity covariance , which is not invariant under gaussian noise . <eos> in our paper we develop the first practical algorithm for independent component analysis that is provably invariant under gaussian noise . <eos> the two main contributions of this work are as follows : 1 . <eos> we develop and implement a more efficient version of a gaussian noise invariant decorrelation ( quasi-orthogonalization ) algorithm using hessians of the cumulant functions . <eos> 2 . <eos> we propose a very simple and efficient fixed-point gi-ica ( gradient iteration ica ) algorithm , which is compatible with quasi-orthogonalization , as well as with the usual pca-based whitening in the noiseless case . <eos> the algorithm is based on a special form of gradient iteration ( different from gradient descent ) . <eos> we provide an analysis of our algorithm demonstrating fast convergence following from the basic properties of cumulants . <eos> we also present a number of experimental comparisons with the existing methods , showing superior results on noisy data and very competitive performance in the noiseless case .
we consider the online principal component analysis ( pca ) for contaminated samples ( containing outliers ) which are revealed sequentially to the principal components ( pcs ) estimator . <eos> due to their sensitiveness to outliers , previous online pca algorithms fail in this case and their results can be arbitrarily bad . <eos> here we propose the online robust pca algorithm , which is able to improve the pcs estimation upon an initial one steadily , even when faced with a constant fraction of outliers . <eos> we show that the final result of the proposed online rpca has an acceptable degradation from the optimum . <eos> actually , under mild conditions , online rpca achieves the maximal robustness with a $ 50\ % $ breakdown point . <eos> moreover , online rpca is shown to be efficient for both storage and computation , since it need not re-explore the previous samples as in traditional robust pca algorithms . <eos> this endows online rpca with scalability for large scale data .
we propose a novel convex relaxation of sparse principal subspace estimation based on the convex hull of rank- $ d $ projection matrices ( the fantope ) . <eos> the convex problem can be solved efficiently using alternating direction method of multipliers ( admm ) . <eos> we establish a near-optimal convergence rate , in terms of the sparsity , ambient dimension , and sample size , for estimation of the principal subspace of a general covariance matrix without assuming the spiked covariance model . <eos> in the special case of $ d=1 $ , our result implies the near- optimality of dspca even when the solution is not rank 1 . <eos> we also provide a general theoretical framework for analyzing the statistical properties of the method for arbitrary input matrices that extends the applicability and provable guarantees to a wide array of settings . <eos> we demonstrate this with an application to kendall 's tau correlation matrices and transelliptical component analysis .
we model a one-shot learning '' situation , where very few ( scalar ) observations $ y_1 , ... , y_n $ are available . <eos> associated with each observation $ y_i $ is a very high-dimensional vector $ x_i $ , which provides context for $ y_i $ and enables us to predict subsequent observations , given their own context . <eos> one of the salient features of our analysis is that the problems studied here are easier when the dimension of $ x_i $ is large ; in other words , prediction becomes easier when more context is provided . <eos> the proposed methodology is a variant of principal component regression ( pcr ) . <eos> our rigorous analysis sheds new light on pcr . <eos> for instance , we show that classical pcr estimators may be inconsistent in the specified setting , unless they are multiplied by a scalar $ c > 1 $ ; that is , unless the classical estimator is expanded . <eos> this expansion phenomenon appears to be somewhat novel and contrasts with shrinkage methods ( $ c < 1 $ ) , which are far more common in big data analyses. ``
we introduce the randomized dependence coefficient ( rdc ) , a measure of non-linear dependence between random variables of arbitrary dimension based on the hirschfeld-gebelein-r ? nyi maximum correlation coefficient . <eos> rdc is defined in terms of correlation of random non-linear copula projections ; it is invariant with respect to marginal distribution transformations , has low computational cost and is easy to implement : just five lines of r code , included at the end of the paper .
the sparse additive model for text modeling involves the sum-of-exp computing , with consuming costs for large scales . <eos> moreover , the assumption of equal background across all classes/topics may be too strong . <eos> this paper extends to propose sparse additive model with low rank background ( sam-lrb ) , and simple yet efficient estimation . <eos> particularly , by employing a double majorization bound , we approximate the log-likelihood into a quadratic lower-bound with the sum-of-exp terms absent . <eos> the constraints of low rank and sparsity are then simply embodied by nuclear norm and $ \ell_1 $ -norm regularizers . <eos> interestingly , we find that the optimization task in this manner can be transformed into the same form as that in robust pca . <eos> consequently , parameters of supervised sam-lrb can be efficiently learned using an existing algorithm for robust pca based on accelerated proximal gradient . <eos> besides the supervised case , we extend sam-lrb to also favor unsupervised and multifaceted scenarios . <eos> experiments on real world data demonstrate the effectiveness and efficiency of sam-lrb , showing state-of-the-art performances .
in text analysis documents are represented as disorganized bags of words , models of count features are typically based on mixing a small number of topics \cite { lda , sam } . <eos> recently , it has been observed that for many text corpora documents evolve into one another in a smooth way , with some features dropping and new ones being introduced . <eos> the counting grid \cite { cguai } models this spatial metaphor literally : it is multidimensional grid of word distributions learned in such a way that a document 's own distribution of features can be modeled as the sum of the histograms found in a window into the grid . <eos> the major drawback of this method is that it is essentially a mixture and all the content much be generated by a single contiguous area on the grid . <eos> this may be problematic especially for lower dimensional grids . <eos> in this paper , we overcome to this issue with the \emph { componential counting grid } which brings the componential nature of topic models to the basic counting grid . <eos> we also introduce a generative kernel based on the document 's grid usage and a visualization strategy useful for understanding large text corpora . <eos> we evaluate our approach on document classification and multimodal retrieval obtaining state of the art results on standard benchmarks .
nonnegative matrix factorization ( nmf ) is a popular data analysis method , the objective of which is to decompose a matrix with all nonnegative components into the product of two other nonnegative matrices . <eos> in this work , we describe a new simple and efficient algorithm for multi-factor nonnegative matrix factorization problem ( { mfnmf } ) , which generalizes the original nmf problem to more than two factors . <eos> furthermore , we extend the mfnmf algorithm to incorporate a regularizer based on dirichlet distribution over normalized columns to encourage sparsity in the obtained factors . <eos> our sparse nmf algorithm affords a closed form and an intuitive interpretation , and is more efficient in comparison with previous works that use fix point iterations . <eos> we demonstrate the effectiveness and efficiency of our algorithms on both synthetic and real data sets .
discriminative methods for learning structured models have enabled wide-spread use of very rich feature representations . <eos> however , the computational cost of feature extraction is prohibitive for large-scale or time-sensitive applications , often dominating the cost of inference in the models . <eos> significant efforts have been devoted to sparsity-based model selection to decrease this cost . <eos> such feature selection methods control computation statically and miss the opportunity to fine-tune feature extraction to each input at run-time . <eos> we address the key challenge of learning to control fine-grained feature extraction adaptively , exploiting non-homogeneity of the data . <eos> we propose an architecture that uses a rich feedback loop between extraction and prediction . <eos> the run-time control policy is learned using efficient value-function approximation , which adaptively determines the value of information of features at the level of individual variables for each input . <eos> we demonstrate significant speedups over state-of-the-art methods on two challenging datasets . <eos> for articulated pose estimation in video , we achieve a more accurate state-of-the-art model that is simultaneously 4 $ \times $ faster while using only a small fraction of possible features , with similar results on an ocr task .
we address the scalability of symbolic planning under uncertainty with factored states and actions . <eos> prior work has focused almost exclusively on factored states but not factored actions , and on value iteration ( vi ) compared to policy iteration ( pi ) . <eos> our ? rst contribution is a novel method for symbolic policy backups via the application of constraints , which is used to yield a new ef ? cient symbolic imple- mentation of modi ? ed pi ( mpi ) for factored action spaces . <eos> while this approach improves scalability in some cases , naive handling of policy constraints comes with its own scalability issues . <eos> this leads to our second and main contribution , symbolic opportunistic policy iteration ( opi ) , which is a novel convergent al- gorithm lying between vi and mpi . <eos> the core idea is a symbolic procedure that applies policy constraints only when they reduce the space and time complexity of the update , and otherwise performs full bellman backups , thus automatically adjusting the backup per state . <eos> we also give a memory bounded version of this algorithm allowing a space-time tradeoff . <eos> empirical results show signi ? cantly improved scalability over the state-of-the-art .
this paper presents four major results towards solving decentralized partially observable markov decision problems ( decpomdps ) culminating in an algorithm that outperforms all existing algorithms on all but one standard infinite-horizon benchmark problems . <eos> ( 1 ) we give an integer program that solves collaborative bayesian games ( cbgs ) . <eos> the program is notable because its linear relaxation is very often integral . <eos> ( 2 ) we show that a decpomdp with bounded belief can be converted to a pomdp ( albeit with actions exponential in the number of beliefs ) . <eos> these actions correspond to strategies of a cbg . <eos> ( 3 ) we present a method to transform any decpomdp into a decpomdp with bounded beliefs ( the number of beliefs is a free parameter ) using optimal ( not lossless ) belief compression . <eos> ( 4 ) we show that the combination of these results opens the door for new classes of decpomdp algorithms based on previous pomdp algorithms . <eos> we choose one such algorithm , point-based valued iteration , and modify it to produce the first tractable value iteration method for decpomdps which outperforms existing algorithms .
in this paper , we study monte carlo tree search ( mcts ) in zero-sum extensive-form games with perfect information and simultaneous moves . <eos> we present a general template of mcts algorithms for these games , which can be instantiated by various selection methods . <eos> we formally prove that if a selection method is $ \epsilon $ -hannan consistent in a matrix game and satisfies additional requirements on exploration , then the mcts algorithm eventually converges to an approximate nash equilibrium ( ne ) of the extensive-form game . <eos> we empirically evaluate this claim using regret matching and exp3 as the selection methods on randomly generated and worst case games . <eos> we confirm the formal result and show that additional mcts variants also converge to approximate ne on the evaluated games .
in search advertising , the search engine needs to select the most profitable advertisements to display , which can be formulated as an instance of online learning with partial feedback , also known as the stochastic multi-armed bandit ( mab ) problem . <eos> in this paper , we show that the naive application of mab algorithms to search advertising for advertisement selection will produce sample selection bias that harms the search engine by decreasing expected revenue and ? estimation of the largest mean ? <eos> ( elm ) bias that harms the advertisers by increasing game-theoretic player-regret . <eos> we then propose simple bias-correction methods with benefits to both the search engine and the advertisers .
we provide several applications of optimistic mirror descent , an online learning algorithm based on the idea of predictable sequences . <eos> first , we recover the mirror-prox algorithm , prove an extension to holder-smooth functions , and apply the results to saddle-point type problems . <eos> second , we prove that a version of optimistic mirror descent ( which has a close relation to the exponential weights algorithm ) can be used by two strongly-uncoupled players in a finite zero-sum matrix game to converge to the minimax equilibrium at the rate of o ( log t / t ) . <eos> this addresses a question of daskalakis et al , 2011 . <eos> further , we consider a partial information version of the problem . <eos> we then apply the results to approximate convex programming and show a simple algorithm for the approximate max-flow problem .
we design and analyze minimax-optimal algorithms for online linear optimization games where the player 's choice is unconstrained . <eos> the player strives to minimize regret , the difference between his loss and the loss of a post-hoc benchmark strategy . <eos> the standard benchmark is the loss of the best strategy chosen from a bounded comparator set , whereas we consider a broad range of benchmark functions . <eos> we consider the problem as a sequential multi-stage zero-sum game , and we give a thorough analysis of the minimax behavior of the game , providing characterizations for the value of the game , as well as both the player 's and the adversary 's optimal strategy . <eos> we show how these objects can be computed efficiently under certain circumstances , and by selecting an appropriate benchmark , we construct a novel hedging strategy for an unconstrained betting game .
we study the power of different types of adaptive ( nonoblivious ) adversaries in the setting of prediction with expert advice , under both full-information and bandit feedback . <eos> we measure the player 's performance using a new notion of regret , also known as policy regret , which better captures the adversary 's adaptiveness to the player 's behavior . <eos> in a setting where losses are allowed to drift , we characterize -- -in a nearly complete manner -- - the power of adaptive adversaries with bounded memories and switching costs . <eos> in particular , we show that with switching costs , the attainable rate with bandit feedback is $ t^ { 2/3 } $ . <eos> interestingly , this rate is significantly worse than the $ \sqrt { t } $ rate attainable with switching costs in the full-information case . <eos> via a novel reduction from experts to bandits , we also show that a bounded memory adversary can force $ t^ { 2/3 } $ regret even in the full information case , proving that switching costs are easier to control than bounded memory adversaries . <eos> our lower bounds rely on a new stochastic adversary strategy that generates loss processes with strong dependencies .
performance guarantees for online learning algorithms typically take the form of regret bounds , which express that the cumulative loss overhead compared to the best expert in hindsight is small . <eos> in the common case of large but structured expert sets we typically wish to keep the regret especially small compared to simple experts , at the cost of modest additional overhead compared to more complex others . <eos> we study which such regret trade-offs can be achieved , and how . <eos> we analyse regret w.r.t . <eos> each individual expert as a multi-objective criterion in the simple but fundamental case of absolute loss . <eos> we characterise the achievable and pareto optimal trade-offs , and the corresponding optimal strategies for each sample size both exactly for each finite horizon and asymptotically .
we study the power of different types of adaptive ( nonoblivious ) adversaries in the setting of prediction with expert advice , under both full-information and bandit feedback . <eos> we measure the player 's performance using a new notion of regret , also known as policy regret , which better captures the adversary 's adaptiveness to the player 's behavior . <eos> in a setting where losses are allowed to drift , we characterize -- -in a nearly complete manner -- - the power of adaptive adversaries with bounded memories and switching costs . <eos> in particular , we show that with switching costs , the attainable rate with bandit feedback is $ t^ { 2/3 } $ . <eos> interestingly , this rate is significantly worse than the $ \sqrt { t } $ rate attainable with switching costs in the full-information case . <eos> via a novel reduction from experts to bandits , we also show that a bounded memory adversary can force $ t^ { 2/3 } $ regret even in the full information case , proving that switching costs are easier to control than bounded memory adversaries . <eos> our lower bounds rely on a new stochastic adversary strategy that generates loss processes with strong dependencies .
many applications in machine learning require optimizing unknown functions defined over a high-dimensional space from noisy samples that are expensive to obtain . <eos> we address this notoriously hard challenge , under the assumptions that the function varies only along some low-dimensional subspace and is smooth ( i.e. , it has a low norm in a reproducible kernel hilbert space ) . <eos> in particular , we present the si-bo algorithm , which leverages recent low-rank matrix recovery techniques to learn the underlying subspace of the unknown function and applies gaussian process upper confidence sampling for optimization of the function . <eos> we carefully calibrate the exploration ? exploitation tradeoff by allocating sampling budget to subspace estimation and function optimization , and obtain the first subexponential cumulative regret bounds and convergence rates for bayesian optimization in high-dimensions under noisy observations . <eos> numerical results demonstrate the effectiveness of our approach in difficult scenarios .
undirected graphical models , such as gaussian graphical models , ising , and multinomial/categorical graphical models , are widely used in a variety of applications for modeling distributions over a large number of variables . <eos> these standard instances , however , are ill-suited to modeling count data , which are increasingly ubiquitous in big-data settings such as genomic sequencing data , user-ratings data , spatial incidence data , climate studies , and site visits . <eos> existing classes of poisson graphical models , which arise as the joint distributions that correspond to poisson distributed node-conditional distributions , have a major drawback : they can only model negative conditional dependencies for reasons of normalizability given its infinite domain . <eos> in this paper , our objective is to modify the poisson graphical model distribution so that it can capture a rich dependence structure between count-valued variables . <eos> we begin by discussing two strategies for truncating the poisson distribution and show that only one of these leads to a valid joint distribution ; even this model , however , has limitations on the types of variables and dependencies that may be modeled . <eos> to address this , we propose two novel variants of the poisson distribution and their corresponding joint graphical model distributions . <eos> these models provide a class of poisson graphical models that can capture both positive and negative conditional dependencies between count-valued variables . <eos> one can learn the graph structure of our model via penalized neighborhood selection , and we demonstrate the performance of our methods by learning simulated networks as well as a network from microrna-sequencing data .
conditional random fields , which model the distribution of a multivariate response conditioned on a set of covariates using undirected graphs , are widely used in a variety of multivariate prediction applications . <eos> popular instances of this class of models such as categorical-discrete crfs , ising crfs , and conditional gaussian based crfs , are not however best suited to the varied types of response variables in many applications , including count-valued responses . <eos> we thus introduce a ? novel subclass of crfs ? , derived by imposing node-wise conditional distributions of response variables conditioned on the rest of the responses and the covariates as arising from univariate exponential families . <eos> this allows us to derive novel multivariate crfs given any univariate exponential distribution , including the poisson , negative binomial , and exponential distributions . <eos> also in particular , it addresses the common crf problem of specifying feature '' functions determining the interactions between response variables and covariates . <eos> we develop a class of tractable penalized $ m $ -estimators to learn these crf distributions from data , as well as a unified sparsistency analysis for this general class of crfs showing exact structure recovery can be achieved with high probability . ''
while graphs with continuous node attributes arise in many applications , state-of-the-art graph kernels for comparing continuous-attributed graphs suffer from a high runtime complexity ; for instance , the popular shortest path kernel scales as $ \mathcal { o } ( n^4 ) $ , where $ n $ is the number of nodes . <eos> in this paper , we present a class of path kernels with computational complexity $ \mathcal { o } ( n^2 ( m + \delta^2 ) ) $ , where $ \delta $ is the graph diameter and $ m $ the number of edges . <eos> due to the sparsity and small diameter of real-world graphs , these kernels scale comfortably to large graphs . <eos> in our experiments , the presented kernels outperform state-of-the-art kernels in terms of speed and accuracy on classification benchmark datasets .
the detection of anomalous activity in graphs is a statistical problem that arises in many applications , such as network surveillance , disease outbreak detection , and activity monitoring in social networks . <eos> beyond its wide applicability , graph structured anomaly detection serves as a case study in the difficulty of balancing computational complexity with statistical power . <eos> in this work , we develop from first principles the generalized likelihood ratio test for determining if there is a well connected region of activation over the vertices in the graph in gaussian noise . <eos> because this test is computationally infeasible , we provide a relaxation , called the lov\'asz extended scan statistic ( less ) that uses submodularity to approximate the intractable generalized likelihood ratio . <eos> we demonstrate a connection between less and maximum a-posteriori inference in markov random fields , which provides us with a poly-time algorithm for less . <eos> using electrical network theory , we are able to control type 1 error for less and prove conditions under which less is risk consistent . <eos> finally , we consider specific graph models , the torus , $ k $ -nearest neighbor graphs , and $ \epsilon $ -random graphs . <eos> we show that on these graphs our results provide near-optimal performance by matching our results to known lower bounds .
we show that either explicitly or implicitly , various well-known graph-based models exhibit a common significant \emph { harmonic } structure in its target function -- the value of a vertex is approximately the weighted average of the values of its adjacent neighbors . <eos> understanding of such structure and analysis of the loss defined over such structure help reveal important properties of the target function over a graph . <eos> in this paper , we show that the variation of the target function across a cut can be upper and lower bounded by the ratio of its harmonic loss and the cut cost . <eos> we use this to develop an analytical tool and analyze 5 popular models in graph-based learning : absorbing random walks , partially absorbing random walks , hitting times , pseudo-inverse of graph laplacian , and eigenvectors of the laplacian matrices . <eos> our analysis well explains several open questions of these models reported in the literature . <eos> furthermore , it provides theoretical justifications and guidelines for their practical use . <eos> simulations on synthetic and real datasets support our analysis .
gaussian graphical models ( ggms ) or gauss markov random fields are widely used in many applications , and the trade-off between the modeling capacity and the efficiency of learning and inference has been an important research problem . <eos> in this paper , we study the family of ggms with small feedback vertex sets ( fvss ) , where an fvs is a set of nodes whose removal breaks all the cycles . <eos> exact inference such as computing the marginal distributions and the partition function has complexity $ o ( k^ { 2 } n ) $ using message-passing algorithms , where k is the size of the fvs , and n is the total number of nodes . <eos> we propose efficient structure learning algorithms for two cases : 1 ) all nodes are observed , which is useful in modeling social or flight networks where the fvs nodes often correspond to a small number of high-degree nodes , or hubs , while the rest of the networks is modeled by a tree . <eos> regardless of the maximum degree , without knowing the full graph structure , we can exactly compute the maximum likelihood estimate in $ o ( kn^2+n^2\log n ) $ if the fvs is known or in polynomial time if the fvs is unknown but has bounded size . <eos> 2 ) the fvs nodes are latent variables , where structure learning is equivalent to decomposing a inverse covariance matrix ( exactly or approximately ) into the sum of a tree-structured matrix and a low-rank matrix . <eos> by incorporating efficient inference into the learning steps , we can obtain a learning algorithm using alternating low-rank correction with complexity $ o ( kn^ { 2 } +n^ { 2 } \log n ) $ per iteration . <eos> we also perform experiments using both synthetic data as well as real data of flight delays to demonstrate the modeling capacity with fvss of various sizes . <eos> we show that empirically the family of ggms of size $ o ( \log n ) $ strikes a good balance between the modeling capacity and the efficiency .
we consider energy minimization for undirected graphical models , also known as map-inference problem for markov random fields . <eos> although combinatorial methods , which return a provably optimal integral solution of the problem , made a big progress in the past decade , they are still typically unable to cope with large-scale datasets . <eos> on the other hand , large scale datasets are typically defined on sparse graphs , and convex relaxation methods , such as linear programming relaxations often provide good approximations to integral solutions . <eos> we propose a novel method of combining combinatorial and convex programming techniques to obtain a global solution of the initial combinatorial problem . <eos> based on the information obtained from the solution of the convex relaxation , our method confines application of the combinatorial solver to a small fraction of the initial graphical model , which allows to optimally solve big problems . <eos> we demonstrate the power of our approach on a computer vision energy minimization benchmark .
lifting attempts to speedup probabilistic inference by exploiting symmetries in the model . <eos> exact lifted inference methods , like their propositional counterparts , work by recursively decomposing the model and the problem . <eos> in the propositional case , there exist formal structures , such as decomposition trees ( dtrees ) , that represent such a decomposition and allow us to determine the complexity of inference a priori . <eos> however , there is currently no equivalent structure nor analogous complexity results for lifted inference . <eos> in this paper , we introduce fo-dtrees , which upgrade propositional dtrees to the first-order level . <eos> we show how these trees can characterize a lifted inference solution for a probabilistic logical model ( in terms of a sequence of lifted operations ) , and make a theoretical analysis of the complexity of lifted inference in terms of the novel notion of lifted width for the tree .
discovering hierarchical regularities in data is a key problem in interacting with large datasets , modeling cognition , and encoding knowledge . <eos> a previous bayesian solution -- -kingman 's coalescent -- -provides a convenient probabilistic model for data represented as a binary tree . <eos> unfortunately , this is inappropriate for data better described by bushier trees . <eos> we generalize an existing belief propagation framework of kingman 's coalescent to the beta coalescent , which models a wider range of tree structures . <eos> because of the complex combinatorial search over possible structures , we develop new sampling schemes using sequential monte carlo and dirichlet process mixture models , which render inference efficient and tractable . <eos> we present results on both synthetic and real data that show the beta coalescent outperforms kingman 's coalescent on real datasets and is qualitatively better at capturing data in bushy hierarchies .
we present a novel mcmc sampler for dirichlet process mixture models that can be used for conjugate or non-conjugate prior distributions . <eos> the proposed sampler can be massively parallelized to achieve significant computational gains . <eos> a non-ergodic restricted gibbs iteration is mixed with split/merge proposals to produce a valid sampler . <eos> each regular cluster is augmented with two sub-clusters to construct likely split moves . <eos> unlike many previous parallel samplers , the proposed sampler accurately enforces the correct stationary distribution of the markov chain without the need for approximate models . <eos> empirical results illustrate that the new sampler exhibits better convergence properties than current methods .
inspired by a two-level theory that unifies agenda setting and ideological framing , we propose supervised hierarchical latent dirichlet allocation ( shlda ) which jointly captures documents ' multi-level topic structure and their polar response variables . <eos> our model extends the nested chinese restaurant process to discover a tree-structured topic hierarchy and uses both per-topic hierarchical and per-word lexical regression parameters to model the response variables . <eos> experiments in a political domain and on sentiment analysis tasks show that shlda improves predictive accuracy while adding a new dimension of insight into how topics under discussion are framed .
cross language text classi ? cation is an important learning task in natural language processing . <eos> a critical challenge of cross language learning lies in that words of different languages are in disjoint feature spaces . <eos> in this paper , we propose a two-step representation learning method to bridge the feature spaces of different languages by exploiting a set of parallel bilingual documents . <eos> speci ? cally , we ? rst formulate a matrix completion problem to produce a complete parallel document-term matrix for all documents in two languages , and then induce a cross-lingual document representation by applying latent semantic indexing on the obtained matrix . <eos> we use a projected gradient descent algorithm to solve the formulated matrix completion problem with convergence guarantees . <eos> the proposed approach is evaluated by conducting a set of experiments with cross language sentiment classi ? cation tasks on amazon product reviews . <eos> the experimental results demonstrate that the proposed learning approach outperforms a number of comparison cross language representation learning methods , especially when the number of parallel bilingual documents is small .
continuous-valued word embeddings learned by neural language models have recently been shown to capture semantic and syntactic information about words very well , setting performance records on several word similarity tasks . <eos> the best results are obtained by learning high-dimensional embeddings from very large quantities of data , which makes scalability of the training method a critical factor . <eos> we propose a simple and scalable new approach to learning word embeddings based on training log-bilinear models with noise-contrastive estimation . <eos> our approach is simpler , faster , and produces better results than the current state-of-the art method of mikolov et al . ( 2013a ) . <eos> we achieve results comparable to the best ones reported , which were obtained on a cluster , using four times less data and more than an order of magnitude less computing time . <eos> we also investigate several model types and find that the embeddings learned by the simpler models perform at least as well as those learned by the more complex ones .
time series often have a temporal hierarchy , with information that is spread out over multiple time scales . <eos> common recurrent neural networks , however , do not explicitly accommodate such a hierarchy , and most research on them has been focusing on training algorithms rather than on their basic architecture . <eos> in this pa- per we study the effect of a hierarchy of recurrent neural networks on processing time series . <eos> here , each layer is a recurrent network which receives the hidden state of the previous layer as input . <eos> this architecture allows us to perform hi- erarchical processing on difficult temporal tasks , and more naturally capture the structure of time series . <eos> we show that they reach state-of-the-art performance for recurrent networks in character-level language modelling when trained with sim- ple stochastic gradient descent . <eos> we also offer an analysis of the different emergent time scales .
biological tissue is often composed of cells with similar morphologies replicated throughout large volumes and many biological applications rely on the accurate identification of these cells and their locations from image data . <eos> here we develop a generative model that captures the regularities present in images composed of repeating elements of a few different types . <eos> formally , the model can be described as convolutional sparse block coding . <eos> for inference we use a variant of convolutional matching pursuit adapted to block-based representations . <eos> we extend the k-svd learning algorithm to subspaces by retaining several principal vectors from the svd decomposition instead of just one . <eos> good models with little cross-talk between subspaces can be obtained by learning the blocks incrementally . <eos> we perform extensive experiments on simulated images and the inference algorithm consistently recovers a large proportion of the cells with a small number of false positives . <eos> we fit the convolutional model to noisy gcamp6 two-photon images of spiking neurons and to nissl-stained slices of cortical tissue and show that it recovers cell body locations without supervision . <eos> the flexibility of the block-based representation is reflected in the variability of the recovered cell shapes .
imaging neuroscience links brain activation maps to behavior and cognition via correlational studies . <eos> due to the nature of the individual experiments , based on eliciting neural response from a small number of stimuli , this link is incomplete , and unidirectional from the causal point of view . <eos> to come to conclusions on the function implied by the activation of brain regions , it is necessary to combine a wide exploration of the various brain functions and some inversion of the statistical inference . <eos> here we introduce a methodology for accumulating knowledge towards a bidirectional link between observed brain activity and the corresponding function . <eos> we rely on a large corpus of imaging studies and a predictive engine . <eos> technically , the challenges are to find commonality between the studies without denaturing the richness of the corpus . <eos> the key elements that we contribute are labeling the tasks performed with a cognitive ontology , and modeling the long tail of rare paradigms in the corpus . <eos> to our knowledge , our approach is the first demonstration of predicting the cognitive content of completely new brain images . <eos> to that end , we propose a method that predicts the experimental paradigms across different studies .
hermitian positive definite matrices ( hpd ) recur throughout statistics and machine learning . <eos> in this paper we develop \emph { geometric optimisation } for globally optimising certain nonconvex loss functions arising in the modelling of data via elliptically contoured distributions ( ecds ) . <eos> we exploit the remarkable structure of the convex cone of positive definite matrices which allows one to uncover hidden geodesic convexity of objective functions that are nonconvex in the ordinary euclidean sense . <eos> going even beyond manifold convexity we show how further metric properties of hpd matrices can be exploited to globally optimise several ecd log-likelihoods that are not even geodesic convex . <eos> we present key results that help recognise this geometric structure , as well as obtain efficient fixed-point algorithms to optimise the corresponding objective functions . <eos> to our knowledge , ours are the most general results on geometric optimisation of hpd matrices known so far . <eos> experiments reveal the benefits of our approach -- -it avoids any eigenvalue computations which makes it very competitive .
recently , [ valiant and valiant ] showed that a class of distributional properties , which includes such practically relevant properties as entropy , the number of distinct elements , and distance metrics between pairs of distributions , can be estimated given a sublinear sized sample . <eos> specifically , given a sample consisting of independent draws from any distribution over at most n distinct elements , these properties can be estimated accurately using a sample of size o ( n / log n ) . <eos> we propose a novel modification of this approach and show : 1 ) theoretically , our estimator is optimal ( to constant factors , over worst-case instances ) , and 2 ) in practice , it performs exceptionally well for a variety of estimation tasks , on a variety of natural distributions , for a wide range of parameters . <eos> perhaps unsurprisingly , the key step in this approach is to first use the sample to characterize the unseen '' portion of the distribution . <eos> this goes beyond such tools as the good-turing frequency estimation scheme , which estimates the total probability mass of the unobserved portion of the distribution : we seek to estimate the `` shape '' of the unobserved portion of the distribution . <eos> this approach is robust , general , and theoretically principled ; we expect that it may be fruitfully used as a component within larger machine learning and data analysis systems. ``
this paper extends factorized asymptotic bayesian ( fab ) inference for latent feature models~ ( lfms ) . <eos> fab inference has not been applicable to models , including lfms , without a specific condition on the hesqsian matrix of a complete log-likelihood , which is required to derive a factorized information criterion '' ~ ( fic ) . <eos> our asymptotic analysis of the hessian matrix of lfms shows that fic of lfms has the same form as those of mixture models . <eos> fab/lfms have several desirable properties ( e.g. , automatic hidden states selection and parameter identifiability ) and empirically perform better than state-of-the-art indian buffet processes in terms of model selection , prediction , and computational efficiency . ''
structure learning algorithms for graphical models have focused almost exclusively on stable environments in which the underlying generative process does not change ; that is , they assume that the generating model is globally stationary . <eos> in real-world environments , however , such changes often occur without warning or signal . <eos> real-world data often come from generating models that are only locally stationary . <eos> in this paper , we present losst , a novel , heuristic structure learning algorithm that tracks changes in graphical model structure or parameters in a dynamic , real-time manner . <eos> we show by simulation that the algorithm performs comparably to batch-mode learning when the generating graphical structure is globally stationary , and significantly better when it is only locally stationary .
we propose a semiparametric procedure for estimating high dimensional sparse inverse covariance matrix . <eos> our method , named alice , is applicable to the elliptical family . <eos> computationally , we develop an efficient dual inexact iterative projection ( $ { \rm d_2 } $ p ) algorithm based on the alternating direction method of multipliers ( admm ) . <eos> theoretically , we prove that the alice estimator achieves the parametric rate of convergence in both parameter estimation and model selection . <eos> moreover , alice calibrates regularizations when estimating each column of the inverse covariance matrix . <eos> so it not only is asymptotically tuning free , but also achieves an improved finite sample performance . <eos> we present numerical simulations to support our theory , and a real data example to illustrate the effectiveness of the proposed estimator .
we address the problem of learning a sparse bayesian network structure for continuous variables in a high-dimensional space . <eos> the constraint that the estimated bayesian network structure must be a directed acyclic graph ( dag ) makes the problem challenging because of the huge search space of network structures . <eos> most previous methods were based on a two-stage approach that prunes the search space in the first stage and then searches for a network structure that satisfies the dag constraint in the second stage . <eos> although this approach is effective in a low-dimensional setting , it is difficult to ensure that the correct network structure is not pruned in the first stage in a high-dimensional setting . <eos> in this paper , we propose a single-stage method , called a* lasso , that recovers the optimal sparse bayesian network structure by solving a single optimization problem with a* search algorithm that uses lasso in its scoring system . <eos> our approach substantially improves the computational efficiency of the well-known exact methods based on dynamic programming . <eos> we also present a heuristic scheme that further improves the efficiency of a* lasso without significantly compromising the quality of solutions and demonstrate this on benchmark bayesian networks and real data .
penalized m-estimators are used in diverse areas of science and engineering to fit high-dimensional models with some low-dimensional structure . <eos> often , the penalties are \emph { geometrically decomposable } , \ie\ can be expressed as a sum of ( convex ) support functions . <eos> we generalize the notion of irrepresentable to geometrically decomposable penalties and develop a general framework for establishing consistency and model selection consistency of m-estimators with such penalties . <eos> we then use this framework to derive results for some special cases of interest in bioinformatics and statistical learning .
in a closed-loop brain-computer interface ( bci ) , adaptive decoders are used to learn parameters suited to decoding the user 's neural response . <eos> feedback to the user provides information which permits the neural tuning to also adapt . <eos> we present an approach to model this process of co-adaptation between the encoding model of the neural signal and the decoding algorithm as a multi-agent formulation of the linear quadratic gaussian ( lqg ) control problem . <eos> in simulation we characterize how decoding performance improves as the neural encoding and adaptive decoder optimize , qualitatively resembling experimentally demonstrated closed-loop improvement . <eos> we then propose a novel , modified decoder update rule which is aware of the fact that the encoder is also changing and show it can improve simulated co-adaptation dynamics . <eos> our modeling approach offers promise for gaining insights into co-adaptation as well as improving user learning of bci control in practical settings .
movement primitives ( mp ) are a well-established approach for representing modular and re-usable robot movement generators . <eos> many state-of-the-art robot learning successes are based mps , due to their compact representation of the inherently continuous and high dimensional robot movements . <eos> a major goal in robot learning is to combine multiple mps as building blocks in a modular control architecture to solve complex tasks . <eos> to this effect , a mp representation has to allow for blending between motions , adapting to altered task variables , and co-activating multiple mps in parallel . <eos> we present a probabilistic formulation of the mp concept that maintains a distribution over trajectories . <eos> our probabilistic approach allows for the derivation of new operations which are essential for implementing all aforementioned properties in one framework . <eos> in order to use such a trajectory distribution for robot movement control , we analytically derive a stochastic feedback controller which reproduces the given trajectory distribution . <eos> we evaluate and compare our approach to existing methods on several simulated as well as real robot scenarios .
in order to learn effective control policies for dynamical systems , policy search methods must be able to discover successful executions of the desired task . <eos> while random exploration can work well in simple domains , complex and high-dimensional tasks present a serious challenge , particularly when combined with high-dimensional policies that make parameter-space exploration infeasible . <eos> we present a method that uses trajectory optimization as a powerful exploration strategy that guides the policy search . <eos> a variational decomposition of a maximum likelihood policy objective allows us to use standard trajectory optimization algorithms such as differential dynamic programming , interleaved with standard supervised learning for the policy itself . <eos> we demonstrate that the resulting algorithm can outperform prior methods on two challenging locomotion tasks .
we consider the problem of learning good trajectories for manipulation tasks . <eos> this is challenging because the criterion defining a good trajectory varies with users , tasks and environments . <eos> in this paper , we propose a co-active online learning framework for teaching robots the preferences of its users for object manipulation tasks . <eos> the key novelty of our approach lies in the type of feedback expected from the user : the human user does not need to demonstrate optimal trajectories as training data , but merely needs to iteratively provide trajectories that slightly improve over the trajectory currently proposed by the system . <eos> we argue that this co-active preference feedback can be more easily elicited from the user than demonstrations of optimal trajectories , which are often challenging and non-intuitive to provide on high degrees of freedom manipulators . <eos> nevertheless , theoretical regret bounds of our algorithm match the asymptotic rates of optimal trajectory algorithms . <eos> we also formulate a score function to capture the contextual information and demonstrate the generalizability of our algorithm on a variety of household tasks , for whom , the preferences were not only influenced by the object being manipulated but also by the surrounding environment .
how humans achieve long-term goals in an uncertain environment , via repeated trials and noisy observations , is an important problem in cognitive science . <eos> we investigate this behavior in the context of a multi-armed bandit task . <eos> we compare human behavior to a variety of models that vary in their representational and computational complexity . <eos> our result shows that subjects ' choices , on a trial-to-trial basis , are best captured by a forgetful '' bayesian iterative learning model in combination with a partially myopic decision policy known as knowledge gradient . <eos> this model accounts for subjects ' trial-by-trial choice better than a number of other previously proposed models , including optimal bayesian learning and risk minimization , epsilon-greedy and win-stay-lose-shift . <eos> it has the added benefit of being closest in performance to the optimal bayesian model than all the other heuristic models that have the same computational complexity ( all are significantly less complex than the optimal model ) . <eos> these results constitute an advancement in the theoretical understanding of how humans negotiate the tension between exploration and exploitation in a noisy , imperfectly known environment . ''
humans and animals readily utilize active sensing , or the use of self-motion , to focus sensory and cognitive resources on the behaviorally most relevant stimuli and events in the environment . <eos> understanding the computational basis of natural active sensing is important both for advancing brain sciences and for developing more powerful artificial systems . <eos> recently , a goal-directed , context-sensitive , bayesian control strategy for active sensing , termed c-dac ( context-dependent active controller ) , was proposed ( ahmad & yu , 2013 ) . <eos> in contrast to previously proposed algorithms for human active vision , which tend to optimize abstract statistical objectives and therefore can not adapt to changing behavioral context or task goals , c-dac directly minimizes behavioral costs and thus , automatically adapts itself to different task conditions . <eos> however , c-dac is limited as a model of human active sensing , given its computational/representational requirements , especially for more complex , real-world situations . <eos> here , we propose a myopic approximation to c-dac , which also takes behavioral costs into account , but achieves a significant reduction in complexity by looking only one step ahead . <eos> we also present data from a human active visual search experiment , and compare the performance of the various models against human behavior . <eos> we find that c-dac and its myopic variant both achieve better fit to human data than infomax ( butko & movellan , 2010 ) , which maximizes expected cumulative future information gain . <eos> in summary , this work provides novel experimental results that differentiate theoretical models for human active sensing , as well as a novel active sensing algorithm that retains the context-sensitivity of the optimal controller while achieving significant computational savings .
this paper addresses the problem of automatic generation of features for value function approximation in reinforcement learning . <eos> bellman error basis functions ( bebfs ) have been shown to improve the error of policy evaluation with function approximation , with a convergence rate similar to that of value iteration . <eos> we propose a simple , fast and robust algorithm based on random projections , which generates bebfs for sparse feature spaces . <eos> we provide a finite sample analysis of the proposed method , and prove that projections logarithmic in the dimension of the original space guarantee a contraction in the error . <eos> empirical results demonstrate the strength of this method in domains in which choosing a good state representation is challenging .
an important challenge in markov decision processes is to ensure robustness with respect to unexpected or adversarial system behavior while taking advantage of well-behaving parts of the system . <eos> we consider a problem setting where some unknown parts of the state space can have arbitrary transitions while other parts are purely stochastic . <eos> we devise an algorithm that is adaptive to potentially adversarial behavior and show that it achieves similar regret bounds as the purely stochastic case .
natural actor-critics are a popular class of policy search algorithms for finding locally optimal policies for markov decision processes . <eos> in this paper we address a drawback of natural actor-critics that limits their real-world applicability - their lack of safety guarantees . <eos> we present a principled algorithm for performing natural gradient descent over a constrained domain . <eos> in the context of reinforcement learning , this allows for natural actor-critic algorithms that are guaranteed to remain within a known safe region of policy space . <eos> while deriving our class of constrained natural actor-critic algorithms , which we call projected natural actor-critics ( pnacs ) , we also elucidate the relationship between natural gradient descent and mirror descent .
most provably efficient learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration . <eos> we study an alternative approach for efficient exploration , posterior sampling for reinforcement learning ( psrl ) . <eos> this algorithm proceeds in repeated episodes of known duration . <eos> at the start of each episode , psrl updates a prior distribution over markov decision processes and takes one sample from this posterior . <eos> psrl then follows the policy that is optimal for this sample during the episode . <eos> the algorithm is conceptually simple , computationally efficient and allows an agent to encode prior knowledge in a natural way . <eos> we establish an $ \tilde { o } ( \tau s \sqrt { at } ) $ bound on the expected regret , where $ t $ is time , $ \tau $ is the episode length and $ s $ and $ a $ are the cardinalities of the state and action spaces . <eos> this bound is one of the first for an algorithm not based on optimism and close to the state of the art for any reinforcement learning algorithm . <eos> we show through simulation that psrl significantly outperforms existing algorithms with similar regret bounds .
in the last decade , policy gradient methods have significantly grown in popularity in the reinforcement -- learning field . <eos> in particular , they have been largely employed in motor control and robotic applications , thanks to their ability to cope with continuous state and action domains and partial observable problems . <eos> policy gradient researches have been mainly focused on the identification of effective gradient directions and the proposal of efficient estimation algorithms . <eos> nonetheless , the performance of policy gradient methods is determined not only by the gradient direction , since convergence properties are strongly influenced by the choice of the step size : small values imply slow convergence rate , while large values may lead to oscillations or even divergence of the policy parameters . <eos> step -- size value is usually chosen by hand tuning and still little attention has been paid to its automatic selection . <eos> in this paper , we propose to determine the learning rate by maximizing a lower bound to the expected performance gain . <eos> focusing on gaussian policies , we derive a lower bound that is second -- order polynomial of the step size , and we show how a simplified version of such lower bound can be maximized when the gradient is estimated from trajectory samples . <eos> the properties of the proposed approach are empirically evaluated in a linear -- quadratic regulator problem .
a long term goal of interactive reinforcement learning is to incorporate non-expert human feedback to solve complex tasks . <eos> state-of-the-art methods have approached this problem by mapping human information to reward and value signals to indicate preferences and then iterating over them to compute the necessary control policy . <eos> in this paper we argue for an alternate , more effective characterization of human feedback : policy shaping . <eos> we introduce advise , a bayesian approach that attempts to maximize the information gained from human feedback by utilizing it as direct labels on the policy . <eos> we compare advise to state-of-the-art approaches and highlight scenarios where it outperforms them and importantly is robust to infrequent and inconsistent human feedback .
approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods . <eos> as our first main result , we show that an important subset of the latter methodology is , in fact , a limiting special case of a general formulation of the former methodology ; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods , and permits one to directly interpolate between them . <eos> the resulting continuum adjusts the strength of the markov assumption in policy improvement and , as such , can be seen as dual in spirit to the continuum in td ( $ \lambda $ ) -style algorithms in policy evaluation . <eos> as our second main result , we show for a substantial subset of soft-greedy value function approaches that , while having the potential to avoid policy oscillation and policy chattering , this subset can never converge toward any optimal policy , except in a certain pathological case . <eos> consequently , in the context of approximations , the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality .
pomdps provide a principled framework for planning under uncertainty , but are computationally intractable , due to the ? curse of dimensionality ? <eos> and the ? curse of history ? . <eos> this paper presents an online lookahead search algorithm that alleviates these difficulties by limiting the search to a set of sampled scenarios . <eos> the execution of all policies on the sampled scenarios is summarized using a determinized sparse partially observable tree ( despot ) , which is a sparsely sampled belief tree . <eos> our algorithm , named regularized despot ( r-despot ) , searches the despot for a policy that optimally balances the size of the policy and the accuracy on its value estimate obtained through sampling . <eos> we give an output-sensitive performance bound for all policies derived from the despot , and show that r-despot works well if a small optimal policy exists . <eos> we also give an anytime approximation to r-despot . <eos> experiments show strong results , compared with two of the fastest online pomdp algorithms .
tetris is a popular video game that has been widely used as a benchmark for various optimization techniques including approximate dynamic programming ( adp ) algorithms . <eos> a close look at the literature of this game shows that while adp algorithms , that have been ( almost ) entirely based on approximating the value function ( value function based ) , have performed poorly in tetris , the methods that search directly in the space of policies by learning the policy parameters using an optimization black box , such as the cross entropy ( ce ) method , have achieved the best reported results . <eos> this makes us conjecture that tetris is a game in which good policies are easier to represent , and thus , learn than their corresponding value functions . <eos> so , in order to obtain a good performance with adp , we should use adp algorithms that search in a policy space , instead of the more traditional ones that search in a value function space . <eos> in this paper , we put our conjecture to test by applying such an adp algorithm , called classification-based modified policy iteration ( cbmpi ) , to the game of tetris . <eos> our extensive experimental results show that for the first time an adp algorithm , namely cbmpi , obtains the best results reported in the literature for tetris in both small $ 10\times 10 $ and large $ 10\times 20 $ boards . <eos> although the cbmpi 's results are similar to those achieved by the ce method in the large board , cbmpi uses considerably fewer ( almost 1/10 ) samples ( call to the generative model of the game ) than ce .
we consider how to transfer knowledge from previous tasks to a current task in long-lived and bounded agents that must solve a sequence of mdps over a finite lifetime . <eos> a novel aspect of our transfer approach is that we reuse reward functions . <eos> while this may seem counterintuitive , we build on the insight of recent work on the optimal rewards problem that guiding an agent 's behavior with reward functions other than the task-specifying reward function can help overcome computational bounds of the agent . <eos> specifically , we use good guidance reward functions learned on previous tasks in the sequence to incrementally train a reward mapping function that maps task-specifying reward functions into good initial guidance reward functions for subsequent tasks . <eos> we demonstrate that our approach can substantially improve the agent 's performance relative to other approaches , including an approach that transfers policies .
in this paper , we study the challenging problem of tracking the trajectory of a moving object in a video with possibly very complex background . <eos> in contrast to most existing trackers which only learn the appearance of the tracked object online , we take a different approach , inspired by recent advances in deep learning architectures , by putting more emphasis on the ( unsupervised ) feature learning problem . <eos> specifically , by using auxiliary natural images , we train a stacked denoising autoencoder offline to learn generic image features that are more robust against variations . <eos> this is then followed by knowledge transfer from offline training to the online tracking process . <eos> online tracking involves a classification neural network which is constructed from the encoder part of the trained autoencoder as a feature extractor and an additional classification layer . <eos> both the feature extractor and the classifier can be further tuned to adapt to appearance changes of the moving object . <eos> comparison with the state-of-the-art trackers on some challenging benchmark video sequences shows that our deep learning tracker is very efficient as well as more accurate .
motivated by recent progress in natural image statistics , we use newly available datasets with ground truth optical flow to learn the local statistics of optical flow and rigorously compare the learned model to prior models assumed by computer vision optical flow algorithms . <eos> we find that a gaussian mixture model with 64 components provides a significantly better model for local flow statistics when compared to commonly used models . <eos> we investigate the source of the gmms success and show it is related to an explicit representation of flow boundaries . <eos> we also learn a model that jointly models the local intensity pattern and the local optical flow . <eos> in accordance with the assumptions often made in computer vision , the model learns that flow boundaries are more likely at intensity boundaries . <eos> however , when evaluated on a large dataset , this dependency is very weak and the benefit of conditioning flow estimation on the local intensity pattern is marginal .
association field models have been used to explain human contour grouping performance and to explain the mean frequency of long-range horizontal connections across cortical columns in v1 . <eos> however , association fields essentially depend on pairwise statistics of edges in natural scenes . <eos> we develop a spectral test of the sufficiency of pairwise statistics and show that there is significant higher-order structure . <eos> an analysis using a probabilistic spectral embedding reveals curvature-dependent components to the association field , and reveals a challenge for biological learning algorithms .
we study optimal image encoding based on a generative approach with non-linear feature combinations and explicit position encoding . <eos> by far most approaches to unsupervised learning learning of visual features , such as sparse coding or ica , account for translations by representing the same features at different positions . <eos> some earlier models used a separate encoding of features and their positions to facilitate invariant data encoding and recognition . <eos> all probabilistic generative models with explicit position encoding have so far assumed a linear superposition of components to encode image patches . <eos> here , we for the first time apply a model with non-linear feature superposition and explicit position encoding . <eos> by avoiding linear superpositions , the studied model represents a closer match to component occlusions which are ubiquitous in natural images . <eos> in order to account for occlusions , the non-linear model encodes patches qualitatively very different from linear models by using component representations separated into mask and feature parameters . <eos> we first investigated encodings learned by the model using artificial data with mutually occluding components . <eos> we find that the model extracts the components , and that it can correctly identify the occlusive components with the hidden variables of the model . <eos> on natural image patches , the model learns component masks and features for typical image components . <eos> by using reverse correlation , we estimate the receptive fields associated with the model 's hidden units . <eos> we find many gabor-like or globular receptive fields as well as fields sensitive to more complex structures . <eos> our results show that probabilistic models that capture occlusions and invariances can be trained efficiently on image patches , and that the resulting encoding represents an alternative model for the neural encoding of images in the primary visual cortex .
human eye movements provide a rich source of information into the human visual processing . <eos> the complex interplay between the task and the visual stimulus is believed to determine human eye movements , yet it is not fully understood . <eos> this has precluded the development of reliable dynamic eye movement prediction systems . <eos> our work makes three contributions towards addressing this problem . <eos> first , we complement one of the largest and most challenging static computer vision datasets , voc 2012 actions , with human eye movement annotations collected under the task constraints of action and context recognition . <eos> our dataset is unique among eyetracking datasets for still images in terms of its large scale ( over 1 million fixations , 9157 images ) , task control and action from a single image emphasis . <eos> second , we introduce models to automatically discover areas of interest ( aoi ) and introduce novel dynamic consistency metrics , based on them . <eos> our method can automatically determine the number and spatial support of the aois , in addition to their locations . <eos> based on such encodings , we show that , on unconstrained read-world stimuli , task instructions have significant influence on visual behavior . <eos> finally , we leverage our large scale dataset in conjunction with powerful machine learning techniques and computer vision features , to introduce novel dynamic eye movement prediction methods which learn task-sensitive reward functions from eye movement data and efficiently integrate these rewards to plan future saccades based on inverse optimal control . <eos> we show that the propose methodology achieves state of the art scanpath modeling results .
we propose a new weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video . <eos> as part of the proposed approach we develop a generalization of the max-path search algorithm , which allows us to efficiently search over a structured space of multiple spatio-temporal paths , while also allowing to incorporate context information into the model . <eos> instead of using spatial annotations , in the form of bounding boxes , to guide the latent model during training , we utilize human gaze data in the form of a weak supervisory signal . <eos> this is achieved by incorporating gaze , along with the classification , into the structured loss within the latent svm learning framework . <eos> experiments on a challenging benchmark dataset , ucf-sports , show that our model is more accurate , in terms of classification , and achieves state-of-the-art results in localization . <eos> in addition , we show how our model can produce top-down saliency maps conditioned on the classification label and localized latent paths .
many methods have been proposed to recover the intrinsic scene properties such as shape , reflectance and illumination from a single image . <eos> however , most of these models have been applied on laboratory datasets . <eos> in this work we explore the synergy effects between intrinsic scene properties recovered from an image , and the objects and attributes present in the scene . <eos> we cast the problem in a joint energy minimization framework ; thus our model is able to encode the strong correlations between intrinsic properties ( reflectance , shape , illumination ) , objects ( table , tv-monitor ) , and materials ( wooden , plastic ) in a given scene . <eos> we tested our approach on the nyu and pascal datasets , and observe both qualitative and quantitative improvements in the overall accuracy .
randomized decision trees and forests have a rich history in machine learning and have seen considerable success in application , perhaps particularly so for computer vision . <eos> however , they face a fundamental limitation : given enough data , the number of nodes in decision trees will grow exponentially with depth . <eos> for certain applications , for example on mobile or embedded processors , memory is a limited resource , and so the exponential growth of trees limits their depth , and thus their potential accuracy . <eos> this paper proposes decision jungles , revisiting the idea of ensembles of rooted decision directed acyclic graphs ( dags ) , and shows these to be compact and powerful discriminative models for classification . <eos> unlike conventional decision trees that only allow one path to every node , a dag in a decision jungle allows multiple paths from the root to each leaf . <eos> we present and compare two new node merging algorithms that jointly optimize both the features and the structure of the dags efficiently . <eos> during training , node splitting and node merging are driven by the minimization of exactly the same objective function , here the weighted sum of entropies at the leaves . <eos> results on varied datasets show that , compared to decision forests and several other baselines , decision jungles require dramatically less memory while considerably improving generalization .
a common assumption in machine vision is that the training and test samples are drawn from the same distribution . <eos> however , there are many problems when this assumption is grossly violated , as in bio-medical applications where different acquisitions can generate drastic variations in the appearance of the data due to changing experimental conditions . <eos> this problem is accentuated with 3d data , for which annotation is very time-consuming , limiting the amount of data that can be labeled in new acquisitions for training . <eos> in this paper we present a multi-task learning algorithm for domain adaptation based on boosting . <eos> unlike previous approaches that learn task-specific decision boundaries , our method learns a single decision boundary in a shared feature space , common to all tasks . <eos> we use the boosting-trick to learn a non-linear mapping of the observations in each task , with no need for specific a-priori knowledge of its global analytical form . <eos> this yields a more parameter-free domain adaptation approach that successfully leverages learning on new tasks where labeled data is scarce . <eos> we evaluate our approach on two challenging bio-medical datasets and achieve a significant improvement over the state-of-the-art .
visual clutter , the perception of an image as being crowded and disordered , affects aspects of our lives ranging from object detection to aesthetics , yet relatively little effort has been made to model this important and ubiquitous percept . <eos> our approach models clutter as the number of proto-objects segmented from an image , with proto-objects defined as groupings of superpixels that are similar in intensity , color , and gradient orientation features . <eos> we introduce a novel parametric method of merging superpixels by modeling mixture of weibull distributions on similarity distance statistics , then taking the normalized number of proto-objects following partitioning as our estimate of clutter perception . <eos> we validated this model using a new $ \text { 90 } - $ image dataset of realistic scenes rank ordered by human raters for clutter , and showed that our method not only predicted clutter extremely well ( spearman 's $ \rho = 0.81 $ , $ p < 0.05 $ ) , but also outperformed all existing clutter perception models and even a behavioral object segmentation ground truth . <eos> we conclude that the number of proto-objects in an image affects clutter perception more than the number of objects or features .
recent work on mid-level visual representations aims to capture information at the level of complexity higher than typical visual words '' , but lower than full-blown semantic objects . <eos> several approaches have been proposed to discover mid-level visual elements , that are both 1 ) representative , i.e . frequently occurring within a visual dataset , and 2 ) visually discriminative . <eos> however , the current approaches are rather ad hoc and difficult to analyze and evaluate . <eos> in this work , we pose visual element discovery as discriminative mode seeking , drawing connections to the the well-known and well-studied mean-shift algorithm . <eos> given a weakly-labeled image collection , our method discovers visually-coherent patch clusters that are maximally discriminative with respect to the labels . <eos> one advantage of our formulation is that it requires only a single pass through the data . <eos> we also propose the purity-coverage plot as a principled way of experimentally analyzing and evaluating different visual discovery approaches , and compare our method against prior work on the paris street view dataset . <eos> we also evaluate our method on the task of scene classification , demonstrating state-of-the-art performance on the mit scene-67 dataset . ''
how does the human visual system compute the speed of a coherent motion stimulus that contains motion energy in different spatiotemporal frequency bands ? <eos> here we propose that perceived speed is the result of optimal integration of speed information from independent spatiotemporal frequency tuned channels . <eos> we formalize this hypothesis with a bayesian observer model that treats the channel activity as independent cues , which are optimally combined with a prior expectation for slow speeds . <eos> we test the model against behavioral data from a 2afc speed discrimination task with which we measured subjects ' perceived speed of drifting sinusoidal gratings with different contrasts and spatial frequencies , and of various combinations of these single gratings . <eos> we find that perceived speed of the combined stimuli is independent of the relative phase of the underlying grating components , and that the perceptual biases and discrimination thresholds are always smaller for the combined stimuli , supporting the cue combination hypothesis . <eos> the proposed bayesian model fits the data well , accounting for perceptual biases and thresholds of both simple and combined stimuli . <eos> fits are improved if we assume that the channel responses are subject to divisive normalization , which is in line with physiological evidence . <eos> our results provide an important step toward a more complete model of visual motion perception that can predict perceived speeds for stimuli of arbitrary spatial structure .
modern visual recognition systems are often limited in their ability to scale to large numbers of object categories . <eos> this limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows . <eos> one remedy is to leverage data from other sources -- such as text data -- both to train visual models and to constrain their predictions . <eos> in this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text . <eos> we demonstrate that this model matches state-of-the-art performance on the 1000-class imagenet object recognition challenge while making more semantically reasonable errors , and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training . <eos> semantic knowledge improves such zero-shot predictions by up to 65 % , achieving hit rates of up to 10 % across thousands of novel labels never seen by the visual model .
learning a visual concept from a small number of positive examples is a significant challenge for machine learning algorithms . <eos> current methods typically fail to find the appropriate level of generalization in a concept hierarchy for a given set of visual examples . <eos> recent work in cognitive science on bayesian models of generalization addresses this challenge , but prior results assumed that objects were perfectly recognized . <eos> we present an algorithm for learning visual concepts directly from images , using probabilistic predictions generated by visual classifiers as the input to a bayesian generalization model . <eos> as no existing challenge data tests this paradigm , we collect and make available a new , large-scale dataset for visual concept learning using the imagenet hierarchy as the source of possible concepts , with human annotators to provide ground truth labels as to whether a new image is an instance of each concept using a paradigm similar to that used in experiments studying word learning in children . <eos> we compare the performance of our system to several baseline algorithms , and show a significant advantage results from combining visual classifiers with the ability to identify an appropriate level of abstraction using bayesian generalization .
one approach to computer object recognition and modeling the brain 's ventral stream involves unsupervised learning of representations that are invariant to common transformations . <eos> however , applications of these ideas have usually been limited to 2d affine transformations , e.g. , translation and scaling , since they are easiest to solve via convolution . <eos> in accord with a recent theory of transformation-invariance , we propose a model that , while capturing other common convolutional networks as special cases , can also be used with arbitrary identity-preserving transformations . <eos> the model 's wiring can be learned from videos of transforming objects -- -or any other grouping of images into sets by their depicted object . <eos> through a series of successively more complex empirical tests , we study the invariance/discriminability properties of this model with respect to different transformations . <eos> first , we empirically confirm theoretical predictions for the case of 2d affine transformations . <eos> next , we apply the model to non-affine transformations : as expected , it performs well on face verification tasks requiring invariance to the relatively smooth transformations of 3d rotation-in-depth and changes in illumination direction . <eos> surprisingly , it can also tolerate clutter transformations '' which map an image of a face on one background to an image of the same face on a different background . <eos> motivated by these empirical findings , we tested the same model on face verification benchmark tasks from the computer vision literature : labeled faces in the wild , pubfig and a new dataset we gathered -- -achieving strong performance in these highly unconstrained cases as well . ''
deep neural networks ( dnns ) have recently shown outstanding performance on the task of whole image classification . <eos> in this paper we go one step further and address the problem of object detection -- not only classifying but also precisely localizing objects of various classes using dnns . <eos> we present a simple and yet powerful formulation of object detection as a regression to object masks . <eos> we define a multi-scale inference procedure which is able to produce a high-resolution object detection at a low cost by a few network applications . <eos> the approach achieves state-of-the-art performance on pascal 2007 voc .
applying linear templates is an integral part of many object detection systems and accounts for a significant portion of computation time . <eos> we describe a method that achieves a substantial end-to-end speedup over the best current methods , without loss of accuracy . <eos> our method is a combination of approximating scores by vector quantizing feature windows and a number of speedup techniques including cascade . <eos> our procedure allows speed and accuracy to be traded off in two ways : by choosing the number of vector quantization levels , and by choosing to rescore windows or not . <eos> our method can be directly plugged into any recognition system that relies on linear templates . <eos> we demonstrate our method to speed up the original exemplar svm detector [ 1 ] by an order of magnitude and deformable part models [ 2 ] by two orders of magnitude with no loss of accuracy .
category models for objects or activities typically rely on supervised learning requiring sufficiently large training sets . <eos> transferring knowledge from known categories to novel classes with no or only a few labels however is far less researched even though it is a common scenario . <eos> in this work , we extend transfer learning with semi-supervised learning to exploit unlabeled instances of ( novel ) categories with no or only a few labeled instances . <eos> our proposed approach propagated semantic transfer combines three main ingredients . <eos> first , we transfer information from known to novel categories by incorporating external knowledge , such as linguistic or expert-specified information , e.g. , by a mid-level layer of semantic attributes . <eos> second , we exploit the manifold structure of novel classes . <eos> more specifically we adapt a graph-based learning algorithm - so far only used for semi-supervised learning - to zero-shot and few-shot learning . <eos> third , we improve the local neighborhood in such graph structures by replacing the raw feature-based representation with a mid-level object- or attribute-based representation . <eos> we evaluate our approach on three challenging datasets in two different applications , namely on animals with attributes and imagenet for image classification and on mpii composites for activity recognition . <eos> our approach consistently outperforms state-of-the-art transfer and semi-supervised approaches on all datasets .
in visual recognition problems , the common data distribution mismatches between training and testing make domain adaptation essential . <eos> however , image data is difficult to manually divide into the discrete domains required by adaptation algorithms , and the standard practice of equating datasets with domains is a weak proxy for all the real conditions that alter the statistics in complex ways ( lighting , pose , background , resolution , etc . ) <eos> we propose an approach to automatically discover latent domains in image or video datasets . <eos> our formulation imposes two key properties on domains : maximum distinctiveness and maximum learnability . <eos> by maximum distinctiveness , we require the underlying distributions of the identified domains to be different from each other ; by maximum learnability , we ensure that a strong discriminative model can be learned from the domain . <eos> we devise a nonparametric representation and efficient optimization procedure for distinctiveness , which , when coupled with our learnability constraint , can successfully discover domains among both training and test data . <eos> we extensively evaluate our approach on object recognition and human activity recognition tasks .
all the existing multi-task local learning methods are defined on homogeneous neighborhood which consists of all data points from only one task . <eos> in this paper , different from existing methods , we propose local learning methods for multi-task classification and regression problems based on heterogeneous neighborhood which is defined on data points from all tasks . <eos> specifically , we extend the k-nearest-neighbor classifier by formulating the decision function for each data point as a weighted voting among the neighbors from all tasks where the weights are task-specific . <eos> by defining a regularizer to enforce the task-specific weight matrix to approach a symmetric one , a regularized objective function is proposed and an efficient coordinate descent method is developed to solve it . <eos> for regression problems , we extend the kernel regression to multi-task setting in a similar way to the classification case . <eos> experiments on some toy data and real-world datasets demonstrate the effectiveness of our proposed methods .
a probabilistic model based on the horseshoe prior is proposed for learning dependencies in the process of identifying relevant features for prediction . <eos> exact inference is intractable in this model . <eos> however , expectation propagation offers an approximate alternative . <eos> because the process of estimating feature selection dependencies may suffer from over-fitting in the model proposed , additional data from a multi-task learning scenario are considered for induction . <eos> the same model can be used in this setting with few modifications . <eos> furthermore , the assumptions made are less restrictive than in other multi-task methods : the different tasks must share feature selection dependencies , but can have different relevant features and model coefficients . <eos> experiments with real and synthetic data show that this model performs better than other multi-task alternatives from the literature . <eos> the experiments also show that the model is able to induce suitable feature selection dependencies for the problems considered , only from the training data .
we introduce a novel formulation of multi-task learning ( mtl ) called parametric task learning ( ptl ) that can systematically handle infinitely many tasks parameterized by a continuous parameter . <eos> our key finding is that , for a certain class of ptl problems , the path of optimal task-wise solutions can be represented as piecewise-linear functions of the continuous task parameter . <eos> based on this fact , we employ a parametric programming technique to obtain the common shared representation across all the continuously parameterized tasks efficiently . <eos> we show that our ptl formulation is useful in various scenarios such as learning under non-stationarity , cost-sensitive learning , and quantile regression , and demonstrate the usefulness of the proposed method experimentally in these scenarios .
we propose a boosting method , directboost , a greedy coordinate descent algorithm that builds an ensemble classifier of weak classifiers through directly minimizing empirical classification error over labeled training examples ; once the training classification error is reduced to a local coordinatewise minimum , directboost runs a greedy coordinate ascent algorithm that continuously adds weak classifiers to maximize any targeted arbitrarily defined margins until reaching a local coordinatewise maximum of the margins in a certain sense . <eos> experimental results on a collection of machine-learning benchmark datasets show that directboost gives consistently better results than adaboost , logitboost , lpboost with column generation and brownboost , and is noise tolerant when it maximizes an n'th order bottom sample margin .
we propose to train an ensemble with the help of a reservoir in which the learning algorithm can store a limited number of samples . <eos> this novel approach lies in the area between offline and online ensemble approaches and can be seen either as a restriction of the former or an enhancement of the latter . <eos> we identify some basic strategies that can be used to populate this reservoir and present our main contribution , dubbed greedy edge expectation maximization ( geem ) , that maintains the reservoir content in the case of boosting by viewing the samples through their projections into the weak classifier response space . <eos> we propose an efficient algorithmic implementation which makes it tractable in practice , and demonstrate its efficiency experimentally on several compute-vision data-sets , on which it outperforms both online and offline methods in a memory constrained setting .
we go beyond the notion of pairwise similarity and look into search problems with $ k $ -way similarity functions . <eos> in this paper , we focus on problems related to \emph { 3-way jaccard } similarity : $ \mathcal { r } ^ { 3way } = \frac { |s_1 \cap s_2 \cap s_3| } { |s_1 \cup s_2 \cup s_3| } $ , $ s_1 , s_2 , s_3 \in \mathcal { c } $ , where $ \mathcal { c } $ is a size $ n $ collection of sets ( or binary vectors ) . <eos> we show that approximate $ \mathcal { r } ^ { 3way } $ similarity search problems admit fast algorithms with provable guarantees , analogous to the pairwise case . <eos> our analysis and speedup guarantees naturally extend to $ k $ -way resemblance . <eos> in the process , we extend traditional framework of \emph { locality sensitive hashing ( lsh ) } to handle higher order similarities , which could be of independent theoretical interest . <eos> the applicability of $ \mathcal { r } ^ { 3way } $ search is shown on the google sets '' application . <eos> in addition , we demonstrate the advantage of $ \mathcal { r } ^ { 3way } $ resemblance over the pairwise case in improving retrieval quality . ''
